INPUT/OUTPUT


                                    5
        INPUT/OUTPUT
In addition to providing abstractions such as processes, address spaces, and
files, an operating system also controls all the computer's I/O (Input/Output) de-
vices.  It must issue commands to the devices, catch interrupts, and handle errors.
It should also provide an interface between the devices and the rest of the system
that is simple and easy to use.  To the extent possible, the interface should be the
same for all devices (device independence).  The I/O code represents a significant
fraction of the total operating system. How the operating system manages I/O is
the subject of this chapter.
This chapter is organized as follows.        We will look first at some of the prin-
ciples of I/O hardware and then at I/O software in general.  I/O software can be
structured in layers, with each having a well-defined task.  We will look at these
layers to see what they do and how they fit together.
Next, we will look at several I/O devices in detail: disks, clocks, keyboards,
and displays. For each device we will look at its hardware and software.  Finally,
we will consider power management.
5.1 PRINCIPLES OF I/O HARDWARE
Different people look at I/O hardware in different ways. Electrical engineers
look at it in terms of chips, wires, power supplies, motors, and all the other physi-
cal components that comprise the hardware. Programmers look at the interface
                                    337



338                               INPUT/OUTPUT                                 CHAP. 5
presented to the software--the commands the hardware accepts, the functions it
carries out, and the errors that can be reported back. In this book we are concerned
with programming I/O devices, not designing, building, or maimtaining them, so
our interest is in how the hardware is programmed, not how it works inside. Never-
theless, the programming of many I/O devices is often intimately connected with
their internal operation.  In the next three sections we will provide a little general
background on I/O hardware as it relates to programming.        It may be regarded as a
review and expansion of the introductory material in Sec. 1.3.
5.1.1 I/O Devices
     I/O devices can be roughly divided into two categories: block devices and
character devices.  A block device is one that stores information in fixed-size
blocks, each one with its own address. Common block sizes range from 512 to
65,536 bytes. All transfers are in units of one or more entire (consecutive) blocks.
The essential property of a block device is that it is possible to read or write each
block independently of all the other ones. Hard disks, Blu-ray discs, and USB
sticks are common block devices.
     If you look very closely, the boundary between devices that are block address-
able and those that are not is not well defined. Everyone agrees that a disk is a
block addressable device because no matter where the arm currently is, it is always
possible to seek to another cylinder and then wait for the required block to rotate
under the head. Now consider an old-fashioned tape drive still used, sometimes, for
making disk backups (because tapes are cheap).            Tapes contain a sequence of
blocks. If the tape drive is given a command to read block N, it can always rewind
the tape and go forward until it comes to block N. This operation is analogous to a
disk doing a seek, except that it takes much longer. Also, it may or may not be pos-
sible to rewrite one block in the middle of a tape. Even if it were possible to use
tapes as random access block devices, that is stretching the point somewhat: they
are normally not used that way.
     The other type of I/O device is the character device. A character device deliv-
ers or accepts a stream of characters, without regard to any block structure.            It is
not addressable and does not have any seek operation. Printers, network interfaces,
mice (for pointing), rats (for psychology lab experiments), and most other devices
that are not disk-like can be seen as character devices.
     This classification scheme is not perfect. Some devices do not fit in.    Clocks,
for example, are not block addressable. Nor do they generate or accept character
streams. All they do is cause interrupts at well-defined intervals. Memory-mapped
screens do not fit the model well either. Nor do touch screens, for that matter. Still,
the model of block and character devices is general enough that it can be used as a
basis for making some of the operating system software dealing with I/O device in-
dependent. The file system, for example, deals just with abstract block devices and
leaves the device-dependent part to lower-level software.



SEC. 5.1                     PRINCIPLES OF I/O HARDWARE                                 339
I/O devices cover a huge range in speeds, which puts considerable pressure on
the software to perform well over many orders of magnitude in data rates.          Figure
5-1 shows the data rates of some common devices. Most of these devices tend to
get faster as time goes on.
                              Device                Data rate
                    Keyboard                    10 bytes/sec
                    Mouse                       100 bytes/sec
                    56K modem                       7 KB/sec
                    Scanner at 300 dpi              1 MB/sec
                    Digital camcorder               3.5 MB/sec
                    4x Blu-ray disc             18 MB/sec
                    802.11n Wireless            37.5 MB/sec
                    USB 2.0                     60 MB/sec
                    FireWire 800                100 MB/sec
                    Gigabit Ethernet            125 MB/sec
                    SATA 3 disk drive           600 MB/sec
                    USB 3.0                     625 MB/sec
                    SCSI Ultra 5 bus            640 MB/sec
                    Single-lane PCIe 3.0 bus    985 MB/sec
                    Thunderbolt 2 bus               2.5 GB/sec
                    SONET OC-768 network            5 GB/sec
                    Figure 5-1. Some typical device, network, and bus data rates.
5.1.2 Device Controllers
I/O units often consist of a mechanical component and an electronic compo-
nent.  It is possible to separate the two portions to provide a more modular and
general    design.  The  electronic  component  is  called    the  device          controller  or
adapter.   On personal computers, it often takes the form of a chip on the par-
entboard or a printed circuit card that can be inserted into a (PCIe) expansion slot.
The mechanical component is the device itself. This arrangement is shown in
Fig. 1-6.
The controller card usually has a connector on it, into which a cable leading to
the device itself can be plugged. Many controllers can handle two, four, or even
eight identical devices. If the interface between the controller and device is a stan-
dard interface, either an official ANSI, IEEE, or ISO standard or a de facto one,
then companies can make controllers or devices that fit that interface. Many com-
panies, for example, make disk drives that match the SATA, SCSI, USB, Thunder-
bolt, or FireWire (IEEE 1394) interfaces.



340                                    INPUT/OUTPUT                            CHAP. 5
      The interface between the controller and the device is often a very low-level
one.  A disk, for example, might be formatted with 2,000,000 sectors of 512 bytes
per track.  What actually comes off the drive, however, is a serial bit stream, start-
ing with a preamble, then the 4096 bits in a sector, and finally a checksum, or
ECC (Error-Correcting Code).           The preamble is written when the disk is for-
matted and contains the cylinder and sector number, the sector size, and similar
data, as well as synchronization information.
      The controller's job is to convert the serial bit stream into a block of bytes and
perform any error correction necessary. The block of bytes is typically first assem-
bled, bit by bit, in a buffer inside the controller. After its checksum has been veri-
fied and the block has been declared to be error free, it can then be copied to main
memory.
      The controller for an LCD display monitor also works as a bit serial device at
an equally low level.  It reads bytes containing the characters to be displayed from
memory and generates the signals to modify the polarization of the backlight for
the corresponding pixels in order to write them on screen.       If it were not for the
display controller, the operating system programmer would have to explicitly pro-
gram the electric fields of all pixels. With the controller, the operating system ini-
tializes the controller with a few parameters, such as the number of characters or
pixels per line and number of lines per screen, and lets the controller take care of
actually driving the electric fields.
      In a very short time, LCD screens have completely replaced the old CRT
(Cathode Ray Tube) monitors.           CRT monitors fire a beam of electrons onto a flu-
orescent screen. Using magnetic fields, the system is able to bend the beam and
draw pixels on the screen. Compared to LCD screens, CRT monitors were bulky,
power    hungry,  and  fragile.  Moreover,  the  resolution  on  todayÂ´s  (Retina)  LCD
screens is so good that the human eye is unable to distinguish individual pixels. It
is hard to imagine today that laptops in the past came with a small CRT screen that
made them more than 20 cm deep with a nice work-out weight of around 12 kilos.
5.1.3 Memory-Mapped I/O
      Each controller has a few registers that are used for communicating with the
CPU.     By writing into these registers, the operating system can command the de-
vice to deliver data, accept data, switch itself on or off, or otherwise perform some
action.  By reading from these registers, the operating system can learn what the
device's state is, whether it is prepared to accept a new command, and so on.
      In addition to the control registers, many devices have a data buffer that the op-
erating system can read and write. For example, a common way for computers to
display pixels on the screen is to have a video RAM, which is basically just a data
buffer, available for programs or the operating system to write into.
      The issue thus arises of how the CPU communicates with the control registers
and also with the device data buffers. Two alternatives exist.   In the first approach,



SEC. 5.1                 PRINCIPLES OF I/O HARDWARE                                      341
each control register is assigned an I/O port number, an 8- or 16-bit integer. The
set of all the I/O ports form the I/O port space, which is protected so that ordinary
user programs cannot access it (only the operating system can).                Using a special
I/O instruction such as
     IN REG,PORT,
the CPU can read in control register PORT and store the result in CPU register
REG. Similarly, using
     OUT PORT,REG
the CPU can write the contents of REG to a control register. Most early computers,
including nearly all mainframes, such as the IBM 360 and all of its successors,
worked this way.
     In this scheme, the address spaces for memory and I/O are different, as shown
in Fig. 5-2(a). The instructions
     IN R0,4
and
     MOV R0,4
are completely different in this design. The former reads the contents of I/O port 4
and puts it in R0 whereas the latter reads the contents of memory word 4 and puts it
in R0.  The 4s in these examples refer to different and unrelated address spaces.
           Two address              One address space      Two address spaces
0xFFFF...                Memory
                         I/O ports
        0
                  (a)               (b)                                        (c)
        Figure 5-2. (a) Separate I/O and memory space. (b) Memory-mapped I/O.
        (c) Hybrid.
     The second approach, introduced with the PDP-11, is to map all the control
registers into the memory space, as shown in Fig. 5-2(b).  Each control register is
assigned a unique memory address to which no memory is assigned. This system is
called memory-mapped I/O.         In most systems, the assigned addresses are at or
near the top of the address space.  A hybrid scheme, with memory-mapped I/O
data buffers and separate I/O ports for the control registers, is shown in Fig. 5-2(c).



342                        INPUT/OUTPUT                                CHAP. 5
The x86 uses this architecture, with addresses 640K to 1M - 1 being reserved for
device data buffers in IBM PC compatibles, in addition to I/O ports 0 to 64K - 1.
     How do these schemes actually work in practice?  In all cases, when the CPU
wants to read a word, either from memory or from an I/O port, it puts the address it
needs on the bus' address lines and then asserts a READ signal on a bus' control
line.  A second signal line is used to tell whether I/O space or memory space is
needed.  If it is memory space, the memory responds to the request.    If it is I/O
space, the I/O device responds to the request.      If there is only memory space [as in
Fig. 5-2(b)], every memory module and every I/O device compares the address
lines to the range of addresses that it services. If the address falls in its range, it re-
sponds to the request.  Since no address is ever assigned to both memory and an
I/O device, there is no ambiguity and no conflict.
     These two schemes for addressing the controllers have different strengths and
weaknesses. Let us start with the advantages of memory-mapped I/O. Firstof all, if
special I/O instructions are needed to read and write the device control registers,
access to them requires the use of assembly code since there is no way to execute
an IN or OUT instruction in C or C++.     Calling such a procedure adds overhead to
controlling I/O.  In contrast, with memory-mapped I/O, device control registers are
just variables in memory and can be addressed in C the same way as any other var-
iables. Thus with memory-mapped I/O, an I/O device driver can be written entirely
in C. Without memory-mapped I/O, some assembly code is needed.
     Second, with memory-mapped I/O, no special protection mechanism is needed
to keep user processes from performing I/O. All the operating system has to do is
refrain from putting that portion of the address space containing the control regis-
ters in any user's virtual address space. Better yet, if each device has its control
registers on a different page of the address space, the operating system can give a
user control over specific devices but not others by simply including the desired
pages in its page table. Such a scheme can allow different device drivers to be
placed in different address spaces, not only reducing kernel size but also keeping
one driver from interfering with others.
     Third, with memory-mapped I/O, every instruction that can reference memory
can also reference control registers. For example, if there is an instruction, TEST,
that tests a memory word for 0, it can also be used to test a control register for 0,
which might be the signal that the device is idle and can accept a new command.
The assembly language code might look like this:
     LOOP:   TEST PORT  4              // check if port 4 is 0
             BEQ READY                 // if it is 0, go to ready
             BRANCH LOOP               // otherwise, continue testing
     READY:
If memory-mapped I/O is not present, the control register must first be read into
the CPU, then tested, requiring two instructions instead of just one.  In the case of



SEC. 5.1                        PRINCIPLES OF I/O HARDWARE                                        343
the loop given above, a fourth instruction has to be added, slightly slowing down
the responsiveness of detecting an idle device.
    In computer design, practically everything involves trade-offs, and that is the
case here, too. Memory-mapped I/O also has its disadvantages.               First, most com-
puters nowadays have some form of caching of memory words. Caching a device
control register would be disastrous. Consider the assembly-code loop given above
in  the  presence   of   caching.  The  first  reference  to  PORT  4  would  cause           it  to  be
cached. Subsequent references would just take the value from the cache and not
even ask the device. Then when the device finally became ready, the software
would have no way of finding out. Instead, the loop would go on forever.
    To prevent this situation with memory-mapped I/O, the hardware has to be able
to selectively disable caching, for example, on a per-page basis. This feature adds
extra complexity to both the hardware and the operating system, which has to man-
age the selective caching.
    Second, if there is only one address space, then all memory modules and all
I/O devices must examine all memory references to see which ones to respond to.
If the computer has a single bus, as in Fig. 5-3(a), having everyone look at every
address is straightforward.
                                                              CPU reads and writes of memory
                                                              go over this high-bandwidth bus
    CPU       Memory               I/O            CPU            Memory                  I/O
         All addresses (memory                                              This memory port is
              and I/O) go here          Bus                                 to allow I/O devices
                                                                            access to memory
                    (a)                                                (b)
         Figure 5-3. (a) A single-bus architecture. (b) A dual-bus memory architecture.
    However, the trend in modern personal computers is to have a dedicated high-
speed memory bus, as shown in Fig. 5-3(b). The bus is tailored to optimize memo-
ry performance, with no compromises for the sake of slow I/O devices. x86 sys-
tems     can  have  multiple    buses   (memory,  PCIe,   SCSI,  and   USB),             as  shown    in
Fig. 1-12.
    The trouble with having a separate memory bus on memory-mapped machines
is that the I/O devices have no way of seeing memory addresses as they go by on
the memory bus, so they have no way of responding to them. Again, special meas-
ures have to be taken to make memory-mapped I/O work on a system with multiple



344                               INPUT/OUTPUT                                CHAP. 5
buses. One possibility is to first send all memory references to the memory.  If the
memory fails to respond, then the CPU tries the other buses. This design can be
made to work but requires additional hardware complexity.
      A second possible design is to put a snooping device on the memory bus to
pass all addresses presented to potentially interested I/O devices. The problem here
is that I/O devices may not be able to process requests at the speed the memory
can.
      A third possible design, and one that would well match the design sketched in
Fig. 1-12, is to filter addresses in the memory controller. In that case, the memory
controller chip contains range registers that are preloaded at boot time.     For ex-
ample, 640K to 1M - 1 could be marked as a nonmemory range. Addresses that
fall within one of the ranges marked as nonmemory are forwarded to devices in-
stead of to memory. The disadvantage of this scheme is the need for figuring out at
boot time which memory addresses are not really memory addresses. Thus each
scheme  has  arguments  for  and  against  it,   so     compromises  and  trade-offs         are
inevitable.
5.1.4 Direct Memory Access
      No matter whether a CPU does or does not have memory-mapped I/O, it needs
to address the device controllers to exchange data with them. The CPU can request
data from an I/O controller one byte at a time, but doing so wastes the CPU's time,
so a different scheme, called DMA (Direct Memory Access) is often used. To
simplify the explanation, we assume that the CPU accesses all devices and memory
via a single system bus that connects the CPU, the memory, and the I/O devices, as
shown in Fig. 5-4. We already know that the real organization in modern systems is
more complicated, but all the principles are the same. The operating system can
use only DMA if the hardware has a DMA controller, which most systems do.
Sometimes this controller is integrated into disk controllers and other controllers,
but such a design requires a separate DMA controller for each device. More com-
monly, a single DMA controller is available (e.g., on the parentboard) for regulat-
ing transfers to multiple devices, often concurrently.
      No matter where it is physically located, the DMA controller has access to the
system bus independent of the CPU, as shown in Fig. 5-4.   It contains several reg-
isters that can be written and read by the CPU.         These include a memory address
register, a byte count register, and one or more control registers. The control regis-
ters specify the I/O port to use, the direction of the transfer (reading from the I/O
device or writing to the I/O device), the transfer unit (byte at a time or word at a
time), and the number of bytes to transfer in one burst.
      To explain how DMA works, let us first look at how disk reads occur when
DMA is not used. First the disk controller reads the block (one or more sectors)
from the drive serially, bit by bit, until the entire block is in the controller's internal
buffer. Next, it computes the checksum to verify that no read errors have occurred.



SEC. 5.1                       PRINCIPLES OF I/O HARDWARE                                            345
                                                                        Drive
                 1. CPU
                 programs      DMA                             Disk                          Main
CPU              the DMA       controller                   controller                       memory
                 controller                                             Buffer
                               Address
                               Count
                               Control     4. Ack
               Interrupt when              2. DMA requests
               done                        transfer to memory           3. Data transferred
                                                                                                     Bus
                             Figure 5-4. Operation of a DMA transfer.
Then the controller causes an interrupt. When the operating system starts running,
it can read the disk block from the controller's buffer a byte or a word at a time by
executing a loop, with each iteration reading one byte or word from a controller de-
vice register and storing it in main memory.
When DMA is used, the procedure is different. First the CPU programs the
DMA controller by setting its registers so it knows what to transfer where (step 1
in Fig. 5-4).  It also issues a command to the disk controller telling it to read data
from the disk into its internal buffer and verify the checksum. When valid data are
in the disk controller's buffer, DMA can begin.
The DMA controller initiates the transfer by issuing a read request over the bus
to the disk controller (step 2).      This read request looks like any other read request,
and the disk controller does not know (or care) whether it came from the CPU or
from a DMA controller. Typically, the memory address to write to is on the bus'
address lines, so when the disk controller fetches the next word from its internal
buffer, it knows where to write it. The write to memory is another standard bus
cycle (step 3).  When the write is complete, the disk controller sends an acknowl-
edgement signal to the DMA controller, also over the bus (step 4). The DMA con-
troller then increments the memory address to use and decrements the byte count.
If the byte count is still greater than 0, steps 2 through 4 are repeated until the
count reaches 0.     At that time, the DMA controller interrupts the CPU to let it
know that the transfer is now complete. When the operating system starts up, it
does not have to copy the disk block to memory; it is already there.
DMA controllers vary considerably in their sophistication. The simplest ones
handle one transfer at a time, as described above.             More complex ones can be pro-
grammed to handle multiple transfers at the same time. Such controllers have mul-
tiple sets of registers internally, one for each channel. The CPU starts by loading
each set of registers with the relevant parameters for its transfer. Each transfer must



346                                    INPUT/OUTPUT                               CHAP. 5
use a different device controller.     After each word is transferred (steps 2 through 4)
in Fig. 5-4, the DMA controller decides which device to service next.     It may be set
up to use a round-robin algorithm, or it may have a priority scheme design to favor
some devices over others. Multiple requests to different device controllers may be
pending at the same time, provided that there is an unambiguous way to tell the ac-
knowledgements apart. Often a different acknowledgement line on the bus is used
for each DMA channel for this reason.
     Many buses can operate in two modes: word-at-a-time mode and block mode.
Some DMA controllers can also operate in either mode.        In the former mode, the
operation is as described above: the DMA controller requests the transfer of one
word and gets it.  If the CPU also wants the bus, it has to wait.   The mechanism is
called cycle stealing because the device controller sneaks in and steals an occa-
sional bus cycle from the CPU once in a while, delaying it slightly. In block mode,
the DMA controller tells the device to acquire the bus, issue a series of transfers,
then release the bus. This form of operation is called burst mode.        It is more ef-
ficient than cycle stealing because acquiring the bus takes time and multiple words
can be transferred for the price of one bus acquisition. The down side to burst
mode is that it can block the CPU and other devices for a substantial period if a
long burst is being transferred.
     In the model we have been discussing, sometimes called fly-by mode, the
DMA controller tells the device controller to transfer the data directly to main
memory. An alternative mode that some DMA controllers use is to have the device
controller send the word to the DMA controller, which then issues a second bus re-
quest to write the word to wherever it is supposed to go. This scheme requires an
extra bus cycle per word transferred, but is more flexible in that it can also perform
device-to-device copies and even memory-to-memory copies (by first issuing a
read to memory and then issuing a write to memory at a different address).
     Most  DMA     controllers    use  physical  memory  addresses  for   their   transfers.
Using physical addresses requires the operating system to convert the virtual ad-
dress of the intended memory buffer into a physical address and write this physical
address into the DMA controller's address register.      An alternative scheme used in
a few DMA controllers is to write virtual addresses into the DMA controller in-
stead. Then the DMA controller must use the MMU to have the virtual-to-physical
translation done. Only in the case that the MMU is part of the memory (possible,
but rare), rather than part of the CPU, can virtual addresses be put on the bus.
     We mentioned earlier that the disk first reads data into its internal buffer before
DMA can start. You may be wondering why the controller does not just store the
bytes in main memory as soon as it gets them from the disk.         In other words, why
does it need an internal buffer?       There are two reasons. First, by doing internal
buffering, the disk controller can verify the checksum before starting a transfer.         If
the checksum is incorrect, an error is signaled and no transfer is done.
     The second reason is that once a disk transfer has started, the bits keep arriving
from the disk at a constant rate, whether the controller is ready for them or not.         If



SEC. 5.1            PRINCIPLES OF I/O HARDWARE                                            347
the controller tried to write data directly to memory, it would have to go over the
system bus for each word transferred.         If the bus were busy due to some other de-
vice using it (e.g., in burst mode), the controller would have to wait.              If the next
disk word arrived before the previous one had been stored, the controller would
have to store it somewhere.        If the bus were very busy, the controller might end up
storing quite a few words and having a lot of administration to do as well. When
the block is buffered internally, the bus is not needed until the DMA begins, so the
design of the controller is much simpler because the DMA transfer to memory is
not time critical.  (Some older controllers did, in fact, go directly to memory with
only a small amount of internal buffering, but when the bus was very busy, a trans-
fer might have had to be terminated with an overrun error.)
Not all computers use DMA.         The argument against it is that the main CPU is
often far faster than the DMA controller and can do the job much faster (when the
limiting factor is not the speed of the I/O device). If there is no other work for it to
do, having the (fast) CPU wait for the (slow) DMA controller to finish is pointless.
Also, getting rid of the DMA controller and having the CPU do all the work in
software saves money, important on low-end (embedded) computers.
5.1.5 Interrupts Revisited
We briefly introduced interrupts in Sec. 1.3.4, but there is more to be said. In a
typical personal computer system, the interrupt structure is as shown in Fig. 5-5.
At the hardware level, interrupts work as follows. When an I/O device has finished
the work given to it, it causes an interrupt (assuming that interrupts have been
enabled by the operating system).  It does this by asserting a signal on a bus line
that it has been assigned. This signal is detected by the interrupt controller chip on
the parentboard, which then decides what to do.
                                   Interrupt     1. Device is finished
          CPU       3. CPU acks    controller
                    interrupt                                     Disk
                                                                         Keyboard
                                                    11  12  1
                                                 10            2  Clock
                                                 9             3
                                                 8             4
                    2. Controller                    7  6   5
                    issues                                               Printer
                    interrupt
                                                                  Bus
Figure 5-5. How an interrupt happens. The connections between the devices and
the controller actually use interrupt lines on the bus rather than dedicated wires.
If no other interrupts are pending, the interrupt controller handles the interrupt
immediately.   However, if another interrupt is in progress, or another device has
made a simultaneous request on a higher-priority interrupt request line on the bus,



348                                  INPUT/OUTPUT                              CHAP. 5
the device is just ignored for the moment.  In this case it continues to assert an in-
terrupt signal on the bus until it is serviced by the CPU.
     To handle the interrupt, the controller puts a number on the address lines speci-
fying which device wants attention and asserts a signal to interrupt the CPU.
     The interrupt signal causes the CPU to stop what it is doing and start doing
something else. The number on the address lines is used as an index into a table
called the interrupt vector to fetch a new program counter. This program counter
points to the start of the corresponding interrupt-service procedure. Typically traps
and interrupts use the same mechanism from this point on, often sharing the same
interrupt vector. The location of the interrupt vector can be hardwired into the ma-
chine or it can be anywhere in memory, with a CPU register (loaded by the operat-
ing system) pointing to its origin.
     Shortly after it starts running, the interrupt-service procedure acknowledges
the interrupt by writing a certain value to one of the interrupt controller's I/O ports.
This acknowledgement tells the controller that it is free to issue another interrupt.
By having the CPU delay this acknowledgement until it is ready to handle the next
interrupt, race conditions involving multiple (almost simultaneous) interrupts can
be avoided.  As an aside, some (older) computers do not have a centralized inter-
rupt controller, so each device controller requests its own interrupts.
     The hardware always saves certain information before starting the service pro-
cedure. Which information is saved and where it is saved varies greatly from CPU
to CPU.  As a bare minimum, the program counter must be saved, so the inter-
rupted process can be restarted. At the other extreme, all the visible registers and a
large number of internal registers may be saved as well.
     One issue is where to save this information. One option is to put it in internal
registers that the operating system can read out as needed. A problem with this ap-
proach is that then the interrupt controller cannot be acknowledged until all poten-
tially relevant information has been read out, lest a second interrupt overwrite the
internal registers saving the state. This strategy leads to long dead times when in-
terrupts are disabled and possibly to lost interrupts and lost data.
     Consequently, most CPUs save the information on the stack. However, this ap-
proach, too, has problems. To start with: whose stack? If the current stack is used,
it may well be a user process stack. The stack pointer may not even be legal, which
would cause a fatal error when the hardware tried to write some words at the ad-
dress pointed to. Also, it might point to the end of a page.          After several memory
writes, the page boundary might be exceeded and a page fault generated. Having a
page fault occur during the hardware interrupt processing creates a bigger problem:
where to save the state to handle the page fault?
     If the kernel stack is used, there is a much better chance of the stack pointer
being legal and pointing to a pinned page. However, switching into kernel mode
may require changing MMU contexts and will probably invalidate most or all of
the cache and TLB.  Reloading all of these, statically or dynamically, will increase
the time to process an interrupt and thus waste CPU time.



SEC. 5.1           PRINCIPLES OF I/O HARDWARE                                          349
Precise and Imprecise Interrupts
Another problem is caused by the fact that most modern CPUs are heavily
pipelined and often superscalar (internally parallel).       In older systems, after each
instruction was finished executing, the microprogram or hardware checked to see if
there was an interrupt pending.     If so, the program counter and PSW were pushed
onto the stack and the interrupt sequence begun. After the interrupt handler ran, the
reverse process took place, with the old PSW and program counter popped from
the stack and the previous process continued.
This model makes the implicit assumption that if an interrupt occurs just after
some instruction, all the instructions up to and including that instruction have been
executed completely, and no instructions after it have executed at all. On older ma-
chines, this assumption was always valid. On modern ones it may not be.
For starters, consider the pipeline model of Fig. 1-7(a). What happens if an in-
terrupt occurs while the pipeline is full (the usual case)?  Many instructions are in
various stages of execution. When the interrupt occurs, the value of the program
counter may not reflect the correct boundary between executed instructions and
nonexecuted instructions. In fact, many instructions may have been partially ex-
ecuted, with different instructions being more or less complete.  In this situation,
the program counter most likely reflects the address of the next instruction to be
fetched and pushed into the pipeline rather than the address of the instruction that
just was processed by the execution unit.
On a superscalar machine, such as that of Fig. 1-7(b), things are even worse.
Instructions may be decomposed into micro-operations and the micro-operations
may execute out of order, depending on the availability of internal resources such
as functional units and registers.  At the time of an interrupt, some instructions
started long ago may not have started and others started more recently may be al-
most done.  At the point when an interrupt is signaled, there may be many instruc-
tions in various states of completeness, with less relation between them and the
program counter.
An interrupt that leaves the machine in a well-defined state is called a precise
interrupt (Walker and Cragon, 1995). Such an interrupt has four properties:
1.        The PC (Program Counter) is saved in a known place.
2.        All instructions before the one pointed to by the PC have completed.
3.        No instruction beyond the one pointed to by the PC has finished.
4.        The execution state of the instruction pointed to by the PC is known.
Note that there is no prohibition on instructions beyond the one pointed to by the
PC from starting.  It is just that any changes they make to registers or memory
must be undone before the interrupt happens.   It is permitted that the instruction
pointed to has been executed.       It is also permitted that it has not been executed.



350                            INPUT/OUTPUT                                        CHAP. 5
However, it must be clear which case applies. Often, if the interrupt is an I/O inter-
rupt, the instruction will not yet have started. However, if the interrupt is really a
trap or page fault, then the PC generally points to the instruction that caused the
fault so it can be restarted later. The situation of Fig. 5-6(a) illustrates a precise in-
terrupt. All instructions up to the program counter (316) have completed and none
of those beyond it have started (or have been rolled back to undo their effects).
                          Not executed    332          Not executed    332
                          Not executed    328          10% executed    328
                          Not executed    324          40% executed    324
                          Not executed    320          35% executed    320
     PC                   Fully executed  316  PC      20% executed    316
                          Fully executed  312          60% executed    312
                          Fully executed  308          80% executed    308
                          Fully executed  304          Fully executed  304
                                          300                          300
                          (a)                                (b)
     Figure 5-6. (a) A precise interrupt. (b) An imprecise interrupt.
     An interrupt that does not meet these requirements is called an imprecise int-
errupt and makes life most unpleasant for the operating system writer, who now
has to figure out what has happened and what still has to happen. Fig. 5-6(b) illus-
trates an imprecise interrupt, where different instructions near the program counter
are in different stages of completion, with older ones not necessarily more com-
plete than younger ones.  Machines with imprecise interrupts usually vomit a large
amount of internal state onto the stack to give the operating system the possibility
of figuring out what was going on. The code necessary to restart the machine is
typically exceedingly complicated. Also, saving a large amount of information to
memory on every interrupt makes interrupts slow and recovery even worse. This
leads to the ironic situation of having very fast superscalar CPUs sometimes being
unsuitable for real-time work due to slow interrupts.
     Some computers are designed so that some kinds of interrupts and traps are
precise and others are not. For example, having I/O interrupts be precise but traps
due to fatal programming errors be imprecise is not so bad since no attempt need
be made to restart a running process after it has divided by zero. Some machines
have a bit that can be set to force all interrupts to be precise. The downside of set-
ting this bit is that it forces the CPU to carefully log everything it is doing and
maintain shadow copies of registers so it can generate a precise interrupt at any in-
stant. All this overhead has a major impact on performance.
     Some superscalar machines, such as the x86 family, have precise interrupts to
allow old software to work correctly.     The price paid for backward compatibility
with precise interrupts is extremely complex interrupt logic within the CPU to
make sure that when the interrupt controller signals that it wants to cause an inter-
rupt, all instructions up to some point are allowed to finish and none beyond that



SEC. 5.1                PRINCIPLES OF I/O HARDWARE                                       351
point are allowed to have any noticeable effect on the machine state. Here the price
is paid not in time, but in chip area and in complexity of the design.  If precise in-
terrupts were not required for backward compatibility purposes, this chip area
would be available for larger on-chip caches, making the CPU faster. On the other
hand, imprecise interrupts make the operating system far more complicated and
slower, so it is hard to tell which approach is really better.
5.2 PRINCIPLES OF I/O SOFTWARE
Let us now turn away from the I/O hardware and look at the I/O software. First
we will look at its goals and then at the different ways I/O can be done from the
point of view of the operating system.
5.2.1 Goals of the I/O Software
A key concept in the design of I/O software is known as device independence.
What it means is that we should be able to write programs that can access any I/O
device without having to specify the device in advance. For example, a program
that reads a file as input should be able to read a file on a hard disk, a DVD, or on a
USB stick without having to be modified for each different device. Similarly, one
should be able to type a command such as
sort <input >output
and have it work with input coming from any kind of disk or the keyboard and the
output going to any kind of disk or the screen.  It is up to the operating system to
take care of the problems caused by the fact that these devices really are different
and require very different command sequences to read or write.
Closely related to device independence is the goal of uniform naming.                    The
name of a file or a device should simply be a string or an integer and not depend on
the device in any way.  In UNIX, all disks can be integrated in the file-system hier-
archy in arbitrary ways so the user need not be aware of which name corresponds
to which device. For example, a USB stick can be mounted on top of the directory
/usr/ast/backup so that copying a file to /usr/ast/backup/monday copies the file to
the USB stick.  In this way, all files and devices are addressed the same way: by a
path name.
Another important issue for I/O software is error handling.             In general, errors
should be handled as close to the hardware as possible.         If the controller discovers
a read error, it should try to correct the error itself if it can.  If it cannot, then the
device driver should handle it, perhaps by just trying to read the block again. Many
errors are transient, such as read errors caused by specks of dust on the read head,
and will frequently go away if the operation is repeated. Only if the lower layers



352                                   INPUT/OUTPUT                          CHAP. 5
are not able to deal with the problem should the upper layers be told about it.         In
many cases, error recovery can be done transparently at a low level without the
upper levels even knowing about the error.
     Still another important issue is that of synchronous (blocking) vs.         asyn-
chronous   (interrupt-driven)  transfers.   Most  physical  I/O  is   asynchronous--the
CPU starts the transfer and goes off to do something else until the interrupt arrives.
User programs are much easier to write if the I/O operations are blocking--after a
read system call the program is automatically suspended until the data are avail-
able in the buffer.  It is up to the operating system to make operations that are ac-
tually interrupt-driven look blocking to the user programs.       However, some very
high-performance applications need to control all the details of the I/O, so some
operating systems make asynchronous I/O available to them.
     Another issue for the I/O software is buffering. Often data that come off a de-
vice cannot be stored directly in their final destination. For example, when a packet
comes in off the network, the operating system does not know where to put it until
it has stored the packet somewhere and examined it. Also, some devices have
severe real-time constraints (for example, digital audio devices), so the data must
be put into an output buffer in advance to decouple the rate at which the buffer is
filled from the rate at which it is emptied, in order to avoid buffer underruns. Buff-
ering involves considerable copying and often has a major impact on I/O per-
formance.
     The final concept that we will mention here is sharable vs. dedicated devices.
Some I/O devices, such as disks, can be used by many users at the same time.            No
problems are caused by multiple users having open files on the same disk at the
same time. Other devices, such as printers, have to be dedicated to a single user
until that user is finished. Then another user can have the printer.  Having two or
more users writing characters intermixed at random to the same page will defi-
nitely not work. Introducing dedicated (unshared) devices also introduces a variety
of problems, such as deadlocks. Again, the operating system must be able to hanfle
both shared and dedicated devices in a way that avoids problems.
5.2.2 Programmed I/O
     There are three fundamentally different ways that I/O can be performed.            In
this section we will look at the first one (programmed I/O).         In the next two sec-
tions we will examine the others (interrupt-driven I/O and I/O using DMA).              The
simplest form of I/O is to have the CPU do all the work. This method is called pro-
grammed I/O.
     It is simplest to illustrate how programmed I/O works by means of an example.
Consider a user process that wants to print the eight-character string ``ABCDE-
FGH'' on the printer via a serial interface. Displays on small embedded systems
sometimes work this way.       The software first assembles the string in a buffer in
user space, as shown in Fig. 5-7(a).



SEC. 5.2                 PRINCIPLES OF I/O SOFTWARE                                          353
             String to
User         be printed
space                    Printed                     Printed
                         page                        page
             ABCD
             EFGH
                                         Next        A            Next           AB
Kernel                                         ABCD                        ABCD
space                                          EFGH                        EFGH
             (a)                               (b)                         (c)
                         Figure 5-7. Steps in printing a string.
The user process then acquires the printer for writing by making a system call
to open it.  If the printer is currently in use by another process, this call will fail
and return an error code or will block until the printer is available, depending on
the operating system and the parameters of the call. Once it has the printer, the user
process makes a system call telling the operating system to print the string on the
printer.
The operating system then (usually) copies the buffer with the string to an
array, say, p, in kernel space, where it is more easily accessed (because the kernel
may have to change the memory map to get at user space).          It then checks to see if
the printer is currently available. If not, it waits until it is. As soon as the printer is
available, the operating system copies the first character to the printer's data regis-
ter, in this example using memory-mapped I/O.        This action activates the printer.
The character may not appear yet because some printers buffer a line or a page be-
fore printing anything.  In Fig. 5-7(b), however, we see that the first character has
been printed and that the system has marked the ``B'' as the next character to be
printed.
As soon as it has copied the first character to the printer, the operating system
checks to see if the printer is ready to accept another one. Generally, the printer has
a second register, which gives its status.     The act of writing to the data register
causes the status to become not ready. When the printer controller has processed
the current character, it indicates its availability by setting some bit in its status reg-
ister or putting some value in it.
At this point the operating system waits for the printer to become ready again.
When that happens, it prints the next character, as shown in Fig. 5-7(c). This loop
continues until the entire string has been printed. Then control returns to the user
process.
The       actions  followed  by     the  operating   system  are  briefly  summarized        in
Fig. 5-8. First the data are copied to the kernel. Then the operating system enters a



354                                       INPUT/OUTPUT                             CHAP. 5
tight loop, outputting the characters one at a time. The essential aspect of program-
med I/O, clearly illustrated in this figure, is that after outputting a character, the
CPU continuously polls the device to see if it is ready to accept another one. This
behavior is often called polling or busy waiting.
copy    from  user(buffer, p, count);            /* p is the kernel buffer */
for (i = 0; i < count; i++) {                    /* loop on every character */
      while (*printer   status  reg != READY) ;  /* loop until ready */
      *printer  data    register = p[i];         /* output one character */
}
return  to    user( );
                Figure 5-8. Writing a string to the printer using programmed I/O.
Programmed I/O is simple but has the disadvantage of tying up the CPU full time
until all the I/O is done. If the time to ``print'' a character is very short (because all
the printer is doing is copying the new character to an internal buffer), then busy
waiting is fine. Also, in an embedded system, where the CPU has nothing else to
do, busy waiting is fine. However, in more complex systems, where the CPU has
other work to do, busy waiting is inefficient. A better I/O method is needed.
5.2.3 Interrupt-Driven I/O
      Now let us consider the case of printing on a printer that does not buffer char-
acters but prints each one as it arrives.        If the printer can print, say 100 charac-
ters/sec, each character takes 10 msec to print. This means that after every charac-
ter is written to the printer's data register, the CPU will sit in an idle loop for 10
msec waiting to be allowed to output the next character. This is more than enough
time to do a context switch and run some other process for the 10 msec that would
otherwise be wasted.
      The way to allow the CPU to do something else while waiting for the printer to
become ready is to use interrupts. When the system call to print the string is made,
the buffer is copied to kernel space, as we showed earlier, and the first character is
copied to the printer as soon as it is willing to accept a character.              At that point the
CPU calls the scheduler and some other process is run.         The process that asked for
the string to be printed is blocked until the entire string has printed. The work done
on the system call is shown in Fig. 5-9(a).
      When the printer has printed the character and is prepared to accept the next
one, it generates an interrupt. This interrupt stops the current process and saves its
state.  Then the printer interrupt-service procedure is run.   A crude version of this
code is shown in Fig. 5-9(b).             If there are no more characters to print, the interrupt
handler takes some action to unblock the user. Otherwise, it outputs the next char-
acter, acknowledges the interrupt, and returns to the process that was running just
before the interrupt, which continues from where it left off.



SEC. 5.2                          PRINCIPLES OF I/O SOFTWARE                                          355
copy   from    user(buffer, p, count);                    if (count == 0) {
enable    interrupts( );                                     unblock         user( );
while (*printer    status   reg != READY) ;               } else {
*printer  data    register = p[0];                           *printer        data  register = p[i];
scheduler( );                                                count = count - 1;
                                                                 i = i + 1;
                                                          }
                                                          acknowledge        interrupt( );
                                                          return    from     interrupt( );
          (a)                                                           (b)
          Figure 5-9.     Writing a string to the printer using interrupt-driven I/O.       (a) Code
          executed at the time the print system call is made. (b) Interrupt service procedure
          for the printer.
5.2.4 I/O Using DMA
      An obvious disadvantage of interrupt-driven I/O is that an interrupt occurs on
every character. Interrupts take time, so this scheme wastes a certain amount of
CPU time.       A solution is to use DMA.           Here the idea is to let the DMA controller
feed the characters to the printer one at time, without the CPU being bothered.                       In
essence, DMA is programmed I/O, only with the DMA controller doing all the
work, instead of the main CPU. This strategy requires special hardware (the DMA
controller) but frees up the CPU during the I/O to do other work. An outline of the
code is given in Fig. 5-10.
copy   from    user(buffer, p, count);              acknowledge         interrupt( );
set   up  DMA     controller( );                    unblock  user( );
scheduler( );                                       return   from   interrupt( );
             (a)                                                   (b)
          Figure 5-10.      Printing a string using DMA.    (a) Code executed when the print
          system call is made. (b) Interrupt-service procedure.
      The big win with DMA is reducing the number of interrupts from one per
character to one per buffer printed.         If there are many characters and interrupts are
slow, this can be a major improvement. On the other hand, the DMA controller is
usually much slower than the main CPU.              If the DMA controller is not capable of
driving the device at full speed, or the CPU usually has nothing to do anyway
while     waiting  for      the   DMA   interrupt,  then     interrupt-driven          I/O  or  even  pro-
grammed I/O may be better. Most of the time, though, DMA is worth it.



356                                INPUT/OUTPUT                                  CHAP. 5
5.3 I/O SOFTWARE LAYERS
     I/O software is typically organized in four layers, as shown in Fig. 5-11.  Each
layer has a well-defined function to perform and a well-defined interface to the ad-
jacent layers. The functionality and interfaces differ from system to system, so the
discussion that follows, which examines all the layers starting at the bottom, is not
specific to one machine.
                          User-level I/O software
                          Device-independent operating system software
                                       Device drivers
                                   Interrupt handlers
                                       Hardware
                 Figure 5-11. Layers of the I/O software system.
5.3.1 Interrupt Handlers
     While programmed I/O is occasionally useful, for most I/O, interrupts are an
unpleasant fact of life and cannot be avoided. They should be hidden away, deep in
the bowels of the operating system, so that as little of the operating system as pos-
sible knows about them. The best way to hide them is to have the driver starting an
I/O operation block until the I/O has completed and the interrupt occurs. The driver
can block itself, for example, by doing a down on a semaphore, a wait on a condi-
tion variable, a receive on a message, or something similar.
     When the interrupt happens, the interrupt procedure does whatever it has to in
order to handle the interrupt. Then it can unblock the driver that was waiting for it.
In some cases it will just complete up on a semaphore. In others it will do a signal
on a condition variable in a monitor.  In still others, it will send a message to the
blocked driver.  In all cases the net effect of the interrupt will be that a driver that
was previously blocked will now be able to run. This model works best if drivers
are structured as kernel processes, with their own states, stacks, and program
counters.
     Of course, reality is not quite so simple. Processing an interrupt is not just a
matter of taking the interrupt, doing an up on some semaphore, and then executing
an IRET instruction to return from the interrupt to the previous process. There is a
great deal more work involved for the operating system.                 We will now give an out-
line of this work as a series of steps that must be performed in software after the
hardware interrupt has completed.      It should be noted that the details are highly



SEC. 5.3                         I/O SOFTWARE LAYERS                                   357
system dependent, so some of the steps listed below may not be needed on a partic-
ular machine, and steps not listed may be required. Also, the steps that do occur
may be in a different order on some machines.
1.        Save any registers (including the PSW) that have not already been
          saved by the interrupt hardware.
2.        Set up a context for the interrupt-service procedure. Doing this may
          involve setting up the TLB, MMU and a page table.
3.        Set up a stack for the interrupt service-procedure.
4.        Acknowledge the interrupt controller. If there is no centralized inter-
          rupt controller, reenable interrupts.
5.        Copy the registers from where they were saved (possibly some stack)
          to the process table.
6.        Run the interrupt-service procedure. It will extract information from
          the interrupting device controller's registers.
7.        Choose which process to run next.      If the interrupt has caused some
          high-priority process that was blocked to become ready, it may be
          chosen to run now.
8.        Set up the MMU context for the process to run next. Some TLB set-
          up may also be needed.
9.        Load the new process' registers, including its PSW.
10.       Start running the new process.
As can be seen, interrupt processing is far from trivial.      It also takes a considerable
number of CPU instructions, especially on machines in which virtual memory is
present and page tables have to be set up or the state of the MMU stored (e.g., the
R and M bits).   On some machines the TLB and CPU cache may also have to be
managed when switching between user and kernel modes, which takes additional
machine cycles.
5.3.2 Device Drivers
Earlier in this chapter we looked at what device controllers do.  We saw that
each controller has some device registers used to give it commands or some device
registers used to read out its status or both. The number of device registers and the
nature of the commands vary radically from device to device. For example, a
mouse driver has to accept information from the mouse telling it how far it has
moved and which buttons are currently depressed.           In contrast, a disk driver may



358             INPUT/OUTPUT                           CHAP. 5
have to know all about sectors, tracks, cylinders, heads, arm motion, motor drives,
head settling times, and all the other mechanics of making the disk work properly.
Obviously, these drivers will be very different.
     Consequently, each I/O device attached to a computer needs some device-spe-
cific code for controlling it. This code, called the device driver, is generally writ-
ten by the device's manufacturer and delivered along with the device. Since each
operating system needs its own drivers, device manufacturers commonly supply
drivers for several popular operating systems.
     Each device driver normally handles one device type, or at most, one class of
closely related devices. For example, a SCSI disk driver can usually handle multi-
ple SCSI disks of different sizes and different speeds, and perhaps a SCSI Blu-ray
disk as well. On the other hand, a mouse and joystick are so different that different
drivers are usually required. However, there is no technical restriction on having
one device driver control multiple unrelated devices.  It is just not a good idea in
most cases.
     Sometimes though, wildly different devices are based on the same underlying
technology. The best-known example is probably USB, a serial bus technology that
is not called ``universal'' for nothing. USB devices include disks, memory sticks,
cameras, mice, keyboards, mini-fans, wireless network cards, robots, credit card
readers, rechargeable shavers, paper shredders, bar code scanners, disco balls, and
portable thermometers. They all use USB and yet they all do very different things.
The trick is that USB drivers are typically stacked, like a TCP/IP stack in networks.
At the bottom, typically in hardware, we find the USB link layer (serial I/O) that
handles hardware stuff like signaling and decoding a stream of signals to USB
packets. It is used by higher layers that deal with the data packets and the common
functionality for USB that is shared by most devices. On top of that, finally, we
find the higher-layer APIs such as the interfaces for mass storage, cameras, etc.
Thus, we still have separate device drivers, even though they share part of the pro-
tocol stack.
     In order to access the device's hardware, actually, meaning the controller's reg-
isters, the device driver normally has to be part of the operating system kernel, at
least with current architectures. Actually, it is possible to construct drivers that run
in user space, with system calls for reading and writing the device registers. This
design isolates the kernel from the drivers and the drivers from each other, elimi-
nating a major source of system crashes--buggy drivers that interfere with the ker-
nel in one way or another. For building highly reliable systems, this is definitely
the way to go.  An example of a system in which the device drivers run as user
processes is MINIX 3 (www.minix3.org). However, since most other desktop oper-
ating systems expect drivers to run in the kernel, that is the model we will consider
here.
     Since the designers of every operating system know that pieces of code (driv-
ers) written by outsiders will be installed in it, it needs to have an architecture that
allows such installation. This means having a well-defined model of what a driver



SEC. 5.3             I/O SOFTWARE LAYERS                                                   359
does and how it interacts with the rest of the operating system. Device drivers are
normally positioned below the rest of the operating system, as is illustrated in
Fig. 5-12.
                                                               User process
             User                        User
             space                       program
                                         Rest of the operating system
             Kernel
             space
                     Printer             Camcorder                     CD-ROM
                     driver              driver                        driver
          Hardware   Printer controller  Camcorder controller  CD-ROM controller
            Devices
Figure 5-12.         Logical positioning of device drivers.  In reality all communication
between drivers and device controllers goes over the bus.
Operating systems usually classify drivers into one of a small number of cate-
gories. The most common categories are the block devices, such as disks, which
contain multiple data blocks that can be addressed independently, and the charac-
ter devices, such as keyboards and printers, which generate or accept a stream of
characters.
Most operating systems define a standard interface that all block drivers must
support and a second standard interface that all character drivers must support.
These interfaces consist of a number of procedures that the rest of the operating
system can call to get the driver to do work for it. Typical procedures are those to
read a block (block device) or write a character string (character device).
In some systems, the operating system is a single binary program that contains
all of the drivers it will need compiled into it. This scheme was the norm for years



360                               INPUT/OUTPUT                           CHAP. 5
with UNIX systems because they were run by computer centers and I/O devices
rarely changed.  If a new device was added, the system administrator simply re-
compiled the kernel with the new driver to build a new binary.
     With the advent of personal computers, with their myriad I/O devices, this
model no longer worked. Few users are capable of recompiling or relinking the
kernel, even if they have the source code or object modules, which is not always
the case. Instead, operating systems, starting with MS-DOS, went over to a model
in which drivers were dynamically loaded into the system during execution. Dif-
ferent systems handle loading drivers in different ways.
     A device driver has several functions. The most obvious one is to accept
abstract read and write requests from the device-independent software above it and
see that they are carried out. But there are also a few other functions they must per-
form.  For example, the driver must initialize the device, if needed.    It may also
need to manage its power requirements and log events.
     Many device drivers have a similar general structure.         A typical driver starts
out by checking the input parameters to see if they are valid. If not, an error is re-
turned. If they are valid, a translation from abstract to concrete terms may be need-
ed. For a disk driver, this may mean converting a linear block number into the
head, track, sector, and cylinder numbers for the disk's geometry.
     Next the driver may check if the device is currently in use.   If it is, the request
will be queued for later processing.     If the device is idle, the hardware status will
be examined to see if the request can be handled now.           It may be necessary to
switch the device on or start a motor before transfers can be begun. Once the de-
vice is on and ready to go, the actual control can begin.
     Controlling the device means issuing a sequence of commands to it. The driver
is the place where the command sequence is determined, depending on what has to
be done. After the driver knows which commands it is going to issue, it starts writ-
ing them into the controller's device registers. After each command is written to
the controller, it may be necessary to check to see if the controller accepted the
command and is prepared to accept the next one. This sequence continues until all
the commands have been issued.           Some controllers can be given a linked list of
commands (in memory) and told to read and process them all by itself without fur-
ther help from the operating system.
     After the commands have been issued, one of two situations will apply.               In
many cases the device driver must wait until the controller does some work for it,
so it blocks itself until the interrupt comes in to unblock it. In other cases, howev-
er, the operation finishes without delay, so the driver need not block.  As an ex-
ample of the latter situation, scrolling the screen requires just writing a few bytes
into the controller's registers.  No mechanical motion is needed, so the entire oper-
ation can be completed in nanoseconds.
     In the former case, the blocked driver will be awakened by the interrupt. In the
latter case, it will never go to sleep.  Either way, after the operation has been com-
pleted, the driver must check for errors.  If everything is all right, the driver may



SEC. 5.3           I/O SOFTWARE LAYERS                                                   361
have some data to pass to the device-independent software (e.g., a block just read).
Finally, it returns some status information for error reporting back to its caller.      If
any other requests are queued, one of them can now be selected and started.              If
nothing is queued, the driver blocks waiting for the next request.
This simple model is only a rough approximation to reality. Many factors make
the code much more complicated.     For one thing, an I/O device may complete
while a driver is running, interrupting the driver. The interrupt may cause a device
driver to run. In fact, it may cause the current driver to run. For example, while the
network driver is processing an incoming packet, another packet may arrive. Con-
sequently, drivers have to be reentrant, meaning that a running driver has to
expect that it will be called a second time before the first call has completed.
In a hot-pluggable system, devices can be added or removed while the com-
puter is running.  As a result, while a driver is busy reading from some device, the
system may inform it that the user has suddenly removed that device from the sys-
tem. Not only must the current I/O transfer be aborted without damaging any ker-
nel data structures, but any pending requests for the now-vanished device must also
be gracefully removed from the system and their callers given the bad news. Fur-
thermore, the unexpected addition of new devices may cause the kernel to juggle
resources (e.g., interrupt request lines), taking old ones away from the driver and
giving it new ones in their place.
Drivers are not allowed to make system calls, but they often need to interact
with the rest of the kernel. Usually, calls to certain kernel procedures are permitted.
For example, there are usually calls to allocate and deallocate hardwired pages of
memory for use as buffers. Other useful calls are needed to manage the MMU,
timers, the DMA controller, the interrupt controller, and so on.
5.3.3 Device-Independent I/O Software
Although some of the I/O software is device specific, other parts of it are de-
vice independent. The exact boundary between the drivers and the device-indepen-
dent software is system (and device) dependent, because some functions that could
be done in a device-independent way may actually be done in the drivers, for ef-
ficiency or other reasons. The functions shown in Fig. 5-13 are typically done in
the device-independent software.
                   Uniform interfacing for device drivers
                   Buffering
                   Error reporting
                   Allocating and releasing dedicated devices
                   Providing a device-independent block size
          Figure 5-13. Functions of the device-independent I/O software.



362                                          INPUT/OUTPUT                                      CHAP. 5
     The basic function of the device-independent software is to perform the I/O
functions that are common to all devices and to provide a uniform interface to the
user-level software. We will now look at the above issues in more detail.
Uniform Interfacing for Device Drivers
     A major issue in an operating system is how to make all I/O devices and driv-
ers look more or less the same.              If disks, printers, keyboards, and so on, are all in-
terfaced in different ways, every time a new device comes along, the operating sys-
tem must be modified for the new device. Having to hack on the operating system
for each new device is not a good idea.
     One aspect of this issue is the interface between the device drivers and the rest
of the operating system.       In Fig. 5-14(a) we illustrate a situation in which each de-
vice driver has a different interface to the operating system. What this means is that
the driver functions available for the system to call differ from driver to driver.                 It
might also mean that the kernel functions that the driver needs also differ from
driver to driver. Taken together, it means that interfacing each new driver requires a
lot of new programming effort.
            Operating system                                                 Operating system
SATA  disk  driver  USB  disk  driver  SCSI  disk  driver  SATA disk driver  USB disk driver SCSI disk driver
                         (a)                                                 (b)
            Figure 5-14. (a) Without a standard driver interface. (b) With a standard driver
            interface.
     In contrast, in Fig. 5-14(b), we show a different design in which all drivers
have the same interface. Now it becomes much easier to plug in a new driver, pro-
viding it conforms to the driver interface.                It also means that driver writers know
what is expected of them.              In practice, not all devices are absolutely identical, but
usually there are only a small number of device types and even these are generally
almost the same.
     The way this works is as follows. For each class of devices, such as disks or
printers, the operating system defines a set of functions that the driver must supply.
For a disk these would naturally include read and write, but also turning the power



SEC. 5.3                   I/O SOFTWARE LAYERS                                          363
on and off, formatting, and other disky things. Often the driver holds a table with
pointers into itself for these functions.  When the driver is loaded, the operating
system records the address of this table of function pointers, so when it needs to
call one of the functions, it can make an indirect call via this table. This table of
function pointers defines the interface between the driver and the rest of the operat-
ing system. All devices of a given class (disks, printers, etc.) must obey it.
Another aspect of having a uniform interface is how I/O devices are named.
The device-independent software takes care of mapping symbolic device names
onto the proper driver. For example, in UNIX a device name, such as /dev/disk0,
uniquely specifies the i-node for a special file, and this i-node contains the major
device number, which is used to locate the appropriate driver. The i-node also
contains the minor device number, which is passed as a parameter to the driver in
order to specify the unit to be read or written. All devices have major and minor
numbers, and all drivers are accessed by using the major device number to select
the driver.
Closely related to naming is protection. How does the system prevent users
from accessing devices that they are not entitled to access?  In both UNIX and
Windows, devices appear in the file system as named objects, which means that the
usual protection rules for files also apply to I/O devices. The system administrator
can then set the proper permissions for each device.
Buffering
Buffering is also an issue, both for block and character devices, for a variety of
reasons.     To see one of them, consider a process that wants to read data from an
(ADSL--Asymmetric Digital Subscriber Line) modem, something many people
use at home to connect to the Internet.    One possible strategy for dealing with the
incoming characters is to have the user process do a read system call and block
waiting for one character. Each arriving character causes an interrupt.         The inter-
rupt-service procedure hands the character to the user process and unblocks it.
After putting the character somewhere, the process reads another character and
blocks again. This model is indicated in Fig. 5-15(a).
The trouble with this way of doing business is that the user process has to be
started up for every incoming character. Allowing a process to run many times for
short runs is inefficient, so this design is not a good one.
An improvement is shown in Fig. 5-15(b).   Here the user process provides an
n-character buffer in user space and does a read of n characters. The interrupt-ser-
vice procedure puts incoming characters in this buffer until it is completely full.
Only then does it wakes up the user process. This scheme is far more efficient than
the previous one, but it has a drawback: what happens if the buffer is paged out
when a character arrives?  The buffer could be locked in memory, but if many
processes start locking pages in memory willy nilly, the pool of available pages
will shrink and performance will degrade.



364                                   INPUT/OUTPUT                                            CHAP.  5
                        User process
User
space
Kernel                                                      2                              2
space
                                                            1                              1      3
           Modem                      Modem         Modem                                  Modem
           (a)                        (b)                   (c)                            (d)
        Figure 5-15. (a) Unbuffered input. (b) Buffering in user space. (c) Buffering in
        the kernel followed by copying to user space. (d) Double buffering in the kernel.
     Yet another approach is to create a buffer inside the kernel and have the inter-
rupt handler put the characters there, as shown in Fig. 5-15(c).     When this buffer is
full, the page with the user buffer is brought in, if needed, and the buffer copied
there in one operation. This scheme is far more efficient.
     However, even this improved scheme suffers from a problem: What happens to
characters that arrive while the page with the user buffer is being brought in from
the disk?  Since the buffer is full, there is no place to put them.    A way out is to
have a second kernel buffer.  After the first buffer fills up, but before it has been
emptied, the second one is used, as shown in Fig. 5-15(d). When the second buffer
fills up, it is available to be copied to the user (assuming the user has asked for it).
While the second buffer is being copied to user space, the first one can be used for
new characters.  In this way, the two buffers take turns: while one is being copied
to user space, the other is accumulating new input. A buffering scheme like this is
called double buffering.
     Another common form of buffering is the circular buffer.        It consists of a re-
gion of memory and two pointers. One pointer points to the next free word, where
new data can be placed.   The other pointer points to the first word of data in the
buffer that has not been removed yet. In many situations, the hardware advances
the first pointer as it adds new data (e.g., just arriving from the network) and the
operating system advances the second pointer as it removes and processes data.
Both pointers wrap around, going back to the bottom when they hit the top.
     Buffering is also important on output. Consider, for example, how output is
done to the modem without buffering using the model of Fig. 5-15(b).                       The user
process executes a write system call to output n characters. The system has two
choices at this point.  It can block the user until all the characters have been writ-
ten, but this could take a very long time over a slow telephone line.                      It could also
release the user immediately and do the I/O while the user computes some more,



SEC. 5.3                        I/O SOFTWARE LAYERS                                        365
but this leads to an even worse problem: how does the user process know that the
output has been completed and it can reuse the buffer? The system could generate
a signal or software interrupt, but that style of programming is difficult and prone
to race conditions.   A much better solution is for the kernel to copy the data to a
kernel buffer, analogous to Fig. 5-15(c) (but the other way), and unblock the caller
immediately. Now it does not matter when the actual I/O has been completed. The
user is free to reuse the buffer the instant it is unblocked.
     Buffering is a widely used technique, but it has a downside as well. If data get
buffered too many times, performance suffers.          Consider, for example, the network
of Fig. 5-16.     Here a user does a system call to write to the network. The kernel
copies the packet to a kernel buffer to allow the user to proceed immediately (step
1). At this point the user program can reuse the buffer.
                                User process
          User
          space
          Kernel          1                                                     5
          space
                          2                                                     4
                                Network
                                controller
                             3
                                              Network
                  Figure 5-16. Networking may involve many copies of a packet.
     When the driver is called, it copies the packet to the controller for output (step
2).  The reason it does not output to the wire directly from kernel memory is that
once a packet transmission has been started, it must continue at a uniform speed.
The driver cannot guarantee that it can get to memory at a uniform speed because
DMA channels and other I/O devices may be stealing many cycles. Failing to get a
word on time would ruin the packet.           By buffering the packet inside the controller,
this problem is avoided.
     After the packet has been copied to the controller's internal buffer, it is copied
out onto the network (step 3).  Bits arrive at the receiver shortly after being sent, so
just after the last bit has been sent, that bit arrives at the receiver, where the packet
has been buffered in the controller. Next the packet is copied to the receiver's ker-
nel buffer (step 4).  Finally, it is copied to the receiving process' buffer (step 5).
Usually, the receiver then sends back an acknowledgement. When the sender gets
the acknowledgement, it is free to send the next packet.       However, it should be
clear that all this copying is going to slow down the transmission rate considerably
because all the steps must happen sequentially.



366                                 INPUT/OUTPUT                              CHAP. 5
Error Reporting
     Errors are far more common in the context of I/O than in other contexts. When
they occur, the operating system must handle them as best it can.  Many errors are
device specific and must be handled by the appropriate driver, but the framework
for error handling is device independent.
     One class of I/O errors is programming errors. These occur when a process
asks for something impossible, such as writing to an input device (keyboard, scan-
ner, mouse, etc.) or reading from an output device (printer, plotter, etc.).  Other er-
rors are providing an invalid buffer address or other parameter, and specifying an
invalid device (e.g., disk 3 when the system has only two disks), and so on.            The
action to take on these errors is straightforward: just report back an error code to
the caller.
     Another class of errors is the class of actual I/O errors, for example, trying to
write a disk block that has been damaged or trying to read from a camcorder that
has been switched off.    In these circumstances, it is up to the driver to determine
what to do.  If the driver does not know what to do, it may pass the problem back
up to device-independent software.
     What this software does depends on the environment and the nature of the
error.  If it is a simple read error and there is an interactive user available, it may
display a dialog box asking the user what to do. The options may include retrying a
certain number of times, ignoring the error, or killing the calling process.  If there
is no user available, probably the only real option is to have the system call fail
with an error code.
     However, some errors cannot be handled this way. For example, a critical data
structure, such as the root directory or free block list, may have been destroyed.       In
this case, the system may have to display an error message and terminate. There is
not much else it can do.
Allocating and Releasing Dedicated Devices
     Some devices, such as printers, can be used only by a single process at any
given moment.      It is up to the operating system to examine requests for device
usage and accept or reject them, depending on whether the requested device is
available or not.  A simple way to handle these requests is to require processes to
perform opens on the special files for devices directly. If the device is unavailable,
the open fails. Closing such a dedicated device then releases it.
     An alternative approach is to have special mechanisms for requesting and
releasing dedicated devices.  An attempt to acquire a device that is not available
blocks the caller instead of failing. Blocked processes are put on a queue.         Sooner
or later, the requested device becomes available and the first process on the queue
is allowed to acquire it and continue execution.



SEC. 5.3                       I/O SOFTWARE LAYERS                                          367
Device-Independent Block Size
Different disks may have different sector sizes.        It is up to the device-indepen-
dent software to hide this fact and provide a uniform block size to higher layers,
for example, by treating several sectors as a single logical block.       In this way, the
higher layers deal only with abstract devices that all use the same logical block
size, independent of the physical sector size. Similarly, some character devices de-
liver their data one byte at a time (e.g., mice), while others deliver theirs in larger
units (e.g., Ethernet interfaces). These differences may also be hidden.
5.3.4 User-Space I/O Software
Although most of the I/O software is within the operating system, a small por-
tion of it consists of libraries linked together with user programs, and even whole
programs running outside the kernel. System calls, including the I/O system calls,
are normally made by library procedures. When a C program contains the call
count = write(fd, buffer, nbytes);
the library procedure write might be linked with the program and contained in the
binary program present in memory at run time.           In other systems, libraries can be
loaded during program execution.       Either way, the collection of all these library
procedures is clearly part of the I/O system.
While these procedures do little more than put their parameters in the ap-
propriate place for the system call, other I/O procedures actually do real work.            In
particular, formatting of input and output is done by library procedures.    One ex-
ample from C is printf, which takes a format string and possibly some variables as
input, builds an ASCII string, and then calls write to output the string.  As an ex-
ample of printf, consider the statement
printf("The square of %3d is          %6d\n", i, i*i);
It formats a string consisting of the 14-character string ``The square of '' followed
by the value i as a 3-character string, then the 4-character string `` is '', then i2 as 6
characters, and finally a line feed.
An example of a similar procedure for input is scanf, which reads input and
stores it into variables described in a format string using the same syntax as printf.
The standard I/O library contains a number of procedures that involve I/O and all
run as part of user programs.
Not all user-level I/O software consists of library procedures. Another impor-
tant category is the spooling system.    Spooling is a way of dealing with dedicated
I/O devices in a multiprogramming system. Consider a typical spooled device: a
printer.  Although it would be technically easy to let any user process open the
character special file for the printer, suppose a process opened it and then did noth-
ing for hours. No other process could print anything.



368                              INPUT/OUTPUT                                             CHAP. 5
     Instead what is done is to create a special process, called a daemon, and a spe-
cial directory, called a spooling directory.       To print a file, a process first generates
the entire file to be printed and puts it in the spooling directory. It is up to the dae-
mon, which is the only process having permission to use the printer's special file,
to print the files in the directory. By protecting the special file against direct use by
users, the problem of having someone keeping it open unnecessarily long is elimi-
nated.
     Spooling is used not only for printers.         It is also used in other I/O situations.
For example, file transfer over a network often uses a network daemon. To send a
file somewhere, a user puts it in a network spooling directory.     Later on, the net-
work daemon takes it out and transmits it. One particular use of spooled file trans-
mission is the USENET News system (now part of Google Groups).                            This network
consists of millions of machines around the world communicating using the Inter-
net. Thousands of news groups exist on many topics.  To post a news message, the
user invokes a news program, which accepts the message to be posted and then
deposits it in a spooling directory for transmission to other machines later. The en-
tire news system runs outside the operating system.
     Figure 5-17 summarizes the I/O system, showing all the layers and the princi-
pal functions of each layer. Starting at the bottom, the layers are the hardware, in-
terrupt handlers, device drivers, device-independent software, and finally the user
processes.
                                              I/O
             Layer                            reply  I/O functions
        I/O  User processes      Make I/O call; format I/O; spooling
request
             Device-independent  Naming, protection, blocking, buffering, allocation
             software
             Device drivers      Set up device registers; check status
             Interrupt handlers  Wake up driver when I/O completed
             Hardware            Perform I/O operation
             Figure 5-17. Layers of the I/O system and the main functions of each layer.
     The arrows in Fig. 5-17 show the flow of control. When a user program tries to
read a block from a file, for example, the operating system is invoked to carry out
the call. The device-independent software looks for it, say, in the buffer cache.              If
the needed block is not there, it calls the device driver to issue the request to the
hardware to go get it from the disk. The process is then blocked until the disk oper-
ation has been completed and the data are safely available in the caller's buffer.



SEC. 5.3                    I/O SOFTWARE LAYERS                                       369
When the disk is finished, the hardware generates an interrupt. The interrupt
handler is run to discover what has happened, that is, which device wants attention
right now.   It then extracts the status from the device and wakes up the sleeping
process to finish off the I/O request and let the user process continue.
5.4 DISKS
Now we will begin studying some real I/O devices.      We will begin with disks,
which are conceptually simple, yet very important.     After that we will examine
clocks, keyboards, and displays.
5.4.1 Disk Hardware
Disks come in a variety of types. The most common ones are the magnetic
hard disks.  They are characterized by the fact that reads and writes are equally
fast, which makes them suitable as secondary memory (paging, file systems, etc.).
Arrays of these disks are sometimes used to provide highly reliable storage. For
distribution of programs, data, and movies, optical disks (DVDs and Blu-ray) are
also important. Finally, solid-state disks are increasingly popular as they are fast
and do not contain moving parts.  In the following sections we will discuss mag-
netic disks as an example of the hardware and then describe the software for disk
devices in general.
Magnetic Disks
Magnetic disks are organized into cylinders, each one containing as many
tracks as there are heads stacked vertically. The tracks are divided into sectors,
with the number of sectors around the circumference typically being 8 to 32 on
floppy disks, and up to several hundred on hard disks. The number of heads varies
from 1 to about 16.
Older disks have little electronics and just deliver a simple serial bit stream.
On these disks, the controller does most of the work.  On other disks, in particular,
IDE (Integrated Drive Electronics) and SATA (Serial ATA) disks, the disk drive
itself contains a microcontroller that does considerable work and allows the real
controller to issue a set of higher-level commands. The controller often does track
caching, bad-block remapping, and much more.
A device feature that has important implications for the disk driver is the possi-
bility of a controller doing seeks on two or more drives at the same time. These are
known as overlapped seeks.        While the controller and software are waiting for a
seek to complete on one drive, the controller can initiate a seek on another drive.
Many controllers can also read or write on one drive while seeking on one or more
other drives, but a floppy disk controller cannot read or write on two drives at the



370                                INPUT/OUTPUT                                        CHAP. 5
same time. (Reading or writing requires the controller to move bits on a microsec-
ond time scale, so one transfer uses up most of its computing power.)                  The situa-
tion is different for hard disks with integrated controllers, and in a system with
more than one of these hard drives they can operate simultaneously, at least to the
extent of transferring between the disk and the controller's buffer memory. Only
one transfer between the controller and the main memory is possible at once, how-
ever. The ability to perform two or more operations at the same time can reduce the
average access time considerably.
     Figure 5-18 compares parameters of the standard storage medium for the origi-
nal IBM PC with parameters of a disk made three decades later to show how much
disks changed in that time. It is interesting to note that not all parameters have im-
proved as much.    Average seek time is almost 9 times better than it was, transfer
rate is 16,000 times better, while capacity is up by a factor of 800,000. This pattern
has to do with relatively gradual improvements in the moving parts, but much
higher bit densities on the recording surfaces.
        Parameter                  IBM 360-KB floppy disk           WD 3000 HLFS hard disk
Number of cylinders                   40                            36,481
Tracks per cylinder                   2                             255
Sectors per track                     9                                63 (avg)
Sectors per disk                      720                           586,072,368
Bytes per sector                      512                           512
Disk capacity                         360 KB                        300 GB
Seek time (adjacent cylinders)        6 msec                           0.7 msec
Seek time (average case)              77 msec                          4.2 msec
Rotation time                         200 msec                         6 msec
Time to transfer 1 sector             22 msec                          1.4 sec
        Figure 5-18. Disk parameters for the original IBM PC 360-KB floppy disk and a
        Western Digital WD 3000 HLFS (``Velociraptor'') hard disk.
     One thing to be aware of in looking at the specifications of modern hard disks
is that the geometry specified, and used by the driver software, is almost always
different from the physical format.   On old disks, the number of sectors per track
was the same for all cylinders. Modern disks are divided into zones with more sec-
tors on the outer zones than the inner ones. Fig. 5-19(a) illustrates a tiny disk with
two zones. The outer zone has 32 sectors per track; the inner one has 16 sectors per
track.  A real disk, such as the WD 3000 HLFS, typically has 16 or more zones,
with the number of sectors increasing by about 4% per zone as one goes out from
the innermost to the outermost zone.
     To hide the details of how many sectors each track has, most modern disks
have a virtual geometry that is presented to the operating system. The software is
instructed to act as though there are x cylinders, y heads, and z sectors per track.



SEC.    5.4                                                            DISKS                                   371
                     30  31                   0      1                                 23  24  0   1
                 29                                     2
             28                                             3                      22                  2
        27                                                      4              21                          3
    26                                                              5      20
                                                                                                               4
25                                                                     6
                         14 15             0  1                            19
24                       10 11 1213              23                     7                                      5
23                                               4
                                                 5                      8  18                                     6
22                                   9  8  67
                                                                       9   17
    21                                                                                                         7
                                                                    10
        20                                                      11             16                          89
            19                                              12                     15                  10
                 18  17                          14     13                             14          11
                         16                15                                              13  12
            Figure 5-19. (a) Physical geometry of a disk with two zones. (b) A possible vir-
            tual geometry for this disk.
The controller then remaps a request for (x, y, z) onto the real cylinder, head, and
sector.     A possible virtual geometry for the physical disk of Fig. 5-19(a) is shown
in Fig. 5-19(b).         In both cases the disk has 192 sectors, only the published arrange-
ment is different than the real one.
    For PCs, the maximum values for these three parameters are often (65535, 16,
and 63), due to the need to be backward compatible with the limitations of the
original IBM PC.                        On this machine, 16-, 4-, and 6-bit fields were used to specify
these numbers, with cylinders and sectors numbered starting at 1 and heads num-
bered starting at 0. With these parameters and 512 bytes per sector, the largest pos-
sible disk is 31.5 GB. To get around this limit, all modern disks now support a sys-
tem called logical block addressing, in which disk sectors are just numbered con-
secutively starting at 0, without regard to the disk geometry.
RAID
    CPU performance has been increasing exponentially over the past decade,
roughly doubling every 18 months. Not so with disk performance.                                    In the 1970s,
average seek times on minicomputer disks were 50 to 100 msec. Now seek times
are still a few msec.                         In most technical industries (say, automobiles or aviation), a
factor of 5 to 10 performance improvement in two decades would be major news
(imagine 300-MPG cars), but in the computer industry it is an embarrassment.
Thus the gap between CPU performance and (hard) disk performance has become
much larger over time. Can anything be done to help?



372                             INPUT/OUTPUT                                 CHAP. 5
     Yes! As we have seen, parallel processing is increasingly being used to speed
up CPU performance.  It has occurred to various people over the years that parallel
I/O might be a good idea, too.  In their 1988 paper, Patterson et al. suggested six
specific disk organizations that could be used to improve disk performance, re-
liability, or both (Patterson et al., 1988).  These ideas were quickly adopted by in-
dustry and have led to a new class of I/O device called a RAID.     Patterson et al.
defined RAID as Redundant Array of Inexpensive Disks, but industry redefined
the I to be ``Independent'' rather than ``Inexpensive'' (maybe so they could charge
more?).    Since a villain was also needed (as in RISC vs. CISC, also due to Patter-
son), the bad guy here was the SLED (Single Large Expensive Disk).
     The fundamental idea behind a RAID is to install a box full of disks next to the
computer, typically a large server, replace the disk controller card with a RAID
controller, copy the data over to the RAID, and then continue normal operation.         In
other words, a RAID should look like a SLED to the operating system but have
better performance and better reliability. In the past, RAIDs consisted almost ex-
clusively of a RAID SCSI controller plus a box of SCSI disks, because the per-
formance was good and modern SCSI supports up to 15 disks on a single con-
troller. Nowadays, many manufacturers also offer (less expensive) RAIDs based on
SATA.      In this way, no software changes are required to use the RAID, a big sell-
ing point for many system administrators.
     In addition to appearing like a single disk to the software, all RAIDs have the
property that the data are distributed over the drives, to allow parallel operation.
Several different schemes for doing this were defined by Patterson et al.            Now-
adays, most manufacturers refer to the seven standard configurations as RAID
level 0 through RAID level 6.   In addition, there are a few other minor levels that
we will not discuss. The term ``level'' is something of a misnomer since no hier-
archy is involved; there are simply seven different organizations possible.
     RAID level 0 is illustrated in Fig. 5-20(a).  It consists of viewing the virtual
single disk simulated by the RAID as being divided up into strips of k sectors each,
with sectors 0 to k - 1 being strip 0, sectors k to 2k - 1 strip 1, and so on. For
k = 1, each strip is a sector; for k = 2 a strip is two sectors, etc. The RAID level 0
organization writes consecutive strips over the drives in round-robin fashion, as
depicted in Fig. 5-20(a) for a RAID with four disk drives.
     Distributing data over multiple drives like this is called striping. For example,
if the software issues a command to read a data block consisting of four consecu-
tive strips starting at a strip boundary, the RAID controller will break this com-
mand up into four separate commands, one for each of the four disks, and have
them operate in parallel. Thus we have parallel I/O without the software knowing
about it.
     RAID level 0 works best with large requests, the bigger the better. If a request
is larger than the number of drives times the strip size, some drives will get multi-
ple requests, so that when they finish the first request they start the second one.     It
is up to the controller to split the request up and feed the proper commands to the



SEC. 5.4                              DISKS                                            373
proper disks in the right sequence and then assemble the results in memory cor-
rectly. Performance is excellent and the implementation is straightforward.
RAID level 0 works worst with operating systems that habitually ask for data
one sector at a time. The results will be correct, but there is no parallelism and
hence no performance gain. Another disadvantage of this organization is that the
reliability is potentially worse than having a SLED.       If a RAID consists of four
disks, each with a mean time to failure of 20,000 hours, about once every 5000
hours a drive will fail and all the data will be completely lost.         A SLED with a
mean time to failure of 20,000 hours would be four times more reliable. Because
no redundancy is present in this design, it is not really a true RAID.
The next option, RAID level 1, shown in Fig. 5-20(b), is a true RAID. It dupli-
cates all the disks, so there are four primary disks and four backup disks.  On a
write, every strip is written twice.  On a read, either copy can be used, distributing
the load over more drives. Consequently, write performance is no better than for a
single drive, but read performance can be up to twice as good. Fault tolerance is
excellent: if a drive crashes, the copy is simply used instead. Recovery consists of
simply installing a new drive and copying the entire backup drive to it.
Unlike levels 0 and 1, which work with strips of sectors, RAID level 2 works
on a word basis, possibly even a byte basis. Imagine splitting each byte of the sin-
gle virtual disk into a pair of 4-bit nibbles, then adding a Hamming code to each
one to form a 7-bit word, of which bits 1, 2, and 4 were parity bits. Further imagine
that the seven drives of Fig. 5-20(c) were synchronized in terms of arm position
and rotational position. Then it would be possible to write the 7-bit Hamming
coded word over the seven drives, one bit per drive.
The Thinking Machines CM-2 computer used this scheme, taking 32-bit data
words and adding 6 parity bits to form a 38-bit Hamming word, plus an extra bit
for word parity, and spread each word over 39 disk drives. The total throughput
was immense, because in one sector time it could write 32 sectors worth of data.
Also, losing one drive did not cause problems, because loss of a drive amounted to
losing 1 bit in each 39-bit word read, something the Hamming code could handle
on the fly.
On the down side, this scheme requires all the drives to be rotationally syn-
chronized, and it only makes sense with a substantial number of drives (even with
32 data drives and 6 parity drives, the overhead is 19%).  It also asks a lot of the
controller, since it must do a Hamming checksum every bit time.
RAID level 3 is a simplified version of RAID level 2.                   It is illustrated in
Fig. 5-20(d). Here a single parity bit is computed for each data word and written to
a parity drive.  As in RAID level 2, the drives must be exactly synchronized, since
individual data words are spread over multiple drives.
At first thought, it might appear that a single parity bit gives only error detec-
tion, not error correction. For the case of random undetected errors, this observa-
tion is true.  However, for the case of a drive crashing, it provides full 1-bit error
correction since the position of the bad bit is known.     In the event that a drive



374                INPUT/OUTPUT                                                         CHAP.  5
     Figure 5-20.  RAID levels 0 through 6. Backup and parity drives are shown shaded.



SEC. 5.4                                 DISKS                                         375
crashes, the controller just pretends that all its bits are 0s.  If a word has a parity
error, the bit from the dead drive must have been a 1, so it is corrected. Although
both RAID levels 2 and 3 offer very high data rates, the number of separate I/O re-
quests per second they can handle is no better than for a single drive.
RAID levels 4 and 5 work with strips again, not individual words with parity,
and do not require synchronized drives.  RAID level 4 [see Fig. 5-20(e)] is like
RAID level 0, with a strip-for-strip parity written onto an extra drive. For example,
if each strip is k bytes long, all the strips are EXCLUSIVE ORed together, re-
sulting in a parity strip k bytes long.  If a drive crashes, the lost bytes can be
recomputed from the parity drive by reading the entire set of drives.
This design protects against the loss of a drive but performs poorly for small
updates.  If one sector is changed, it is necessary to read all the drives in order to
recalculate the parity, which must then be rewritten. Alternatively, it can read the
old user data and the old parity data and recompute the new parity from them.
Even with this optimization, a small update requires two reads and two writes.
As a consequence of the heavy load on the parity drive, it may become a bot-
tleneck. This bottleneck is eliminated in RAID level 5 by distributing the parity
bits uniformly over all the drives, round-robin fashion, as shown in Fig. 5-20(f).
However, in the event of a drive crash, reconstructing the contents of the failed
drive is a complex process.
Raid level 6 is similar to RAID level 5, except that an additional parity block is
used. In other words, the data is striped across the disks with two parity blocks in-
stead of one. As a result, writes are bit more expensive because of the parity calcu-
lations, but reads incur no performance penalty. It does offer more reliability (im-
agine what happens if RAID level 5 encounters a bad block just when it is rebuild-
ing its array).
5.4.2 Disk Formatting
A hard disk consists of a stack of aluminum, alloy, or glass platters typically
3.5 inch in diameter (or 2.5 inch on notebook computers).        On each platter is
deposited a thin magnetizable metal oxide. After manufacturing, there is no infor-
mation whatsoever on the disk.
Before the disk can be used, each platter must receive a low-level format done
by software. The format consists of a series of concentric tracks, each containing
some number of sectors, with short gaps between the sectors. The format of a sec-
tor is shown in Fig. 5-21.
          Preamble                       Data                            ECC
                                Figure 5-21. A disk sector.



376                              INPUT/OUTPUT                                     CHAP. 5
     The preamble starts with a certain bit pattern that allows the hardware to rec-
ognize the start of the sector.  It also contains the cylinder and sector numbers and
some other information.     The size of the data portion is determined by the low-
level formatting program. Most disks use 512-byte sectors.        The ECC field con-
tains redundant information that can be used to recover from read errors. The size
and content of this field varies from manufacturer to manufacturer, depending on
how much disk space the designer is willing to give up for higher reliability and
how complex an ECC code the controller can handle.          A 16-byte ECC field is not
unusual.  Furthermore, all hard disks have some number of spare sectors allocated
to be used to replace sectors with a manufacturing defect.
     The position of sector 0 on each track is offset from the previous track when
the low-level format is laid down. This offset, called cylinder skew, is done to im-
prove performance. The idea is to allow the disk to read multiple tracks in one con-
tinuous operation without losing data. The nature of the problem can be seen by
looking at Fig. 5-19(a).    Suppose that a request needs 18 sectors starting at sector 0
on the innermost track. Reading the first 16 sectors takes one disk rotation, but a
seek is needed to move outward one track to get the 17th sector.  By the time the
head has moved one track, sector 0 has rotated past the head so an entire rotation is
needed until it comes by again. That problem is eliminated by offsetting the sectors
as shown in Fig. 5-22.
     The amount of cylinder skew depends on the drive geometry. For example, a
10,000-RPM (Revolutions Per Minute) drive rotates in 6 msec.      If a track contains
300 sectors, a new sector passes under the head every 20 sec. If the track-to-track
seek time is 800 sec, 40 sectors will pass by during the seek, so the cylinder skew
should be at least 40 sectors, rather than the three sectors shown in Fig. 5-22.       It is
worth mentioning that switching between heads also takes a finite time, so there is
head skew as well as cylinder skew, but head skew is not very large, usually much
less than one sector time.
     As a result of the low-level formatting, disk capacity is reduced, depending on
the sizes of the preamble, intersector gap, and ECC, as well as the number of spare
sectors reserved. Often the formatted capacity is 20% lower than the unformatted
capacity. The spare sectors do not count toward the formatted capacity, so all disks
of a given type have exactly the same capacity when shipped, independent of how
many bad sectors they actually have (if the number of bad sectors exceeds the
number of spares, the drive will be rejected and not shipped).
     There is considerable confusion about disk capacity because some manufact-
urers advertised the unformatted capacity to make their drives look larger than they
in reality are. For example, let us consider a drive whose unformatted capacity is
200 Ã 109 bytes. This might be sold as a 200-GB disk.       However, after formatting,
posibly only 170 Ã 109 bytes are available for data.   To add to the confusion, the
operating system will probably report this capacity as 158 GB, not 170 GB, be-
cause software considers a memory of 1 GB to be 230 (1,073,741,824) bytes, not
109 (1,000,000,000) bytes. It would be better if this were reported as 158 GiB.



SEC. 5.4                                                              DISKS                                                            377
                                              15             16          17          18                             Direction of disk
                                  14                         19       20          21             19                     rotation
                           13         17        18                                         22            20
                               16                      21    22       23       24                    23
                  12                      20                 25       26                25       26             21
                       15          19           23 24                       27    28                     24
              11             18        22              26 27 28       29 30 31 0           29        27             22
                   14     17      21            25                                             30 31          25
           10                  20                         2829 30 31  0  1  2                            28             23
               13                  21 22 23 24                                 3        1                         26
                       16  18 19              2425 2627                           4        2         0     29
                                                                                  5           3                           24
           9   12     15                                                             6                        30    27
                                                                                     7         4      1
                      119114211751123180621192121242703                                 8      5        2     31
           8   11                                                                 9 10 11                           28    25
                                                                                               6     3
                                                          1213 141516171819                   7              0
           7      10                                                                       8         4            29    26
                                                       10 11 12 13 14 15 16          9           5         1
                                                                                           6                    30
                                          13                                                          2               27
               6                                                                     7
                          8        10           10 11 12                 9     8                 3         31
                   5                       9                                               4         0          28
                               7                          8  7           6     5
                           4           6                                                      1          29
                                                       5     4           3        2              30
                                   3                                              31
                                                2            1           0
                              Figure 5-22. An illustration of cylinder skew.
To make things even worse, in the world of data communications, 1 Gbps
means 1,000,000,000 bits/sec because the prefix giga really does mean 109 (a kilo-
meter is 1000 meters, not 1024 meters, after all).                                                       Only with memory and disk
sizes do kilo, mega, giga, and tera mean 210, 220, 230, and 240, respectively.
To avoid confusion, some authors use the prefixes kilo, mega, giga, and tera to
mean 103, 106, 109, and 1012 respectively, while using kibi, mebi, gibi, and tebi to
mean 210, 220, 230, and 240, respectively. However, the use of the ``b'' prefixes is
relatively rare. Just in case you like really big numbers, the prefixes following tebi
are pebi, exbi, zebi, and yobi, so a yobibyte is a whole bunch of bytes (280 to be
precise).
Formatting also affects performance.                                           If a 10,000-RPM disk has 300 sectors
per track of 512 bytes each, it takes 6 msec to read the 153,600 bytes on a track for
a data rate of 25,600,000 bytes/sec or 24.4 MB/sec.                                                      It is not possible to go faster
than this, no matter what kind of interface is present, even if it is a SCSI interface
at 80 MB/sec or 160 MB/sec.
Actually reading continuously at this rate requires a large buffer in the con-
troller. Consider, for example, a controller with a one-sector buffer that has been
given a command to read two consecutive sectors. After reading the first sector
from the disk and doing the ECC calculation, the data must be transferred to main



378                                 INPUT/OUTPUT                                                CHAP. 5
memory. While this transfer is taking place, the next sector will fly by the head.
When the copy to memory is complete, the controller will have to wait almost an
entire rotation time for the second sector to come around again.
     This problem can be eliminated by numbering the sectors in an interleaved
fashion when formatting the disk. In Fig. 5-23(a), we see the usual numbering pat-
tern (ignoring cylinder skew here).         In Fig. 5-23(b), we see single interleaving,
which gives the controller some breathing space between consecutive sectors in
order to copy the buffer to main memory.
        7       0                           7       0                     5                  0
     6             1                     3             4               2                        3
     5             2                     6             1               7                        6
        4       3                           2       5                     4                  1
           (a)                                 (b)                           (c)
        Figure 5-23. (a) No interleaving. (b) Single interleaving. (c) Double interleaving.
     If the copying process is very slow, the double interleaving of Fig. 5-24(c)
may be needed.     If the controller has a buffer of only one sector, it does not matter
whether the copying from the buffer to main memory is done by the controller, the
main CPU, or a DMA chip; it still takes some time.        To avoid the need for inter-
leaving, the controller should be able to buffer an entire track. Most modern con-
trollers can buffer many entire tracks.
     After low-level formatting is completed, the disk is partitioned. Logically, each
partition is like a separate disk.  Partitions are needed to allow multiple operating
systems to coexist.   Also, in some cases, a partition can be used for swapping.                   In
the x86 and most other computers, sector 0 contains the MBR (Master Boot
Record), which contains some boot code plus the partition table at the end.                        The
MBR, and thus support for partition tables, first appeared in IBM PCs in 1983 to
support the then-massive 10-MB hard drive in the PC XT.     Disks have grown a bit
since then. As MBR partition entries in most systems are limited to 32 bits, the
maximum disk size that can be supported with 512 B sectors is 2 TB. For this rea-
son, most operating since now also support the new GPT (GUID Partition Table),
which supports disk sizes up to 9.4 ZB (9,444,732,965,739,290,426,880 bytes). At
the time this book went to press, this was considered a lot of bytes.
     The partition table gives the starting sector and size of each partition.                     On the
x86, the MBR partition table has room for four partitions.        If all of them are for
Windows, they will be called C:, D:, E:, and F: and treated as separate drives.                    If
three of them are for Windows and one is for UNIX, then Windows will call its
partitions C:, D:, and E:.  If a USB drive is added, it will be F:.    To be able to boot
from the hard disk, one partition must be marked as active in the partition table.



SEC. 5.4                              DISKS                                             379
The final step in preparing a disk for use is to perform a high-level format of
each partition (separately).  This operation lays down a boot block, the free storage
administration (free list or bitmap), root directory, and an empty file system.          It
also puts a code in the partition table entry telling which file system is used in the
partition because many operating systems support multiple incompatible file sys-
tems (for historical reasons). At this point the system can be booted.
When the power is turned on, the BIOS runs initially and then reads in the
master boot record and jumps to it. This boot program then checks to see which
partition is active.  Then it reads in the boot sector from that partition and runs it.
The boot sector contains a small program that generally loads a larger bootstrap
loader that searches the file system to find the operating system kernel. That pro-
gram is loaded into memory and executed.
5.4.3 Disk Arm Scheduling Algorithms
In this section we will look at some issues related to disk drivers in general.
First, consider how long it takes to read or write a disk block. The time required is
determined by three factors:
1.        Seek time (the time to move the arm to the proper cylinder).
2.        Rotational delay (how long for the proper sector to appear under the
          reading head).
3.        Actual data transfer time.
For most disks, the seek time dominates the other two times, so reducing the mean
seek time can improve system performance substantially.
If the disk driver accepts requests one at a time and carries them out in that
order, that is, FCFS (First-Come, First-Served), little can be done to optimize
seek time. However, another strategy is possible when the disk is heavily loaded. It
is likely that while the arm is seeking on behalf of one request, other disk requests
may be generated by other processes. Many disk drivers maintain a table, indexed
by cylinder number, with all the pending requests for each cylinder chained toget-
her in a linked list headed by the table entries.
Given this kind of data structure, we can improve upon the first-come, first-
served scheduling algorithm. To see how, consider an imaginary disk with 40 cyl-
inders. A request comes in to read a block on cylinder 11. While the seek to cylin-
der 11 is in progress, new requests come in for cylinders 1, 36, 16, 34, 9, and 12, in
that order. They are entered into the table of pending requests, with a separate link-
ed list for each cylinder. The requests are shown in Fig. 5-24.
When the current request (for cylinder 11) is finished, the disk driver has a
choice of which request to handle next. Using FCFS, it would go next to cylinder
1, then to 36, and so on. This algorithm would require arm motions of 10, 35, 20,
18, 25, and 3, respectively, for a total of 111 cylinders.



380                                 INPUT/OUTPUT                                  CHAP. 5
                           Initial  Pending
                        position    requests
         X              X      XX        X                                     X      X
      0     5              10       15        20               25     30          35  Cylinder
Time                                        Sequence of seeks
            Figure 5-24. Shortest Seek First (SSF) disk scheduling algorithm.
      Alternatively, it could always handle the closest request next, to minimize seek
time. Given the requests of Fig. 5-24, the sequence is 12, 9, 16, 1, 34, and 36,
shown as the jagged line at the bottom of Fig. 5-24.           With this sequence, the arm
motions are 1, 3, 7, 15, 33, and 2, for a total of 61 cylinders. This algorithm, called
SSF (Shortest Seek First), cuts the total arm motion almost in half compared to
FCFS.
      Unfortunately, SSF has a problem. Suppose more requests keep coming in
while the requests of Fig. 5-24 are being processed. For example, if, after going to
cylinder 16, a new request for cylinder 8 is present, that request will have priority
over cylinder 1. If a request for cylinder 13 then comes in, the arm will next go to
13, instead of 1. With a heavily loaded disk, the arm will tend to stay in the middle
of the disk most of the time, so requests at either extreme will have to wait until a
statistical fluctuation in the load causes there to be no requests near the middle. Re-
quests far from the middle may get poor service. The goals of minimal response
time and fairness are in conflict here.
      Tall buildings also have to deal with this trade-off. The problem of scheduling
an elevator in a tall building is similar to that of scheduling a disk arm. Requests
come in continuously calling the elevator to floors (cylinders) at random. The com-
puter running the elevator could easily keep track of the sequence in which cus-
tomers pushed the call button and service them using FCFS or SSF.
      However, most elevators use a different algorithm in order to reconcile the
mutually conflicting goals of efficiency and fairness. They keep moving in the
same direction until there are no more outstanding requests in that direction, then
they switch directions. This algorithm, known both in the disk world and the ele-
vator world as the elevator algorithm, requires the software to maintain 1 bit: the
current direction bit, UP or DOWN.            When a request finishes, the disk or elevator
driver checks the bit.     If it is UP, the arm or cabin is moved to the next highest
pending request. If no requests are pending at higher positions, the direction bit is
reversed. When the bit is set to DOWN, the move is to the next lowest requested
position, if any. If no request is pending, it just stops and waits.



SEC. 5.4                                    DISKS                                                 381
      Figure 5-25 shows the elevator algorithm using the same seven requests as
Fig. 5-24, assuming the direction bit was initially UP.           The order in which the cyl-
inders are serviced is 12, 16, 34, 36, 9, and 1, which yields arm motions of 1, 4, 18,
2, 27, and 8, for a total of 60 cylinders. In this case the elevator algorithm is slight-
ly better than SSF, although it is usually worse. One nice property the elevator al-
gorithm has is that given any collection of requests, the upper bound on the total
motion is fixed: it is just twice the number of cylinders.
                         Initial
                      position
         X            X         XX       X                                              X      X
      0           5      10          15          20           25       30                  35  Cylinder
Time                                     Sequence of seeks
                     Figure 5-25. The elevator algorithm for scheduling disk requests.
      A slight modification of this algorithm that has a smaller variance in response
times (Teory, 1972) is to always scan in the same direction. When the highest-num-
bered cylinder with a pending request has been serviced, the arm goes to the
lowest-numbered cylinder with a pending request and then continues moving in an
upward direction. In effect, the lowest-numbered cylinder is thought of as being
just above the highest-numbered cylinder.
      Some disk controllers provide a way for the software to inspect the current sec-
tor number under the head. With such a controller, another optimization is pos-
sible.      If two or more requests for the same cylinder are pending, the driver can
issue a request for the sector that will pass under the head next. Note that when
multiple tracks are present in a cylinder, consecutive requests can be for different
tracks      with  no  penalty.  The  controller  can  select  any  of  its  heads       almost    in-
stantaneously (head selection involves neither arm motion nor rotational delay).
      If the disk has the property that seek time is much faster than the rotational
delay, then a different optimization should be used. Pending requests should be
sorted by sector number, and as soon as the next sector is about to pass under the
head, the arm should be zipped over to the right track to read or write it.
      With a modern hard disk, the seek and rotational delays so dominate per-
formance that reading one or two sectors at a time is very inefficient. For this rea-
son, many disk controllers always read and cache multiple sectors, even when only
one is requested. Typically any request to read a sector will cause that sector and
much or all the rest of the current track to be read, depending upon how much



382                                INPUT/OUTPUT                                  CHAP. 5
space is available in the controller's cache memory. The hard disk described in Fig.
5-18 has a 4-MB cache, for example. The use of the cache is determined dynam-
ically by the controller. In its simplest mode, the cache is divided into two sections,
one for reads and one for writes.  If a subsequent read can be satisfied out of the
controller's cache, it can return the requested data immediately.
     It is worth noting that the disk controller's cache is completely independent of
the operating system's cache. The controller's cache usually holds blocks that have
not actually been requested, but which were convenient to read because they just
happened to pass under the head as a side effect of some other read.       In contrast,
any cache maintained by the operating system will consist of blocks that were ex-
plicitly read and which the operating system thinks might be needed again in the
near future (e.g., a disk block holding a directory block).
     When several drives are present on the same controller, the operating system
should maintain a pending request table for each drive separately. Whenever any
drive is idle, a seek should be issued to move its arm to the cylinder where it will
be needed next (assuming the controller allows overlapped seeks). When the cur-
rent transfer finishes, a check can be made to see if any drives are positioned on the
correct cylinder. If one or more are, the next transfer can be started on a drive that
is already on the right cylinder. If none of the arms is in the right place, the driver
should issue a new seek on the drive that just completed a transfer and wait until
the next interrupt to see which arm gets to its destination first.
     It is important to realize that all of the above disk-scheduling algorithms tacitly
assume that the real disk geometry is the same as the virtual geometry.    If it is not,
then scheduling disk requests makes no sense because the operating system cannot
really tell whether cylinder 40 or cylinder 200 is closer to cylinder 39.        On the
other hand, if the disk controller can accept multiple outstanding requests, it can
use these scheduling algorithms internally.        In that case, the algorithms are still
valid, but one level down, inside the controller.
5.4.4 Error Handling
     Disk manufacturers are constantly pushing the limits of the technology by
increasing linear bit densities.  A track midway out on a 5.25-inch disk has a cir-
cumference of about 300 mm.       If the track holds 300 sectors of 512 bytes, the lin-
ear recording density may be about 5000 bits/mm taking into account the fact that
some space is lost to preambles, ECCs, and intersector gaps. Recording 5000
bits/mm requires an extremely uniform substrate and a very fine oxide coating. Un-
fortunately, it is not possible to manufacture a disk to such specifications without
defects.  As soon as manufacturing technology has improved to the point where it
is possible to operate flawlessly at such densities, disk designers will go to higher
densities to increase the capacity. Doing so will probably reintroduce defects.
     Manufacturing defects introduce bad sectors, that is, sectors that do not cor-
rectly read back the value just written to them. If the defect is very small, say, only



SEC. 5.4                                                        DISKS                                                                     383
a few bits, it is possible to use the bad sector and just let the ECC correct the errors
every time. If the defect is bigger, the error cannot be masked.
     There are two general approaches to bad blocks: deal with them in the con-
troller or deal with them in the operating system.                                           In the former approach, before
the disk is shipped from the factory, it is tested and a list of bad sectors is written
onto the disk. For each bad sector, one of the spares is substituted for it.
     There are two ways to do this substitution.                                         In Fig. 5-26(a), we see a single
disk track with 30 data sectors and two spares. Sector 7 is defective. What the con-
troller can do is remap one of the spares as sector 7 as shown in Fig. 5-26(b).                                                           The
other way is to shift all the sectors up one, as shown in Fig. 5-26(c). In both cases
the controller has to know which sector is which.                                              It can keep track of this infor-
mation through internal tables (one per track) or by rewriting the preambles to give
the  remapped            sector           numbers.      If  the     preambles                  are  rewritten,  the           method          of
Fig. 5-26(c) is more work (because 23 preambles must be rewritten) but ultimately
gives better performance because an entire track can still be read in one rotation.
             29      0   1   2                              29 7        0   1   2                               28 29      0   1   23
         28                     3                       28                         3                        27
     27                            4                27                                4                 26                             4
26                                    5             26                                   5          25                                    5
25       Spare                         6    25          Replacement                       6         24                                     6
24       sectors       Bad                  24              sector                                  23
23                   sector              8  23                                              8       22                                       7
22                                       9  22                                              9       21                                       8
21                                    10            21                                   10         20                                    9
     20                            11               20                                11                19                             10
     19                      13 12                      19                      13 12                   18                         1211
         18  17  16  15  14                                 18  17  16  15  14                              17  16     15  14  13
                  (a)                                               (b)                                                (c)
             Figure 5-26.          (a) A disk track with a bad sector.                (b) Substituting a spare for the
             bad sector. (c) Shifting all the sectors to bypass the bad one.
     Errors can also develop during normal operation after the drive has been in-
stalled. The first line of defense upon getting an error that the ECC cannot handle
is to just try the read again. Some read errors are transient, that is, are caused by
specks of dust under the head and will go away on a second attempt.                                                           If the con-
troller notices that it is getting repeated errors on a certain sector, it can switch to a
spare before the sector has died completely.                                    In this way, no data are lost and the
operating system and user do not even notice the problem. Usually, the method of
Fig. 5-26(b) has to be used since the other sectors might now contain data. Using
the method of Fig. 5-26(c) would require not only rewriting the preambles, but
copying all the data as well.
     Earlier we said there were two general approaches to handling errors: handle
them in the controller or in the operating system.                                             If the controller does not have
the capability to transparently remap sectors as we have discussed, the operating



384                             INPUT/OUTPUT                                      CHAP. 5
system must do the same thing in software. This means that it must first acquire a
list of bad sectors, either by reading them from the disk, or simply testing the entire
disk itself. Once it knows which sectors are bad, it can build remapping tables.            If
the operating system wants to use the approach of Fig. 5-26(c), it must shift the
data in sectors 7 through 29 up one sector.
     If the operating system is handling the remapping, it must make sure that bad
sectors do not occur in any files and also do not occur in the free list or bitmap.
One way to do this is to create a secret file consisting of all the bad sectors.      If this
file is not entered into the file system, users will not accidentally read it (or worse
yet, free it).
     However, there is still another problem: backups.  If the disk is backed up file
by file, it is important that the backup utility not try to copy the bad block file.        To
prevent this, the operating system has to hide the bad block file so well that even a
backup utility cannot find it.  If the disk is backed up sector by sector rather than
file by file, it will be difficult, if not impossible, to prevent read errors during back-
up. The only hope is that the backup program has enough smarts to give up after 10
failed reads and continue with the next sector.
     Bad sectors are not the only source of errors. Seek errors caused by mechanical
problems in the arm also occur. The controller keeps track of the arm position in-
ternally. To perform a seek, it issues a command to the arm motor to move the arm
to the new cylinder. When the arm gets to its destination, the controller reads the
actual cylinder number from the preamble of the next sector. If the arm is in the
wrong place, a seek error has occurred.
     Most hard disk controllers correct seek errors automatically, but most of the
old floppy controllers used in the 1980s and 1990s just set an error bit and left the
rest to the driver. The driver handled this error by issuing a recalibrate command,
to move the arm as far out as it would go and reset the controller's internal idea of
the current cylinder to 0. Usually this solved the problem. If it did not, the drive
had to be repaired.
     As we have just seen, the controller is really a specialized little computer, com-
plete with software, variables, buffers, and occasionally, bugs. Sometimes an unu-
sual sequence of events, such as an interrupt on one drive occurring simultaneously
with a recalibrate command for another drive will trigger a bug and cause the con-
troller to go into a loop or lose track of what it was doing. Controller designers us-
ually plan for the worst and provide a pin on the chip which, when asserted, forces
the controller to forget whatever it was doing and reset itself. If all else fails, the
disk driver can set a bit to invoke this signal and reset the controller. If that does
not help, all the driver can do is print a message and give up.
     Recalibrating a disk makes a funny noise but otherwise normally is not disturb-
ing. However, there is one situation where recalibration is a problem: systems with
real-time constraints. When a video is being played off (or served from) a hard
disk, or files from a hard disk are being burned onto a Blu-ray disc, it is essential
that the bits arrive from the hard disk at a uniform rate. Under these circumstances,



SEC. 5.4                             DISKS                                               385
recalibrations insert gaps into the bit stream and are unacceptable. Special drives,
called AV disks (Audio Visual disks), which never recalibrate are available for
such applications.
Anecdotally, a highly convincing demonstration of how advanced disk con-
trollers have become was given by the Dutch hacker Jeroen Domburg, who hacked
a modern disk controller to make it run custom code. It turns out the disk controller
is equipped with a fairly powerful multicore (!) ARM processor and has easily
enough resources to run Linux. If the bad guys hack your hard drive in this way,
they will be able to see and modify all data you transfer to and from the disk. Even
reinstalling the operating from scratch will not remove the infection, as the disk
controller itself is malicious and serves as a permanent backdoor. Alternatively,
you can collect a stack of broken hard drives from your local recycling center and
build your own cluster computer for free.
5.4.5 Stable Storage
As we have seen, disks sometimes make errors. Good sectors can suddenly be-
come bad sectors.    Whole drives can die unexpectedly.      RAIDs protect against a
few sectors going bad or even a drive falling out.    However, they do not protect
against write errors laying down bad data in the first place. They also do not pro-
tect against crashes during writes corrupting the original data without replacing
them by newer data.
For some applications, it is essential that data never be lost or corrupted, even
in the face of disk and CPU errors. Ideally, a disk should simply work all the time
with no errors.  Unfortunately, that is not achievable. What is achievable is a disk
subsystem that has the following property: when a write is issued to it, the disk ei-
ther correctly writes the data or it does nothing, leaving the existing data intact.
Such a system is called stable storage and is implemented in software (Lampson
and Sturgis, 1979).  The goal is to keep the disk consistent at all costs.  Below we
will describe a slight variant of the original idea.
Before describing the algorithm, it is important to have a clear model of the
possible errors. The model assumes that when a disk writes a block (one or more
sectors), either the write is correct or it is incorrect and this error can be detected
on a subsequent read by examining the values of the ECC fields.             In principle,
guaranteed error detection is never possible because with a, say, 16-byte ECC field
guarding a 512-byte sector, there are 24096 data values and only 2144 ECC values.
Thus if a block is garbled during writing but the ECC is not, there are billions upon
billions of incorrect combinations that yield the same ECC.  If any of them occur,
the error will not be detected. On the whole, the probability of random data having
the proper 16-byte ECC is about 2-144, which is small enough that we will call it
zero, even though it is really not.
The model also assumes that a correctly written sector can spontaneously go
bad and become unreadable. However, the assumption is that such events are so



386                               INPUT/OUTPUT                                   CHAP. 5
rare that having the same sector go bad on a second (independent) drive during a
reasonable time interval (e.g., 1 day) is small enough to ignore.
     The model also assumes the CPU can fail, in which case it just stops. Any disk
write in progress at the moment of failure also stops, leading to incorrect data in
one sector and an incorrect ECC that can later be detected. Under all these condi-
tions, stable storage can be made 100% reliable in the sense of writes either work-
ing correctly or leaving the old data in place. Of course, it does not protect against
physical disasters, such as an earthquake happening and the computer falling 100
meters into a fissure and landing in a pool of boiling magma. It is tough to recover
from this condition in software.
     Stable storage uses a pair of identical disks with the corresponding blocks
working together to form one error-free block.  In the absence of errors, the corres-
ponding blocks on both drives are the same. Either one can be read to get the same
result. To achieve this goal, the following three operations are defined:
     1.  Stable writes. A stable write consists of first writing the block on
         drive 1, then reading it back to verify that it was written correctly.    If
         it was not, the write and reread are done again up to n times until they
         work. After n consecutive failures, the block is remapped onto a spare
         and the operation repeated until it succeeds, no matter how many
         spares have to be tried. After the write to drive 1 has succeeded, the
         corresponding block on drive 2 is written and reread, repeatedly if
         need be, until it, too, finally succeeds. In the absence of CPU crashes,
         when a stable write completes, the block has correctly been written
         onto both drives and verified on both of them.
     2.  Stable reads. A stable read first reads the block from drive 1.         If this
         yields an incorrect ECC, the read is tried again, up to n times.        If all
         of these give bad ECCs, the corresponding block is read from drive 2.
         Given the fact that a successful stable write leaves two good copies of
         the block behind, and our assumption that the probability of the same
         block spontaneously going bad on both drives in a reasonable time in-
         terval is negligible, a stable read always succeeds.
     3.  Crash recovery. After a crash, a recovery program scans both disks
         comparing corresponding blocks.        If a pair of blocks are both good
         and the same, nothing is done.   If one of them has an ECC error, the
         bad block is overwritten with the corresponding good block.       If a pair
         of blocks are both good but different, the block from drive 1 is written
         onto drive 2.
     In the absence of CPU crashes, this scheme always works because stable
writes always write two valid copies of every block and spontaneous errors are as-
sumed never to occur on both corresponding blocks at the same time. What about



SEC. 5.4                                   DISKS                                                          387
in the presence of CPU crashes during stable writes? It depends on precisely when
the crash occurs. There are five possibilities, as depicted in Fig. 5-27.
                      ECC
          Disk        error     Disk          Disk                Disk                        Disk
       1         2           1        2    1         2         1         2                 1         2
       Old       Old                  Old  New       Old       New                         New       New
Crash                        Crash         Crash                    Crash                               Crash
            (a)                 (b)             (c)                 (d)                         (e)
                      Figure 5-27. Analysis of the influence of crashes on stable writes.
In Fig. 5-27(a), the CPU crash happens before either copy of the block is writ-
ten. During recovery, neither will be changed and the old value will continue to
exist, which is allowed.
In Fig. 5-27(b), the CPU crashes during the write to drive 1, destroying the
contents of the block. However the recovery program detects this error and restores
the block on drive 1 from drive 2. Thus the effect of the crash is wiped out and the
old state is fully restored.
In Fig. 5-27(c), the CPU crash happens after drive 1 is written but before drive
2 is written. The point of no return has been passed here: the recovery program
copies the block from drive 1 to drive 2. The write succeeds.
Fig. 5-27(d) is like Fig. 5-27(b): during recovery, the good block overwrites the
bad block. Again, the final value of both blocks is the new one.
Finally, in Fig. 5-27(e) the recovery program sees that both blocks are the
same, so neither is changed and the write succeeds here, too.
Various          optimizations        and  improvements   are  possible     to  this       scheme.        For
starters, comparing all the blocks pairwise after a crash is doable, but expensive. A
huge improvement is to keep track of which block was being written during a sta-
ble write so that only one block has to be checked during recovery. Some com-
puters have a small amount of nonvolatile RAM, which is a special CMOS memo-
ry powered by a lithium battery. Such batteries last for years, possibly even the
whole life of the computer.           Unlike main memory, which is lost after a crash, non-
volatile RAM is not lost after a crash. The time of day is normally kept here (and
incremented by a special circuit), which is why computers still know what time it
is even after having been unplugged.
Suppose that a few bytes of nonvolatile RAM are available for operating sys-
tem purposes. The stable write can put the number of the block it is about to update
in nonvolatile RAM before starting the write. After successfully completing the
stable write, the block number in nonvolatile RAM is overwritten with an invalid



388                             INPUT/OUTPUT                                CHAP. 5
block number, for example, -1.  Under these conditions, after a crash the recovery
program can check the nonvolatile RAM to see if a stable write happened to be in
progress during the crash, and if so, which block was being written when the
crashed happened. The two copies of the block can then be checked for correctness
and consistency.
     If nonvolatile RAM is not available, it can be simulated as follows. At the start
of a stable write, a fixed disk block on drive 1 is overwritten with the number of
the block to be stably written. This block is then read back to verify it.  After get-
ting it correct, the corresponding block on drive 2 is written and verified. When the
stable write completes correctly, both blocks are overwritten with an invalid block
number and verified. Again here, after a crash it is easy to determine whether or
not a stable write was in progress during the crash.     Of course, this technique re-
quires eight extra disk operations to write a stable block, so it should be used
exceedingly sparingly.
     One last point is worth making. We assumed that only one spontaneous decay
of a good block to a bad block happens per block pair per day.   If enough days go
by, the other one might go bad, too. Therefore, once a day a complete scan of both
disks must be done, repairing any damage. That way, every morning both disks are
always identical.  Even if both blocks in a pair go bad within a period of a few
days, all errors are repaired correctly.
5.5 CLOCKS
     Clocks (also called timers) are essential to the operation of any multipro-
grammed system for a variety of reasons. They maintain the time of day and pre-
vent one process from monopolizing the CPU, among other things. The clock soft-
ware can take the form of a device driver, even though a clock is neither a block
device, like a disk, nor a character device, like a mouse. Our examination of clocks
will follow the same pattern as in the previous section: first a look at clock hard-
ware and then a look at the clock software.
5.5.1 Clock Hardware
     Two types of clocks are commonly used in computers, and both are quite dif-
ferent from the clocks and watches used by people. The simpler clocks are tied to
the 110- or 220-volt power line and cause an interrupt on every voltage cycle, at 50
or 60 Hz. These clocks used to dominate, but are rare nowadays.
     The other kind of clock is built out of three components: a crystal oscillator, a
counter, and a holding register, as shown in Fig. 5-28.  When a piece of quartz
crystal is properly cut and mounted under tension, it can be made to generate a
periodic signal of very great accuracy, typically in the range of several hundred
megahertz to a few gigahertz, depending on the crystal chosen. Using electronics,



SEC. 5.5                            CLOCKS                                                  389
this base signal can be multiplied by a small integer to get frequencies up to several
gigahertz or even more.           At least one such circuit is usually found in any computer,
providing a synchronizing signal to the computer's various circuits. This signal is
fed into the counter to make it count down to zero. When the counter gets to zero,
it causes a CPU interrupt.
              Crystal oscillator
                                              Counter is decremented at each pulse
                                              Holding register is used to load the counter
                                  Figure 5-28. A programmable clock.
Programmable clocks typically have several modes of operation.            In one-shot
mode, when the clock is started, it copies the value of the holding register into the
counter and then decrements the counter at each pulse from the crystal. When the
counter gets to zero, it causes an interrupt and stops until it is explicitly started
again by the software. In square-wave mode, after getting to zero and causing the
interrupt, the holding register is automatically copied into the counter, and the
whole process is repeated again indefinitely. These periodic interrupts are called
clock ticks.
The advantage of the programmable clock is that its interrupt frequency can be
controlled by software. If a 500-MHz crystal is used, then the counter is pulsed
every 2 nsec. With (unsigned) 32-bit registers, interrupts can be programmed to oc-
cur at intervals from 2 nsec to 8.6 sec. Programmable clock chips usually contain
two or three independently programmable clocks and have many other options as
well (e.g., counting up instead of down, interrupts disabled, and more).
To prevent the current time from being lost when the computer's power is
turned off, most computers have a battery-powered backup clock, implemented
with the kind of low-power circuitry used in digital watches. The battery clock can
be read at startup.  If the backup clock is not present, the software may ask the user
for the current date and time. There is also a standard way for a networked system
to get the current time from a remote host.   In any case the time is then translated
into the number of clock ticks since 12 A.M.  UTC (Universal Coordinated Time)
(formerly known as Greenwich Mean Time) on Jan. 1, 1970, as UNIX does, or
since some other benchmark moment. The origin of time for Windows is Jan. 1,
1980. At every clock tick, the real time is incremented by one count. Usually util-
ity programs are provided to manually set the system clock and the backup clock
and to synchronize the two clocks.



390                       INPUT/OUTPUT                                       CHAP. 5
5.5.2 Clock Software
     All the clock hardware does is generate interrupts at known intervals. Every-
thing else involving time must be done by the software, the clock driver. The exact
duties of the clock driver vary among operating systems, but usually include most
of the following:
     1.  Maintaining the time of day.
     2.  Preventing processes from running longer than they are allowed to.
     3.  Accounting for CPU usage.
     4.  Handling the alarm system call made by user processes.
     5.  Providing watchdog timers for parts of the system itself.
     6.  Doing profiling, monitoring, and statistics gathering.
     The first clock function, maintaining the time of day (also called the real time)
is not difficult. It just requires incrementing a counter at each clock tick, as men-
tioned before. The only thing to watch out for is the number of bits in the time-of-
day counter. With a clock rate of 60 Hz, a 32-bit counter will overflow in just over
2 years. Clearly the system cannot store the real time as the number of ticks since
Jan. 1, 1970 in 32 bits.
     Three approaches can be taken to solve this problem. The first way is to use a
64-bit counter, although doing so makes maintaining the counter more expensive
since it has to be done many times a second. The second way is to maintain the
time of day in seconds, rather than in ticks, using a subsidiary counter to count
ticks until a whole second has been accumulated. Because 232 seconds is more than
136 years, this method will work until the twenty-second century.
     The third approach is to count in ticks, but to do that relative to the time the
system was booted, rather than relative to a fixed external moment. When the back-
up clock is read or the user types in the real time, the system boot time is calcu-
lated from the current time-of-day value and stored in memory in any convenient
form. Later, when the time of day is requested, the stored time of day is added to
the counter to get the current time of day. All three approaches are shown in
Fig. 5-29.
     The second clock function is preventing processes from running too long.
Whenever a process is started, the scheduler initializes a counter to the value of
that process' quantum in clock ticks. At every clock interrupt, the clock driver
decrements the quantum counter by 1. When it gets to zero, the clock driver calls
the scheduler to set up another process.
     The third clock function is doing CPU accounting. The most accurate way to
do it is to start a second timer, distinct from the main system timer, whenever a
process is started up. When that process is stopped, the timer can be read out to tell



SEC. 5.5                               CLOCKS                                            391
             64 bits                   32 bits                                  32 bits
       Time  of day in  ticks                                                   Counter in ticks
                                       Time of day       Number of ticks
                                       in seconds   in current second
                                                                                System boot time
                                                                                in seconds
             (a)                                    (b)                         (c)
                        Figure  5-29.  Three ways to maintain the time of day.
how long the process has run. To do things right, the second timer should be saved
when an interrupt occurs and restored afterward.
A less accurate, but simpler, way to do accounting is to maintain a pointer to
the process table entry for the currently running process in a global variable. At
every clock tick, a field in the current process' entry is incremented. In this way,
every clock tick is ``charged'' to the process running at the time of the tick. A
minor problem with this strategy is that if many interrupts occur during a process'
run, it is still charged for a full tick, even though it did not get much work done.
Properly accounting for the CPU during interrupts is too expensive and is rarely
done.
In many systems, a process can request that the operating system give it a
warning after a certain interval. The warning is usually a signal, interrupt, message,
or something similar. One application requiring such warnings is networking, in
which a packet not acknowledged within a certain time interval must be retrans-
mitted. Another application is computer-aided instruction, where a student not pro-
viding a response within a certain time is told the answer.
If the clock driver had enough clocks, it could set a separate clock for each re-
quest. This not being the case, it must simulate multiple virtual clocks with a single
physical clock. One way is to maintain a table in which the signal time for all
pending timers is kept, as well as a variable giving the time of the next one. When-
ever the time of day is updated, the driver checks to see if the closest signal has oc-
curred. If it has, the table is searched for the next one to occur.
If many signals are expected, it is more efficient to simulate multiple clocks by
chaining all the pending clock requests together, sorted on time, in a linked list, as
shown in Fig. 5-30. Each entry on the list tells how many clock ticks following the
previous one to wait before causing a signal. In this example, signals are pending
for 4203, 4207, 4213, 4215, and 4216.
In Fig. 5-30, the next interrupt occurs in 3 ticks. On each tick, Next signal is
decremented. When it gets to 0, the signal corresponding to the first item on the list
is caused, and that item is removed from the list. Then Next signal is set to the
value in the entry now at the head of the list, in this example, 4.



392                         INPUT/OUTPUT                                           CHAP. 5
                            Current time           Next signal
                 Clock         4200                3
                 header
                         3  4                   6  2            1              X
                 Figure 5-30. Simulating multiple timers with a single clock.
     Note that during a clock interrupt, the clock driver has several things to do--
increment the real time, decrement the quantum and check for 0, do CPU ac-
counting, and decrement the alarm counter. However, each of these operations has
been carefully arranged to be very fast because they have to be repeated many
times a second.
     Parts of the operating system also need to set timers. These are called watch-
dog timers and are frequently used (especially in embedded devices) to detect
problems such as hangs. For instance, a watchdog timer may reset a system that
stops running. While the system is running, it regularly resets the timer, so that it
never expires. In that case, expiration of the timer proves that the system has not
run for a long time, and leads to corrective action--such as a full-system reset.
     The mechanism used by the clock driver to handle watchdog timers is the same
as for user signals. The only difference is that when a timer goes off, instead of
causing a signal, the clock driver calls a procedure supplied by the caller. The pro-
cedure is part of the caller's code. The called procedure can do whatever is neces-
sary, even causing an interrupt, although within the kernel interrupts are often
inconvenient and signals do not exist. That is why the watchdog mechanism is pro-
vided.  It is worth nothing that the watchdog mechanism works only when the
clock driver and the procedure to be called are in the same address space.
     The last thing in our list is profiling. Some operating systems provide a mech-
anism by which a user program can have the system build up a histogram of its
program counter, so it can see where it is spending its time. When profiling is a
possibility, at every tick the driver checks to see if the current process is being pro-
filed, and if so, computes the bin number (a range of addresses) corresponding to
the current program counter. It then increments that bin by one. This mechanism
can also be used to profile the system itself.
5.5.3 Soft Timers
     Most computers have a second programmable clock that can be set to cause
timer interrupts at whatever rate a program needs. This timer is in addition to the
main system timer whose functions were described above. As long as the interrupt
frequency is low, there is no problem using this second timer for application-spe-
cific purposes. The trouble arrives when the frequency of the application-specific



SEC. 5.5                         CLOCKS                                                   393
timer is very high. Below we will briefly describe a software-based timer scheme
that works well under many circumstances, even at fairly high frequencies. The
idea is due to Aron and Druschel (1999). For more details, please see their paper.
Generally, there are two ways to manage I/O: interrupts and polling. Interrupts
have low latency, that is, they happen immediately after the event itself with little
or no delay.  On the other hand, with modern CPUs, interrupts have a substantial
overhead due to the need for context switching and their influence on the pipeline,
TLB, and cache.
The alternative to interrupts is to have the application poll for the event expect-
ed itself. Doing this avoids interrupts, but there may be substantial latency because
an event may happen directly after a poll, in which case it waits almost a whole
polling interval. On the average, the latency is half the polling interval.
Interrupt latency today is barely better than that of computers in the 1970s. On
most minicomputers, for example, an interrupt took four bus cycles: to stack the
program counter and PSW and to load a new program counter and PSW.                  Now-
adays dealing with the pipeline, MMU, TLB, and cache adds a great deal to the
overhead. These effects are likely to get worse rather than better in time, thus can-
celing out faster clock rates.   Unfortunately, for certain applications, we want nei-
ther the overhead of interrupts nor the latency of polling.
Soft timers avoid interrupts. Instead, whenever the kernel is running for some
other reason, just before it returns to user mode it checks the real-time clock to see
if a soft timer has expired. If it has expired, the scheduled event (e.g., packet trans-
mission or checking for an incoming packet) is performed, with no need to switch
into kernel mode since the system is already there. After the work has been per-
formed, the soft timer is reset to go off again.  All that has to be done is copy the
current clock value to the timer and add the timeout interval to it.
Soft timers stand or fall with the rate at which kernel entries are made for other
reasons. These reasons include:
1.        System calls.
2.        TLB misses.
3.        Page faults.
4.        I/O interrupts.
5.        The CPU going idle.
To see how often these events happen, Aron and Druschel made measurements
with several CPU loads, including a fully loaded Web server, a Web server with a
compute-bound background job, playing real-time audio from the Internet, and
recompiling the UNIX kernel. The average entry rate into the kernel varied from 2
to 18 sec, with about half of these entries being system calls. Thus to a first-order
approximation, having a soft timer go off, say, every 10 sec is doable, albeit with



394                                INPUT/OUTPUT                                  CHAP. 5
an occasional missed deadline. Being 10 sec late from time to time is often better
than having interrupts eat up 35% of the CPU.
     Of course, there will be periods when there are no system calls, TLB misses, or
page faults, in which case no soft timers will go off.      To put an upper bound on
these intervals, the second hardware timer can be set to go off, say, every 1 msec.
If the application can live with only 1000 activations per second for occasional in-
tervals, then the combination of soft timers and a low-frequency hardware timer
may be better than either pure interrupt-driven I/O or pure polling.
5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR
     Every general-purpose computer has a keyboard and monitor (and sometimes a
mouse) to allow people to interact with it. Although the keyboard and monitor are
technically separate devices, they work closely together. On mainframes, there are
frequently many remote users, each with a device containing a keyboard and an at-
tached display as a unit. These devices have historically been called terminals.
People frequently still use that term, even when discussing personal computer
keyboards and monitors (mostly for lack of a better term).
5.6.1 Input Software
     User input comes primarily from the keyboard and mouse (or somtimes touch
screens), so let us look at those. On a personal computer, the keyboard contains an
embedded      microprocessor  which  usually     communicates      through    a  specialized
serial  port  with  a  controller  chip      on  the  parentboard  (although     increasingly
keyboards are connected to a USB port).          An interrupt is generated whenever a key
is struck and a second one is generated whenever a key is released.              At each of
these keyboard interrupts, the keyboard driver extracts the information about what
happens from the I/O port associated with the keyboard. Everything else happens
in software and is pretty much independent of the hardware.
     Most of the rest of this section can be best understood when thinking of typing
commands to a shell window (command-line interface).         This is how programmers
commonly work.      We will discuss graphical interfaces below. Some devices, in
particular touch screens, are used for input and output. We have made an (arbi-
trary) choice to discuss them in the section on output devices.       We will discuss
graphical interfaces later in this chapter.
Keyboard Software
     The number in the I/O register is the key number, called the scan code, not the
ASCII code. Normal keyboards have fewer than 128 keys, so only 7 bits are need-
ed to represent the key number.    The eighth bit is set to 0 on a key press and to 1 on



SEC. 5.6        USER INTERFACES: KEYBOARD, MOUSE, MONITOR                               395
a key release.  It is up to the driver to keep track of the status of each key (up or
down). So all the hardware does is give press and release interrupts. Software does
the rest.
When the A key is struck, for example, the scan code (30) is put in an I/O reg-
ister.  It is up to the driver to determine whether it is lowercase, uppercase, CTRL-
A, ALT-A, CTRL-ALT-A, or some other combination. Since the driver can tell
which keys have been struck but not yet released (e.g., SHIFT), it has enough
information to do the job.
For example, the key sequence
DEPRESS SHIFT, DEPRESS A, RELEASE A, RELEASE SHIFT
indicates an uppercase A. However, the key sequence
DEPRESS SHIFT, DEPRESS A, RELEASE SHIFT, RELEASE A
also indicates an uppercase A.  Although this keyboard interface puts the full bur-
den on the software, it is extremely flexible. For example, user programs may be
interested in whether a digit just typed came from the top row of keys or the
numeric keypad on the side. In principle, the driver can provide this information.
Two possible philosophies can be adopted for the driver.             In the first one, the
driver's job is just to accept input and pass it upward unmodified.  A program read-
ing from the keyboard gets a raw sequence of ASCII codes. (Giving user programs
the scan codes is too primitive, as well as being highly keyboard dependent.)
This philosophy is well suited to the needs of sophisticated screen editors such
as emacs, which allow the user to bind an arbitrary action to any character or se-
quence of characters.  It does, however, mean that if the user types dste instead of
date and then corrects the error by typing three backspaces and ate, followed by a
carriage return, the user program will be given all 11 ASCII codes typed, as fol-
lows:
d s t e    a t e CR
Not all programs want this much detail. Often they just want the corrected
input, not the exact sequence of how it was produced. This observation leads to the
second philosophy: the driver handles all the intraline editing and just delivers cor-
rected lines to the user programs. The first philosophy is character oriented; the
second one is line oriented.    Originally they were referred to as raw mode and
cooked mode, respectively. The POSIX standard uses the less-picturesque term
canonical mode to describe line-oriented mode.       Noncanonical mode is equiv-
alent to raw mode, although many details of the behavior can be changed. POSIX-
compatible systems provide several library functions that support selecting either
mode and changing many parameters.
If the keyboard is in canonical (cooked) mode, characters must be stored until
an entire line has been accumulated, because the user may subsequently decide to
erase part of it. Even if the keyboard is in raw mode, the program may not yet have



396                             INPUT/OUTPUT                                   CHAP. 5
requested input, so the characters must be buffered to allow type ahead. Either a
dedicated buffer can be used or buffers can be allocated from a pool.    The former
puts a fixed limit on type ahead; the latter does not. This issue arises most acutely
when the user is typing to a shell window (command-line window in Windows)
and has just issued a command (such as a compilation) that has not yet completed.
Subsequent characters typed have to be buffered because the shell is not ready to
read them. System designers who do not permit users to type far ahead ought to be
tarred and feathered, or worse yet, be forced to use their own system.
     Although the keyboard and monitor are logically separate devices, many users
have grown accustomed to seeing the characters they have just typed appear on the
screen. This process is called echoing.
     Echoing is complicated by the fact that a program may be writing to the screen
while the user is typing (again, think about typing to a shell window).  At the very
least, the keyboard driver has to figure out where to put the new input without its
being overwritten by program output.
     Echoing also gets complicated when more than 80 characters have to be dis-
played in a window with 80-character lines (or some other number). Depending on
the application, wrapping around to the next line may be appropriate. Some drivers
just truncate lines to 80 characters by throwing away all characters beyond column
80.
     Another problem is tab handling.      It is usually up to the driver to compute
where the cursor is currently located, taking into account both output from pro-
grams and output from echoing, and compute the proper number of spaces to be
echoed.
     Now we come to the problem of device equivalence. Logically, at the end of a
line of text, one wants a carriage return, to move the cursor back to column 1, and
a line feed, to advance to the next line.  Requiring users to type both at the end of
each line would not sell well.  It is up to the device driver to convert whatever
comes in to the format used by the operating system.  In UNIX, the Enter key is
converted to a line feed for internal storage; in Windows it is converted to a car-
riage return followed by a line feed.
     If the standard form is just to store a line feed (the UNIX convention), then
carriage returns (created by the Enter key) should be turned into line feeds.  If the
internal format is to store both (the Windows convention), then the driver should
generate a line feed when it gets a carriage return and a carriage return when it gets
a line feed.  No matter what the internal convention, the monitor may require both
a line feed and a carriage return to be echoed in order to get the screen updated
properly.  On a multiuser system such as a mainframe, different users may have
different types of terminals connected to it and it is up to the keyboard driver to get
all the different carriage-return/line-feed combinations converted to the internal
system standard and arrange for all echoing to be done right.
     When operating in canonical mode, some of the input characters have special
meanings.     Figure 5-31 shows all of the special characters required by the POSIX



SEC. 5.6       USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                 397
standard.  The defaults are all control characters that should not conflict with text
input or codes used by programs; all except the last two can be changed under pro-
gram control.
               Character  POSIX name                         Comment
               CTRL-H     ERASE                 Backspace one character
               CTRL-U     KILL                  Erase entire line being typed
               CTRL-V     LNEXT                 Interpret next character literally
               CTRL-S     STOP                  Stop output
               CTRL-Q     START                 Start output
               DEL        INTR                  Interrupt process (SIGINT)
               CTRL-\     QUIT                  Force core dump (SIGQUIT)
               CTRL-D     EOF                   End of file
               CTRL-M     CR                    Carriage return (unchangeable)
               CTRL-J     NL                    Line feed (unchangeable)
               Figure 5-31. Characters that are handled specially in canonical mode.
The ERASE character allows the user to rub out the character just typed.                 It is
usually the backspace (CTRL-H). It is not added to the character queue but instead
removes the previous character from the queue. It should be echoed as a sequence
of three characters, backspace, space, and backspace, in order to remove the previ-
ous character from the screen.   If the previous character was a tab, erasing it de-
pends on how it was processed when it was typed.              If it is immediately expanded
into spaces, some extra information is needed to determine how far to back up.           If
the tab itself is stored in the input queue, it can be removed and the entire line just
output again.  In most systems, backspacing will only erase characters on the cur-
rent line. It will not erase a carriage return and back up into the previous line.
When the user notices an error at the start of the line being typed in, it is often
convenient to erase the entire line and start again. The KILL character erases the
entire line. Most systems make the erased line vanish from the screen, but a few
older ones echo it plus a carriage return and line feed because some users like to
see the old line. Consequently, how to echo KILL is a matter of taste.                As with
ERASE it is usually not possible to go further back than the current line. When a
block of characters is killed, it may or may not be worth the trouble for the driver
to return buffers to the pool, if one is used.
Sometimes the ERASE or KILL characters must be entered as ordinary data.
The LNEXT character serves as an escape character. In UNIX CTRL-V is the de-
fault. As an example, older UNIX systems often used the @ sign for KILL, but the
Internet mail system uses addresses of the form linda@cs.washington.edu.              Some-
one who feels more comfortable with older conventions might redefine KILL as @,
but then need to enter an @ sign literally to address email. This can be done by
typing CTRL-V @. The CTRL-V itself can be entered literally by typing CTRL-V



398                              INPUT/OUTPUT                     CHAP. 5
twice consecutively.    After seeing a CTRL-V, the driver sets a flag saying that the
next character is exempt from special processing. The LNEXT character itself is not
entered in the character queue.
     To allow users to stop a screen image from scrolling out of view, control codes
are provided to freeze the screen and restart it later.  In UNIX these are STOP,
(CTRL-S) and START, (CTRL-Q), respectively. They are not stored but are used to
set and clear a flag in the keyboard data structure. Whenever output is attempted,
the flag is inspected.  If it is set, no output occurs. Usually, echoing is also sup-
pressed along with program output.
     It is often necessary to kill a runaway program being debugged. The INTR
(DEL) and QUIT (CTRL-\) characters can be used for this purpose.  In UNIX,
DEL sends the SIGINT signal to all the processes started up from that keyboard.
Implementing DEL can be quite tricky because UNIX was designed from the be-
ginning to handle multiple users at the same time. Thus in the general case, there
may be many processes running on behalf of many users, and the DEL key must
signal only the user's own processes. The hard part is getting the information from
the driver to the part of the system that handles signals, which, after all, has not
asked for this information.
     CTRL-\ is similar to DEL, except that it sends the SIGQUIT signal, which
forces a core dump if not caught or ignored. When either of these keys is struck,
the driver should echo a carriage return and line feed and discard all accumulated
input to allow for a fresh start. The default value for INTR is often CTRL-C instead
of DEL, since many programs use DEL interchangeably with the backspace for
editing.
     Another special character is EOF (CTRL-D), which in UNIX causes any pend-
ing read requests for the terminal to be satisfied with whatever is available in the
buffer, even if the buffer is empty. Typing CTRL-D at the start of a line causes the
program to get a read of 0 bytes, which is conventionally interpreted as end-of-file
and causes most programs to act the same way as they would upon seeing end-of-
file on an input file.
Mouse Software
     Most PCs have a mouse, or sometimes a trackball, which is just a mouse lying
on its back. One common type of mouse has a rubber ball inside that protrudes
through a hole in the bottom and rotates as the mouse is moved over a rough sur-
face. As the ball rotates, it rubs against rubber rollers placed on orthogonal shafts.
Motion in the east-west direction causes the shaft parallel to the y-axis to rotate;
motion in the north-south direction causes the shaft parallel to the x-axis to rotate.
     Another popular type is the optical mouse, which is equipped with one or more
light-emitting diodes and photodetectors on the bottom. Early ones had to operate
on a special mousepad with a rectangular grid etched onto it so the mouse could
count lines crossed. Modern optical mice have an image-processing chip in them



SEC. 5.6      USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                 399
and make continuous low-resolution photos of the surface under them, looking for
changes from image to image.
Whenever a mouse has moved a certain minimum distance in either direction
or a button is depressed or released, a message is sent to the computer. The mini-
mum distance is about 0.1 mm (although it can be set in software).         Some people
call this unit a mickey.  Mice (or occasionally, mouses) can have one, two, or three
buttons, depending on the designers' estimate of the users' intellectual ability to
keep track of more than one button. Some mice have wheels that can send addi-
tional data back to the computer. Wireless mice are the same as wired mice except
that instead of sending their data back to the computer over a wire, they use
low-power radios, for example, using the Bluetooth standard.
The message to the computer contains three items: x, y, buttons. The first
item is the change in x position since the last message.  Then comes the change in
y position since the last message. Finally, the status of the buttons is included. The
format of the message depends on the system and the number of buttons the mouse
has. Usually, it takes 3 bytes.    Most mice report back a maximum of 40 times/sec,
so the mouse may have moved multiple mickeys since the last report.
Note that the mouse indicates only changes in position, not absolute position
itself.  If the mouse is picked up and put down gently without causing the ball to
rotate, no messages will be sent.
Many GUIs distinguish between single clicks and double clicks of a mouse
button.  If two clicks are close enough in space (mickeys) and also close enough in
time (milliseconds), a double click is signaled. The maximum for ``close enough''
is up to the software, with both parameters usually being user settable.
5.6.2 Output Software
Now let us consider output software. First we will look at simple output to a
text window, which is what programmers normally prefer to use.             Then we will
consider graphical user interfaces, which other users often prefer.
Text Windows
Output is simpler than input when the output is sequentially in a single font,
size, and color. For the most part, the program sends characters to the current win-
dow and they are displayed there. Usually, a block of characters, for example, a
line, is written in one system call.
Screen    editors  and    many     other  sophisticated  programs    need  to  be  able  to
update the screen in complex ways such as replacing one line in the middle of the
screen.   To accommodate this need, most output drivers support a series of com-
mands to move the cursor, insert and delete characters or lines at the cursor, and so
on. These commands are often called escape sequences.              In the heyday of the
dumb 25 Ã 80 ASCII terminal, there were hundreds of terminal types, each with its



400                           INPUT/OUTPUT                                            CHAP. 5
own escape sequences.  As a consequence, it was difficult to write software that
worked on more than one terminal type.
     One solution, which was introduced in Berkeley UNIX, was a terminal data-
base called termcap.   This software package defined a number of basic actions,
such as moving the cursor to (row, column).      To move the cursor to a particular lo-
cation, the software, say, an editor, used a generic escape sequence which was then
converted to the actual escape sequence for the terminal being written to.            In this
way, the editor worked on any terminal that had an entry in the termcap database.
Much UNIX software still works this way, even on personal computers.
     Eventually, the industry saw the need for standardizing the escape sequence, so
an ANSI standard was developed. Some of the values are shown in Fig. 5-32.
Escape sequence                                  Meaning
ESC [ n A              Move up n lines
ESC [ n B              Move down n lines
ESC [ n C              Move right n spaces
ESC [ n D              Move left n spaces
ESC [ m ; n H          Move cursor to (m,n)
ESC [ s J              Clear screen from cursor (0 to end, 1 from start, 2 all)
ESC [ s K              Clear line from cursor (0 to end, 1 from start, 2 all)
ESC [ n L              Insert n lines at cursor
ESC [ n M              Delete n lines at cursor
ESC [ n P              Delete n chars at cursor
ESC [ n @              Insert n chars at cursor
ESC [ n m              Enable rendition n (0 = normal, 4 = bold, 5 = blinking, 7 = reverse)
ESC M                  Scroll the screen backward if the cursor is on the top line
     Figure 5-32. The ANSI escape sequences accepted by the terminal driver on out-
     put.  ESC denotes the ASCII escape character (0x1B), and n, m, and s are optio-
     nal numeric parameters.
     Consider how these escape sequences might be used by a text editor. Suppose
that the user types a command telling the editor to delete all of line 3 and then
close up the gap between lines 2 and 4.          The editor might send the following
escape sequence over the serial line to the terminal:
     ESC [ 3 ; 1 H ESC [ 0 K ESC [ 1 M
(where the spaces are used above only to separate the symbols; they are not trans-
mitted). This sequence moves the cursor to the start of line 3, erases the entire line,
and then deletes the now-empty line, causing all the lines starting at 5 to move up
one line. Then what was line 4 becomes line 3; what was line 5 becomes line 4,
and so on. Analogous escape sequences can be used to add text to the middle of the
display. Words can be added or removed in a similar way.



SEC. 5.6         USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                 401
The X Window System
Nearly all UNIX systems base their user interface on the X Window System
(often just called X), developed at M.I.T. as part of project Athena in the 1980s. It
is very portable and runs entirely in user space. It was originally intended for con-
necting a large number of remote user terminals with a central compute server, so
it is logically split into client software and host software, which can potentially run
on different computers.  On modern personal computers, both parts can run on the
same machine.    On Linux systems, the popular Gnome and KDE desktop environ-
ments run on top of X.
When X is running on a machine, the software that collects input from the
keyboard and mouse and writes output to the screen is called the X server.          It has
to keep track of which window is currently selected (where the mouse pointer is),
so it knows which client to send any new keyboard input to. It communicates with
running   programs  (possible   over  a  network)  called   X  clients.  It  sends  them
keyboard and mouse input and accepts display commands from them.
It may seem odd that the X server is always inside the user's computer while
the X client may be off on a remote compute server, but just think of the X server's
main job: displaying bits on the screen, so it makes sense to be near the user. From
the program's point of view, it is a client telling the server to do things, like display
text and geometric figures. The server (in the local PC) just does what it is told, as
do all servers.
The arrangement of client and server is shown in Fig. 5-33 for the case where
the X client and X server are on different machines. But when running Gnome or
KDE on a single machine, the client is just some application program using the X
library talking to the X server on the same machine (but using a TCP connection
over sockets, the same as it would do in the remote case).
The reason it is possible to run the X Window System on top of UNIX (or an-
other operating system) on a single machine or over a network is that what X really
defines is the X protocol between the X client and the X server, as shown in
Fig. 5-33.  It does not matter whether the client and server are on the same ma-
chine, separated by 100 meters over a local area network, or are thousands of kilo-
meters apart and connected by the Internet. The protocol and operation of the sys-
tem is identical in all cases.
X is just a windowing system.            It is not a complete GUI.  To get a complete
GUI, others layer of software are run on top of it. One layer is Xlib, which is a set
of library procedures for accessing the X functionality. These procedures form the
basis of the X Window System and are what we will examine below, but they are
too primitive for most user programs to access directly. For example, each mouse
click is reported separately, so that determining that two clicks really form a double
click has to be handled above Xlib.
To make programming with X easier, a toolkit consisting of the Intrinsics is
supplied as part of X.   This layer manages buttons, scroll bars, and other GUI



402                                          INPUT/OUTPUT                             CHAP. 5
                     Remote host
                Window          Application
                manager          program
                         Motif                                                        Window
User                 Intrinsics
space
                         Xlib
                     X client                                               X server
Kernel                   UNIX                                               UNIX
space
                     Hardware                                               Hardware
                                              X protocol
                                             Network
                Figure 5-33. Clients and servers in the M.I.T. X Window System.
elements, called widgets.        To make a true GUI interface, with a uniform look and
feel, another layer is needed (or several of them). One example is Motif, shown in
Fig. 5-33, which is the basis of the Common Desktop Environment used on Solaris
and other commercial UNIX systems Most applications make use of calls to Motif
rather than Xlib. Gnome and KDE have a similar structure to Fig. 5-33, only with
different libraries. Gnome uses the GTK+ library and KDE uses the Qt library.
Whether having two GUIs is better than one is debatable.
     Also worth noting is that window management is not part of X itself. The de-
cision to leave it out was fully intentional. Instead, a separate X client process, cal-
led a window manager, controls the creation, deletion, and movement of windows
on the screen.  To manage windows, it sends commands to the X server telling it
what to do. It often runs on the same machine as the X client, but in theory can run
anywhere.
     This   modular  design,      consisting  of      several  layers  and  multiple  programs,
makes X highly portable and flexible.                 It has been ported to most versions of
UNIX, including Solaris, all variants of BSD, AIX, Linux, and so on, making it
possible for application developers to have a standard user interface for multiple
platforms.  It has also been ported to other operating systems.             In contrast, in Win-
dows, the windowing and GUI systems are mixed together in the GDI and located
in the kernel, which makes them harder to maintain, and of, course, not portable.
     Now let us take a brief look at X as viewed from the Xlib level. When an X
program starts, it opens a connection to one or more X servers--let us call them
workstations even though they might be collocated on the same machine as the X



SEC. 5.6       USER INTERFACES: KEYBOARD, MOUSE, MONITOR                               403
program itself. X considers this connection to be reliable in the sense that lost and
duplicate messages are handled by the networking software and it does not have to
worry about communication errors. Usually, TCP/IP is used between the client and
server.
Four kinds of messages go over the connection:
1.        Drawing commands from the program to the workstation.
2.        Replies by the workstation to program queries.
3.        Keyboard, mouse, and other event announcements.
4.        Error messages.
Most drawing commands are sent from the program to the workstation as one-
way messages.  No reply is expected.     The reason for this design is that when the
client and server processes are on different machines, it may take a substantial
period of time for the command to reach the server and be carried out. Blocking
the application program during this time would slow it down unnecessarily. On the
other hand, when the program needs information from the workstation, it simply
has to wait until the reply comes back.
Like Windows, X is highly event driven. Events flow from the workstation to
the program, usually in response to some human action such as keyboard strokes,
mouse movements, or a window being uncovered. Each event message is 32 bytes,
with the first byte giving the event type and the next 31 bytes providing additional
information. Several dozen kinds of events exist, but a program is sent only those
events that it has said it is willing to handle. For example, if a program does not
want to hear about key releases, it is not sent any key-release events.  As in Win-
dows, events are queued, and programs read events from the input queue. However,
unlike Windows, the operating system never calls procedures within the applica-
tion program on its own.   It does not even know which procedure handles which
event.
A key concept in X is the resource.      A resource is a data structure that holds
certain information. Application programs create resources on workstations.            Re-
sources can be shared among multiple processes on the workstation. Resources
tend to be short-lived and do not survive workstation reboots. Typical resources in-
clude windows, fonts, colormaps (color palettes), pixmaps (bitmaps), cursors, and
graphic contexts. The latter are used to associate properties with windows and are
similar in concept to device contexts in Windows.
A rough, incomplete skeleton of an X program is shown in Fig. 5-34. It begins
by including some required headers and then declaring some variables.        It then
connects to the X server specified as the parameter to XOpenDisplay.     Then it allo-
cates a window resource and stores a handle to it in win.  In practice, some ini-
tialization would happen here. After that it tells the window manager that the new
window exists so the window manager can manage it.



404                                    INPUT/OUTPUT                                        CHAP. 5
#include <X11/Xlib.h>
#include <X11/Xutil.h>
main(int argc, char *argv[])
{
     Display disp;                                       /* server identifier */
     Window win;                                         /* window identifier */
     GC gc;                                              /* graphic context identifier */
     XEvent event;                                       /* storage for one event */
     int running = 1;
     disp = XOpenDisplay("display      name");           /* connect to the X server */
     win = XCreateSimpleWindow(disp, ... ); /* allocate memory for new window */
     XSetStandardProperties(disp, ...);         /* announces window to window mgr */
     gc = XCreateGC(disp, win, 0, 0);           /* create graphic context */
     XSelectInput(disp, win, ButtonPressMask | KeyPressMask | ExposureMask);
     XMapRaised(disp, win);                     /* display window; send Expose event */
     while (running) {
         XNextEvent(disp, &event);                       /* get next event */
         switch (event.type) {
             case Expose:          ...;  break;          /* repaint window */
             case ButtonPress:     ...;  break;          /* process mouse click */
             case Keypress:        ...;  break;          /* process keyboard input */
         }
     }
     XFreeGC(disp, gc);                         /* release graphic context */
     XDestroyWindow(disp, win);                 /* deallocate window's memory space */
     XCloseDisplay(disp);                       /* tear down network connection */
}
             Figure 5-34. A skeleton of an X Window application program.
     The call to XCreateGC creates a graphic context in which properties of the
window are stored.      In a more complete program, they might be initialized here.
The next statement, the call to XSelectInput, tells the X server which events the
program  is  prepared    to   handle.    In  this  case  it  is  interested    in  mouse   clicks,
keystrokes, and windows being uncovered.           In practice, a real program would be
interested in other events as well.      Finally, the call to XMapRaised maps the new
window onto the screen as the uppermost window.                  At this point the window be-
comes visible on the screen.
     The main loop consists of two statements and is logically much simpler than
the corresponding loop in Windows. The first statement here gets an event and the
second one dispatches on the event type for processing. When some event indicates
that the program has finished, running is set to 0 and the loop terminates. Before
exiting, the program releases the graphic context, window, and connection.



SEC. 5.6  USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                    405
It is worth mentioning that not everyone likes a GUI.       Many programmers pre-
fer a traditional command-line oriented interface of the type discussed in Sec. 5.6.1
above.   X handles this via a client program called xterm.  This program emulates a
venerable VT102 intelligent terminal, complete with all the escape sequences.
Thus editors such as vi and emacs and other software that uses termcap work in
these windows without modification.
Graphical User Interfaces
Most      personal  computers  offer  a  GUI    (Graphical  User  Interface).          The
acronym GUI is pronounced ``gooey.''
The GUI was invented by Douglas Engelbart and his research group at the
Stanford Research Institute.  It was then copied by researchers at Xerox PARC.
One fine day, Steve Jobs, cofounder of Apple, was touring PARC and saw a GUI
on a Xerox computer and said something to the effect of ``Holy mackerel. This is
the future of computing.'' The GUI gave him the idea for a new computer, which
became the Apple Lisa. The Lisa was too expensive and was a commercial failure,
but its successor, the Macintosh, was a huge success.
When Microsoft got a Macintosh prototype so it could develop Microsoft
Office on it, it begged Apple to license the interface to all comers so it would be-
come the new industry standard. (Microsoft made much more money from Office
than from MS-DOS, so it was willing to abandon MS-DOS to have a better plat-
form for Office.)   The Apple executive in charge of the Macintosh, Jean-Louis
GasseÂ´e, refused and Steve Jobs was no longer around to overrule him. Eventually,
Microsoft got a license for elements of the interface.      This formed the basis of
Windows.  When Windows began to catch on, Apple sued Microsoft, claiming
Microsoft had exceeded the license, but the judge disagreed and Windows went on
to overtake the Macintosh.    If GasseÂ´e had agreed with the many people within
Apple who also wanted to license the Macintosh software to everyone and his
uncle, Apple would have become insanely rich on licensing fees alone and Win-
dows would not exist now.
Leaving aside touch-enabled interfaces for the moment, a GUI has four essen-
tial elements, denoted by the characters WIMP.  These letters stand for Windows,
Icons, Menus, and Pointing device, respectively. Windows are rectangular blocks
of screen area used to run programs. Icons are little symbols that can be clicked on
to cause some action to happen. Menus are lists of actions from which one can be
chosen.   Finally, a pointing device is a mouse, trackball, or other hardware device
used to move a cursor around the screen to select items.
The GUI software can be implemented in either user-level code, as is done in
UNIX systems, or in the operating system itself, as in the case in Windows.
Input for GUI systems still uses the keyboard and mouse, but output almost al-
ways goes to a special hardware board called a graphics adapter.             A graphics
adapter contains a special memory called video RAM that holds the images that



406                             INPUT/OUTPUT                              CHAP. 5
appear on the screen.   Graphics adapters often have powerful 32- or 64-bit CPUs
and up to 4 GB of their own RAM, separate from the computer's main memory.
     Each graphics adapter supports some number of screen sizes. Common sizes
(horizontal Ã vertical in pixels) are 1280 Ã 960, 1600 Ã 1200, 1920 Ã1080, 2560 Ã
1600, and 3840 Ã 2160.  Many resolutions in practice are in the ratio of 4:3, which
fits the aspect ratio of NTSC and PAL television sets and thus gives square pixels
on the same monitors used for television sets.  Higher resolutions are intended for
wide-screen monitors whose aspect ratio matches them.  At a resolution of just
1920 Ã 1080 (the size of full HD videos), a color display with 24 bits per pixel re-
quires about 6.2 MB of RAM just to hold the image, so with 256 MB or more, the
graphics adapter can hold many images at once.        If the full screen is refreshed 75
times/sec, the video RAM must be capable of delivering data continuously at 445
MB/sec.
     Output software for GUIs is a massive topic. Many 1500-page books have
been written about the Windows GUI alone (e.g., Petzold, 2013; Rector and New-
comer, 1997; and Simon, 1997).         Clearly, in this section, we can only scratch the
surface and present a few of the underlying concepts.  To make the discussion con-
crete, we will describe the Win32 API, which is supported by all 32-bit versions of
Windows.  The output software for other GUIs is roughly comparable in a general
sense, but the details are very different.
     The basic item on the screen is a rectangular area called a window.    A win-
dow's position and size are uniquely determined by giving the coordinates (in pix-
els) of two diagonally opposite corners.    A window may contain a title bar, a menu
bar, a tool bar, a vertical scroll bar, and a horizontal scroll bar. A typical window is
shown in Fig. 5-35.    Note that the Windows coordinate system puts the origin in
the upper left-hand corner and has y increase downward, which is different from
the Cartesian coordinates used in mathematics.
     When a window is created, the parameters specify whether it can be moved by
the user, resized by the user, or scrolled (by dragging the thumb on the scroll bar)
by the user. The main window produced by most programs can be moved, resized,
and scrolled, which has enormous consequences for the way Windows programs
are written. In particular, programs must be informed about changes to the size of
their windows and must be prepared to redraw the contents of their windows at any
time, even when they least expect it.
     As a consequence, Windows programs are message oriented. User actions in-
volving the keyboard or mouse are captured by Windows and converted into mes-
sages to the program owning the window being addressed. Each program has a
message queue to which messages relating to all its windows are sent. The main
loop of the program consists of fishing out the next message and processing it by
calling an internal procedure for that message type.   In some cases, Windows itself
may call these procedures directly, bypassing the message queue. This model is
quite different from the UNIX model of procedural code that makes system calls to
interact with the operating system. X, however, is event oriented.



SEC.    5.6         USER INTERFACES: KEYBOARD,             MOUSE, MONITOR              407
(0, 0)                                                                                (1023, 0)
        (200, 100)
                                Title bar
        Menu bar    File  Edit  View                Tools  Options  Help
                                      11  12  1
        Tool bar                10               2
                                9                3
                                8                4
                                      7   6   5
                                                                           Thumb
                                Client area
                                                                          Scroll bar
        Window
(0, 767)                                                                            (1023, 767)
             Figure 5-35. A sample window located at (200, 100) on an XGA display.
To make this programming model clearer, consider the example of Fig. 5-36.
Here we see the skeleton of a main program for Windows.                   It is not complete and
does no error checking, but it shows enough detail for our purposes. It starts by in-
cluding a header file, windows.h, which contains many macros, data types, con-
stants, function prototypes, and other information needed by Windows programs.
The main program starts with a declaration giving its name and parameters.
The WINAPI macro is an instruction to the compiler to use a certain parameter-pas-
sing convention and will not be of further concern to us. The first parameter, h, is
an instance handle and is used to identify the program to the rest of the system.      To
some extent, Win32 is object oriented, which means that the system contains ob-
jects (e.g., programs, files, and windows) that have some state and associated code,
called methods, that operate on that state. Objects are referred to using handles,
and in this case, h identifies the program. The second parameter is present only for
reasons of backward compatibility. It is no longer actually used. The third parame-
ter, szCmd, is a zero-terminated string containing the command line that started the
program, even if it was not started from a command line. The fourth parameter,



408                               INPUT/OUTPUT                                       CHAP. 5
#include <windows.h>
int WINAPI WinMain(HINSTANCE h, HINSTANCE, hprev, char *szCmd, int iCmdShow)
{
     WNDCLASS wndclass;                   /* class object for this window */
     MSG msg;                             /* incoming messages are stored here */
     HWND hwnd;                           /* handle (pointer) to the window object */
     /* Initialize wndclass */            /* tells which procedure to call */
     wndclass.lpfnWndProc = WndProc;
     wndclass.lpszClassName = "Program name";          /* text for title bar */
     wndclass.hIcon = LoadIcon(NULL, IDI     APPLICATION);      /* load program icon */
     wndclass.hCursor = LoadCursor(NULL, IDC           ARROW);  /* load mouse cursor */
     RegisterClass(&wndclass);            /* tell Windows about wndclass */
     hwnd = CreateWindow ( ... )          /* allocate storage for the window */
     ShowWindow(hwnd, iCmdShow);          /* display the window on the screen */
     UpdateWindow(hwnd);                  /* tell the window to paint itself */
     while (GetMessage(&msg, NULL, 0, 0)) {            /* get message from queue */
        TranslateMessage(&msg);           /* translate the message */
        DispatchMessage(&msg);            /* send msg to the appropriate procedure */
     }
     return(msg.wParam);
}
long CALLBACK WndProc(HWND hwnd, UINT message, UINT wParam, long lParam)
{
     /* Declarations go here. */
     switch (message) {
        case WM       CREATE:     ... ;  return ... ;  /* create window */
        case WM       PAINT:      ... ;  return ... ;  /* repaint contents of window */
        case WM       DESTROY:    ... ;  return ... ;  /* destroy window */
     }
}    return(DefWindowProc(hwnd, message, wParam, lParam)); /* default */
                      Figure 5-36. A skeleton of a Windows main program.
iCmdShow, tells whether the program's initial window should occupy the entire
screen, part of the screen, or none of the screen (task bar only).
     This declaration illustrates a widely used Microsoft convention called Hungar-
ian notation.  The name is a play on Polish notation, the postfix system invented
by the Polish logician J. Lukasiewicz for representing algebraic formulas without
using precedence or parentheses. Hungarian notation was invented by a Hungarian
programmer at Microsoft, Charles Simonyi, and uses the first few characters of an
identifier to specify the type. The allowed letters and types include c (character), w
(word, now meaning an unsigned 16-bit integer), i (32-bit signed integer), l (long,



SEC. 5.6     USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                   409
also a 32-bit signed integer), s (string), sz (string terminated by a zero byte), p
(pointer), fn (function), and h (handle).     Thus szCmd is a zero-terminated string
and iCmdShow is an integer, for example. Many programmers believe that en-
coding the type in variable names this way has little value and makes Windows
code hard to read. Nothing analogous to this convention is present in UNIX.
Every window must have an associated class object that defines its properties.
In Fig. 5-36, that class object is wndclass.  An object of type WNDCLASS has 10
fields, four of which are initialized in Fig. 5-36. In an actual program, the other six
would be initialized as well. The most important field is lpfnWndProc, which is a
long (i.e., 32-bit) pointer to the function that handles the messages directed to this
window. The other fields initialized here tell which name and icon to use in the
title bar, and which symbol to use for the mouse cursor.
After wndclass has been initialized, RegisterClass is called to pass it to Win-
dows.  In particular, after this call Windows knows which procedure to call when
various events occur that do not go through the message queue. The next call, Cre-
ateWindow, allocates memory for the window's data structure and returns a handle
for referencing it later. The program then makes two more calls in a row, to put the
window's outline on the screen, and finally fill it in completely.
At this point we come to the program's main loop, which consists of getting a
message, having certain translations done to it, and then passing it back to Win-
dows to have Windows invoke WndProc to process it.          To answer the question of
whether this whole mechanism could have been made simpler, the answer is yes,
but it was done this way for historical reasons and we are now stuck with it.
Following the main program is the procedure WndProc, which handles the
various messages that can be sent to the window. The use of CALLBACK here, like
WINAPI above, specifies the calling sequence to use for parameters. The first pa-
rameter is the handle of the window to use.   The second parameter is the message
type. The third and fourth parameters can be used to provide additional infor-
mation when needed.
Message types WM     CREATE and WM            DESTROY are sent at the start and end
of the program, respectively. They give the program the opportunity, for example,
to allocate memory for data structures and then return it.
The third message type, WM  PAINT, is an instruction to the program to fill in
the window.  It is called not only when the window is first drawn, but often during
program execution as well.  In contrast to text-based systems, in Windows a pro-
gram cannot assume that whatever it draws on the screen will stay there until it re-
moves it. Other windows can be dragged on top of this one, menus can be pulled
down over it, dialog boxes and tool tips can cover part of it, and so on. When these
items are removed, the window has to be redrawn.          The way Windows tells a pro-
gram to redraw a window is to send it a WM    PAINT message.        As a friendly ges-
ture, it also provides information about what part of the window has been overwrit-
ten, in case it is easier or faster to regenerate that part of the window instead of
redrawing the whole thing from scratch.



410                         INPUT/OUTPUT                                          CHAP. 5
     There are two ways Windows can get a program to do something. One way is
to post a message to its message queue.    This method is used for keyboard input,
mouse input, and timers that have expired. The other way, sending a message to the
window, involves having Windows directly call WndProc itself. This method is
used for all other events.  Since Windows is notified when a message is fully proc-
essed, it can refrain from making a new call until the previous one is finished.            In
this way race conditions are avoided.
     There are many more message types.    To avoid erratic behavior should an un-
expected message arrive, the program should call DefWindowProc at the end of
WndProc to let the default handler take care of the other cases.
     In summary, a Windows program normally creates one or more windows with
a class object for each one. Associated with each program is a message queue and
a set of handler procedures. Ultimately, the program's behavior is driven by the in-
coming events, which are processed by the handler procedures. This is a very dif-
ferent model of the world than the more procedural view that UNIX takes.
     Drawing to the screen is handled by a package consisting of hundreds of pro-
cedures that are bundled together to form the GDI (Graphics Device Interface).
It can handle text and graphics and is designed to be platform and device indepen-
dent. Before a program can draw (i.e., paint) in a window, it needs to acquire a de-
vice context, which is an internal data structure containing properties of the win-
dow, such as the font, text color, background color, and so on. Most GDI calls use
the device context, either for drawing or for getting or setting the properties.
     Various ways exist to acquire the device context.       A simple example of its
acquisition and use is
     hdc = GetDC(hwnd);
     TextOut(hdc, x, y, psText, iLength);
     ReleaseDC(hwnd, hdc);
The first statement gets a handle to a device content, hdc. The second one uses the
device context to write a line of text on the screen, specifying the (x, y) coordinates
of where the string starts, a pointer to the string itself, and its length. The third call
releases the device context to indicate that the program is through drawing for the
moment. Note that hdc is used in a way analogous to a UNIX file descriptor. Also
note that ReleaseDC contains redundant information (the use of hdc uniquely
specifies a window).    The use of redundant information that has no actual value is
common in Windows.
     Another interesting note is that when hdc is acquired in this way, the program
can write only in the client area of the window, not in the title bar and other parts
of it. Internally, in the device context's data structure, a clipping region is main-
tained. Any drawing outside the clipping region is ignored.       However, there is an-
other way to acquire a device context, GetWindowDC, which sets the clipping re-
gion to the entire window. Other calls restrict the clipping region in other ways.
Having multiple calls that do almost the same thing is characteristic of Windows.



SEC. 5.6  USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                   411
A complete treatment of the GDI is out of the question here. For the interested
reader, the references cited above provide additional information. Nevertheless,
given how important it is, a few words about the GDI are probably worthwhile.
GDI has various procedure calls to get and release device contexts, obtain infor-
mation about device contexts, get and set device context attributes (e.g., the back-
ground color), manipulate GDI objects such as pens, brushes, and fonts, each of
which has its own attributes. Finally, of course, there are a large number of GDI
calls to actually draw on the screen.
The drawing procedures fall into four categories: drawing lines and curves,
drawing filled areas, managing bitmaps, and displaying text.  We saw an example
of drawing text above, so let us take a quick look at one of the others. The call
Rectangle(hdc, xleft, ytop, xright, ybottom);
draws a filled rectangle whose corners are (xleft, ytop) and (xright, ybottom).       For
example,
Rectangle(hdc, 2, 1, 6, 4);
will draw the rectangle shown in Fig. 5-37. The line width and color and fill color
are taken from the device context. Other GDI calls are similar in flavor.
               0             1         2  3  4  5  6  7  8
            0
            1
            2
            3
            4
            5
            6
            7
Figure 5-37. An example rectangle drawn using Rectangle. Each box represents
one pixel.
Bitmaps
The GDI procedures are examples of vector graphics. They are used to place
geometric figures and text on the screen.       They can be scaled easily to larger or
smaller screens (provided the number of pixels on the screen is the same).         They
are also relatively device independent. A collection of calls to GDI procedures can
be assembled in a file that can describe a complex drawing. Such a file is called a



412                                 INPUT/OUTPUT                  CHAP. 5
Windows metafile and is widely used to transmit drawings from one Windows pro-
gram to another. Such files have extension .wmf.
     Many Windows programs allow the user to copy (part of) a drawing and put it
on the Windows clipboard. The user can then go to another program and paste the
contents of the clipboard into another document. One way of doing this is for the
first program to represent the drawing as a Windows metafile and put it on the clip-
board in .wmf format. Other ways also exist.
     Not all the images that computers manipulate can be generated using vector
graphics. Photographs and videos, for example, do not use vector graphics.              In-
stead, these items are scanned in by overlaying a grid on the image. The average
red, green, and blue values of each grid square are then sampled and saved as the
value of one pixel. Such a file is called a bitmap.  There are extensive facilities in
Windows for manipulating bitmaps.
     Another use for bitmaps is for text. One way to represent a particular character
in some font is as a small bitmap.  Adding text to the screen then becomes a matter
of moving bitmaps.
     One general way to use bitmaps is through a procedure called BitBlt. It is cal-
led as follows:
     BitBlt(dsthdc, dx, dy, wid, ht, srchdc, sx, sy, rasterop);
In its simplest form, it copies a bitmap from a rectangle in one window to a rectan-
gle in another window (or the same one).      The first three parameters specify the
destination window and position. Then come the width and height.  Next come the
source window and position. Note that each window has its own coordinate sys-
tem, with (0, 0) in the upper left-hand corner of the window. The last parameter
will be described below. The effect of
     BitBlt(hdc2, 1, 2, 5, 7, hdc1, 2, 2, SRCCOPY);
is shown in Fig. 5-38. Notice carefully that the entire 5 Ã 7 area of the letter A has
been copied, including the background color.
     BitBlt can do more than just copy bitmaps. The last parameter gives the possi-
bility of performing Boolean operations to combine the source bitmap and the
destination bitmap. For example, the source can be ORed into the destination to
merge with it.   It can also be EXCLUSIVE ORed into it, which maintains the char-
acteristics of both source and destination.
     A problem with bitmaps is that they do not scale.           A character that is in a box
of 8 Ã 12 on a display of 640 Ã 480 will look reasonable. However, if this bitmap is
copied to a printed page at 1200 dots/inch, which is 10,200 bits Ã 13,200 bits, the
character width (8 pixels) will be 8/1200 inch or 0.17 mm.       In addition, copying
between devices with different color properties or between monochrome and color
does not work well.
     For this reason, Windows also supports a data structure called a DIB (Device
Independent Bitmap). Files using this format use the extension .bmp. These files



SEC. 5.6           USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                           413
   0   2  4  6  8                                      0  2  4  6  8
0                                                   0
2                                                   2
4                                                   4
6                                                   6
8                                                   8
                                     0  2  4  6  8                                      0  2  4  6  8
                                  0                                                  0
   Window 1                       2                    Window 1                      2
                                  4                                                  4
                                  6                                                  6
                                  8                                                  8
                   Window 2                                           Window 2
                             (a)                                                (b)
                   Figure 5-38.      Copying bitmaps using BitBlt. (a) Before. (b) After.
have file and information headers and a color table before the pixels. This infor-
mation makes it easier to move bitmaps between dissimilar devices.
Fonts
In versions of Windows before 3.1, characters were represented as bitmaps and
copied onto the screen or printer using BitBlt.                 The problem with that, as we just
saw, is that a bitmap that makes sense on the screen is too small for the printer.
Also, a different bitmap is needed for each character in each size.                           In other words,
given the bitmap for A in 10-point type, there is no way to compute it for 12-point
type. Because every character of every font might be needed for sizes ranging from
4 point to 120 point, a vast number of bitmaps were needed. The whole system was
just too cumbersome for text.
The solution was the introduction of TrueType fonts, which are not bitmaps but
outlines of the characters. Each TrueType character is defined by a sequence of
points around its perimeter. All the points are relative to the (0, 0) origin.                         Using
this system, it is easy to scale the characters up or down. All that has to be done is
to multiply each coordinate by the same scale factor. In this way, a TrueType char-
acter can be scaled up or down to any point size, even fractional point sizes. Once
at the proper size, the points can be connected using the well-known follow-the-
dots algorithm taught in kindergarten (note that modern kindergartens use splines
for smoother results).            After the outline has been completed, the character can be
filled in.      An example of some characters scaled to three different point sizes is
given in Fig. 5-39.
Once the filled character is available in mathematical form, it can be rasterized,
that is, converted to a bitmap at whatever resolution is desired. By first scaling and
then rasterizing, we can be sure that the characters displayed on the screen or
printed on the printer will be as close as possible, differing only in quantization



414                               INPUT/OUTPUT                                              CHAP. 5
        20 pt:
        53 pt:
        81 pt:
                Figure 5-39. Some examples of character outlines at different point sizes.
error.  To improve the quality still more, it is possible to embed hints in each char-
acter telling how to do the rasterization. For example, both serifs on the top of the
letter T should be identical, something that might not otherwise be the case due to
roundoff error. Hints improve the final appearance.
Touch Screens
     More and more the screen is used as an input device also. Especially on smart-
phones, tablets and other ultra-portable devices it is convenient to tap and swipe
away at the screen with your finger (or a stylus). The user experience is different
and more intuitive than with a mouse-like device, since the user interacts directly
with the objects on the screen. Research has shown that even orangutans and other
primates like little children are capable of operating touch-based devices.
     A touch device is not necessarily a screen. Touch devices fall into two cate-
gories: opaque and transparent. A typical opaque touch device is the touchpad on a
notebook computer. An example of a transparent device is the touch screen on a
smartphone or tablet. In this section, however, we limit ourselves to touch screens.
     Like many things that have come into fashion in the computer industry, touch
screens are not exactly new. As early as 1965, E.A. Johnson of the British Royal
Radar   Establishment  described  a  (capacitive)    touch  display  that,                  while  crude,
served as precursor of the displays we find today. Most modern touch screens are
either resistive or capacitive.
     Resistive screens have a flexible plastic surface on top. The plastic in itself is
nothing too special, except that is more scratch resistant than your garden variety



SEC. 5.6  USER INTERFACES: KEYBOARD, MOUSE, MONITOR                                      415
plastic. However, a thin film of ITO (Indium Tin Oxide) or some similar con-
ducive material) is printed in thin lines onto the surface's underside. Beneath it, but
not quite touching it, is a second surface also coated with a layer of ITO. On the
top surface, the charge runs in the vertical direction and there are conductive con-
nections at the top and bottom. In the bottom layer the charge runs horizontally and
there are connections on the left and right. When you touch the screen, you dent
the plastic so that the top layer of ITO touches the bottom layer. To find out the
exact position of the finger or stylus touching it, all you need to do is measure the
resistance in both directions at all the horizontal positions of the bottom and all the
vertical positions of the top layer.
Capacitive Screens have two hard surfaces, typically glass, each coated with
ITO. A typical configuration is to have ITO added to each surface in parallel lines,
where the lines in the top layer are perpendicular to those in the bottom layer. For
instance, the top layer may be coated in thin lines in a vertical direction, while the
bottom layer has a similarly striped pattern in the horizontal direction. The two
charged surfaces, separated by air, form a grid of really small capacitors. Voltages
are applied alternately to the horizontal and vertical lines, while the voltage values,
which are affected by the capacitance of each intersection, are read out on the other
ones. When you put your finger onto the screen, you change the local capacitance.
By very accurately measuring the miniscule voltage changes everywhere, it is pos-
sible to discover the location of the finger on the screen. This operation is repeated
many times per second with the coordinates touched fed to the device driver as a
stream of (x, y) pairs. Further processing, such as determining whether pointing,
pinching, expanding, or swiping is taking place is done by the operating system.
What is nice about resistive screens is that the pressure determines the outcome
of the measurements. In other words, it will work even if you are wearing gloves in
cold weather. This is not true of capacitive screens, unless you wear special gloves.
For instance, you can sew a conductive thread (like silver-plated nylon) through the
fingertips of the gloves, or if you are not a needling person, buy them ready-made.
Alternatively, you cut off the tips of your gloves and be done in 10 seconds.
What is not so nice about resistive screens is that they typically cannot support
multitouch, a technique that detects multiple touches at the same time. It allows
you to manipulate objects on the screen with two or more fingers. People (and per-
haps also orangutans) like multitouch because it enables them to use pinch-and-ex-
pand gestures with two fingers to enlarge or shrink a picture or document. Imagine
that the two fingers are at (3, 3) and (8, 8). As a result, the resistive screen may
notice a change in resistance on the x = 3 and x = 8 vertical lines, and the y = 3 and
y = 8 horizontal lines. Now consider a different scenario with the fingers at (3, 8)
and (8, 3), which are the opposite corners of the rectangle whose corners are (3, 3),
(8, 3), (8, 8), and (3, 8). The resistance in precisely the same lines has changed, so
the software has no way of telling which of the two scenarios holds. This problem
is called ghosting. Because capacitive screens send a stream of (x, y) coordinates,
they are more adept at supporting multitouch.



416                                INPUT/OUTPUT                     CHAP. 5
     Manipulating a touch screen with just a single finger is still fairly WIMPy--
you just replace the mouse pointer with your stylus or index finger. Multitouch is a
bit more complicated. Touching the screen with five fingers is like pushing five
mouse pointers across the screen at the same time and clearly changes things for
the window manager. Multitouch screens have become ubiquitous and increasingly
sensitive and accurate. Nevertheless, it is unclear whether the Five Point Palm
Exploding Heart Technique has any effect on the CPU.
5.7 THIN CLIENTS
     Over the years, the main computing paradigm has oscillated between cent-
ralized and decentralized computing. The first computers, such as the ENIAC,
were, in fact, personal computers, albeit large ones, because only one person could
use one at once. Then came timesharing systems, in which many remote users at
simple terminals shared a big central computer. Next came the PC era, in which the
users had their own personal computers again.
     While the decentralized PC model has advantages, it also has some severe
disadvantages that are only beginning to be taken seriously. Probably the biggest
problem is that each PC has a large hard disk and complex software that must be
maintained. For example, when a new release of the operating system comes out, a
great deal of work has to be done to perform the upgrade on each machine sepa-
rately. At most corporations, the labor costs of doing this kind of software mainte-
nance dwarf the actual hardware and software costs. For home users, the labor is
technically free, but few people are capable of doing it correctly and fewer still
enjoy doing it. With a centralized system, only one or a few machines have to be
updated and those machines have a staff of experts to do the work.
     A related issue is that users should make regular backups of their gigabyte file
systems, but few of them do. When disaster strikes, a great deal of moaning and
wringing of hands tends to follow. With a centralized system, backups can be made
every night by automated tape robots.
     Another advantage is that resource sharing is easier with centralized systems.
A system with 256 remote users, each with 256 MB of RAM, will have most of
that RAM idle most of the time. With a centralized system with 64 GB of RAM, it
never happens that some user temporarily needs a lot of RAM but cannot get it be-
cause it is on someone else's PC.      The same argument holds for disk space and
other resources.
     Finally, we are starting to see a shift from PC-centric computing to Web-cen-
tric computing. One area where this shift is very far along is email. People used to
get their email delivered to their home machine and read it there. Nowadays, many
people log into Gmail, Hotmail, or Yahoo and read their mail there. The next step
is for people to log into other Websites to do word processing, build spreadsheets,



SEC. 5.7                              THIN CLIENTS                                        417
and other things that used to require PC software. It is even possible that eventually
the only software people run on their PC is a Web browser, and maybe not even
that.
      It is probably a fair conclusion to say that most users want high-performance
interactive computing but do not really want to administer a computer. This has led
researchers to reexamine timesharing using dumb terminals (now politely called
thin clients) that meet modern terminal expectations.         X was a step in this direc-
tion and dedicated X terminals were popular for a little while but they fell out of
favor because they cost as much as PCs, could do less, and still needed some soft-
ware maintenance. The holy grail would be a high-performance interactive com-
puting system in which the user machines had no software at all. Interestingly
enough, this goal is achievable.
      One of the best known thin clients is the Chromebook.               It is pushed actively
by Google, but with a wide variety of manufacturers providing a wide variety of
models.      The notebook runs ChromeOS which is based on Linux and the Chrome
Web browser and is assumed to be online all the time. Most other software is
hosted on the Web in the form of Web Apps, making the software stack on the
Chromebook itself considerably thinner than in most traditional notebooks. On the
other hand, a system that runs a full Linux stack, and a Chrome browser, it is not
exactly anorexic either.
5.8 POWER MANAGEMENT
      The first general-purpose electronic computer, the ENIAC, had 18,000 vacuum
tubes and consumed 140,000 watts of power.          As a result, it ran up a nontrivial
electricity bill. After the invention of the transistor, power usage dropped dramati-
cally and the computer industry lost interest in power requirements. However, now-
adays power management is back in the spotlight for several reasons, and the oper-
ating system is playing a role here.
      Let us start with desktop PCs.  A desktop PC often has a 200-watt power sup-
ply (which is typically 85% efficient, that is, loses 15% of the incoming energy to
heat).  If 100 million of these machines are turned on at once worldwide, together
they    use  20,000  megawatts    of  electricity.  This  is  the  total  output  of  20  aver-
age-sized nuclear power plants.       If power requirements could be cut in half, we
could get rid of 10 nuclear power plants. From an environmental point of view, get-
ting rid of 10 nuclear power plants (or an equivalent number of fossil-fuel plants) is
a big win and well worth pursuing.
      The other place where power is a big issue is on battery-powered computers,
including notebooks, handhelds, and Webpads, among others. The heart of the
problem is that the batteries cannot hold enough charge to last very long, a few
hours at most. Furthermore, despite massive research efforts by battery companies,
computer companies, and consumer electronics companies, progress is glacial.              To



418                     INPUT/OUTPUT                                CHAP. 5
an industry used to a doubling of performance every 18 months (Moore's law),
having no progress at all seems like a violation of the laws of physics, but that is
the current situation.  As a consequence, making computers use less energy so
existing batteries last longer is high on everyone's agenda. The operating system
plays a major role here, as we will see below.
     At the lowest level, hardware vendors are trying to make their electronics more
energy efficient. Techniques used include reducing transistor size, employing dy-
namic voltage scaling, using low-swing and adiabatic buses, and similar techni-
ques. These are outside the scope of this book, but interested readers can find a
good survey in a paper by Venkatachalam and Franz (2005).
     There are two general approaches to reducing energy consumption. The first
one is for the operating system to turn off parts of the computer (mostly I/O de-
vices) when they are not in use because a device that is off uses little or no energy.
The second one is for the application program to use less energy, possibly degrad-
ing the quality of the user experience, in order to stretch out battery time. We will
look at each of these approaches in turn, but first we will say a little bit about hard-
ware design with respect to power usage.
5.8.1 Hardware Issues
     Batteries come in two general types: disposable and rechargeable. Disposable
batteries (most commonly AAA, AA, and D cells) can be used to run handheld de-
vices, but do not have enough energy to power notebook computers with large
bright screens.  A rechargeable battery, in contrast, can store enough energy to
power a notebook for a few hours. Nickel cadmium batteries used to dominate
here, but they gave way to nickel metal hydride batteries, which last longer and do
not pollute the environment quite as badly when they are eventually discarded.
Lithium ion batteries are even better, and may be recharged without first being
fully drained, but their capacities are also severely limited.
     The general approach most computer vendors take to battery conservation is to
design the CPU, memory, and I/O devices to have multiple states: on, sleeping,
hibernating, and off. To use the device, it must be on. When the device will not be
needed for a short time, it can be put to sleep, which reduces energy consumption.
When it is not expected to be needed for a longer interval, it can be made to hiber-
nate, which reduces energy consumption even more. The trade-off here is that get-
ting a device out of hibernation often takes more time and energy than getting it
out of sleep state. Finally, when a device is off, it does nothing and consumes no
power.  Not all devices have all these states, but when they do, it is up to the oper-
ating system to manage the state transitions at the right moments.
     Some computers have two or even three power buttons. One of these may put
the whole computer in sleep state, from which it can be awakened quickly by typ-
ing a character or moving the mouse. Another may put the computer into hiberna-
tion, from which wakeup takes far longer. In both cases, these buttons typically do



SEC. 5.8                POWER MANAGEMENT                                                419
nothing except send a signal to the operating system, which does the rest in soft-
ware.  In some countries, electrical devices must, by law, have a mechanical power
switch that breaks a circuit and removes power from the device, for safety reasons.
To comply with this law, another switch may be needed.
Power management brings up a number of questions that the operating system
has to deal with. Many of them relate to resource hibernation--selectively and
temporarily turning off devices, or at least reducing their power consumption when
they are idle. Questions that must be answered include these: Which devices can be
controlled?  Are they on/off, or are there intermediate states?  How much power is
saved in the low-power states?  Is energy expended to restart the device?             Must
some context be saved when going to a low-power state? How long does it take to
go back to full power?  Of course, the answers to these questions vary from device
to device, so the operating system must be able to deal with a range of possibilities.
Various researchers have examined notebook computers to see where the pow-
er goes.  Li et al. (1994) measured various workloads and came to the conclusions
shown in Fig. 5-40.     Lorch and Smith (1998) made measurements on other ma-
chines and came to the conclusions shown in Fig. 5-40.       Weiser et al. (1994) also
made measurements but did not publish the numerical values. They simply stated
that the top three energy sinks were the display, hard disk, and CPU, in that order.
While these numbers do not agree closely, possibly because the different brands of
computers measured indeed have different energy requirements, it seems clear that
the display, hard disk, and CPU are obvious targets for saving energy. On devices
like smartphones, there may be other power drains, like the radio and GPS. Al-
though we focus on displays, disks, CPUs and memory in this section, the princ-
ples are the same for other peripherals.
             Device     Li et al. (1994)  Lorch and Smith (1998)
             Display            68%                     39%
             CPU                12%                     18%
             Hard disk          20%                     12%
             Modem                                      6%
             Sound                                      2%
             Memory             0.5%                    1%
             Other                                      22%
             Figure 5-40. Power consumption of various parts of a notebook computer.
5.8.2 Operating System Issues
The operating system plays a key role in energy management.                           It controls all
the devices, so it must decide what to shut down and when to shut it down.              If it
shuts down a device and that device is needed again quickly, there may be an



420                              INPUT/OUTPUT                    CHAP. 5
annoying delay while it is restarted.  On the other hand, if it waits too long to shut
down a device, energy is wasted for nothing.
     The trick is to find algorithms and heuristics that let the operating system make
good decisions about what to shut down and when. The trouble is that ``good'' is
highly subjective.  One user may find it acceptable that after 30 seconds of not
using the computer it takes 2 seconds for it to respond to a keystroke. Another user
may swear a blue streak under the same conditions. In the absence of audio input,
the computer cannot tell these users apart.
The Display
     Let us now look at the big spenders of the energy budget to see what can be
done about each one. One of the biggest items in everyone's energy budget is the
display. To get a bright sharp image, the screen must be backlit and that takes sub-
stantial energy. Many operating systems attempt to save energy here by shutting
down the display when there has been no activity for some number of minutes.
Often the user can decide what the shutdown interval is, thus pushing the trade-off
between frequent blanking of the screen and draining the battery quickly back to
the user (who probably really does not want it).  Turning off the display is a sleep
state because it can be regenerated (from the video RAM) almost instantaneously
when any key is struck or the pointing device is moved.
     One possible improvement was proposed by Flinn and Satyanarayanan (2004).
They suggested having the display consist of some number of zones that can be in-
dependently powered up or down.        In Fig. 5-41, we depict 16 zones, using dashed
lines to separate them. When the cursor is in window 2, as shown in Fig. 5-41(a),
only the four zones in the lower righthand corner have to be lit up. The other 12
can be dark, saving 3/4 of the screen power.
     When the user moves the cursor to window 1, the zones for window 2 can be
darkened and the zones behind window 1 can be turned on. However, because win-
dow 1 straddles 9 zones, more power is needed.    If the window manager can sense
what is happening, it can automatically move window 1 to fit into four zones, with
a kind of snap-to-zone action, as shown in Fig. 5-41(b).         To achieve this reduction
from 9/16 of full power to 4/16 of full power, the window manager has to under-
stand power management or be capable of accepting instructions from some other
piece of the system that does. Even more sophisticated would be the ability to par-
tially illuminate a window that was not completely full (e.g., a window containing
short lines of text could be kept dark on the right-hand side).
The Hard Disk
     Another major villain is the hard disk.  It takes substantial energy to keep it
spinning at high speed, even if there are no accesses. Many computers, especially
notebooks, spin the disk down after a certain number of minutes of being idle.



SEC. 5.8                     POWER MANAGEMENT                                           421
                                                  Window 1
          Window 1
                             Window 2                                    Window 2
Zone
                    (a)                                             (b)
      Figure 5-41.  The use of zones for backlighting the display.  (a) When window 2
      is selected, it is not moved.  (b) When window 1 is selected, it moves to reduce
      the number of zones illuminated.
When it is next needed, it is spun up again.      Unfortunately, a stopped disk is hiber-
nating rather than sleeping because it takes quite a few seconds to spin it up again,
which causes noticeable delays for the user.
In addition, restarting the disk consumes considerable energy.           As a conse-
quence, every disk has a characteristic time, Td , that is its break-even point, often
in the range 5 to 15 sec. Suppose that the next disk access is expected to come
some time t  in the future.  If t < Td , it takes less energy to keep the disk spinning
rather than spin it down and then spin it up so quickly.            If t > Td , the energy saved
makes it worth spinning the disk down and then up again much later.                     If a good
prediction could be made (e.g., based on past access patterns), the operating sys-
tem could make good shutdown predictions and save energy. In practice, most sys-
tems are conservative and stop the disk only after a few minutes of inactivity.
Another way to save disk energy is to have a substantial disk cache in RAM.
If a needed block is in the cache, an idle disk does not have to be restarted to sat-
isfy the read. Similarly, if a write to the disk can be buffered in the cache, a stop-
ped disk does not have to restarted just to handle the write. The disk can remain off
until the cache fills up or a read miss happens.
Another way to avoid unnecessary disk starts is for the operating system to
keep running programs informed about the disk state by sending them messages or
signals. Some programs have discretionary writes that can be skipped or delayed.
For example, a word processor may be set up to write the file being edited to disk
every few minutes.  If at the moment it would normally write the file out, the word
processor knows that the disk is off, it can delay this write until it is turned on.
The CPU
The CPU can also be managed to save energy.       A notebook CPU can be put to
sleep in software, reducing power usage to almost zero. The only thing it can do in
this state is wake up when an interrupt occurs.   Therefore, whenever the CPU goes
idle, either waiting for I/O or because there is no work to do, it goes to sleep.



422                             INPUT/OUTPUT                                                         CHAP. 5
       On many computers, there is a relationship between CPU voltage, clock cycle,
and power usage. The CPU voltage can often be reduced in software, which saves
energy but also reduces the clock cycle (approximately linearly). Since power con-
sumed is proportional to the square of the voltage, cutting the voltage in half makes
the CPU about half as fast but at 1/4 the power.
       This property can be exploited for programs with well-defined deadlines, such
as multimedia viewers that have to decompress and display a frame every 40 msec,
but go idle if they do it faster. Suppose that a CPU uses x joules while running full
blast for 40 msec and x/4 joules running at half speed.                      If a multimedia viewer can
decompress and display a frame in 20 msec, the operating system can run at full
power for 20 msec and then shut down for 20 msec for a total energy usage of x/2
joules. Alternatively, it can run at half power and just make the deadline, but use
only   x/4 joules instead.      A comparison of running at full speed and full power for
some time interval and at half speed and one-quarter power for twice as long is
shown in Fig. 5-42.        In both cases the same work is done, but in Fig. 5-42(b) only
half the energy is consumed doing it.
       1.00                                                         1.00
       0.75                                                         0.75
Power  0.50                                                  Power  0.50
       0.25                                                         0.25
       0     0             T/2         T                            0     0  T/2                     T
                Time                                                         Time
                (a)                                                          (b)
             Figure 5-42.  (a) Running at full clock speed.         (b) Cutting voltage by two cuts
             clock speed by two and power consumption by four.
       In a similar vein, if a user is typing at 1 char/sec, but the work needed to proc-
ess the character takes 100 msec, it is better for the operating system to detect the
long idle periods and slow the CPU down by a factor of 10.                   In short, running
slowly is more energy efficient than running quickly.
       Interestingly, scaling down the CPU cores does not always imply a reduction
in performance. Hruby et al. (2013) show that sometimes the performance of the
network stack improves with slower cores. The explanation is that a core can be too
fast for its own good. For instance, imagine a CPU with several fast cores, where
one core is responsible for the transmission of network packets on behalf of a pro-
ducer running on another core. The producer and the network stack communicate
directly via shared memory and they both run on dedicated cores. The producer
performs a fair amount of computation and cannot quite keep up with the core of
the network stack. On a typical run, the network will transmit all it has to transmit
and poll the shared memory for some amount of time to see if there is really no



SEC. 5.8                       POWER MANAGEMENT                                          423
more data to transmit. Finally, it will give up and go to sleep, because continuous
polling is very bad for power consumption. Shortly after, the producer provides
more data, but now the network stack is fast sleep. Waking up the stack takes time
and slows down the throughput. One possible solution is never to sleep, but this is
not attractive either because doing so would increase the power consumption--ex-
actly the opposite of what we are trying to achieve. A much more attractive solu-
tion is to run the network stack on a slower core, so that it is constantly busy (and
thus never sleeps), while still reducing the power consumption. If the network core
is slowed down carefully, its performance will be better than a configuration where
all cores are blazingly fast.
The Memory
Two possible options exist for saving energy with the memory. First, the cache
can be flushed and then switched off.  It can always be reloaded from main memo-
ry with no loss of information. The reload can be done dynamically and quickly, so
turning off the cache is entering a sleep state.
A more drastic option is to write the contents of main memory to the disk, then
switch off the main memory itself. This approach is hibernation, since virtually all
power can be cut to memory at the expense of a substantial reload time, especially
if the disk is off, too. When the memory is cut off, the CPU either has to be shut off
as well or has to execute out of ROM. If the CPU is off, the interrupt that wakes it
up has to cause it to jump to code in ROM so the memory can be reloaded before
being used. Despite all the overhead, switching off the memory for long periods of
time (e.g., hours) may be worth it if restarting in a few seconds is considered much
more desirable than rebooting the operating system from disk, which often takes a
minute or more.
Wireless Communication
Increasingly many portable computers have a wireless connection to the out-
side world (e.g., the Internet). The radio transmitter and receiver required are often
first-class power hogs.  In particular, if the radio receiver is always on in order to
listen for incoming email, the battery may drain fairly quickly.  On the other hand,
if the radio is switched off after, say, 1 minute of being idle, incoming messages
may be missed, which is clearly undesirable.
One efficient solution to this problem has been proposed by Kravets and Krish-
nan (1998).  The heart of their solution exploits the fact that mobile computers
communicate with fixed base stations that have large memories and disks and no
power constraints. What they propose is to have the mobile computer send a mes-
sage to the base station when it is about to turn off the radio. From that time on, the
base station buffers incoming messages on its disk.  The mobile computer may in-
dicate explicitly how long it is planning to sleep, or simply inform the base station



424                                  INPUT/OUTPUT                            CHAP. 5
when it switches on the radio again.      At that point any accumulated messages can
be sent to it.
     Outgoing messages that are generated while the radio is off are buffered on the
mobile computer.  If the buffer threatens to fill up, the radio is turned on and the
queue transmitted to the base station.
     When should the radio be switched off? One possibility is to let the user or the
application program decide. Another is to turn it off after some number of seconds
of idle time. When should it be switched on again? Again, the user or program
could decide, or it could be switched on periodically to check for inbound traffic
and transmit any queued messages. Of course, it also should be switched on when
the output buffer is close to full. Various other heuristics are possible.
     An example of a wireless technology supporting such a power-management
scheme can be found in 802.11 (``WiFi'') networks. In 802.11, a mobile computer
can notify the access point that it is going to sleep but it will wake up before the
base station sends the next beacon frame. The access point sends out these frames
periodically. At that point the access point can tell the mobile computer that it has
data pending. If there is no such data, the mobile computer can sleep again until
the next beacon frame.
Thermal Management
     A somewhat different, but still energy-related issue, is thermal management.
Modern CPUs get extremely hot due to their high speed.            Desktop machines nor-
mally have an internal electric fan to blow the hot air out of the chassis. Since
reducing power consumption is usually not a driving issue with desktop machines,
the fan is usually on all the time.
     With notebooks, the situation is different. The operating system has to monitor
the temperature continuously.        When it gets close to the maximum allowable tem-
perature, the operating system has a choice.       It can switch on the fan, which makes
noise and consumes power.         Alternatively, it can reduce power consumption by
reducing  the   backlighting  of     the  screen,  slowing  down  the  CPU,  being  more
aggressive about spinning down the disk, and so on.
     Some input from the user may be valuable as a guide. For example, a user
could specify in advance that the noise of the fan is objectionable, so the operating
system would reduce power consumption instead.
Battery Management
     In ye olde days, a battery just provided current until it was fully drained, at
which time it stopped. Not any more. Mobile devices now use smart batteries now,
which can communicate with the operating system. Upon request from the operat-
ing system, they can report on things like their maximum voltage, current voltage,
maximum charge, current charge, maximum drain rate, current drain rate, and



SEC. 5.8                     POWER MANAGEMENT                                             425
more. Most mobile devices have programs that can be run to query and display all
these parameters. Smart batteries can also be instructed to change various opera-
tional parameters under control of the operating system.
Some notebooks have multiple batteries. When the operating system detects
that one battery is about to go, it has to arrange for a graceful cutover to the next
one, without causing any glitches during the transition. When the final battery is on
its last legs, it is up to the operating system to warn the user and then cause an
orderly shutdown, for example, making sure that the file system is not corrupted.
Driver Interface
Several operating systems have an elaborate mechanism for doing power man-
agement called ACPI (Advanced Configuration and Power Interface).                The op-
erating system can send any conformant driver commands asking it to report on the
capabilities of its devices and their current states. This feature is especially impor-
tant when combined with plug and play because just after it is booted, the operat-
ing system does not even know what devices are present, let alone their properties
with respect to energy consumption or power manageability.
It can also send commands to drivers instructing them to cut their power levels
(based on the capabilities that it learned earlier, of course). There is also some traf-
fic the other way.  In particular, when a device such as a keyboard or a mouse
detects activity after a period of idleness, this is a signal to the system to go back to
(near) normal operation.
5.8.3 Application Program Issues
So far we have looked at ways the operating system can reduce energy usage
by various kinds of devices. But there is another approach as well: tell the pro-
grams to use less energy, even if this means providing a poorer user experience
(better a poorer experience than no experience when the battery dies and the lights
go out).  Typically, this information is passed on when the battery charge is below
some threshold.     It is then up to the programs to decide between degrading perfor-
mance to lengthen battery life or to maintain performance and risk running out of
energy.
One question that comes up here asks how a program can degrade its perfor-
mance     to  save  energy.  This  question  has  been  studied  by  Flinn  and    Satya-
narayanan (2004).   They provided four examples of how degraded performance
can save energy. We will now look at these.
In this study, information is presented to the user in various forms. When no
degradation is present, the best possible information is presented.  When degrada-
tion is present, the fidelity (accuracy) of the information presented to the user is
worse than what it could have been. We will see examples of this shortly.



426                       INPUT/OUTPUT                                         CHAP. 5
     In order to measure the energy usage, Flinn and Satyanarayanan devised a soft-
ware tool called PowerScope. What it does is provide a power-usage profile of a
program.   To use it, a computer must be hooked up to an external power supply
through a software-controlled digital multimeter. Using the multimeter, software is
able to read out the number of milliamperes coming in from the power supply and
thus determine the instantaneous power being consumed by the computer. What
PowerScope does is periodically sample the program counter and the power usage
and write these data to a file. After the program has terminated, the file is analyzed
to give the energy usage of each procedure. These measurements formed the basis
of  their  observations.  Hardware  energy-saving    measures  were  also  used         and
formed the baseline against which the degraded performance was measured.
     The first program measured was a video player.  In undegraded mode, it plays
30 frames/sec in full resolution and in color.  One form of degradation is to aban-
don the color information and display the video in black and white. Another form
of degradation is to reduce the frame rate, which leads to flicker and gives the
movie a jerky quality. Still another form of degradation is to reduce the number of
pixels in both directions, either by lowering the spatial resolution or making the
displayed image smaller. Measures of this type saved about 30% of the energy.
     The second program was a speech recognizer.     It sampled the microphone to
construct a waveform.     This waveform could either be analyzed on the notebook
computer or be sent over a radio link for analysis on a fixed computer. Doing this
saves CPU energy but uses energy for the radio. Degradation was accomplished by
using a smaller vocabulary and a simpler acoustic model. The win here was about
35%.
     The next example was a map viewer that fetched the map over the radio link.
Degradation consisted of either cropping the map to smaller dimensions or telling
the remote server to omit smaller roads, thus requiring fewer bits to be transmitted.
Again here a gain of about 35% was achieved.
     The fourth experiment was with transmission of JPEG images to a Web brow-
ser. The JPEG standard allows various algorithms, trading image quality against
file size. Here the gain averaged only 9%.      Still, all in all, the experiments showed
that by accepting some quality degradation, the user can run longer on a given bat-
tery.
5.9 RESEARCH ON INPUT/OUTPUT
     There is a fair amount of research on input/output. Some of it is focused on
specific devices, rather than I/O in general. Other work focuses on the entire I/O
infrastructure. For instance, the Streamline architecture aims to provide applica-
tion-tailored I/O that minimizes overhead due to copying, context switching, sig-
naling and poor use of the cache and TLB (DeBruijn et al., 2011). It builds on the
notion of Beltway Buffers, advanced circular buffers that are more efficient than



SEC. 5.9            RESEARCH ON INPUT/OUTPUT                                          427
existing buffering systems (DeBruijn and Bos, 2008). Streamline is especially use-
ful for demanding network applications. Megapipe (Han et al., 2012) is another
network I/O architecture for message-oriented workloads. It creates per-core bidi-
rectional channels between the kernel and user space, on which the systems layers
abstractions like lightweight sockets. The sockets are not quite POSIX-compliant,
so applications need to be adapted to benefit from the more efficient I/O.
Often, the goal of the research is to improve performance of a specific device
in one way or another. Disk systems are a case in point. Disk-arm scheduling algo-
rithms are an ever-popular research area. Sometimes the focus is on improved
peformance (Gonzalez-Ferez et al., 2012; Prabhakar et al., 2013; and Zhang et al.,
2012b) but sometimes it is on lower energy usage (Krish et al., 2013; Nijim et al.,
2013; and Zhang et al., 2012a).  With the popularity of server consolidation using
virtual machines, disk scheduling for virtualized systems has become a hot topic
(Jin et al., 2013; and Ling et al., 2012).
Not all topics are new though. That old standby, RAID, still gets plenty of
attention (Chen et al., 2013; Moon and Reddy; 2013; and Timcenko and Djordje-
vic, 2013) as do SSDs (Dayan et al., 2013; Kim et al., 2013; and Luo et al., 2013).
On the theoretical front, some researchers are looking at modeling disk systems in
order to better understand their performance under different workloads (Li et al.,
2013b; and Shen and Qi, 2013).
Disks are not the only I/O device in the spotlight. Another key research area
relating to I/O is networking.   Topics include energy usage (Hewage and Voigt,
2013; and Hoque et al., 2013), networks for data centers (Haitjema, 2013; Liu et
al., 2103; and Sun et al., 2013), quality of service (Gupta, 2013; Hemkumar and
Vinaykumar, 2012; and Lai and Tang, 2013), and performance (Han et al., 2012;
and Soorty, 2012).
Given the large number of computer scientists with notebook computers and
given the microscopic battery lifetime on most of them, it should come as no sur-
prise that there is tremendous interest in using software techniques to reduce power
consumption. Among the specialized topics being looked at are balancing the clock
speed on different cores to achieve sufficient performance without wasting power
(Hruby 2013), energy usage and quality of service (Holmbacka et al., 2013), esti-
mating energy usage in real time (Dutta et al., 2013), providing OS services to
manage energy usage (Weissel, 2012) examining the energy cost of security (Kabri
and Seret, 2009), and scheduling for multimedia (Wei et al., 2010).
Not everyone is interested in notebooks, though.  Some computer scientists
think big and want to save megawatts at data centers (Fetzer and Knauth, 2012;
Schwartz et al., 2012; Wang et al., 2013b; and Yuan et al., 2012).
At the other end of the spectrum, a very hot topic is energy use in sensor net-
works (Albath et al., 2013; Mikhaylov and Tervonen, 2013; Rasaneh and Baniro-
stam, 2013; and Severini et al., 2012).
Somewhat surprisingly, even the lowly clock is still a subject of research.           To
provide good resolution, some operating systems run the clock at 1000 Hz, which



428                              INPUT/OUTPUT                         CHAP. 5
leads to substantial overhead. Getting rid of this overhead is where the research
comes in (Tsafir et al., 2005).
     Similarly, interrupt latency is still a concern for research groups, especially in
the area of real-time operating systems. Since these are often found embedded in
critical systems (like controls of brake and steering systems), permitting interrupts
only at very specific preemption points enables the system to control the possible
interleavings and permits the use of formal verification to improve dependability
(Blackham et al., 2012).
     Device drivers are also still a very active research area. Many operating system
crashes are caused by buggy device drivers. In Symdrive, the authors present a
framework to test device drivers without actually talking to devices (Renzelmann
et al., 2012). As an alternative approach, Rhyzik et al. (2009) show how device
drivers can be constructed automatically from specifications, with fewer chances of
bugs.
     Thin clients are also a topic of interest, especially mobile devices connected to
the cloud (Hocking, 2011; and Tuan-Anh et al., 2013). Finally, there are some
papers on unusual topics such as buildings as big I/O devices (Dawson-Haggerty et
al., 2013).
5.10 SUMMARY
     Input/output is an often neglected, but important, topic.  A substantial fraction
of any operating system is concerned with I/O.  I/O can be accomplished in one of
three ways. First, there is programmed I/O, in which the main CPU inputs or out-
puts each byte or word and sits in a tight loop waiting until it can get or send the
next one. Second, there is interrupt-driven I/O, in which the CPU starts an I/O
transfer for a character or word and goes off to do something else until an interrupt
arrives signaling completion of the I/O.     Third, there is DMA, in which a separate
chip manages the complete transfer of a block of data, given an interrupt only
when the entire block has been transferred.
     I/O can be structured in four levels: the interrupt-service procedures, the device
drivers, the device-independent I/O software, and the I/O libraries and spoolers that
run in user space. The device drivers handle the details of running the devices and
providing uniform interfaces to the rest of the operating system. The device-inde-
pendent I/O software does things like buffering and error reporting.
     Disks come in a variety of types, including magnetic disks, RAIDs, flash
drives, and optical disks.  On rotating disks, disk arm scheduling algorithms can
often be used to improve disk performance, but the presence of virtual geometries
complicates matters.  By pairing two disks, a stable storage medium with certain
useful properties can be constructed.
     Clocks are used for keeping track of the real time, limiting how long processes
can run, handling watchdog timers, and doing accounting.



SEC. 5.10                               SUMMARY                                      429
    Character-oriented terminals have a variety of issues concerning special char-
acters that can be input and special escape sequences that can be output. Input can
be in raw mode or cooked mode, depending on how much control the program
wants over the input. Escape sequences on output control cursor movement and
allow for inserting and deleting text on the screen.
    Most UNIX systems use the X Window System as the basis of the user inter-
face.  It consists of programs that are bound to special libraries that issue drawing
commands and an X server that writes on the display.
    Many personal computers use GUIs for their output. These are based on the
WIMP paradigm: windows, icons, menus, and a pointing device.                 GUI-based pro-
grams are generally event driven, with keyboard, mouse, and other events being
sent to the program for processing as soon as they happen.       In UNIX systems, the
GUIs almost always run on top of X.
    Thin clients have some advantages over standard PCs, notably simplicity and
less maintenance for users.
    Finally, power management is a major issue for phones, tablets, and notebooks
because battery lifetimes are limited and for desktop and server machines because
of an organization's energy bills. Various techniques can be employed by the oper-
ating system to reduce power consumption. Programs can also help out by sacrific-
ing some quality for longer battery lifetimes.
                                        PROBLEMS
1.  Advances in chip technology have made it possible to put an entire controller, includ-
    ing all the bus access logic, on an inexpensive chip. How does that affect the model of
    Fig. 1-6?
2.  Given the speeds listed in Fig. 5-1, is it possible to scan documents from a scanner and
    transmit them over an 802.11g network at full speed? Defend your answer.
3.  Figure 5-3(b) shows one way of having memory-mapped I/O even in the presence of
    separate buses for memory and I/O devices, namely, to first try the memory bus and if
    that fails try the I/O bus.   A clever computer science student has thought of an im-
    provement on this idea: try both in parallel, to speed up the process of accessing I/O
    devices. What do you think of this idea?
4.  Explain    the  tradeoffs  between  precise  and  imprecise  interrupts  on  a  superscalar
    machine.
5.  A DMA controller has five channels. The controller is capable of requesting a 32-bit
    word every 40 nsec.  A response takes equally long. How fast does the bus have to be
    to avoid being a bottleneck?
6.  Suppose that a system uses DMA for data transfer from disk controller to main memo-
    ry. Further assume that it takes t1 nsec on average to acquire the bus and t2 nsec to
    transfer one word over the bus (t1 >> t2). After the CPU has programmed the DMA



430                                    INPUT/OUTPUT                                  CHAP. 5
     controller, how long will it take to transfer 1000 words from the disk controller to main
     memory, if (a) word-at-a-time mode is used, (b) burst mode is used? Assume that com-
     manding the disk controller requires acquiring the bus to send one word and acknowl-
     edging a transfer also requires acquiring the bus to send one word.
7.   One mode that some DMA controllers use is to have the device controller send the
     word to the DMA controller, which then issues a second bus request to write to mem-
     ory.  How can this mode be used to perform memory to memory copy?        Discuss any
     advantage or disadvantage of using this method instead of using the CPU to perform
     memory to memory copy.
8.   Suppose that a computer can read or write a memory word in 5 nsec. Also suppose that
     when an interrupt occurs, all 32 CPU registers, plus the program counter and PSW are
     pushed onto the stack. What is the maximum number of interrupts per second this ma-
     chine can process?
9.   CPU architects know that operating system writers hate imprecise interrupts. One way
     to please the OS folks is for the CPU to stop issuing new instructions when an interrupt
     is signaled, but allow all the instructions currently being executed to finish, then force
     the interrupt. Does this approach have any disadvantages? Explain your answer.
10.  In Fig. 5-9(b), the interrupt is not acknowledged until after the next character has been
     output to the printer. Could it have equally well been acknowledged right at the start of
     the interrupt service procedure?  If so, give one reason for doing it at the end, as in the
     text. If not, why not?
11.  A computer has a three-stage pipeline as shown in Fig. 1-7(a).       On each clock cycle,
     one new instruction is fetched from memory at the address pointed to by the PC and
     put into the pipeline and the PC advanced. Each instruction occupies exactly one mem-
     ory word.  The instructions already in the pipeline are each advanced one stage. When
     an interrupt occurs, the current PC is pushed onto the stack, and the PC is set to the ad-
     dress of the interrupt handler. Then the pipeline is shifted right one stage and the first
     instruction of the interrupt handler is fetched into the pipeline. Does this machine have
     precise interrupts? Defend your answer.
12.  A typical printed page of text contains 50 lines of 80 characters each. Imagine that a
     certain printer can print 6 pages per minute and that the time to write a character to the
     printer's output register is so short it can be ignored. Does it make sense to run this
     printer using interrupt-driven I/O if each character printed requires an interrupt that
     takes 50 sec all-in to service?
13.  Explain how an OS can facilitate installation of a new device without any need for
     recompiling the OS.
14.  In which of the four I/O software layers is each of the following done.
     (a) Computing the track, sector, and head for a disk read.
     (b) Writing commands to the device registers.
     (c) Checking to see if the user is permitted to use the device.
     (d) Converting binary integers to ASCII for printing.
15.  A local area network is used as follows. The user issues a system call to write data
     packets to the network.  The operating system then copies the data to a kernel buffer.



CHAP. 5                                 PROBLEMS                                              431
     Then it copies the data to the network controller board.  When all the bytes are safely
     inside the controller, they are sent over the network at a rate of 10 megabits/sec. The
     receiving network controller stores each bit a microsecond after it is sent. When the
     last bit arrives, the destination CPU is interrupted, and the kernel copies the newly arri-
     ved packet to a kernel buffer to inspect it. Once it has figured out which user the packet
     is for, the kernel copies the data to the user space. If we assume that each interrupt and
     its associated processing takes 1 msec, that packets are 1024 bytes (ignore the head-
     ers), and that copying a byte takes 1 sec, what is the maximum rate at which one
     process can pump data to another?  Assume that the sender is blocked until the work is
     finished at the receiving side and an acknowledgement comes back. For simplicity, as-
     sume that the time to get the acknowledgement back is so small it can be ignored.
16.  Why are output files for the printer normally spooled on disk before being printed?
17.  How much cylinder skew is needed for a 7200-RPM disk with a track-to-track seek
     time of 1 msec? The disk has 200 sectors of 512 bytes each on each track.
18.  A disk rotates at 7200 RPM. It has 500 sectors of 512 bytes around the outer cylinder.
     How long does it take to read a sector?
19.  Calculate the maximum data rate in bytes/sec for the disk described in the previous
     problem.
20.  RAID level 3 is able to correct single-bit errors using only one parity drive. What is the
     point of RAID level 2?  After all, it also can only correct one error and takes more
     drives to do so.
21.  A RAID can fail if two or more of its drives crash within a short time interval. Suppose
     that the probability of one drive crashing in a given hour is p.  What is the probability
     of a k-drive RAID failing in a given hour?
22.  Compare RAID level 0 through 5 with respect to read performance, write performance,
     space overhead, and reliability.
23.  How many pebibytes are there in a zebibyte?
24.  Why are optical storage devices inherently capable of higher data density than mag-
     netic storage devices?  Note: This problem requires some knowledge of high-school
     physics and how magnetic fields are generated.
25.  What are the advantages and disadvantages of optical disks versus magnetic disks?
26.  If a disk controller writes the bytes it receives from the disk to memory as fast as it re-
     ceives them, with no internal buffering, is interleaving conceivably useful? Discuss
     your answer.
27.  If a disk has double interleaving, does it also need cylinder skew in order to avoid
     missing data when making a track-to-track seek? Discuss your answer.
28.  Consider a magnetic disk consisting of 16 heads and 400 cylinders. This disk has four
     100-cylinder zones with the cylinders in different zones containing 160, 200, 240. and
     280 sectors, respectively. Assume that each sector contains 512 bytes, average seek
     time between adjacent cylinders is 1 msec, and the disk rotates at 7200 RPM.             Calcu-
     late the (a) disk capacity, (b) optimal track skew, and (c) maximum data transfer rate.



432                                    INPUT/OUTPUT                                    CHAP. 5
29.  A disk manufacturer has two 5.25-inch disks that each have 10,000 cylinders. The
     newer one has double the linear recording density of the older one. Which disk proper-
     ties are better on the newer drive and which are the same? Are any worse on the newer
     one?
30.  A computer manufacturer decides to redesign the partition table of a Pentium hard disk
     to provide more than four partitions. What are some consequences of this change?
31.  Disk requests come in to the disk driver for cylinders 10, 22, 20, 2, 40, 6, and 38, in
     that order. A seek takes 6 msec per cylinder. How much seek time is needed for
     (a) First-come, first served.
     (b) Closest cylinder next.
     (c) Elevator algorithm (initially moving upward).
     In all cases, the arm is initially at cylinder 20.
32.  A slight modification of the elevator algorithm for scheduling disk requests is to al-
     ways scan in the same direction. In what respect is this modified algorithm better than
     the elevator algorithm?
33.  A personal computer salesman visiting a university in South-West Amsterdam remark-
     ed during his sales pitch that his company had devoted substantial effort to making
     their version of UNIX very fast.     As an example, he noted that their disk driver used
     the elevator algorithm and also queued multiple requests within a cylinder in sector
     order.  A student, Harry Hacker, was impressed and bought one.        He took it home and
     wrote a program to randomly read 10,000 blocks spread across the disk. To his amaze-
     ment, the performance that he measured was identical to what would be expected from
     first-come, first-served. Was the salesman lying?
34.  In the discussion of stable storage using nonvolatile RAM, the following point was
     glossed over. What happens if the stable write completes but a crash occurs before the
     operating system can write an invalid block number in the nonvolatile RAM?               Does
     this race condition ruin the abstraction of stable storage? Explain your answer.
35.  In the discussion on stable storage, it was shown that the disk can be recovered to a
     consistent state (a write either completes or does not take place at all) if a CPU crash
     occurs during a write. Does this property hold if the CPU crashes again during a recov-
     ery procedure. Explain your answer.
36.  In the discussion on stable storage, a key assumption is that a CPU crash that corrupts
     a sector leads to an incorrect ECC. What problems might arise in the five crash-recov-
     ery scenarios shown in Figure 5-27 if this assumption does not hold?
37.  The clock interrupt handler on a certain computer requires 2 msec (including process
     switching overhead) per clock tick. The clock runs at 60 Hz. What fraction of the CPU
     is devoted to the clock?
38.  A computer uses a programmable clock in square-wave mode. If a 500 MHz crystal is
     used, what should be the value of the holding register to achieve a clock resolution of
     (a) a millisecond (a clock tick once every millisecond)?
     (b) 100 microseconds?



CHAP. 5                                    PROBLEMS                                     433
39.  A system simulates multiple clocks by chaining all pending clock requests together as
     shown in Fig. 5-30. Suppose the current time is 5000 and there are pending clock re-
     quests for time 5008, 5012, 5015, 5029, and 5037. Show the values of Clock header,
     Current time, and Next signal at times 5000, 5005, and 5013. Suppose a new (pending)
     signal arrives at time 5017 for 5033. Show the values of Clock header, Current time
     and Next signal at time 5023.
40.  Many versions of UNIX use an unsigned 32-bit integer to keep track of the time as the
     number of seconds since the origin of time. When will these systems wrap around
     (year and month)? Do you expect this to actually happen?
41.  A bitmap terminal contains 1600 by 1200 pixels.  To scroll a window, the CPU (or
     controller) must move all the lines of text upward by copying their bits from one part
     of the video RAM to another.   If a particular window is 80 lines high by 80 characters
     wide (6400 characters, total), and a character's box is 8 pixels wide by 16 pixels high,
     how long does it take to scroll the whole window at a copying rate of 50 nsec per byte?
     If all lines are 80 characters long, what is the equivalent baud rate of the terminal?
     Putting a character on the screen takes 5 sec. How many lines per second can be dis-
     played?
42.  After receiving a DEL (SIGINT) character, the display driver discards all output cur-
     rently queued for that display. Why?
43.  A user at a terminal issues a command to an editor to delete the word on line 5 occupy-
     ing character positions 7 through and including 12. Assuming the cursor is not on line
     5 when the command is given, what ANSI escape sequence should the editor emit to
     delete the word?
44.  The designers of a computer system expected that the mouse could be moved at a max-
     imum rate of 20 cm/sec.  If a mickey is 0.1 mm and each mouse message is 3 bytes,
     what is the maximum data rate of the mouse assuming that each mickey is reported
     separately?
45.  The primary additive colors are red, green, and blue, which means that any color can
     be constructed from a linear superposition of these colors.     Is it possible that someone
     could have a color photograph that cannot be represented using full 24-bit color?
46.  One way to place a character on a bitmapped screen is to use BitBlt from a font table.
     Assume that a particular font uses characters that are 16 Ã 24 pixels in true RGB color.
     (a) How much font table space does each character take?
     (b) If copying a byte takes 100 nsec, including overhead, what is the output rate to the
     screen in characters/sec?
47.  Assuming that it takes 2 nsec to copy a byte, how much time does it take to completely
     rewrite the screen of an 80 character Ã 25 line text mode memory-mapped screen?
     What about a 1024 Ã 768 pixel graphics screen with 24-bit color?
48.  In Fig. 5-36 there is a class to RegisterClass. In the corresponding X Window code, in
     Fig. 5-34, there is no such call or anything like it. Why not?
49.  In the text we gave an example of how to draw a rectangle on the screen using the Win-
     dows GDI:



434                                   INPUT/OUTPUT                            CHAP. 5
     Rectangle(hdc, xleft, ytop, xright, ybottom);
     Is there any real need for the first parameter (hdc), and if so, what? After all, the coor-
     dinates of the rectangle are explicitly specified as parameters.
50.  A thin-client terminal is used to display a Web page containing an animated cartoon of
     size 400 pixels Ã 160 pixels running at 10 frames/sec. What fraction of a 100-Mbps
     Fast Ethernet is consumed by displaying the cartoon?
51.  It has been observed that a thin-client system works well with a 1-Mbps network in a
     test. Are any problems likely in a multiuser situation?   (Hint: Consider a large number
     of users watching a scheduled TV show and the same number of users browsing the
     World Wide Web.)
52.  Describe two advantages and two disadvantages of thin client computing?
53.  If a CPU's maximum voltage, V , is cut to V /n, its power consumption drops to 1/n2 of
     its original value and its clock speed drops to 1/n of its original value. Suppose that a
     user is typing at 1 char/sec, but the CPU time required to process each character is 100
     msec. What is the optimal value of n and what is the corresponding energy saving in
     percent compared to not cutting the voltage?   Assume that an idle CPU consumes no
     energy at all.
54.  A notebook computer is set up to take maximum advantage of power saving features
     including shutting down the display and the hard disk after periods of inactivity. A user
     sometimes runs UNIX programs in text mode, and at other times uses the X Window
     System. She is surprised to find that battery life is significantly better when she uses
     text-only programs. Why?
55.  Write a program that simulates stable storage. Use two large fixed-length files on your
     disk to simulate the two disks.
56.  Write a program to implement the three disk-arm scheduling algorithms. Write a driver
     program that generates a sequence of cylinder numbers (0Â­999) at random, runs the
     three algorithms for this sequence and prints out the total distance (number of cylin-
     ders) the arm needs to traverse in the three algorithms.
57.  Write a program to implement multiple timers using a single clock. Input for this pro-
     gram consists of a sequence of four types of commands (S <int>, T, E <int>, P): S
     <int> sets the current time to <int>; T is a clock tick; and E <int> schedules a signal to
     occur at time <int>; P prints out the values of Current time, Next signal, and Clock
     header. Your program should also print out a statement whenever it is time to raise a
     signal.
