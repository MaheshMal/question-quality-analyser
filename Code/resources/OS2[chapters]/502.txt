VIRTUALIZATION AND THE CLOUD


                             7
VIRTUALIZATION AND THE CLOUD
In some situations, an organization has a multicomputer but does not actually
want it. A common example is where a company has an email server, a Web server,
an FTP server, some e-commerce servers, and others. These all run on different
computers in the same equipment rack, all connected by a high-speed network, in
other words, a multicomputer. One reason all these servers run on separate ma-
chines may be that one machine cannot handle the load, but another is reliability:
management simply does not trust the operating system to run 24 hours a day, 365
or 366 days a year, with no failures. By putting each service on a separate com-
puter, if one of the servers crashes, at least the other ones are not affected. This is
good for security also. Even if some malevolent intruder manages to compromise
the Web server, he will not immediately have access to sensitive emails also--a
property sometimes referred to as sandboxing.  While isolation and fault tolerance
are achieved this way, this solution is expensive and hard to manage because so
many machines are involved.
Mind you, these are just two out of many reasons for keeping separate ma-
chines.  For instance, organizations often depend on more than one operating sys-
tem for their daily operations: a Web server on Linux, a mail server on Windows,
an e-commerce server for customers running on OS X, and a few other services
running on various flavors of UNIX. Again, this solution works, but cheap it is def-
initely not.
What to do? A possible (and popular) solution is to use virtual machine tech-
nology, which sounds very hip and modern, but the idea is old, dating back to the
                             471



472                     VIRTUALIZATION AND THE CLOUD                           CHAP. 7
1960s. Even so, the way we use it today is definitely new.  The main idea is that a
VMM (Virtual Machine Monitor) creates the illusion of multiple (virtual) ma-
chines on the same physical hardware. A VMM is also known as a hypervisor. As
discussed in Sec. 1.7.5, we distinguish between type 1 hypervisors which run on
the bare metal, and type 2 hypervisors that may make use of all the wonderful ser-
vices and abstractions offered by an underlying operating system. Either way, vir-
tualization allows a single computer to host multiple virtual machines, each poten-
tially running a completely different operating system.
     The advantage of this approach is that a failure in one virtual machine does not
bring down any others.  On a virtualized system, different servers can run on dif-
ferent virtual machines, thus maintaining the partial-failure model that a multicom-
puter has, but at a lower cost and with easier maintainability.  Moreover, we can
now run multiple different operating systems on the same hardware, benefit from
virtual machine isolation in the face of attacks, and enjoy other good stuff.
     Of course, consolidating servers like this is like putting all your eggs in one
basket. If the server running all the virtual machines fails, the result is even more
catastrophic than the crashing of a single dedicated server. The reason virtuali-
zation works, however, is that most service outages are due not to faulty hardware,
but to ill-designed, unreliable, buggy and poorly configured software, emphatically
including operating systems. With virtual machine technology, the only software
running in the highest privilege mode is the hypervisor, which has two orders of
magnitude fewer lines of code than a full operating system, and thus two orders of
magnitude fewer bugs.   A hypervisor is simpler than an operating system because
it does only one thing: emulate multiple copies of the bare metal (most commonly
the Intel x86 architecture).
     Running software in virtual machines has other advantages in addition to
strong isolation.  One of them is that having fewer physical machines saves money
on hardware and electricity and takes up less rack space.   For a company such as
Amazon or Microsoft, which may have hundreds of thousands of servers doing a
huge variety of different tasks at each data center, reducing the physical demands
on their data centers represents a huge cost savings. In fact, server companies fre-
quently locate their data centers in the middle of nowhere--just to be close to, say,
hydroelectric dams (and cheap energy). Virtualization also helps in trying out new
ideas. Typically, in large companies, individual departments or groups think of an
interesting idea and then go out and buy a server to implement it. If the idea
catches on and hundreds or thousands of servers are needed, the corporate data
center expands.    It is often hard to move the software to existing machines because
each application often needs a different version of the operating system, its own li-
braries, configuration files, and more. With virtual machines, each application can
take its own environment with it.
     Another advantage of virtual machines is that checkpointing and migrating vir-
tual machines (e.g., for load balancing across multiple servers) is much easier than
migrating processes running on a normal operating system. In the latter case, a fair



SEC. 7.1                         HISTORY                                               473
amount of critical state information about every process is kept in operating system
tables, including information relating to open files, alarms, signal handlers, and
more. When migrating a virtual machine, all that have to be moved are the memory
and disk images, since all the operating system tables move, too.
Another use for virtual machines is to run legacy applications on operating sys-
tems (or operating system versions) no longer supported or which do not work on
current hardware. These can run at the same time and on the same hardware as cur-
rent applications. In fact, the ability to run at the same time applications that use
different operating systems is a big argument in favor of virtual machines.
Yet another important use of virtual machines is for software development.             A
programmer who wants to make sure his software works on Windows 7, Windows
8, several versions of Linux, FreeBSD, OpenBSD, NetBSD, and OS X, among
other systems no longer has to get a dozen computers and install different operat-
ing systems on all of them. Instead, he merely creates a dozen virtual machines on
a single computer and installs a different operating system on each one. Of course,
he could have partitioned the hard disk and installed a different operating system in
each partition, but that approach is more difficult.  First of all, standard PCs sup-
port only four primary disk partitions, no matter how big the disk is.       Second, al-
though a multiboot program could be installed in the boot block, it would be neces-
sary to reboot the computer to work on a new operating system.     With virtual ma-
chines, all of them can run at once, since they are really just glorified processes.
Perhaps the most important and buzzword-compliant use case for virtualization
nowadays is found in the cloud.  The key idea of a cloud is straightforward: out-
source your computation or storage needs to a well-managed data center run by a
company specializing in this and staffed by experts in the area. Because the data
center typically belongs to someone else, you will probably have to pay for the use
of the resources, but at least you will not have to worry about the physical ma-
chines, power, cooling, and maintenance. Because of the isolation offered by virtu-
alizaton, cloud-providers can allow multiple clients, even competitors, to share a
single physical machine. Each client gets a piece of the pie.      At the risk of stretch-
ing the cloud metaphor, we mention that early critics maintained that the pie was
only in the sky and that real organizations would not want to put their sensitive
data and computations on someone else's resources. By now, however, virtualized
machines in the cloud are used by countless organization for countless applica-
tions, and while it may not be for all organizations and all data, there is no doubt
that cloud computing has been a success.
7.1 HISTORY
With all the hype surrounding virtualizaton in recent years, we sometimes for-
get that by Internet standards virtual machines are ancient. As early as the 1960s.
IBM experimented with not just one but two independently developed hypervisors:



474                   VIRTUALIZATION AND THE CLOUD     CHAP. 7
SIMMON and CP-40.     While CP-40 was a research project, it was reimplemented
as CP-67 to form the control program of CP/CMS, a virtual machine operating
system for the IBM System/360 Model 67. Later, it was reimplemented again and
released as VM/370 for the System/370 series in 1972.  The System/370 line was
replaced by IBM in the 1990s by the System/390.        This was basically a name
change since the underlying architecture remained the same for reasons of back-
ward compatibility. Of course, the hardware technology was improved and the
newer machines were bigger and faster than the older ones, but as far as virtualiza-
tion was concerned, nothing changed.  In 2000, IBM released the z-series, which
supported 64-bit virtual address spaces but was otherwise backward compatible
with the System/360.  All of these systems supported virtualization decades before
it became popular on the x86.
     In 1974, two computer scientists at UCLA, Gerald Popek and Robert Gold-
berg, published a seminal paper (``Formal Requirements for Virtualizable Third
Generation Architectures'') that listed exactly what conditions a computer architec-
ture should satisfy in order to support virtualization efficiently (Popek and Gold-
berg, 1974).  It is impossible to write a chapter on virtualization without referring
to their work and terminology.      Famously, the well-known x86 architecture that
also originated in the 1970s did not meet these requirements for decades. It was not
the only one. Nearly every architecture since the mainframe also failed the test.
The 1970s were very productive, seeing also the birth of UNIX, Ethernet, the
Cray-1, Microsoft, and Apple--so, despite what your parents may say, the 1970s
were not just about disco!
     In fact, the real Disco revolution started in the 1990s, when researchers at Stan-
ford University developed a new hypervisor by that name and went on to found
VMware, a virtualization giant that offers type 1 and type 2 hypervisors and now
rakes in billions of dollars in revenue (Bugnion et al., 1997, Bugnion et al., 2012).
Incidentally, the distinction between ``type 1'' and ``type 2'' hypervisors is also
from the seventies (Goldberg, 1972).  VMware introduced its first virtualization
solution for x86 in 1999. In its wake other products followed: Xen, KVM, Virtu-
alBox, Hyper-V, Parallels, and many others. It seems the time was right for virtu-
alization, even though the theory had been nailed down in 1974 and for decades
IBM had been selling computers that supported--and heavily used--virtualization.
In 1999, it became popular among the masses, but new it was not, despite the mas-
sive attention it suddenly gained.
7.2 REQUIREMENTS FOR VIRTUALIZATION
     It is important that virtual machines act just like the real McCoy. In particular,
it must be possible to boot them like real machines and install arbitrary operating
systems on them, just as can be done on the real hardware. It is the task of the



SEC. 7.2            REQUIREMENTS FOR VIRTUALIZATION                                     475
hypervisor to provide this illusion and to do it efficiently.  Indeed, hypervisors
should score well in three dimensions:
     1.   Safety: the hypervisor should have full control of the virtualized re-
          sources.
     2.   Fidelity: the behavior of a program on a virtual machine should be
          identical to that of the same program running on bare hardware.
     3.   Efficiency: much of the code in the virtual machine should run with-
          out intervention by the hypervisor.
     An unquestionably safe way to execute the instructions is to consider each in-
struction in turn in an interpreter (such as Bochs) and perform exactly what is
needed for that instruction. Some instructions can be executed directly, but not too
many. For instance, the interpreter may be able to execute an INC (increment) in-
struction simply as is, but instructions that are not safe to execute directly must be
simulated by the interpreter. For instance, we cannot really allow the guest operat-
ing system to disable interrupts for the entire machine or modify the page-table
mappings. The trick is to make the operating system on top of the hypervisor think
that it has disabled interrupts, or changed the machine's page mappings. We will
see how this is done later.  For now, we just want to say that the interpreter may be
safe, and if carefully implemented, perhaps even hi-fi, but the performance sucks.
To also satisfy the performance criterion, we will see that VMMs try to execute
most of the code directly.
     Now let us turn to fidelity. Virtualization has long been a problem on the x86
architecture due to defects in the Intel 386 architecture that were slavishly carried
forward into new CPUs for 20 years in the name of backward compatibility.               In a
nutshell, every CPU with kernel mode and user mode has a set of instructions that
behave differently when executed in kernel mode than when executed in user
mode.    These include instructions that do I/O, change the MMU settings, and so
on.  Popek and Goldberg called these sensitive instructions.   There is also a set of
instructions that cause a trap if executed in user mode.  Popek and Goldberg called
these privileged instructions.  Their paper stated for the first time that a machine
is virtualizable only if the sensitive instructions are a subset of the privileged in-
structions.  In simpler language, if you try to do something in user mode that you
should not be doing in user mode, the hardware should trap.    Unlike the IBM/370,
which had this property, Intel's 386 did not.  Quite a few sensitive 386 instructions
were ignored if executed in user mode or executed with different behavior. For ex-
ample, the POPF instruction replaces the flags register, which changes the bit that
enables/disables interrupts. In user mode, this bit is simply not changed.        As a
consequence, the 386 and its successors could not be virtualized, so they could not
support a hypervisor directly.
     Actually, the situation is even worse than sketched. In addition to the problems
with instructions that fail to trap in user mode, there are instructions that can read



476               VIRTUALIZATION AND THE CLOUD                                 CHAP. 7
sensitive state in user mode without causing a trap.  For example, on x86 proces-
sors prior to 2005, a program can determine whether it is running in user mode or
kernel mode by reading its code-segment selector.     An operating system that did
this and discovered that it was actually in user mode might make an incorrect de-
cision based on this information.
     This problem was finally solved when Intel and AMD introduced virtualization
in their CPUs starting in 2005 (Uhlig, 2005).    On the Intel CPUs it is called VT
(Virtualization Technology); on the AMD CPUs it is called SVM (Secure Vir-
tual Machine).    We will use the term VT in a generic sense below.        Both were
inspired by the IBM VM/370 work, but they are slightly different.        The basic idea
is to create containers in which virtual machines can be run. When a guest operat-
ing system is started up in a container, it continues to run there until it causes an
exception and traps to the hypervisor, for example, by executing an I/O instruction.
The set of operations that trap is controlled by a hardware bitmap set by the hyper-
visor. With these extensions the classical trap-and-emulate virtual machine ap-
proach becomes possible.
     The astute reader may have noticed an apparent contradiction in the descrip-
tion thus far. On the one hand, we have said that x86 was not virtualizable until the
architecture  extensions  introduced  in  2005.  On   the  other  hand,    we  saw       that
VMware launched its first x86 hypervisor in 1999. How can both be true at the
same time?    The answer is that the hypervisors before 2005 did not really run the
original guest operating system. Rather, they rewrote part of the code on the fly to
replace problematic instructions with safe code sequences that emulated the origi-
nal instruction.  Suppose, for instance, that the guest operating system performed a
privileged I/O instruction, or modified one of the CPU's privileged control regis-
ters (like the CR3 register which contains a pointer to the page directory). It is im-
portant that the consequences of such instructions are limited to this virtual ma-
chine and do not affect other virtual machines, or the hypervisor itself.      Thus, an
unsafe I/O instruction was replaced by a trap that, after a safety check, performed
an equivalent instruction and returned the result. Since we are rewriting, we can
use the trick to replace instructions that are sensitive, but not privileged. Other in-
structions execute natively. The technique is known as binary translation; we will
discuss it more detail in Sec. 7.4.
     There is no need to rewrite all sensitive instructions. In particular, user proc-
esses on the guest can typically run without modification. If the instruction is non-
privileged but sensitive and behaves differently in user processes than in the kernel,
that is fine. We are running it in userland anyway. For sensitive instructions that are
privileged, we can resort to the classical trap-and-emulate, as usual. Of course, the
VMM must ensure that it receives the corresponding traps. Typically, the VMM
has a module that executes in the kernel and redirects the traps to its own handlers.
     A different form of virtualization is known as paravirtualization.        It is quite
different from full virtualization, because it never even aims to present a virtual
machine that looks just like the actual underlying hardware. Instead, it presents a



SEC. 7.2             REQUIREMENTS FOR VIRTUALIZATION                                     477
machine-like software interface that explicitly exposes the fact that it is a virtu-
alized environment. For instance, it offers a set of hypercalls, which allow the
guest to send explicit requests to the hypervisor (much as a system call offers ker-
nel services to applications). Guests use hypercalls for privileged sensitive opera-
tions like updating the page tables, but because they do it explicitly in cooperation
with the hypervisor, the overall system can be simpler and faster.
     It should not come as a surprise that paravirtualization is nothing new either.
IBM's VM operating system has offered such a facility, albeit under a different
name, since 1972. The idea was revived by the Denali (Whitaker et al., 2002) and
Xen  (Barham     et  al.,  2003)  virtual  machine  monitors.  Compared  to    full  virtu-
alization, the drawback of paravirtualization is that the guest has to be aware of the
virtual machine API. Typically, this means it should be customized explicitly for
the hypervisor.
     Before we delve more deeply into type 1 and type 2 hypervisors, it is important
to mention that not all virtualization technology tries to trick the guest into believ-
ing that it has the entire system. Sometimes, the aim is simply to allow a process to
run that was originally written for a different operating system and/or architecture.
We therefore distinguish between full system virtualization and process-level vir-
tualization.  While we focus on the former in the remainder of this chapter, proc-
ess-level virtualization technology is used in practice also. Well-known examples
include the WINE compatibility layer that allows Windows application to run on
POSIX-compliant systems like Linux, BSD, and OS X, and the process-level ver-
sion of the QEMU emulator that allows applications for one architecture to run on
another.
7.3 TYPE 1 AND TYPE 2 HYPERVISORS
     Goldberg (1972) distinguished between two approaches to virtualization.             One
kind of hypervisor, dubbed a type 1 hypervisor is illustrated in Fig. 7-1(a).        Tech-
nically, it is like an operating system, since it is the only program running in the
most privileged mode. Its job is to support multiple copies of the actual hardware,
called virtual machines, similar to the processes a normal operating system runs.
     In contrast, a type 2 hypervisor, shown in Fig. 7-1(b), is a different kind of
animal.   It is a program that relies on, say, Windows or Linux to allocate and
schedule resources, very much like a regular process. Of course, the type 2 hyper-
visor still pretends to be a full computer with a CPU and various devices. Both
types of hypervisor must execute the machine's instruction set in a safe manner.
For instance, an operating system running on top of the hypervisor may change and
even mess up its own page tables, but not those of others.
     The operating system running on top of the hypervisor in both cases is called
the guest operating system. For a type 2 hypervisor, the operating system running
on the hardware is called the host operating system.        The first type 2 hypervisor



478                     VIRTUALIZATION AND THE CLOUD                                             CHAP. 7
                                                         Guest OS process
       Excel Word Mplayer Emacs                                                   Host OS
                                                                                  process
                                                         Guest OS
                                       Control           (e.g., Windows)
       Windows          Linux          Domain            Type 2 hypervisor
                    Type 1 hypervisor                             Host OS
                                                                  (e.g., Linux)
                        Hardware                                  Hardware
       (CPU, disk, network, interrupts, etc.)            (CPU, disk, network, interrupts, etc.)
                    Figure 7-1. Location of     type  1  and type 2 hypervisors.
on the x86 market was VMware Workstation (Bugnion et al., 2012). In this sec-
tion, we introduce the general idea. A study of VMware follows in Sec. 7.12.
     Type 2 hypervisors, sometimes referred to as hosted hypervisors, depend for
much of their functionality on a host operating system such as Windows, Linux, or
OS X.  When it starts for the first time, it acts like a newly booted computer and
expects to find a DVD, USB drive, or CD-ROM containing an operating system in
the drive. This time, however, the drive could be a virtual device. For instance, it is
possible to store the image as an ISO file on the hard drive of the host and have the
hypervisor pretend it is reading from a proper DVD drive. It then installs the oper-
ating system to its virtual disk (again really just a Windows, Linux, or OS X file)
by running the installation program found on the DVD.              Once the guest operating
system is installed on the virtual disk, it can be booted and run.
     The various categories of virtualization we have discussed are summarized in
the table of Fig. 7-2 for both type 1 and type 2 hypervisors.               For each combination
of hypervisor and kind of virtualization, some examples are given.
     Virtualizaton method                      Type 1 hypervisor            Type 2 hypervisor
Virtualization without HW support      ESX Server 1.0             VMware Workstation 1
Paravirtualization                     Xen 1.0
Virtualization with HW support         vSphere, Xen, Hyper-V      VMware Fusion, KVM, Parallels
Process virtualization                                            Wine
       Figure 7-2. Examples of hypervisors. Type 1 hypervisors run on the bare metal
       whereas type 2 hypervisors use the services of an existing host operating system.
7.4 TECHNIQUES FOR EFFICIENT VIRTUALIZATION
     Virtualizability and performance are important issues, so let us examine them
more closely.  Assume, for the moment, that we have a type 1 hypervisor sup-
porting one virtual machine, as shown in Fig. 7-3.                Like all type 1 hypervisors, it



SEC. 7.4        TECHNIQUES FOR EFFICIENT VIRTUALIZATION                                               479
runs on the bare metal.  The virtual machine runs as a user process in user mode,
and as such is not allowed to execute sensitive instructions (in the Popek-Goldberg
sense). However, the virtual machine runs a guest operating system that thinks it is
in kernel mode (although, of course, it is not).  We will call this virtual kernel
mode.  The virtual machine also runs user processes, which think they are in user
mode (and really are in user mode).
                         User process
                                                  Virtual user mode
       Virtual                                                                                User
       machine                                                                                mode
                Guest operating system            Virtual kernel mode
                Type 1 hypervisor            Trap on privileged instruction                   Kernel
                                                                                              mode
                                   Hardware
       Figure 7-3. When the operating system in a virtual machine executes a kernel-
       only instruction, it traps to the hypervisor if virtualization technology is present.
What happens when the guest operating system (which thinks it is in kernel
mode) executes an instruction that is allowed only when the CPU really is in kernel
mode? Normally, on CPUs without VT, the instruction fails and the operating sys-
tem crashes. On CPUs with VT, when the guest operating system executes a sensi-
tive instruction, a trap to the hypervisor does occur, as illustrated in Fig. 7-3.                    The
hypervisor can then inspect the instruction to see if it was issued by the guest oper-
ating system in the virtual machine or by a user program in the virtual machine.                      In
the former case, it arranges for the instruction to be carried out; in the latter case, it
emulates what the real hardware would do when confronted with a sensitive in-
struction executed in user mode.
7.4.1 Virtualizing the Unvirtualizable
Building a virtual machine system is relatively straightforward when VT is
available, but what did people do before that? For instance, VMware released a
hypervisor well before the arrival of the virtualization extensions on the x86.
Again, the answer is that the software engineers who built such systems made
clever use of binary translation and hardware features that did exist on the x86,
such as the processor's protection rings.
For many years, the x86 has supported four protection modes or rings. Ring 3
is the least privileged. This is where normal user processes execute. In this ring,
you cannot execute privileged instructions. Ring 0 is the most privileged ring that
allows the execution of any instruction. In normal operation, the kernel runs in



480                   VIRTUALIZATION AND THE CLOUD                                         CHAP. 7
ring 0.  The remaining two rings are not used by any current operating system. In
other words, hypervisors were free to use them as they pleased.       As shown in
Fig. 7-4, many virtualization solutions therefore kept the hypervisor in kernel mode
(ring 0) and the applications in user mode (ring 3), but put the guest operating sys-
tem in a layer of intermediate privilege (ring 1). As a result, the kernel is privileged
relative to the user processes and any attempt to access kernel memory from a user
program leads to an access violation. At the same time, the guest operating sys-
tem's privileged instructions trap to the hypervisor. The hypervisor does some san-
ity checks and then performs the instructions on the guest's behalf.
                         User process
                                                                                           ring 3
     Virtual                                                                               ring 2
     machine
                         Guest operating system                                            ring 1
              (Rewrite binary prior to execution + emulate)
                         Type 1 hypervisor                                                 ring 0
                         Hardware
         Figure 7-4. The binary translator rewrites the guest operating system running in
         ring 1, while the hypervisor runs in ring 0.
     As for the sensitive instructions in the guest's kernel code: the hypervisor
makes sure they no longer exist. To do so, it rewrites the code, one basic block at a
time. A basic block is a short, straight-line sequence of instructions that ends with
a branch. By definition, a basic block contains no jump, call, trap, return, or other
instruction that alters the flow of control, except for the very last instruction which
does precisely that.  Just prior to executing a basic block, the hypervisor first scans
it to see if it contains sensitive instructions (in the Popek and Goldberg sense), and
if so, replaces them with a call to a hypervisor procedure that handles them. The
branch on the last instruction is also replaced by a call into the hypervisor (to make
sure it can repeat the procedure for the next basic block).      Dynamic translation and
emulation sound expensive, but typically are not. Translated blocks are cached, so
no translation is needed in the future. Also, most code blocks do not contain sensi-
tive or privileged instructions and thus can executes natively.  In particular, as long
as the hypervisor configures the hardware carefully (as is done, for instance, by
VMware), the binary translator can ignore all user processes; they execute in non-
privileged mode anyway.
     After a basic block has completed executing, control is returned to the hypervi-
sor, which then locates its successor.  If the successor has already been translated,



SEC. 7.4     TECHNIQUES FOR EFFICIENT VIRTUALIZATION                                      481
it can be executed immediately.    Otherwise, it is first translated, cached, then ex-
ecuted.   Eventually, most of the program will be in the cache and run at close to
full speed.  Various optimizations are used, for example, if a basic block ends by
jumping to (or calling) another one, the final instruction can be replaced by a jump
or call directly to the translated basic block, eliminating all overhead associated
with finding the successor block.  Again, there is no need to replace sensitive in-
structions in user programs; the hardware will just ignore them anyway.
On the other hand, it is common to perform binary translation on all the guest
operating system code running in ring 1 and replace even the privileged sensitive
instructions that, in principle, could be made to trap also. The reason is that traps
are very expensive and binary translation leads to better performance.
So far we have described a type 1 hypervisor. Although type 2 hypervisors are
conceptually different from type 1 hypervisors, they use, by and large, the same
techniques. For instance, VMware ESX Server (a type 1 hypervisor first shipped in
2001) used exactly the same binary translation as the first VMware Workstation (a
type 2 hypervisor released two years earlier).
However, to run the guest code natively and use exactly the same techniques
requires the type 2 hypervisor to manipulate the hardware at the lowest level,
which cannot be done from user space. For instance, it has to set the segment de-
scriptors to exactly the right value for the guest code.  For faithful virtualization,
the guest operating system should also be tricked into thinking that it is the true
and only king of the mountain with full control of all the machine's resources and
with access to the entire address space (4 GB on 32-bit machines). When the king
finds another king (the host kernel) squatting in its address space, the king will not
be amused.
Unfortunately, this is exactly what happens when the guest runs as a user proc-
ess on a regular operating system. For instance, in Linux a user process has access
to just 3 GB of the 4-GB address space, as the remaining 1 GB is reserved for the
kernel. Any access to the kernel memory leads to a trap.  In principle, it is possible
to take the trap and emulate the appropriate actions, but doing so is expensive and
typically requires installing the appropriate trap handler in the host kernel. Another
(obvious) way to solve the two-kings problem, is to reconfigure the system to re-
move the host operating system and actually give the guest the entire address
space. However, doing so is clearly not possible from user space either.
Likewise, the hypervisor needs to handle the interrupts to do the right thing,
for instance when the disk sends an interrupt or a page fault occurs. Also, if the
hypervisor wants to use trap-and-emulate for privileged instructions, it needs to re-
ceive the traps. Again, installing trap/interrupt handlers in the kernel is not possible
for user processes.
Most modern type 2 hypervisors therefore have a kernel module operating in
ring 0 that allows them to manipulate the hardware with privileged instructions. Of
course, manipulating the hardware at the lowest level and giving the guest access
to the full address space is all well and good, but at some point the hypervisor



482                       VIRTUALIZATION AND THE CLOUD                       CHAP. 7
needs  to  clean  it  up  and  restore   the  original  processor  context.  Suppose,    for
instance, that the guest is running when an interrupt arrives from an external de-
vice. Since a type 2 hypervisor depends on the host's device drivers to handle the
interrupt, it needs to reconfigure the hardware completely to run the host operating
system code. When the device driver runs, it finds everything just as it expected it
to be. The hypervisor behaves just like teenagers throwing a party while their par-
ents are away. It is okay to rearrange the furniture completely, as long as they put it
back exactly as they found it before the parents come home.        Going from a hard-
ware configuration for the host kernel to a configuration for the guest operating
system is known as a world switch.       We will discuss it in detail when we discuss
VMware in Sec. 7.12.
     It should now be clear why these hypervisors work, even on unvirtualizable
hardware: sensitive instructions in the guest kernel are replaced by calls to proce-
dures that emulate these instructions. No sensitive instructions issued by the guest
operating system are ever executed directly by the true hardware. They are turned
into calls to the hypervisor, which then emulates them.
7.4.2 The Cost of Virtualization
     One might naively expect that CPUs with VT would greatly outperform soft-
ware techniques that resort to translation, but measurements show a mixed picture
(Adams and Agesen, 2006).      It turns out that the trap-and-emulate approach used
by VT hardware generates a lot of traps, and traps are very expensive on modern
hardware because they ruin CPU caches, TLBs, and branch prediction tables inter-
nal to the CPU.   In contrast, when sensitive instructions are replaced by calls to
hypervisor procedures within the executing process, none of this context-switching
overhead is incurred.     As Adams and Agesen show, depending on the workload,
sometimes software beats hardware. For this reason, some type 1 (and type 2)
hypervisors do binary translation for performance reasons, even though the soft-
ware will execute correctly without it.
     With binary translation, the translated code itself may be either slower or faster
than the original code. Suppose, for instance, that the guest operating system dis-
ables hardware interrupts using the CLI instruction (``clear interrupts''). Depending
on the architecture, this instruction can be very slow, taking many tens of cycles on
certain CPUs with deep pipelines and out-of-order execution.       It should be clear by
now that the guest's wanting to turn off interrupts does not mean the hypervisor
should really turn them off and affect the entire machine. Thus, the hypervisor
must turn them off for the guest without really turning them off. To do so, it may
keep track of a dedicated IF (Interrupt Flag) in the virtual CPU data structure it
maintains for each guest (making sure the virtual machine does not get any inter-
rupts until the interrupts are turned off again). Every occurrence of CLI in the guest
will be replaced by something like ``VirtualCPU.IF = 0'', which is a very cheap move



SEC. 7.4            TECHNIQUES FOR EFFICIENT VIRTUALIZATION                               483
instruction that may take as little as one to three cycles. Thus, the translated code is
faster. Still, with modern VT hardware, usually the hardware beats the software.
    On the other hand, if the guest operating system modifies its page tables, this is
very costly. The problem is that each guest operating system on a virtual machine
thinks it ``owns'' the machine and is at liberty to map any virtual page to any phys-
ical page in memory. However, if one virtual machine wants to use a physical page
that is already in use by another virtual machine (or the hypervisor), something has
to give. We will see in Section 7.6 that the solution is to add an extra level of page
tables to map ``guest physical pages'' to the actual physical pages on the host. Not
surprisingly, mucking around with multiple levels of page tables is not cheap.
7.5 ARE HYPERVISORS MICROKERNELS DONE RIGHT?
    Both type 1 and type 2 hypervisors work with unmodified guest operating sys-
tems, but have to jump through hoops to get good performance. We have seen that
paravirtualization takes a different approach by modifying the source code of the
guest operating system instead. Rather than performing sensitive instructions, the
paravirtualized guest executes hypercalls.    In effect the guest operating system is
acting like a user program making system calls to the operating system (the hyper-
visor).   When this route is taken, the hypervisor must define an interface consisting
of a set of procedure calls that guest operating systems can use.  This set of calls
forms what is effectively an API (Application Programming Interface) even
though it is an interface for use by guest operating systems, not application pro-
grams.
    Going one step further, by removing all the sensitive instructions from the op-
erating system and just having it make hypercalls to get system services like I/O,
we have turned the hypervisor into a microkernel, like that of Fig. 1-26. The idea,
explored in paravirtualization, is that emulating peculiar hardware instructions is
an unpleasant and time-consuming task. It requires a call into the hypervisor and
then emulating the exact semantics of a complicated instruction.   It is far better just
to have the guest operating system call the hypervisor (or microkernel) to do I/O,
and so on.
    Indeed, some researchers have argued that we should perhaps consider hyper-
visors as ``microkernels done right'' (Hand et al., 2005). The first thing to mention
is  that  this  is  a  highly  controversial  topic  and  some  researchers  have  vocally
opposed the notion, arguing that the difference between the two is not fundamental
to begin with (Heiser et al., 2006). Others suggest that compared to microkernels,
hypervisors may not even be that well suited for building secure systems, and
advocate that they be extended with kernel functionality like message passing and
memory sharing (Hohmuth et al., 2004).        Finally, some researchers argue that per-
haps hypervisors are not even ``operating systems research done right'' (Roscoe et
al., 2007). Since nobody said anything about operating system textbooks done right



484                  VIRTUALIZATION AND THE CLOUD                           CHAP. 7
(or wrong)--yet--we think we do right by exploring the similarity between hyper-
visors and microkernels a bit more.
     The main reason the first hypervisors emulated the complete machine was the
lack of availability of source code for the guest operating system (e.g., for Win-
dows) or the vast number of variants (e.g., for Linux).         Perhaps in the future the
hypervisor/microkernel API will be standardized, and subsequent operating sys-
tems will be designed to call it instead of using sensitive instructions. Doing so
would make virtual machine technology easier to support and use.
     The difference between true virtualization and paravirtualization is illustrated
in Fig. 7-5.  Here we have two virtual machines being supported on VT hardware.
On the left is an unmodified version of Windows as the guest operating system.
When a sensitive instruction is executed, the hardware causes a trap to the hypervi-
sor, which then emulates it and returns. On the right is a version of Linux modified
so that it no longer contains any sensitive instructions. Instead, when it needs to do
I/O or change critical internal registers (such as the one pointing to the page
tables), it makes a hypervisor call to get the work done, just like an application pro-
gram making a system call in standard Linux.
     True virtualization                       Paravirtualization
                                 Trap due                             Trap due
     Unmodified Windows          to sensitive  Modified Linux         to hypervisor
                                 instruction                          call
              Type 1 hypervisor                Microkernel
                                 Hardware
              Figure 7-5. True virtualization and paravirtualization
     In Fig. 7-5 we have shown the hypervisor as being divided into two parts sepa-
rated by a dashed line. In reality, only one program is running on the hardware.
One part of it is responsible for interpreting trapped sensitive instructions, in this
case, from Windows.  The other part of it just carries out hypercalls. In the figure
the latter part is labeled ``microkernel.'' If the hypervisor is intended to run only
paravirtualized guest operating systems, there is no need for the emulation of sen-
sitive instructions and we have a true microkernel, which just provides very basic
services such as process dispatching and managing the MMU. The boundary be-
tween a type 1 hypervisor and a microkernel is vague already and will get even less
clear as hypervisors begin acquiring more and more functionality and hypercalls,
as seems likely. Again, this subject is controversial, but it is increasingly clear that
the program running in kernel mode on the bare hardware should be small and reli-
able and consist of thousands, not millions, of lines of code.



SEC. 7.5  ARE HYPERVISORS MICROKERNELS DONE RIGHT?                                           485
Paravirtualizing the guest operating system raises a number of issues.                       First, if
the sensitive instructions are replaced with calls to the hypervisor, how can the op-
erating system run on the native hardware? After all, the hardware does not under-
stand these hypercalls.  And second, what if there are multiple hypervisors avail-
able in the marketplace, such as VMware, the open source Xen originally from the
University of Cambridge, and Microsoft's Hyper-V, all with somewhat different
hypervisor APIs? How can the kernel be modified to run on all of them?
Amsden et al. (2006) have proposed a solution.               In their model, the kernel is
modified to call special procedures whenever it needs to do something sensitive.
Together these procedures, called the VMI (Virtual Machine Interface), form a
low-level layer that interfaces with the hardware or hypervisor.       These procedures
are designed to be generic and not tied to any specific hardware platform or to any
particular hypervisor.
An example of this technique is given in Fig. 7-6 for a paravirtualized version
of Linux they call VMI Linux (VMIL).       When VMI Linux runs on the bare hard-
ware, it has to be linked with a library that issues the actual (sensitive) instruction
needed to do the work, as shown in Fig. 7-6(a). When running on a hypervisor, say
VMware or Xen, the guest operating system is linked with different libraries that
make the appropriate (and different) hypercalls to the underlying hypervisor.                In
this way, the core of the operating system remains portable yet is hypervisor
friendly and still efficient.
          VMI Linux                  VMI Linux               VMI Linux
VMIL /HWinterface lib.           VMIL to Vmware lib.         VMIL to Xen library
               Sensitive                   Hypervisor  call            Hypervisor      call
               instruction
               executed by           VMware                  Xen
               HW
          Hardware                   Hardware                Hardware
          (a)                         (b)                         (c)
          Figure 7-6. VMI Linux running on (a) the bare hardware, (b) VMware, (c) Xen
Other proposals for a virtual machine interface have also been made.                   Another
popular one is called paravirt ops.  The idea is conceptually similar to what we
described above, but different in the details. Essentially, a group of Linux vendors
that included companies like IBM, VMware, Xen, and Red Hat advocated a hyper-
visor-agnostic interface for Linux. The interface, included in the mainline kernel
from version 2.6.23 onward, allows the kernel to talk to whatever hypervisor is
managing the physical hardware.



486                       VIRTUALIZATION AND THE CLOUD                       CHAP. 7
7.6 MEMORY VIRTUALIZATION
     So far we have addressed the issue of how to virtualize the CPU.        But a com-
puter system has more than just a CPU. It also has memory and I/O devices. They
have to be virtualized, too. Let us see how that is done.
     Modern operating systems nearly all support virtual memory, which is basical-
ly a mapping of pages in the virtual address space onto pages of physical memory.
This mapping is defined by (multilevel) page tables.       Typically the mapping is set
in motion by having the operating system set a control register in the CPU that
points to the top-level page table. Virtualization greatly complicates memory man-
agement. In fact, it took hardware manufacturers two tries to get it right.
     Suppose, for example, a virtual machine is running, and the guest operating
system in it decides to map its virtual pages 7, 4, and 3 onto physical pages 10, 11,
and 12, respectively. It builds page tables containing this mapping and loads a
hardware register to point to the top-level page table.    This instruction is sensitive.
On a VT CPU, it will trap; with dynamic translation it will cause a call to a hyper-
visor procedure; on a paravirtualized operating system, it will generate a hypercall.
For simplicity, let us assume it traps into a type 1 hypervisor, but the problem is the
same in all three cases.
     What does the hypervisor do now?     One solution is to actually allocate physi-
cal pages 10, 11, and 12 to this virtual machine and set up the actual page tables to
map the virtual machine's virtual pages 7, 4, and 3 to use them. So far, so good.
     Now suppose a second virtual machine starts and maps its virtual pages 4, 5,
and 6 onto physical pages 10, 11, and 12 and loads the control register to point to
its page tables.  The hypervisor catches the trap, but what should it do?    It cannot
use this mapping because physical pages 10, 11, and 12 are already in use.         It can
find some free pages, say 20, 21, and 22, and use them, but it first has to create new
page tables mapping the virtual pages 4, 5, and 6 of virtual machine 2 onto 20, 21,
and 22. If another virtual machine starts and tries to use physical pages 10, 11, and
12, it has to create a mapping for them.  In general, for each virtual machine the
hypervisor needs to create a shadow page table that maps the virtual pages used
by the virtual machine onto the actual pages the hypervisor gave it.
     Worse yet, every time the guest operating system changes its page tables, the
hypervisor must change the shadow page tables as well.     For example, if the guest
OS remaps virtual page 7 onto what it sees as physical page 200 (instead of 10),
the hypervisor has to know about this change.  The trouble is that the guest operat-
ing system can change its page tables by just writing to memory. No sensitive oper-
ations are required, so the hypervisor does not even know about the change and
certainly cannot update the shadow page tables used by the actual hardware.
     A possible (but clumsy) solution is for the hypervisor to keep track of which
page in the guest's virtual memory contains the top-level page table. It can get this
information the first time the guest attempts to load the hardware register that
points to it because this instruction is sensitive and traps. The hypervisor can create



SEC. 7.6                    MEMORY VIRTUALIZATION                                        487
a shadow page table at this point and also map the top-level page table and the
page tables it points to as read only. A subsequent attempts by the guest operating
system to modify any of them will cause a page fault and thus give control to the
hypervisor, which can analyze the instruction stream, figure out what the guest OS
is trying to do, and update the shadow page tables accordingly.       It is not pretty, but
it is doable in principle.
Another, equally clumsy, solution is to do exactly the opposite. In this case, the
hypervisor simply allows the guest to add new mappings to its page tables at will.
As this is happening, nothing changes in the shadow page tables. In fact, the hyper-
visor is not even aware of it.  However, as soon as the guest tries to access any of
the new pages, a fault will occur and control reverts to the hypervisor.          The hyper-
visor inspects the guest's page tables to see if there is a mapping that it should add,
and if so, adds it and reexecutes the faulting instruction. What if the guest removes
a mapping from its page tables? Clearly, the hypervisor cannot wait for a page fault
to happen, because it will not happen. Removing a mapping from a page table hap-
pens by way of the INVLPG instruction (which is really intended to invalidate a
TLB entry). The hypervisor therefore intercepts this instruction and removes the
mapping from the shadow page table also. Again, not pretty, but it works.
Both of these techniques incur many page faults, and page faults are expensive.
We typically distinguish between ``normal'' page faults that are caused by guest
programs that access a page that has been paged out of RAM, and page faults that
are related to ensuring the shadow page tables and the guest's page tables are in
sync. The former are known as guest-induced page faults, and while they are
intercepted by the hypervisor, they must be reinjected into the guest. This is not
cheap at all. The latter are known as hypervisor-induced page faults and they are
handled by updating the shadow page tables.
Page faults are always expensive, but especially so in virtualized environments,
because they lead to so-called VM exits.     A VM exit is a situation in which the
hypervisor regains control. Consider what the CPU needs to do for such a VM exit.
First, it records the cause of the VM exit, so the hypervisor knows what to do. It
also records the address of the guest instruction that caused the exit. Next, a con-
text switch is done, which includes saving all the registers. Then, it loads the
hypervisor's processor state. Only then can the hypervisor start handling the page
fault, which was expensive to begin with. Oh, and when it is all done, it should re-
verse these steps. The whole process may take tens of thousands of cycles, or
more. No wonder people bend over backward to reduce the number of exits.
In  a     paravirtualized   operating  system,  the    situation  is  different.  Here   the
paravirtualized OS in the guest knows that when it is finished changing some proc-
ess' page table, it had better inform the hypervisor.  Consequently, it first changes
the page table completely, then issues a hypervisor call telling the hypervisor about
the new page table.  Thus, instead of a protection fault on every update to the page
table, there is one hypercall when the whole thing has been updated, obviously a
more efficient way to do business.



488                VIRTUALIZATION AND THE CLOUD      CHAP. 7
Hardware Support for Nested Page Tables
     The cost of handling shadow page tables led chip makers to add hardware sup-
port for nested page tables.  Nested page tables is the term used by AMD. Intel
refers to them as EPT (Extended Page Tables).        They are similar and aim to re-
move most of the overhead by handling the additional page-table manipulation all
in hardware, all without any traps. Interestingly, the first virtualization extensions
in Intel's x86 hardware did not include support for memory virtualization at all.
While these VT-extended processors removed many bottlenecks concerning CPU
virtualization, poking around in page tables was as expensive as ever. It took a few
years for AMD and Intel to produce the hardware to virtualize memory efficiently.
     Recall that even without virtualization, the operating system maintains a map-
ping between the virtual pages and the physical page. The hardware ``walks'' these
page tables to find the physical address that corresponds to a virtual address. Add-
ing more virtual machines simply adds an extra mapping. As an example, suppose
we need to translate a virtual address of a Linux process running on a type 1 hyper-
visor like Xen or VMware ESX Server to a physical address. In addition to the
guest virtual addresses, we now also have guest physical addresses and subse-
quently host physical addresses (sometimes referred to as machine physical
addresses).  We have seen that without EPT, the hypervisor is responsible for
maintaining the shadow page tables explicitly. With EPT, the hypervisor still has
an additional set of page tables, but now the CPU is able to handle much of the
intermediate level in hardware also. In our example, the hardware first walks the
``regular'' page tables to translate the guest virtual address to a guest physical ad-
dress, just as it would do without virtualization. The difference is that it also walks
the extended (or nested) page tables without software intervention to find the host
physical address, and it needs to do this every time a guest physical address is ac-
cessed. The translation is illustrated in Fig. 7-7.
     Unfortunately, the hardware may need to walk the nested page tables more fre-
quently then you might think. Let us suppose that the guest virtual address was not
cached and requires a full page-table lookup. Every level in paging hierarchy
incurs a lookup in the nested page tables. In other words, the number of memory
references grows quadratically with the depth of the hierarchy. Even so, EPT dra-
matically reduces the number of VM exits. Hypervisors no longer need to map the
guest's page table read only and can do away with shadow page-table handling.
Better still, when switching virtual machines, it just changes this mapping, the
same way an operating system changes the mapping when switching processes.
Reclaiming Memory
     Having all these virtual machines on the same physical hardware all with their
own memory pages and all thinking they are the king of the mountain is great--
until we need the memory back. This is particularly important in the event of



SEC.  7.6                      MEMORY VIRTUALIZATION                                                              489
63                      48 47     39 38              30 29   21 20                          12  11                0
                          Level 1 offset  Level   2  offset  Level 3 offset  Level 4 offset         page  offset
      Guest pointer to         +
      level 1 page table
                   Guest pointer to entry in
                          level 1 page table
                   Look up in nested page tables             +
                                                             Guest pointer to entry in
                                                             level 2 page table
                                                             Look up in nested page tables                etc.
      Figure 7-7. Extended/nested page tables are walked every time a guest physical
      address is accessed--including the accesses for each level of the guest's page ta-
      bles.
overcommitment of memory, where the hypervisor pretends that the total amount
of memory for all virtual machines combined is more than the total amount of
physical memory present on the system. In general, this is a good idea, because it
allows the hypervisor to admit more and more beefy virtual machines at the same
time. For instance, on a machine with 32 GB of memory, it may run three virtual
machines each thinking it has 16 GB of memory. Clearly, this does not fit. Howev-
er, perhaps the three machines do not really need the maximum amount of physical
memory at the same time.       Or perhaps they share pages that have the same content
(such as the Linux kernel) in different virtual machines in an optimization known
as deduplication.  In that case, the three virtual machines use a total amount of
memory that is less than 3 times 16 GB.              We will discuss deduplication later; for
the moment the point is that what looks like a good distribution now, may be a
poor distribution as the workloads change.                   Maybe virtual machine 1 needs more
memory, while virtual machine 2 could do with fewer pages. In that case, it would
be nice if the hypervisor could transfer resources from one virtual machine to an-
other and make the system as a whole benefit. The question is, how can we take
away memory pages safely if that memory is given to a virtual machine already?
In principle, we could use yet another level of paging. In case of memory
shortage, the hypervisor would then page out some of the virtual machine's pages,
just as an operating system may page out some of an application's pages. The
drawback of this approach is that the hypervisor should do this, and the hypervisor
has no clue about which pages are the most valuable to the guest. It is very likely
to page out the wrong ones. Even if it does pick the right pages to swap (i.e., the
pages that the guest OS would also have picked), there is still more trouble ahead.



490                VIRTUALIZATION AND THE CLOUD                                   CHAP. 7
For instance, suppose that the hypervisor pages out a page P. A little later, the
guest OS also decides to page out this page to disk. Unfortunately, the hypervisor's
swap space and the guest's swap space are not the same. In other words, the hyper-
visor must first page the contents back into memory, only to see the guest write it
back out to disk immediately. Not very efficient.
     A common solution is to use a trick known as ballooning, where a small bal-
loon module is loaded in each VM as a pseudo device driver that talks to the hyper-
visor. The balloon module may inflate at the hypervisor's request by allocating
more and more pinned pages, and deflate by deallocating these pages. As the bal-
loon inflates, memory scarcity in the guest increases. The guest operating system
will respond by paging out what it believes are the least valuable pages--which is
just what we wanted. Conversely, as the balloon deflates, more memory becomes
available for the guest to allocate.  In other words, the hypervisor tricks the operat-
ing system into making tough decisions for it.     In politics, this is known as passing
the buck (or the euro, pound, yen, etc.).
7.7 I/O VIRTUALIZATION
     Having looked at CPU and memory virtualization, we next examine I/O virtu-
alization.  The guest operating system will typically start out probing the hardware
to find out what kinds of I/O devices are attached.         These probes will trap to the
hypervisor.  What should the hypervisor do?        One approach is for it to report back
that the disks, printers, and so on are the ones that the hardware actually has.          The
guest will then load device drivers for these devices and try to use them. When the
device drivers try to do actual I/O, they will read and write the device's hardware
device registers.  These instructions are sensitive and will trap to the hypervisor,
which could then copy the needed values to and from the hardware registers, as
needed.
     But here, too, we have a problem. Each guest OS could think it owns an entire
disk partition, and there may be many more virtual machines (hundreds) than there
are actual disk partitions. The usual solution is for the hypervisor to create a file or
region on the actual disk for each virtual machine's physical disk. Since the guest
OS is trying to control a disk that the real hardware has (and which the hypervisor
understands), it can convert the block number being accessed into an offset into the
file or disk region being used for storage and do the I/O.
     It is also possible for the disk that the guest is using to be different from the
real one. For example, if the actual disk is some brand-new high-performance disk
(or RAID) with a new interface, the hypervisor could advertise to the guest OS that
it has a plain old IDE disk and let the guest OS install an IDE disk driver. When
this driver issues IDE disk commands, the hypervisor converts them into com-
mands to drive the new disk. This strategy can be used to upgrade the hardware
without changing the software.        In fact, this ability of virtual machines to remap



SEC. 7.7                     I/O VIRTUALIZATION                                         491
hardware devices was one of the reasons VM/370 became popular: companies
wanted to buy new and faster hardware but did not want to change their software.
Virtual machine technology made this possible.
Another interesting trend related to I/O is that the hypervisor can take the role
of a virtual switch. In this case, each virtual machine has a MAC address and the
hypevisor switches frames from one virtual machine to another--just like an Ether-
net switch would do. Virtual switches have several advantages. For instance, it is
very easy to reconfigure them. Also, it is possible to augment the switch with addi-
tional functionality, for instance for additional security.
I/O MMUs
Another I/O problem that must be solved somehow is the use of DMA, which
uses absolute memory addresses.    As might be expected, the hypervisor has to
intervene here and remap the addresses before the DMA starts.        However, hard-
ware already exists with an I/O MMU, which virtualizes the I/O the same way the
MMU virtualizes the memory.       I/O MMU exists in different forms and shapes for
many processor architectures. Even if we limit ourselves to the x86, Intel and
AMD have slightly different technology. Still, the idea is the same. This hardware
eliminates the DMA problem.
Just like regular MMUs, the I/O MMU uses page tables to map a memory ad-
dress that a device wants to use (the device address) to a physical address. In a vir-
tual environment, the hypervisor can set up the page tables in such a way that a de-
vice performing DMA will not trample over memory that does not belong to the
virtual machine on whose behalf it is working.
I/O MMUs offer different advantages when dealing with a device in a virtu-
alized world.  Device pass through allows the physical device to be directly as-
signed to a particular virtual machine. In general, it would be ideal if device ad-
dress space were exactly the same as the guest's physical address space. However,
this is unlikely--unless you have an I/O MMU. The MMU allows the addresses to
remapped transparently, and both the device and the virtual machine are blissfully
unaware of the address translation that takes place under the hood.
Device isolation ensures that a device assigned to a virtual machine can direct-
ly access that virtual machine without jeopardizing the integrity of the other guests.
In other words, the I/O MMU prevents rogue DMA traffic, just as a normal MMU
prevents  rogue  memory  accesses  from  processes--in       both    cases  accesses    to
unmapped pages result in faults.
DMA and addresses are not the whole I/O story, unfortunately.        For complete-
ness, we also need to virtualize interrupts, so that the interrupt generated by a de-
vice arrives at the right virtual machine, with the right interrupt number. Modern
I/O MMUs therefore support interrupt remapping.              Say, a device sends a mes-
sage signaled interrupt with number 1. This message first hits the I/O MMU that
will use the interrupt remapping table to translate to a new message destined for



492                   VIRTUALIZATION AND THE CLOUD                     CHAP. 7
the CPU that currently runs the virtual machine and with the vector number that
the VM expects (e.g., 66).
     Finally, having an I/O MMU also helps 32-bit devices access memory above 4
GB. Normally, such devices are unable to access (e.g., DMA to) addresses beyond
4 GB, but the I/O MMU can easily remap the device's lower addresses to any ad-
dress in the physical larger address space.
Device Domains
     A different approach to handling I/O is to dedicate one of the virtual machines
to run a standard operating system and reflect all I/O calls from the other ones to it.
This approach is enhanced when paravirtualization is used, so the command being
issued to the hypervisor actually says what the guest OS wants (e.g., read block
1403 from disk 1) rather than being a series of commands writing to device regis-
ters, in which case the hypervisor has to play Sherlock Holmes and figure out what
it is trying to do. Xen uses this approach to I/O, with the virtual machine that does
I/O called domain 0.
     I/O virtualization is an area in which type 2 hypervisors have a practical advan-
tage over type 1 hypervisors: the host operating system contains the device drivers
for all the weird and wonderful I/O devices attached to the computer.  When an ap-
plication program attempts to access a strange I/O device, the translated code can
call the existing device driver to get the work done.  With a type 1 hypervisor, the
hypervisor must either contain the driver itself, or make a call to a driver in domain
0, which is somewhat similar to a host operating system. As virtual machine tech-
nology matures, future hardware is likely to allow application programs to access
the hardware directly in a secure way, meaning that device drivers can be linked di-
rectly with application code or put in separate user-mode servers (as in MINIX3),
thereby eliminating the problem.
Single Root I/O Virtualization
     Directly assigning a device to a virtual machine is not very scalable. With four
physical networks you can support no more than four virtual machines that way.
For eight virtual machines you need eight network cards, and to run 128 virtual
machines--well, let's just say that it may be hard to find your computer buried
under all those network cables.
     Sharing devices among multiple hypervisors in software is possible, but often
not optimal because an emulation layer (or device domain) interposes itself be-
tween hardware and the drivers and the guest operating systems. The emulated de-
vice frequently does not implement all the advanced functions supported by the
hardware. Ideally, the virtualization technology would offer the equivalence of de-
vice pass through of a single device to multiple hypervisors, without any overhead.
Virtualizing a single device to trick every virtual machine into believing that it has



SEC. 7.7                       I/O VIRTUALIZATION                                       493
exclusive access to its own device is much easier if the hardware actually does the
virtualization for you. On PCIe, this is known as single root I/O virtualization.
Single root I/O virtualization (SR-IOV) allows us to bypass the hypervisor's
involvement in the communication between the driver and the device. Devices that
support   SR-IOV  provide  an  independent        memory    space,  interrupts  and     DMA
streams to each virtual machine that uses it (Intel, 2011). The device appears as
multiple separate devices and each can be configured by separate virtual machines.
For instance, each will have a separate base address register and address space. A
virtual machine maps one of these memory areas (used for instance to configure
the device) into its address space.
SR-IOV provides access to the device in two flavors: PF (Physical Functions)
and (Virtual Functions).   PFs are full PCIe functions and allow the device to be
configured in whatever way the administrator sees fit. Physical functions are not
accessible to guest operating systems. VFs are lightweight PCIe functions that do
not offer such configuration options. They are ideally suited for virtual machines.
In summary, SR-IOV allows devices to be virtualized in (up to) hundreds of virtual
functions that trick virtual machines into believing they are the sole owner of a de-
vice. For example, given an SR-IOV network interface, a virtual machine is able to
handle its virtual network card just like a physical one. Better still, many modern
network cards have separate (circular) buffers for sending and receiving data, dedi-
cated to this virtual machines. For instance, the Intel I350 series of network cards
has eight send and eight receive queues
7.8 VIRTUAL APPLIANCES
Virtual   machines  offer  an        interesting  solution  to  a  problem  that   has  long
plagued users, especially users of open source software: how to install new appli-
cation programs. The problem is that many applications are dependent on numer-
ous other applications and libraries, which are themselves dependent on a host of
other software packages, and so on. Furthermore, there may be dependencies on
particular versions of the compilers, scripting languages, and the operating system.
With virtual machines now available, a software developer can carefully con-
struct a virtual machine, load it with the required operating system, compilers, li-
braries, and application code, and freeze the entire unit, ready to run.        This virtual
machine image can then be put on a CD-ROM or a Website for customers to install
or download. This approach means that only the software developer has to under-
stand all the dependencies. The customers get a complete package that actually
works, completely independent of which operating system they are running and
which other software, packages, and libraries they have installed.          These ``shrink-
wrapped'' virtual machines are often called virtual appliances.           As an example,
Amazon's EC2 cloud has many pre-packaged virtual appliances available for its
clients, which it offers as convenient software services (``Software as a Service'').



494                      VIRTUALIZATION AND THE CLOUD      CHAP. 7
7.9 VIRTUAL MACHINES ON MULTICORE CPUS
     The combination of virtual machines and multicore CPUs creates a whole new
world in which the number of CPUs available can be set by the software. If there
are, say, four cores, and each can run, for example, up to eight virtual machines, a
single (desktop) CPU can be configured as a 32-node multicomputer if need be,
but it can also have fewer CPUs, depending on the software. Never before has it
been possible for an application designer to first choose how many CPUs he wants
and then write the software accordingly. This is clearly a new phase in computing.
     Moreover, virtual machines can share memory.          A typical example where this
is useful is a single server hosting multiple instances of the same operating sys-
tems. All that has to be done is map physical pages into the address spaces of mul-
tiple virtual machines.  Memory sharing is already available in deduplication solu-
tions.  Deduplication does exactly what you think it does: avoids storing the same
data twice. It is a fairly common technique in storage systems, but is now appear-
ing in virtualization as well. In Disco, it was known as transparent page sharing
(which requires modification to the guest), while VMware calls it content-based
page sharing (which does not require any modification). In general, the technique
revolves around scanning the memory of each of the virtual machines on a host and
hashing the memory pages. Should some pages produce an identical hash, the sys-
tem has to first check to see if they really are the same, and if so, deduplicate them,
creating one page with the actual content and two references to that page. Since the
hypervisor controls the nested (or shadow) page tables, this mapping is straightfor-
ward. Of course, when either of the guests modifies a shared page, the change
should not be visible in the other virtual machine(s). The trick is to use copy on
write so the modified page will be private to the writer.
     If virtual machines can share memory, a single computer becomes a virtual
multiprocessor. Since all the cores in a multicore chip share the same RAM, a sin-
gle quad-core chip could easily be configured as a 32-node multiprocessor or a
32-node multicomputer, as needed.
     The combination of multicore, virtual machines, hypervisor, and microkernels
is going to radically change the way people think about computer systems. Current
software cannot deal with the idea of the programmer determining how many
CPUs are needed, whether they should be a multicomputer or a multiprocessor,
and how minimal kernels of one kind or another fit into the picture. Future soft-
ware will have to deal with these issues. If you are a computer science or engineer-
ing student or professional, you could be the one to sort out all this stuff. Go for it!
7.10 LICENSING ISSUES
     Some software is licensed on a per-CPU basis, especially software for compa-
nies. In other words, when they buy a program, they have the right to run it on just
one CPU.  What's a CPU, anyway?    Does this contract give them the right to run



SEC. 7.10                  LICENSING ISSUES                                           495
the software on multiple virtual machines all running on the same physical ma-
chine? Many software vendors are somewhat unsure of what to do here.
The problem is much worse in companies that have a license allowing them to
have n machines running the software at the same time, especially when virtual
machines come and go on demand.
In some cases, software vendors have put an explicit clause in the license for-
bidding the licensee from running the software on a virtual machine or on an unau-
thorized virtual machine.  For companies that run all their software exclusively on
virtual machines, this could be a real problem.         Whether any of these restrictions
will hold up in court and how users respond to them remains to be seen.
7.11 CLOUDS
Virtualization technology played a crucial role in the dizzying rise of cloud
computing. There are many clouds. Some clouds are public and available to any-
one willing to pay for the use of resources, others are private to an organization.
Likewise, different clouds offer different things. Some give their users access to
physical hardware, but most virtualize their environments. Some offer the bare ma-
chines, virtual or not, and nothing more, but others offer software that is ready to
use and can be combined in interesting ways, or platforms that make it easy for
their users to develop new services. Cloud providers typically offer different cate-
gories of resources, such as ``big machines'' versus ``little machines,'' etc.
For all the talk about clouds, few people seem really sure about what they are
exactly. The National Institute of Standards and Technology, always a good source
to fall back on, lists five essential characteristics:
1.  On-demand  self-service.     Users  should          be  able  to  provision  re-
    sources automatically, without requiring human interaction.
2.  Broad network access.        All these resources should be available over
    the network via standard mechanisms so that heterogeneous devices
    can make use of them.
3.  Resource pooling.      The computing resource owned by the provider
    should be pooled to serve multiple users and with the ability to assign
    and reassign resources dynamically. The users generally do not even
    know the exact location of ``their'' resources or even which country
    they are located in.
4.  Rapid elasticity.      It should be possible to acquire and release re-
    sources elastically, perhaps even automatically, to scale immediately
    with the users' demands.
5.  Measured service. The cloud provider meters the resources used in a
    way that matches the type of service agreed upon.



496                  VIRTUALIZATION AND THE CLOUD                         CHAP. 7
7.11.1 Clouds as a Service
     In this section, we will look at clouds with a focus on virtualization and operat-
ing systems. Specifically, we consider clouds that offer direct access to a virtual
machine, which the user can use in any way he sees fit. Thus, the same cloud may
run different operating systems, possibly on the same hardware.           In cloud terms,
this is known as IAAS (Infrastructure As A Service), as opposed to PAAS (Plat-
form As A Service, which delivers an environment that includes things such as a
specific OS, database, Web server, and so on), SAAS (Software As A Service,
which offers access to specific software, such as Microsoft Office 365, or Google
Apps), and many other types of as-a-service. One example of an IAAS cloud is
Amazon EC2, which happens to be based on the Xen hypervisor and counts multi-
ple hundreds of thousands of physical machines. Provided you have the cash, you
can have as much computing power as you need.
     Clouds can transform the way companies do computing. Overall, consolidating
the computing resources in a small number of places (conveniently located near a
power source and cheap cooling) benefits from economy of scale.           Outsourcing
your processing means that you need not worry so much about managing your IT
infrastructure, backups, maintenance, depreciation, scalability, reliability, perfor-
mance, and perhaps security. All of that is done in one place and, assuming the
cloud provider is competent, done well. You would think that IT managers are hap-
pier today than ten years ago. However, as these worries disappeared, new ones
emerged. Can you really trust your cloud provider to keep your sensitive data safe?
Will a competitor running on the same infrastructure be able to infer information
you wanted to keep private? What law(s) apply to your data (for instance, if the
cloud provider is from the United States, is your data subject to the PATRIOT Act,
even if your company is in Europe)?  Once you store all your data in cloud X, will
you be able to get them out again, or will you be tied to that cloud and its provider
forever, something known as vendor lock-in?
7.11.2 Virtual Machine Migration
     Virtualization technology not only allows IAAS clouds to run multiple dif-
ferent operating systems on the same hardware at the same time, it also permits
clever  management.  We  have  already  discussed       the  ability  to  overcommit     re-
sources, especially in combination with deduplication.       Now we will look at anoth-
er management issue: what if a machine needs servicing (or even replacement)
while it is running lots of important machines? Probably, clients will not be happy
if their systems go down because the cloud provider wants to replace a disk drive.
     Hypervisors decouple the virtual machine from the physical hardware. In other
words, it does not really matter to the virtual machine if it runs on this machine or
that machine. Thus, the administrator could simply shut down all the virtual ma-
chines and restart them again on a shiny new machine. Doing so, however, results



SEC. 7.11                            CLOUDS                                            497
in significant downtime. The challenge is to move the virtual machine from the
hardware that needs servicing to the new machine without taking it down at all.
A slightly better approach might be to pause the virtual machine, rather than
shut it down. During the pause, we copy over the memory pages used by the virtual
machine to the new hardware as quickly as possible, configure things correctly in
the new hypervisor and then resume execution. Besides memory, we also need to
transfer storage and network connectivity, but if the machines are close, this can be
relatively fast. We could make the file system network-based to begin with (like
NFS, the network file system), so that it does not matter whether your virtual ma-
chine is running on hardware in server rack 1 or 3. Likewise, the IP address can
simply be switched to the new location. Nevertheless, we still need to pause the
machine for a noticeable amount of time. Less time perhaps, but still noticeable.
Instead, what modern virtualization solutions offer is something known as live
migration.  In other words, they move the virtual machine while it is still opera-
tional. For instance, they employ techniques like pre-copy memory migration.
This means that they copy memory pages while the machine is still serving re-
quests. Most memory pages are not written much, so copying them over is safe.
Remember, the virtual machine is still running, so a page may be modified after it
has already been copied. When memory pages are modified, we have to make sure
that the latest version is copied to the destination, so we mark them as dirty. They
will be recopied later. When most memory pages have been copied, we are left
with a small number of dirty pages. We now pause very briefly to copy the remain-
ing pages and resume the virtual machine at the new location. While there is still a
pause, it is so brief that applications typically are not affected. When the downtime
is not noticeable, it is known as a seamless live migration.
7.11.3 Checkpointing
Decoupling of virtual machine and physical hardware has additional advan-
tages. In particular, we mentioned that we can pause a machine. This in itself is
useful. If the state of the paused machine (e.g., CPU state, memory pages, and stor-
age state) is stored on disk, we have a snapshot of a running machine. If the soft-
ware makes a royal mess of the still-running virtual machine, it is possible to just
roll back to the snapshot and continue as if nothing happened.
The most straightforward way to make a snapshot is to copy everything, in-
cluding the full file system. However, copying a multiterabyte disk may take a
while, even if it is a fast disk. And again, we do not want to pause for long while
we are doing it. The solution is to use copy on write solutions, so that data is cop-
ied only when absolutely necessary.
Snapshotting works quite well, but there are issues. What to do if a machine is
interacting with a remote computer? We can snapshot the system and bring it up
again at a later stage, but the communicating party may be long gone. Clearly, this
is a problem that cannot be solved.



498           VIRTUALIZATION AND THE CLOUD                              CHAP. 7
7.12  CASE STUDY: VMWARE
     Since 1999, VMware, Inc. has been the leading commercial provider of virtu-
alization solutions with products for desktops, servers, the cloud, and now even on
cell phones.  It provides not only hypervisors but also the software that manages
virtual machines on a large scale.
     We will start this case study with a brief history of how the company got start-
ed. We will then describe VMware Workstation, a type 2 hypervisor and the com-
pany's first product, the challenges in its design and the key elements of the solu-
tion. We then describe the evolution of VMware Workstation over the years. We
conclude with a description of ESX Server, VMware's type 1 hypervisor.
7.12.1 The Early History of VMware
     Although the idea of using virtual machines was popular in the 1960s and
1970s in both the computing industry and academic research, interest in virtu-
alization was totally lost after the 1980s and the rise of the personal computer in-
dustry. Only IBM's mainframe division still cared about virtualization. Indeed, the
computer architectures designed at the time, and in particular Intel's x86 architec-
ture, did not provide architectural support for virtualization (i.e., they failed the
Popek/Goldberg criteria). This is extremely unfortunate, since the 386 CPU, a
complete redesign of the 286, was done a decade after the Popek-Goldberg paper,
and the designers should have known better.
     In 1997, at Stanford, three of the future founders of VMware had built a proto-
type hypervisor called Disco (Bugnion et al., 1997), with the goal of running com-
modity operating systems (in particular UNIX) on a very large scale multiproces-
sor then being developed at Stanford: the FLASH machine. During that project, the
authors realized that using virtual machines could solve, simply and elegantly, a
number of hard system software problems: rather than trying to solve these prob-
lems within existing operating systems, one could innovate in a layer below exist-
ing operating systems. The key observation of Disco was that, while the high com-
plexity of modern operating systems made innovation difficult, the relative simpli-
city of a virtual machine monitor and its position in the software stack provided a
powerful foothold to address limitations of operating systems. Although Disco was
aimed at very large servers, and designed for the MIPS architecture, the authors
realized that the same approach could equally apply, and be commercially relevant,
for the x86 marketplace.
     And so, VMware, Inc. was founded in 1998 with the goal of bringing virtu-
alization to the x86 architecture and the personal computer industry. VMware's
first product (VMware Workstation) was the first virtualization solution available
for 32-bit x86-based platforms. The product was first released in 1999, and came in
two variants: VMware Workstation for Linux, a type 2 hypervisor that ran on top
of Linux host operating systems, and VMware Workstation for Windows, which



SEC. 7.12                  CASE STUDY: VMWARE                                          499
similarly ran on top of Windows NT. Both variants had identical functionality:
users could create multiple virtual machines by specifying first the characteristics
of the virtual hardware (such as how much memory to give the virtual machine, or
the size of the virtual disk) and could then install the operating system of their
choice within the virtual machine, typically from the (virtual) CD-ROM.
VMware Workstation was largely aimed at developers and IT professionals.
Before the introduction of virtualization, a developer routinely had two computers
on his desk, a stable one for development and a second one where he could rein-
stall the system software as needed. With virtualization, the second test system
became a virtual machine.
Soon, VMware started developing a second and more complex product, which
would be released as ESX Server in 2001. ESX Server leveraged the same virtu-
alization engine as VMware Workstation, but packaged it as part of a type 1 hyper-
visor. In other words, ESX Server ran directly on the hardware without requiring a
host operating system. The ESX hypervisor was designed for intense workload
consolidation and contained many optimizations to ensure that all resources (CPU,
memory, and I/O) were efficiently and fairly allocated among the virtual machines.
For example, it was the first to introduce the concept of ballooning to rebalance
memory between virtual machines (Waldspurger, 2002).
ESX Server was aimed at the server consolidation market. Before the introduc-
tion of virtualization, IT administrators would typically buy, install, and configure
a new server for every new task or application that they had to run in the data cen-
ter. The result wasthat  the infrastructure was very inefficiently utilized: servers at
the time were typically used at 10% of their capacity (during peaks). With ESX
Server, IT administrators could consolidate many independent virtual machines
into a single server, saving time, money, rack space, and electrical power.
In 2002, VMware introduced its first management solution for ESX Server,
originally called Virtual Center, and today called vSphere. It provided a single
point of management for a cluster of servers running virtual machines: an IT
administrator could now simply log into the Virtual Center application and control,
monitor, or provision thousands of virtual machines running throughout the enter-
prise. With Virtual Center came another critical innovation, VMotion (Nelson et
al., 2005), which allowed the live migration of a running virtual machine over the
network. For the first time, an IT administrator could move a running computer
from one location to another without having to reboot the operating system, restart
applications, or even lose network connections.
7.12.2 VMware Workstation
VMware Workstation was the first virtualization product for 32-bit x86 com-
puters. The subsequent adoption of virtualization had a profound impact on the in-
dustry and on the computer science community: in 2009, the ACM awarded its



500                     VIRTUALIZATION AND THE CLOUD                            CHAP. 7
authors the ACM Software System Award for VMware Workstation 1.0 for Lin-
ux. The original VMware Workstation is described in a detailed technical article
(Bugnion et al., 2012). Here we provide a summary of that paper.
     The idea was that a virtualization layer could be useful on commodity plat-
forms built from x86 CPUs and primarily running the Microsoft Windows operat-
ing systems (a.k.a. the WinTel platform). The benefits of virtualization could help
address some of the known limitations of the WinTel platform, such as application
interoperability, operating system migration, reliability, and security. In addition,
virtualization could easily enable the coexistence of operating system alternatives,
in particular, Linux.
     Although there existed decades' worth of research and commercial develop-
ment of virtualization technology on mainframes, the x86 computing environment
was sufficiently different that new approaches were necessary. For example, main-
frames were vertically integrated, meaning that a single vendor engineered the
hardware, the hypervisor, the operating systems, and most of the applications.
     In contrast, the x86 industry was (and still is) disaggregated into at least four
different categories: (a) Intel and AMD make the processors; (b) Microsoft offers
Windows and the open source community offers Linux; (c) a third group of com-
panies builds the I/O devices and peripherals and their corresponding device driv-
ers; and (d) a fourth group of system integrators such as HP and Dell put together
computer systems for retail sale. For the x86 platform, virtualization would first
need to be inserted without the support of any of these industry players.
     Because this disaggregation was a fact of life, VMware Workstation differed
from classic virtual machine monitors that were designed as part of single-vendor
architectures with explicit support for virtualization. Instead, VMware Workstation
was designed for the x86 architecture and the industry built around it. VMware
Workstation  addressed  these     new   challenges  by  combining    well-known  virtu-
alization techniques, techniques from other domains, and new techniques into a
single solution.
     We now discuss the specific technical challenges in building VMware Work-
station.
7.12.3 Challenges in Bringing Virtualization to the x86
     Recall our definition of hypervisors and virtual machines: hypervisors apply
the well-known principle of adding a level of indirection to the domain of com-
puter hardware. They provide the abstraction of virtual machines: multiple copies
of  the   underlying   hardware,  each  running     an  independent  operating   system
instance. The virtual machines are isolated from other virtual machines, appear
each as a duplicate of the underlying hardware, and ideally run with the same
speed as the real machine. VMware adapted these core attributes of a virtual ma-
chine to an x86-based target platform as follows:



SEC. 7.12                 CASE STUDY: VMWARE                                         501
1.  Compatibility.     The notion of an ``essentially identical environment''
    meant   that  any  x86  operating  system,  and  all  of  its  applications,
    would be able to run without modifications as a virtual machine. A
    hypervisor needed to provide sufficient compatibility at the hardware
    level such that users could run whichever operating system, (down to
    the update and patch version), they wished to install within a particu-
    lar virtual machine, without restrictions.
2.  Performance.       The overhead of the hypervisor had to be sufficiently
    low that users could use a virtual machine as their primary work envi-
    ronment. As a goal, the designers of VMware aimed to run relevant
    workloads at near native speeds, and in the worst case to run them on
    then-current processors with the same performance as if they were
    running natively on the immediately prior generation of processors.
    This was based on the observation that most x86 software was not de-
    signed to run only on the latest generation of CPUs.
3.  Isolation.    A hypervisor had to guarantee the isolation of the virtual
    machine without making any assumptions about the software running
    inside. That is, a hypervisor needed to be in complete control of re-
    sources.    Software running inside virtual machines had to be pre-
    vented from any access that would allow it to subvert the hypervisor.
    Similarly, a hypervisor had to ensure the privacy of all data not be-
    longing to the virtual machine. A hypervisor had to assume that the
    guest operating system could be infected with unknown, malicious
    code (a much bigger concern today than during the mainframe era).
There was an inevitable tension between these three requirements. For ex-
ample, total compatibility in certain areas might lead to a prohibitive impact on
performance, in which case VMware's designers had to compromise. However,
they ruled out any trade-offs that might compromise isolation or expose the hyper-
visor to attacks by a malicious guest. Overall, four major challenges emerged:
1.  The x86 architecture was not virtualizable.           It contained virtu-
    alization-sensitive,  nonprivileged  instructions,    which    violated     the
    Popek and Goldberg criteria for strict virtualization. For example, the
    POPF instruction has a different (yet nontrapping) semantics depend-
    ing on whether the currently running software is allowed to disable
    interrupts or not. This ruled out the traditional trap-and-emulate ap-
    proach to virtualization. Even engineers from Intel Corporation were
    convinced their processors could not be virtualized in any practical
    sense.
2.  The x86 architecture was of daunting complexity.          The x86 archi-
    tecture was a notoriously complicated CISC architecture, including



502                       VIRTUALIZATION AND THE CLOUD                          CHAP.  7
           legacy support for multiple decades of backward compatibility. Over
           the years, it had introduced four main modes of operations (real, pro-
           tected, v8086, and system management), each of which enabled in
           different ways the hardware's segmentation model, paging mechan-
           isms, protection rings, and security features (such as call gates).
     3.    x86 machines had diverse peripherals.      Although there were only
           two major x86 processor vendors, the personal computers of the time
           could contain an enormous variety of add-in cards and devices, each
           with their own vendor-specific device drivers. Virtualizing all these
           peripherals was infeasible. This had dual implications: it applied to
           both the front end (the virtual hardware exposed in the virtual ma-
           chines) and the back end (the real hardware that the hypervisor need-
           ed to be able to control) of peripherals.
     4.    Need for a simple user experience.         Classic hypervisors were in-
           stalled in the factory, similar to the firmware found in today's com-
           puters. Since VMware was a startup, its users would have to add the
           hypervisors to existing systems after the fact. VMware needed a soft-
           ware delivery model with a simple installation experience to encour-
           age adoption.
7.12.4 VMware Workstation: Solution Overview
     This section describes at a high level how VMware Workstation addressed the
challenges mentioned in the previous section.
     VMware Workstation is a type 2 hypervisor that consists of distinct modules.
One important module is the VMM, which is responsible for executing the virtual
machine's instructions.   A second important module is the VMX, which interacts
with the host operating system.
     The section covers first how the VMM solves the nonvirtualizability of the x86
architecture. Then, we describe the operating system-centric strategy used by the
designers throughout the development phase. After that, we describe the design of
the virtual hardware platform, which addresses one-half of the peripheral diversity
challenge. Finally, we discuss the role of the host operating system in VMware
Workstation, and in particular the interaction between the VMM and VMX compo-
nents.
Virtualizing the x86 Architecture
     The VMM runs the actual virtual machine; it enables it to make forward
progress.  A VMM built for a virtualizable architecture uses a technique known as
trap-and-emulate to execute the virtual machine's instruction sequence directly, but



SEC. 7.12                            CASE STUDY: VMWARE                                  503
safely, on the hardware. When this is not possible, one approach is to specify a vir-
tualizable subset of the processor architecture, and port the guest operating systems
to that newly defined platform.              This technique is known as paravirtualization
(Barham et al., 2003; Whitaker et al., 2002) and requires source-code level modifi-
cations of the operating system.     Put bluntly, paravirtualization modifies the guest
to avoid doing anything that the hypervisor cannot handle.           Paravirtualization was
infeasible at VMware because of the compatibility requirement and the need to run
operating systems whose source code was not available, in particular Windows.
An alternative would have been to employ an all-emulation approach. In this,
the instructions of the virtual machines are emulated by the VMM on the hardware
(rather than directly executed). This can be quite efficient; prior experience with
the SimOS (Rosenblum et al., 1997) machine simulator showed that the use of
techniques such as dynamic binary translation running in a user-level program
could limit overhead of complete emulation to a factor-of-five slowdown. Although
this is quite efficient, and certainly useful for simulation purposes, a factor-of-five
slowdown was clearly inadequate and would not meet the desired performance re-
quirements.
The solution to this problem combined two key insights. First, although trap-
and-emulate direct execution could not be used to virtualize the entire x86 archi-
tecture all the time, it could actually be used some of the time. In particular, it
could be used during the execution of application programs, which accounted for
most of the execution time on relevant workloads. The reasons is that these virtu-
alization sensitive instructions are not sensitive all the time; rather they are sensi-
tive only in certain circumstances. For example, the POPF instruction is virtu-
alization-sensitive when the software is expected to be able to disable interrupts
(e.g., when running the operating system), but is not virtualization-sensitive when
software cannot disable interrupts (in practice, when running nearly all user-level
applications).
Figure 7-8 shows the modular building blocks of the original VMware VMM.
We see that it consists of a direct-execution subsystem, a binary translation subsys-
tem, and a decision algorithm to determine which subsystem should be used. Both
subsystems       rely  on  some      shared  modules,  for  example  to  virtualize  memory
through shadow page tables, or to emulate I/O devices.
The direct-execution subsystem is preferred, and the dynamic binary transla-
tion subsystem provides a fallback mechanism whenever direct execution is not
possible. This is the case for example whenever the virtual machine is in such a
state  that  it  could     issue  a  virtualization-sensitive  instruction.  Therefore,  each
subsystem constantly reevaluates the decision algorithm to determine whether a
switch of subsystems is possible (from binary translation to direct execution) or
necessary (from direct execution to binary translation). This algorithm has a num-
ber of input parameters, such as the current execution ring of the virtual machine,
whether interrupts can be enabled at that level, and the state of the segments. For
example, binary translation must be used if any of the following is true:



504                    VIRTUALIZATION AND THE CLOUD                                   CHAP. 7
                  VMM
                                            Decision
                                            Alg.
                       Direct Execution                     Binary translation
                                            Shared modules
                             (shadow MMU, I/O handling, ...)
         Figure 7-8. High-level components of the VMware virtual machine monitor (in
         the absence of hardware support).
     1.  The virtual machine is currently running in kernel mode (ring 0 in the
         x86 architecture).
     2.  The virtual machine can disable interrupts and issue I/O instructions
         (in the x86 architecture, when the I/O privilege level is set to the ring
         level).
     3.  The virtual machine is currently running in real mode, a legacy 16-bit
         execution mode used by the BIOS among other things.
     The actual decision algorithm contains a few additional conditions. The details
can be found in Bugnion et al. (2012). Interestingly, the algorithm does not depend
on the instructions that are stored in memory and may be executed, but only on the
value of a few virtual registers; therefore it can be evaluated very efficiently in just
a handful of instructions.
     The second key insight was that by properly configuring the hardware, particu-
larly using the x86 segment protection mechanisms carefully, system code under
dynamic binary translation could also run at near-native speeds. This is very dif-
ferent than the factor-of-five slowdown normally expected of machine simulators.
     The difference can be explained by comparing how a dynamic binary translator
converts a simple instruction that accesses memory. To emulate such an instruction
in software, a classic binary translator emulating the full x86 instruction-set archi-
tecture would have to first verify whether the effective address is within the range
of the data segment, then convert the address into a physical address, and finally to
copy the referenced word into the simulated register. Of course, these various steps
can be optimized through caching, in a way very similar to how the processor
cached page-table mappings in a translation-lookaside buffer. But even such opti-
mizations would lead to an expansion of individual instructions into an instruction
sequence.
     The VMware binary translator performs none of these steps in software. In-
stead, it configures the hardware so that this simple instruction can be reissued



SEC. 7.12                        CASE STUDY: VMWARE                                       505
with the identical instruction.  This is possible only because the VMware VMM (of
which the binary translator is a component) has previously configured the hard-
ware to match the exact specification of the virtual machine: (a) the VMM uses
shadow page tables, which ensures that the memory management unit can be used
directly (rather than emulated) and (b) the VMM uses a similar shadowing ap-
proach to the segment descriptor tables (which played a big role in the 16-bit and
32-bit software running on older x86 operating systems).
There are, of course, complications and subtleties. One important aspect of the
design is to ensure the integrity of the virtualization sandbox, that is, to ensure that
no software running inside the virtual machine (including malicious software) can
tamper with the VMM. This problem is generally known as software fault isola-
tion and adds run-time overhead to each memory access if the solution is imple-
mented in software. Here also, the VMware VMM uses a different, hardware-based
approach. It splits the address space into two disjoint zones. The VMM reserves
for its own use the top 4 MB of the address space. This frees up the rest (that is, 4
GB - 4 MB, since we are talking about a 32-bit architecture) for the use by the vir-
tual machine. The VMM then configures the segmentation hardware so that no vir-
tual machine instructions (including ones generated by the binary translator) can
ever access the top 4-MB region of the address space.
A Guest Operating System Centric Strategy
Ideally, a VMM should be designed without worrying about the guest operat-
ing system running in the virtual machine, or how that guest operating system con-
figures the hardware. The idea behind virtualization is to make the virtual machine
interface identical to the hardware interface so that all software that runs on the
hardware will also run in a virtual machine. Unfortunately, this approach is practi-
cal only when the architecture is virtualizeable and simple. In the case of x86, the
overwhelming complexity of the architecture was clearly a problem.
The VMware engineers simplified the problem by focusing only on a selection
of supported guest operating systems. In its first release, VMware Workstation sup-
ported officially only Linux, Windows 3.1, Windows 95/98 and Windows NT as
guest operating systems. Over the years, new operating systems were added to the
list with each revision of the software. Nevertheless, the emulation was good
enough that it ran some unexpected operating systems, such as MINIX 3, perfectly,
right out of the box.
This simplification did not change the overall design--the VMM still provided
a faithful copy of the underlying hardware, but it helped guide the development
process. In particular, engineers had to worry only about combinations of features
that were used in practice by the supported guest operating systems.
For example, the x86 architecture contains four privilege rings in protected
mode (ring 0 to ring 3) but no operating system uses ring 1 or ring 2 in practice
(save for OS/2, a long-dead operating system from IBM). So rather than figure out



506                   VIRTUALIZATION AND THE CLOUD         CHAP. 7
how to correctly virtualize ring 1 and ring 2, the VMware VMM simply had code
to detect if a guest was trying to enter into ring 1 or ring 2, and, in that case, would
abort execution of the virtual machine.  This not only removed unnecessary code,
but more importantly it allowed the VMware VMM to assume that ring 1 and ring
2 would never be used by the virtual machine, and therefore that it could use these
rings for its own purposes. In fact, the VMware VMM's binary translator runs at
ring 1 to virtualize ring 0 code.
The Virtual Hardware Platform
     So far, we have primarily discussed the problem associated with the virtu-
alization of the x86 processor. But an x86-based computer is much more than its
processor. It also has a chipset, some firmware, and a set of I/O peripherals to con-
trol disks, network cards, CD-ROM, keyboard, etc.
     The diversity of I/O peripherals in x86 personal computers made it impossible
to match the virtual hardware to the real, underlying hardware. Whereas there were
only a handful of x86 processor models in the market, with only minor variations
in instruction-set level capabilities, there were thousands of I/O devices, most of
which had no publicly available documentation of their interface or functionality.
VMware's key insight was to not attempt to have the virtual hardware match the
specific underlying hardware, but instead have it always match some configuration
composed of selected, canonical I/O devices. Guest operating systems then used
their own existing, built-in mechanisms to detect and operate these (virtual) de-
vices.
     The virtualization platform consisted of a combination of multiplexed and
emulated components. Multiplexing meant configuring the hardware so it can be
directly used by the virtual machine, and shared (in space or time) across multiple
virtual machines. Emulation meant exporting a software simulation of the selected,
canonical hardware component to the virtual machine.       Figure 7-9 shows that
VMware Workstation used multiplexing for processor and memory and emulation
for everything else.
     For the multiplexed hardware, each virtual machine had the illusion of having
one dedicated CPU and a configurable, but a fixed amount of contiguous RAM
starting at physical address 0.
     Architecturally, the emulation of each virtual device was split between a front-
end component, which was visible to the virtual machine, and a back-end compo-
nent, which interacted with the host operating system (Waldspurger and Rosen-
blum, 2012). The front-end was essentially a software model of the hardware de-
vice that could be controlled by unmodified device drivers running inside the virtu-
al machine. Regardless of the specific corresponding physical hardware on the
host, the front end always exposed the same device model.
     For example, the first Ethernet device front end was the AMD PCnet ``Lance''
chip, once a popular 10-Mbps plug-in board on PCs, and the back end provided



SEC. 7.12                              CASE STUDY: VMWARE                                             507
             Virtual Hardware (front end)           Back end
Multiplexed  1 virtual x86 CPU, with the same       Scheduled by the host operating system on
             instruction set extensions as the un-  either a uniprocessor or multiprocessor host
             derlying hardware CUP
             Up to 512 MB of contiguous DRAM        Allocated and managed by the host OS
                                                    (page-by-page)
             PCI Bus                                Fully emulated compliant PCI bus
             4x IDE disks                           Virtual disks (stored as files) or direct access
             7x Buslogic SCSI Disks                 to a given raw device
             1x IDE CD-ROM                          ISO image or emulated access to the real
                                                    CD-ROM
             2x 1.44 MB floppy drives               Physical floppy or floppy image
Emulated     1x VMware graphics card with VGA       Ran in a window and in full-screen mode.
             and SVGA support                       SVGA required VMware SVGA guest driver
             2x serial ports COM1 and COM2          Connect to host serial port or a file
             1x printer (LPT)                       Can connect to host LPT port
             1x keyboard (104-key)                  Fully emulated; keycode events are gen-
                                                    erated when they are received by the VMware
                                                    application
             1x PS-2 mouse                          Same as keyboard
             3x AMD Lance Ethernet cards            Bridge mode and host-only modes
             1x Soundblaster                        Fully emulated
             Figure 7-9. Virtual     hardware  configuration  options  of  the  early  VMware
             Workstation, ca. 2000.
network connectivity to the host's physical network. Ironically, VMware kept sup-
porting the PCnet device long after physical Lance boards were no longer avail-
able, and actually achieved I/O that was orders of magnitude faster than 10 Mbps
(Sugerman et al., 2001).             For storage devices, the original front ends were an IDE
controller and a Buslogic Controller, and the back end was typically either a file in
the host file system, such as a virtual disk or an ISO 9660 image, or a raw resource
such as a drive partition or the physical CD-ROM.
Splitting front ends from back ends had another benefit: a VMware virtual ma-
chine could be copied from computer to another computer, possibly with different
hardware devices. Yet, the virtual machine would not have to install new device
drivers since it only interacted with the front-end component. This attribute, called
hardware-independent encapsulation, has a huge benefit today in server envi-
ronments and in cloud computing. It enabled subsequent innovations such as sus-
pend/resume, checkpointing, and the transparent migration of live virtual machines



508                     VIRTUALIZATION AND THE CLOUD                               CHAP. 7
across physical boundaries (Nelson et al., 2005). In the cloud, it allows customers
to deploy their virtual machines on any available server, without having to worry of
the details of the underlying hardware.
The Role of the Host Operating System
     The final critical design decision in VMware Workstation was to deploy it ``on
top'' of an existing operating system. This classifies it as a type 2 hypervisor. The
choice had two main benefits.
     First,  it  would  address  the  second  part  of  peripheral  diversity  challenge.
VMware implemented the front-end emulation of the various devices, but relied on
the device drivers of the host operating system for the back end. For example,
VMware Workstation would read or write a file in the host file system to emulate a
virtual disk device, or draw in a window of the host's desktop to emulate a video
card. As long as the host operating system had the appropriate drivers, VMware
Workstation could run virtual machines on top of it.
     Second, the product could install and feel like a normal application to a user,
making adoption easier. Like any application, the VMware Workstation installer
simply writes its component files onto an existing host file system, without per-
turbing the hardware configuration (no reformatting of a disk, creating of a disk
partition, or changing of BIOS settings). In fact, VMware Workstation could be in-
stalled and start running virtual machines without requiring even rebooting the host
operating system, at least on Linux hosts.
     However, a normal application does not have the necessary hooks and APIs
necessary for a hypervisor to multiplex the CPU and memory resources, which is
essential to provide near-native performance. In particular, the core x86 virtu-
alization technology described above works only when the VMM runs in kernel
mode and can furthermore control all aspects of the processor without any restric-
tions. This includes the ability to change the address space (to create shadow page
tables), to change the segment tables, and to change all interrupt and exception
handlers.
     A device driver has more direct access to the hardware, in particular if it runs
in kernel mode.  Although it could (in theory) issue any privileged instructions, in
practice a device driver is expected to interact with its operating system using
well-defined APIs, and does not (and should never) arbitrarily reconfigure the
hardware. And since hypervisors call for a massive reconfiguration of the hardware
(including the entire address space, segment tables, exception and interrupt hand-
lers), running the hypervisor as a device driver was also not a realistic option.
     Since none of these assumptions are supported by host operating systems, run-
ning the hypervisor as a device driver (in kernel mode) was also not an option.
     These stringent requirements led to the development of the VMware Hosted
Architecture.    In it, as shown in Fig. 7-10, the software is broken into three sepa-
rate and distinct components.



SEC. 7.12                       CASE STUDY: VMWARE                                  509
           Any           VMX                                      Virtual Machine
           Proc.                                                                    User mode
           Host OS       write()             (v)
Disk               fs
                                             VMM     world
                   scsi                      Driver  switch                         Kernel mode
                                                     (ii)    VMM
                                int handler  (iv)                      int handler
CPU                      (iii)                                    (i)
IDTR
                         Host OS Context                          VMM Context
      Figure 7-10. The VMware Hosted Architecture and its three components: VMX,
      VMM driver and VMM.
These components each have different functions and operate independently
from one another:
1.    A user-space program (the VMX) which the user perceives to be the
      VMware program. The VMX performs all UI functions, starts the vir-
      tual machine, and then performs most of the device emulation (front
      end), and makes regular system calls to the host operating system for
      the back end interactions. There is typically one multithreaded VMX
      process per virtual machine.
2.    A small kernel-mode device driver (the VMX driver), which gets in-
      stalled within the host operating system. It is used primarily to allow
      the VMM to run by temporarily suspending the entire host operating
      system. There is one VMX driver installed in the host operating sys-
      tem, typically at boot time.
3.    The VMM, which includes all the software necessary to multiplex the
      CPU and the memory, including the exception handlers, the trap-and-
      emulate handlers, the binary translator, and the shadow paging mod-
      ule. The VMM runs in kernel mode, but it does not run in the context
      of the host operating system. In other words, it cannot rely directly on
      services offered by the host operating system, but it is also not con-
      strained by any rules or conventions imposed by the host operating
      system. There is one VMM instance for each virtual machine, created
      when the virtual machine starts.



510                                   VIRTUALIZATION AND THE CLOUD                                    CHAP. 7
     VMware Workstation appears to run on top of an existing operating system,
and, in fact, its VMX does run as a process of that operating system. However, the
VMM operates at system level, in full control of the hardware, and without de-
pending on any way on the host operating system.        Figure 7-10 shows the relation-
ship between the entities: the two contexts (host operating system and VMM) are
peers to each other, and each has a user-level and a kernel component. When the
VMM runs (the right half of the figure), it reconfigures the hardware, handles all
I/O interrupts and exceptions, and can therefore safely temporarily remove the host
operating system from its virtual memory. For example, the location of the inter-
rupt table is set within the VMM by assigning the IDTR register to a new address.
Conversely, when the host operating system runs (the left half of the figure), the
VMM and its virtual machine are equally removed from its virtual memory.
     This transition between these two totally independent system-level contexts is
called a world switch.                The name itself emphasizes that everything about the soft-
ware changes during a world switch, in contrast with the regular context switch im-
plemented by an operating system.     Figure 7-11 shows the difference between the
two. The regular context switch between processes ``A'' and ``B'' swaps the user
portion of the address space and the registers of the two processes, but leaves a
number of critical system resources unmodified. For example, the kernel portion of
the address space is identical for all processes, and the exception handlers are also
not modified. In contrast, the world switch changes everything: the entire address
space, all exception handlers, privileged registers, etc. In particular, the kernel ad-
dress space of the host operating system is mapped only when running in the host
operating system context. After the world switch into the VMM context, it has
been removed from the address space altogether, freeing space to run both the
VMM and the virtual machine. Although this sounds complicated, this can be im-
plemented quite efficiently and takes only 45 x86 machine-language instructions to
execute.
                                      Linear Address space
             Context Switch  Process  A (user-space)    Kernel Address space
     Normal                  A
                             Process  B (user-space)    Kernel Address space
                             B
             World Switch    Host OS  VMX (user-space)  Kernel Address space (host OS)
     VMware                  Context
                             VMM      Virtual Machine                                                 VMM
                             Context
             Figure 7-11.             Difference between a normal context switch and a world switch.



SEC. 7.12                 CASE STUDY: VMWARE                                            511
The careful reader will have wondered: what of the guest operating system's
kernel address space? The answer is simply that it is part of the virtual machine ad-
dress space, and is present when running in the VMM context. Therefore, the guest
operating system can use the entire address space, and in particular the same loca-
tions in virtual memory as the host operating system. This is very specifically what
happens when the host and guest operating systems are the same (e.g., both are
Linux). Of course, this all ``just works'' because of the two independent contexts
and the world switch between the two.
The same reader will then wonder: what of the VMM area, at the very top of
the address space? As we discussed above, it is reserved for the VMM itself, and
those portions of the address space cannot be directly used by the virtual machine.
Luckily, that small 4-MB portion is not frequently used by the guest operating sys-
tems since each access to that portion of memory must be individually emulated
and induces noticeable software overhead.
Going back to Fig. 7-10: it further illustrates the various steps that occur when
a disk interrupt happens while the VMM is executing (step i). Of course, the VMM
cannot handle the interrupt since it does not have the back-end device driver. In
(ii), the VMM does a world switch back to the host operating system. Specifically,
the world-switch code returns control to the VMware driver, which in (iii) emulates
the same interrupt that was issued by the disk. So in step (iv), the interrupt handler
of the host operating system runs through its logic, as if the disk interrupt had oc-
curred while the VMware driver (but not the VMM!) was running. Finally, in step
(v), the VMware driver returns control to the VMX application. At this point, the
host operating system may choose to schedule another process, or keep running the
VMware VMX process. If the VMX process keeps running, it will then resume ex-
ecution of the virtual machine by doing a special call into the device driver, which
will generate a world switch back into the VMM context. As you see, this is a neat
trick that hides the entire VMM and virtual machine from the host operating sys-
tem. More importantly, it provides the VMM complete freedom to reprogram the
hardware as it sees fit.
7.12.5 The Evolution of VMware Workstation
The technology landscape has changed dramatically in the decade following
the development of the original VMware Virtual Machine Monitor.
The hosted architecture is still used today for state-of-the-art interactive hyper-
visors such as VMware Workstation, VMware Player, and VMware Fusion (the
product aimed at Apple OS X host operating systems), and even in VMware's
product aimed at cell phones (Barr et al., 2010). The world switch, and its ability to
separate the host operating system context from the VMM context, remains the
foundational mechanism of VMware's hosted products today. Although the imple-
mentation of the world switch has evolved through the years, for example, to



512                      VIRTUALIZATION AND THE CLOUD                           CHAP. 7
support 64-bit systems, the fundamental idea of having totally separate address
spaces for the host operating system and the VMM remains valid today.
      In contrast, the approach to the virtualization of the x86 architecture changed
rather dramatically with the introduction of hardware-assisted virtualization. Hard-
ware-assisted virtualizations, such as Intel VT-x and AMD-v were introduced in
two phases. The first phase, starting in 2005, was designed with the explicit pur-
pose of eliminating the need for either paravirtualization or binary translation
(Uhlig et al., 2005). Starting in 2007, the second phase provided hardware support
in the MMU in the form of nested page tables. This eliminated the need to main-
tain shadow page tables in software. Today, VMware's hypervisors mostly uses a
hardware-based, trap-and-emulate approach (as formalized by Popek and Goldberg
four  decades  earlier)  whenever  the  processor    supports  both  virtualization     and
nested page tables.
      The emergence of hardware support for virtualization had a significant impact
on VMware's guest operating system centric-strategy. In the original VMware
Workstation, the strategy was used to dramatically reduce implementation com-
plexity at the expense of compatibility with the full architecture. Today, full archi-
tectural  compatibility  is  expected   because  of  hardware  support.    The  current
VMware guest operating system-centric strategy focuses on performance optimiza-
tions for selected guest operating systems.
7.12.6 ESX Server: VMware's type 1 Hypervisor
      In 2001, VMware released a different product, called ESX Server, aimed at the
server marketplace. Here, VMware's engineers took a different approach: rather
than creating a type 2 solution running on top of a host operating system, they de-
cided to build a type 1 solution that would run directly on the hardware.
      Figure 7-12 shows the high-level architecture of ESX Server. It combines an
existing component, the VMM, with a true hypervisor running directly on the bare
metal. The VMM performs the same function as in VMware Workstation, which is
to run the virtual machine in an isolated environment that is a duplicate of the x86
architecture. As a matter of fact, the VMMs used in the two products use the same
source code base, and they are largely identical. The ESX hypervisor replaces the
host operating system. But rather than implementing the full functionality expected
of an operating system, its only goal is to run the various VMM instances and to
efficiently manage the physical resources of the machine. ESX Server therefore
contains the usual subsystem found in an operating system, such as a CPU sched-
uler, a memory manager, and an I/O subsystem, with each subsystem optimized to
run virtual machines.
      The absence of a host operating system required VMware to directly address
the issues of peripheral diversity and user experience described earlier. For periph-
eral diversity, VMware restricted ESX Server to run only on well-known and certi-
fied server platforms, for which it had device drivers. As for the user experience,



SEC. 7.12                   CASE STUDY: VMWARE                                        513
                     VM          VM       VM           VM
                     VMM    VMM           VMM          VMM
           ESX
                                 ESX hypervisor
                                     x86
                     Figure 7-12. ESX Server: VMware's type 1 hypervisor.
ESX Server (unlike VMware Workstation) required users to install a new system
image on a boot partition.
Despite the drawbacks, the trade-off made sense for dedicated deployments of
virtualization in data centers, consisting of hundreds or thousands of physical ser-
vers, and often (many) thousands of virtual machines. Such deployments are some-
times referred today as private clouds. There, the ESX Server architecture provides
substantial benefits in terms of performance, scalability, manageability, and fea-
tures. For example:
1.  The CPU scheduler ensures that each virtual machine gets a fair share
    of the CPU (to avoid starvation). It is also designed so that the dif-
    ferent virtual CPUs of a given multiprocessor virtual machine are
    scheduled at the same time.
2.  The memory manager is optimized for scalability, in particular to run
    virtual machines efficiently even when they need more memory than
    is actually available on the computer. To achieve this result, ESX Ser-
    ver first introduced the notion of ballooning and transparent page
    sharing for virtual machines (Waldspurger, 2002).
3.  The I/O subsystem is optimized for performance. Although VMware
    Workstation and ESX Server often share the same front-end emula-
    tion components, the back ends are totally different. In the VMware
    Workstation case, all I/O flows through the host operating system and
    its API, which often adds overhead. This is particularly true in the
    case of networking and storage devices. With ESX Server, these de-
    vice drivers run directly within the ESX hypervisor, without requiring
    a world switch.
4.  The back ends also typically relied on abstractions provided by the
    host operating system. For example, VMware Workstation stores vir-
    tual machine images as regular (but very large) files on the host file
    system. In contrast, ESX Server has VMFS (Vaghani, 2010), a file



514                    VIRTUALIZATION AND THE CLOUD                         CHAP. 7
         system optimized specifically to store virtual machine images and
         ensure high I/O throughput. This allows for extreme levels of per-
         formance. For example, VMware demonstrated back in 2011 that a
         single ESX Server could issue 1 million disk operations per second
         (VMware, 2011).
     5.  ESX Server made it easy to introduce new capabilities, which re-
         quired the tight coordination and specific configuration of multiple
         components    of  a  computer.  For  example,  ESX   Server  introduced
         VMotion, the first virtualization solution that could migrate a live vir-
         tual machine from one machine running ESX Server to another ma-
         chine running ESX Server, while it was running. This achievement re-
         quired the coordination of the memory manager, the CPU scheduler,
         and the networking stack.
     Over the years, new features were added to ESX Server. ESX Server evolved
into ESXi, a small-footprint alternative that is sufficiently small in size to be
pre-installed in the firmware of servers. Today, ESXi is VMware's most important
product and serves as the foundation of the vSphere suite.
7.13 RESEARCH ON VIRTUALIZATION AND THE CLOUD
     Virtualization technology and cloud computing are both extremely active re-
search areas.   The research produced in these fields is way too much to enumerate.
Each has multiple research conferences.  For instance, the Virtual Execution Envi-
ronments (VEE) conference focuses on virtualization in the broadest sense.               You
will find papers on migration deduplication, scaling out, and so on.  Likewise, the
ACM Symposium on Cloud Computing (SOCC) is one of the best-known venues
on cloud computing.    Papers in SOCC include work on fault resilience, scheduling
of data center workloads, management and debugging in clouds, and so on.
     Old topics never really die, as in Penneman et al. (2013), which looks at the
problems of virtualizing the ARM in the light of the Popek and Goldberg criteria.
Security is perpetually a hot topic (Beham et al., 2013; Mao, 2013; and Pearce et
al., 2013), as is reducing energy usage (Botero and Hesselbach, 2013; and Yuan et
al., 2013). With so many data centers now using virtualization technology, the net-
works connecting these machines are also a major subject of research (Theodorou
et al., 2013).  Virtualization in wireless networks is also an up-and-coming subject
(Wang et al., 2013a).
     One interesting area which has seen a lot of interesting research is nested virtu-
alization (Ben-Yehuda et al., 2010; and Zhang et al., 2011).  The idea is that a vir-
tual machine itself can be further virtualized into multiple higher-level virtual ma-
chines, which in turn may be virtualized and so on. One of these projects is appro-
priately called ``Turtles,'' because once you start, ``It's Turtles all the way down!''



SEC. 7.13    RESEARCH ON VIRTUALIZATION AND THE CLOUD                                        515
     One of the nice things about virtualization hardware is that untrusted code can
get direct but safe access to hardware features like page tables, and tagged TLBs.
With this in mind, the Dune project (Belay, 2012) does not aim to provide a ma-
chine abstraction, but rather it provides a process abstraction.             The process is able
to enter Dune mode, an irreversible transition that gives it access to the low-level
hardware. Nevertheless, it is still a process and able to talk to and rely on the ker-
nel. The only difference that it uses the VMCALL instruction to make a system call.
                                           PROBLEMS
1.   Give a reason why a data center might be interested in virtualization.
2.   Give a reason why a company might be interested in running a hypervisor on a ma-
     chine that has been in use for a while.
3.   Give a reason why a software developer might use virtualization on a desktop machine
     being used for development.
4.   Give a reason why an individual at home might be interested in virtualization.
5.   Why do you think virtualization took so long to become popular? After all, the key
     paper was written in 1974 and IBM mainframes had the necessary hardware and soft-
     ware throughout the 1970s and beyond.
6.   Name two kinds of instructions that are sensitive in the Popek and Goldberg sense.
7.   Name three machine instructions that are not sensitive in the Popek and Goldberg
     sense.
8.   What is the difference between full virtualization and paravirtualization? Which do
     you think is harder to do? Explain your answer.
9.   Does it make sense to paravirtualize an operating system if the source code is avail-
     able? What if it is not?
10.  Consider a type 1 hypervisor that can support up to n virtual machines at the same
     time. PCs can have a maximum of four disk primary partitions. Can n be larger than 4?
     If so, where can the data be stored?
11.  Briefly explain the concept of process-level virtualization.
12.  Why do type 2 hypervisors exist? After all, there is nothing they can do that type 1
     hypervisors cannot do and the type 1 hypervisors are generally more efficient as well.
13.  Is virtualization of any use to type 2 hypervisors?
14.  Why was binary translation invented? Do you think it has much of a future? Explain
     your answer.
15.  Explain how the x86's four protection rings can be used to support virtualization.
16.  State one reason as to why a hardware-based approach using VT-enabled CPUs can
     perform poorly when compared to translation-based software approaches.



516                         VIRTUALIZATION AND THE CLOUD                             CHAP. 7
17.  Give one case where a translated code can be faster than the original code, in a system
     using binary translation.
18.  VMware does binary translation one basic block at a time, then it executes the block
     and starts translating the next one. Could it translate the entire program in advance and
     then execute it? If so, what are the advantages and disadvantages of each technique?
19.  What is the difference between a pure hypervisor and a pure microkernel?
20.  Briefly explain why memory is so difficult to virtualize. well in practice? Explain your
     answer.
21.  Running multiple virtual machines on a PC is known to require large amounts of mem-
     ory. Why? Can you think of any ways to reduce the memory usage? Explain.
22.  Explain the concept of shadow page tables, as used in memory virtualization.
23.  One way to handle guest operating systems that change their page tables using ordin-
     ary (nonprivileged) instructions is to mark the page tables as read only and take a trap
     when they are modified.    How else could the shadow page tables be maintained?            Dis-
     cuss the efficiency of your approach vs. the read-only page tables.
24.  Why are balloon drivers used? Is this cheating?
25.  Descibe a situation in which balloon drivers do not work.
26.  Explain the concept of deduplication as used in memory virtualization.
27.  Computers have had DMA for doing I/O for decades. Did this cause any problems be-
     fore there were I/O MMUs?
28.  Give one advantage of cloud computing over running your programs locally. Give one
     disadvantage as well.
29.  Give an example of IAAS, PAAS, and SAAS.
30.  Why is virtual machine migration important? Under what circumstances might it be
     useful?
31.  Migrating virtual machines may be easier than migrating processes, but migration can
     still be difficult. What problems can arise when migrating a virtual machine?
32.  Why is migration of virtual machines from one machine to another easier than migrat-
     ing processes from one machine to another?
33.  What is the difference between live migration and the other kind (dead migration?)?
34.  What were the three main requirements considered while designing VMware?
35.  Why  was  the  enormous    number  of  peripheral  devices  available        a  problem  when
     VMware Workstation was first introduced?
36.  VMware ESXi has been made very small. Why? After all, servers at data centers
     usually have tens of gigabytes of RAM. What difference does a few tens of megabytes
     more or less make?
37.  Do an Internet search to find two real-life examples of virtual appliances.
