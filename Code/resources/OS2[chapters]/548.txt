MULTIPLE PROCESSOR SYSTEMS


                                   8
MULTIPLE PROCESSOR SYSTEMS
Since its inception, the computer industry has been driven by an endless quest
for more and more computing power. The ENIAC could perform 300 operations
per second, easily 1000 times faster than any calculator before it, yet people were
not satisfied with it.  We now have machines millions of times faster than the
ENIAC and still there is a demand for yet more horsepower. Astronomers are try-
ing to make sense of the universe, biologists are trying to understand the implica-
tions of the human genome, and aeronautical engineers are interested in building
safer and more efficient aircraft, and all want more CPU cycles. However much
computing power there is, it is never enough.
In the past, the solution was always to make the clock run faster. Unfortunate-
ly, we have begun to hit some fundamental limits on clock speed.  According to
Einstein's special theory of relativity, no electrical signal can propagate faster than
the speed of light, which is about 30 cm/nsec in vacuum and about 20 cm/nsec in
copper wire or optical fiber. This means that in a computer with a 10-GHz clock,
the signals cannot travel more than 2 cm in total. For a 100-GHz computer the total
path length is at most 2 mm. A 1-THz (1000-GHz) computer will have to be smal-
ler than 100 microns, just to let the signal get from one end to the other and back
once within a single clock cycle.
Making computers this small may be possible, but then we hit another funda-
mental problem: heat dissipation. The faster the computer runs, the more heat it
generates, and the smaller the computer, the harder it is to get rid of this heat. Al-
ready on high-end x86 systems, the CPU cooler is bigger than the CPU itself. All
                                   517



518                      MULTIPLE PROCESSOR SYSTEMS                            CHAP. 8
in all, going from 1 MHz to 1 GHz simply required incrementally better engineer-
ing of the chip manufacturing process. Going from 1 GHz to 1 THz is going to re-
quire a radically different approach.
     One approach to greater speed is through massively parallel computers. These
machines consist of many CPUs, each of which runs at ``normal'' speed (whatever
that may mean in a given year), but which collectively have far more computing
power than a single CPU.  Systems with tens of thousands of CPUs are now com-
mercially available. Systems with 1 million CPUs are already being built in the lab
(Furber et al., 2013).  While there are other potential approaches to greater speed,
such as biological computers, in this chapter we will focus on systems with multi-
ple conventional CPUs.
     Highly parallel computers are frequently used for heavy-duty number crunch-
ing. Problems such as predicting the weather, modeling airflow around an aircraft
wing, simulating the world economy, or understanding drug-receptor interactions
in the brain are all computationally intensive. Their solutions require long runs on
many CPUs at once.      The multiple processor systems discussed in this chapter are
widely used for these and similar problems in science and engineering, among
other areas.
     Another relevant development is the incredibly rapid growth of the Internet.      It
was originally designed as a prototype for a fault-tolerant military control system,
then became popular among academic computer scientists, and long ago acquired
many new uses. One of these is linking up thousands of computers all over the
world to work together on large scientific problems.    In a sense, a system consist-
ing of 1000 computers spread all over the world is no different than one consisting
of 1000 computers in a single room, although the delay and other technical charac-
teristics are different. We will also consider these systems in this chapter.
     Putting 1 million unrelated computers in a room is easy to do provided that
you have enough money and a sufficiently large room. Spreading 1 million unrelat-
ed computers around the world is even easier since it finesses the second problem.
The trouble comes in when you want them to communicate with one another to
work together on a single problem.     As a consequence, a great deal of work has
been done on interconnection technology, and different interconnect technologies
have led to qualitatively different kinds of systems and different software organiza-
tions.
     All  communication   between      electronic  (or  optical)  components   ultimately
comes down to sending messages--well-defined bit strings--between them. The
differences are in the time scale, distance scale, and logical organization involved.
At one extreme are the shared-memory multiprocessors, in which somewhere be-
tween two and about 1000 CPUs communicate via a shared memory.                 In this
model, every CPU has equal access to the entire physical memory, and can read
and write individual words using LOAD and STORE instructions. Accessing a mem-
ory word usually takes 1­10 nsec.      As we shall see, it is now common to put more
than one processing core on a single CPU chip, with the cores sharing access to



SEC. 8.1                          MULTIPROCESSORS                                       519
main memory (and sometimes even sharing caches). In other words, the model of
shared-memory multicomputers may be implemented using physically separate
CPUs, multiple cores on a single CPU, or a combination of the above.                   While this
model, illustrated in Fig. 8-1(a), sounds simple, actually implementing it is not
really so simple and usually involves considerable message passing under the cov-
ers, as we will explain shortly.  However, this message passing is invisible to the
programmers.
                       Local
   CPU                 memory                                          Complete system
                                     M  M               M  M
C        C       C  C                C  C               C  C     C+ M  C+ M            C+ M
C                   C             M  C  Inter-             C  M
         Shared                         connect                        Internet
   memory                         M  C                     C  M
C                   C
                                     C  C               C  C
C        C       C  C                M  M               M  M     C+ M  C+ M            C+ M
            (a)                            (b)                         (c)
         Figure 8-1. (a) A shared-memory multiprocessor. (b) A message-passing multi-
         computer. (c) A wide area distributed system.
Next comes the system of Fig. 8-1(b) in which the CPU-memory pairs are con-
nected by a high-speed interconnect. This kind of system is called a message-pas-
sing multicomputer.    Each memory is local to a single CPU and can be accessed
only by that CPU.      The CPUs communicate by sending multiword messages over
the interconnect. With a good interconnect, a short message can be sent in 10­50
sec, but still far longer than the memory access time of Fig. 8-1(a).       There is no
shared global memory in this design. Multicomputers (i.e., message-passing sys-
tems) are much easier to build than (shared-memory) multiprocessors, but they are
harder to program. Thus each genre has its fans.
The third model, which is illustrated in Fig. 8-1(c), connects complete com-
puter systems over a wide area network, such as the Internet, to form a distributed
system.     Each of these has its own memory and the systems communicate by mes-
sage passing. The only real difference between Fig. 8-1(b) and Fig. 8-1(c) is that in
the latter, complete computers are used and message times are often 10­100 msec.
This long delay forces these loosely coupled systems to be used in different ways
than the tightly coupled systems of Fig. 8-1(b).              The three types of systems differ
in their delays by something like three orders of magnitude. That is the difference
between a day and three years.
This chapter has three major sections, corresponding to each of the three mod-
els of Fig. 8-1.    In each model discussed in this chapter, we start out with a brief



520                         MULTIPLE PROCESSOR SYSTEMS                       CHAP. 8
introduction to the relevant hardware. Then we move on to the software, especially
the operating system issues for that type of system.     As we will see, in each case
different issues are present and different approaches are needed.
8.1 MULTIPROCESSORS
     A shared-memory multiprocessor (or just multiprocessor henceforth) is a
computer system in which two or more CPUs share full access to a common RAM.
A program running on any of the CPUs sees a normal (usually paged) virtual ad-
dress space. The only unusual property this system has is that the CPU can write
some value into a memory word and then read the word back and get a different
value (because another CPU has changed it).      When organized correctly, this prop-
erty forms the basis of interprocessor communication: one CPU writes some data
into memory and another one reads the data out.
     For the most part, multiprocessor operating systems are normal operating sys-
tems. They handle system calls, do memory management, provide a file system,
and manage I/O devices. Nevertheless, there are some areas in which they have
unique features.    These include process synchronization, resource management,
and scheduling. Below we will first take a brief look at multiprocessor hardware
and then move on to these operating systems' issues.
8.1.1 Multiprocessor Hardware
     Although all multiprocessors have the property that every CPU can address all
of memory, some multiprocessors have the additional property that every memory
word can be read as fast as every other memory word. These machines are called
UMA (Uniform Memory Access) multiprocessors. In contrast, NUMA (Nonuni-
form Memory Access) multiprocessors do not have this property. Why this dif-
ference exists will become clear later. We will first examine UMA multiprocessors
and then move on to NUMA multiprocessors.
UMA Multiprocessors with Bus-Based Architectures
     The  simplest  multiprocessors  are  based  on   a  single    bus,  as  illustrated  in
Fig. 8-2(a). Two or more CPUs and one or more memory modules all use the same
bus for communication. When a CPU wants to read a memory word, it first checks
to see if the bus is busy.  If the bus is idle, the CPU puts the address of the word it
wants on the bus, asserts a few control signals, and waits until the memory puts the
desired word on the bus.
     If the bus is busy when a CPU wants to read or write memory, the CPU just
waits until the bus becomes idle. Herein lies the problem with this design.  With
two or three CPUs, contention for the bus will be manageable; with 32 or 64 it will
be unbearable. The system will be totally limited by the bandwidth of the bus, and
most of the CPUs will be idle most of the time.



SEC. 8.1                          MULTIPROCESSORS                                            521
                                                 Private memory                           Shared
                   Shared memory                                                    memory
CPU       CPU      M              CPU       CPU        M             CPU       CPU        M
                                     Cache
     Bus
              (a)                           (b)                                (c)
     Figure 8-2.   Three  bus-based  multiprocessors.  (a)  Without  caching.  (b)  With
     caching. (c) With caching and private memories.
The solution to this problem is to add a cache to each CPU, as depicted in
Fig. 8-2(b).  The cache can be inside the CPU chip, next to the CPU chip, on the
processor board, or some combination of all three. Since many reads can now be
satisfied out of the local cache, there will be much less bus traffic, and the system
can support more CPUs.            In general, caching is not done on an individual word
basis but on the basis of 32- or 64-byte blocks. When a word is referenced, its en-
tire block, called a cache line, is fetched into the cache of the CPU touching it.
Each cache block is marked as being either read only (in which case it can be
present in multiple caches at the same time) or read-write (in which case it may not
be present in any other caches). If a CPU attempts to write a word that is in one or
more remote caches, the bus hardware detects the write and puts a signal on the
bus informing all other caches of the write.     If other caches have a ``clean'' copy,
that is, an exact copy of what is in memory, they can just discard their copies and
let the writer fetch the cache block from memory before modifying it.                     If some
other cache has a ``dirty'' (i.e., modified) copy, it must either write it back to mem-
ory before the write can proceed or transfer it directly to the writer over the bus.
This set of rules is called a cache-coherence protocol and is one of many.
Yet another possibility is the design of Fig. 8-2(c), in which each CPU has not
only a cache, but also a local, private memory which it accesses over a dedicated
(private) bus. To use this configuration optimally, the compiler should place all the
program text, strings, constants and other read-only data, stacks, and local vari-
ables in the private memories. The shared memory is then only used for writable
shared variables. In most cases, this careful placement will greatly reduce bus traf-
fic, but it does require active cooperation from the compiler.
UMA Multiprocessors Using Crossbar Switches
Even with the best caching, the use of a single bus limits the size of a UMA
multiprocessor to about 16 or 32 CPUs.           To go beyond that, a different kind of
interconnection network is needed. The simplest circuit for connecting n CPUs to k



522                       MULTIPLE PROCESSOR SYSTEMS                                    CHAP. 8
memories is the crossbar switch, shown in Fig. 8-3.        Crossbar switches have been
used for decades in telephone switching exchanges to connect a group of incoming
lines to a set of outgoing lines in an arbitrary way.
      At each intersection of a horizontal (incoming) and vertical (outgoing) line is a
crosspoint. A crosspoint is a small electronic switch that can be electrically open-
ed or closed, depending on whether the horizontal and vertical lines are to be con-
nected or not. In Fig. 8-3(a) we see three crosspoints closed simultaneously, allow-
ing connections between the (CPU, memory) pairs (010, 000), (101, 101), and
(110, 010) at the same time. Many other combinations are also possible.                 In fact,
the number of combinations is equal to the number of different ways eight rooks
can be safely placed on a chess board.
                               Memories
             000   001    010  011  100  101  110     111                      Crosspoint
                                                                               switch is open
      000
      001
      010
      011                                                                 (b)
CPUs                                                                           Crosspoint
      100                                                                      switch is closed
      101
      110
      111
                  Closed                                                  (c)
                  crosspoint                  Open
                  switch                      crosspoint
                                              switch
                               (a)
      Figure 8-3.  (a) An 8 × 8 crossbar switch. (b) An open crosspoint.  (c) A closed
      crosspoint.
      One of the nicest properties of the crossbar switch is that it is a nonblocking
network, meaning that no CPU is ever denied the connection it needs because
some crosspoint or line is already occupied (assuming the memory module itself is
available).  Not all interconnects have this fine property.  Furthermore, no advance
planning is needed. Even if seven arbitrary connections are already set up, it is al-
ways possible to connect the remaining CPU to the remaining memory.



SEC. 8.1                           MULTIPROCESSORS                                         523
Contention for memory is still possible, of course, if two CPUs want to access
the same module at the same time.      Nevertheless, by partitioning the memory into
n units, contention is reduced by a factor of n compared to the model of Fig. 8-2.
One of the worst properties of the crossbar switch is the fact that the number of
crosspoints grows as n2. With 1000 CPUs and 1000 memory modules we need a
million crosspoints. Such a large crossbar switch is not feasible.           Nevertheless, for
medium-sized systems, a crossbar design is workable.
UMA Multiprocessors Using Multistage Switching Networks
A completely different multiprocessor design is based on the humble 2 × 2
switch shown in Fig. 8-4(a).       This switch has two inputs and two outputs. Mes-
sages arriving on either input line can be switched to either output line. For our
purposes, messages will contain up to four parts, as shown in Fig. 8-4(b).                 The
Module field tells which memory to use. The Address specifies an address within a
module. The Opcode gives the operation, such as READ or WRITE.               Finally, the op-
tional Value field may contain an operand, such as a 32-bit word to be written on a
WRITE.    The switch inspects the Module field and uses it to determine if the mes-
sage should be sent on X or on Y.
          A                     X      Module          Address       Opcode  Value
          B                     Y
                 (a)                                            (b)
        Figure 8-4. (a) A 2 × 2 switch with two input lines, A and B, and two output
        lines, X and Y. (b) A message format.
Our 2 × 2 switches can be arranged in many ways to build larger multistage
switching networks (Adams et al., 1987; Garofalakis and Stergiou, 2013; and
Kumar and Reddy, 1987).         One possibility is the no-frills, cattle-class omega net-
work, illustrated in Fig. 8-5.     Here we have connected eight CPUs to eight memo-
ries using 12 switches. More generally, for n CPUs and n memories we would need
log2 n  stages,  with  n/2  switches   per     stage,  for  a   total  of    (n/2) log2 n  switches,
which is a lot better than n2 crosspoints, especially for large values of n.
The wiring pattern of the omega network is often called the perfect shuffle,
since the mixing of the signals at each stage resembles a deck of cards being cut in
half and then mixed card-for-card.     To see how the omega network works, suppose
that CPU 011 wants to read a word from memory module 110.                    The CPU sends a
READ message to switch 1D containing the value 110 in the Module field. The
switch takes the first (i.e., leftmost) bit of 110 and uses it for routing. A 0 routes to
the upper output and a 1 routes to the lower one. Since this bit is a 1, the message
is routed via the lower output to 2D.



524                   MULTIPLE PROCESSOR SYSTEMS                            CHAP. 8
                                        3  Stages
CPUs                                                                        Memories
000                   1A                   2A            3A                 000
001      b                                            b           b         001
010                   1B                   2B            3B                 010
011                                                                         011
100                                  b                                      100
                      1C                   2C            3C
101                                                                         101
110             a                                              a            110
111                   1D                   2D            3D                 111
                                  a                a
                      Figure 8-5. An omega switching network.
     All the second-stage switches, including 2D, use the second bit for routing.
This, too, is a 1, so the message is now forwarded via the lower output to 3D. Here
the third bit is tested and found to be a 0.    Consequently, the message goes out on
the upper output and arrives at memory 110, as desired. The path followed by this
message is marked in Fig. 8-5 by the letter a.
     As the message moves through the switching network, the bits at the left-hand
end of the module number are no longer needed. They can be put to good use by
recording the incoming line number there, so the reply can find its way back. For
path a, the incoming lines are 0 (upper input to 1D), 1 (lower input to 2D), and 1
(lower input to 3D), respectively. The reply is routed back using 011, only reading
it from right to left this time.
     At the same time all this is going on, CPU 001 wants to write a word to memo-
ry module 001.  An analogous process happens here, with the message routed via
the upper, upper, and lower outputs, respectively, marked by the letter b.  When it
arrives, its Module field reads 001, representing the path it took. Since these two
requests do not use any of the same switches, lines, or memory modules, they can
proceed in parallel.
     Now consider what would happen if CPU 000 simultaneously wanted to access
memory module 000. Its request would come into conflict with CPU 001's request
at switch 3A.   One of them would then have to wait. Unlike the crossbar switch,
the omega network is a blocking network.           Not every set of requests can be proc-
essed simultaneously. Conflicts can occur over the use of a wire or a switch, as
well as between requests to memory and replies from memory.
     Since it is highly desirable to spread the memory references uniformly across
the modules, one common technique is to use the low-order bits as the module
number.  Consider, for example, a byte-oriented address space for a computer that



SEC. 8.1                         MULTIPROCESSORS                                       525
mostly accesses full 32-bit words. The 2 low-order bits will usually be 00, but the
next 3 bits will be uniformly distributed. By using these 3 bits as the module num-
ber, consecutively words will be in consecutive modules.        A memory system in
which consecutive words are in different modules is said to be interleaved.      Inter-
leaved memories maximize parallelism because most memory references are to
consecutive addresses.      It is also possible to design switching networks that are
nonblocking and offer multiple paths from each CPU to each memory module to
spread the traffic better.
NUMA Multiprocessors
Single-bus UMA multiprocessors are generally limited to no more than a few
dozen CPUs, and crossbar or switched multiprocessors need a lot of (expensive)
hardware and are not that much bigger.     To get to more than 100 CPUs, something
has to give. Usually, what gives is the idea that all memory modules have the same
access time. This concession leads to the idea of NUMA multiprocessors, as men-
tioned above. Like their UMA cousins, they provide a single address space across
all the CPUs, but unlike the UMA machines, access to local memory modules is
faster than access to remote ones. Thus all UMA programs will run without change
on NUMA machines, but the performance will be worse than on a UMA machine.
NUMA machines have three key characteristics that all of them possess and
which together distinguish them from other multiprocessors:
1.        There is a single address space visible to all CPUs.
2.        Access to remote memory is via LOAD and STORE instructions.
3.        Access to remote memory is slower than access to local memory.
When the access time to remote memory is not hidden (because there is no cach-
ing), the system is called NC-NUMA (Non Cache-coherent NUMA).                When the
caches are coherent, the system is called CC-NUMA (Cache-Coherent NUMA).
A   popular  approach       for  building  large  CC-NUMA       multiprocessors  is    the
directory-based multiprocessor. The idea is to maintain a database telling where
each cache line is and what its status is. When a cache line is referenced, the data-
base is queried to find out where it is and whether it is clean or dirty.    Since this
database is queried on every instruction that touches memory, it must be kept in ex-
tremely fast special-purpose hardware that can respond in a fraction of a bus cycle.
To make the idea of a directory-based multiprocessor somewhat more concrete,
let us consider as a simple (hypothetical) example, a 256-node system, each node
consisting of one CPU and 16 MB of RAM connected to the CPU via a local bus.
The total memory is 232 bytes and it is divided up into 226 cache lines of 64 bytes
each. The memory is statically allocated among the nodes, with 0­16M in node 0,
16M­32M in node 1, etc. The nodes are connected by an interconnection network,



526                           MULTIPLE PROCESSOR SYSTEMS                                  CHAP. 8
as shown in Fig. 8-6(a).        Each node also holds the directory entries for the 218
64-byte cache lines comprising its 224-byte memory. For the moment, we will as-
sume that a line can be held in at most one cache.
              Node 0                    Node 1                                           Node 255
           CPU  Memory                  CPU Memory                                   CPU  Memory
Directory
                                                                  ...
              Local bus                         Local bus                                Local bus
                                Interconnection  network
                                        (a)
                                                           218-1
Bits       8             18     6
      Node               Block  Offset
                         (b)                               4                 0
                                                           3                 0
                                                           2                 1  82
                                                           1                 0
                                                           0                 0
                                                                                (c)
           Figure 8-6. (a) A 256-node directory-based multiprocessor. (b) Division of a
           32-bit memory address into fields. (c) The directory at node 36.
     To see how the directory works, let us trace a LOAD instruction from CPU 20
that references a cached line. First the CPU issuing the instruction presents it to its
MMU, which translates it to a physical address, say, 0x24000108.                         The MMU
splits this address into the three parts shown in Fig. 8-6(b).                  In decimal, the three
parts are node 36, line 4, and offset 8. The MMU sees that the memory word refer-
enced is from node 36, not node 20, so it sends a request message through the
interconnection network to the line's home node, 36, asking whether its line 4 is
cached, and if so, where.
     When the request arrives at node 36 over the interconnection network, it is
routed to the directory hardware. The hardware indexes into its table of 218 entries,
one for each of its cache lines, and extracts entry 4.     From Fig. 8-6(c) we see that
the line is not cached, so the hardware issues a fetch for line 4 from the local RAM
and after it arrives sends it back to node 20. It then updates directory entry 4 to in-
dicate that the line is now cached at node 20.



SEC. 8.1                      MULTIPROCESSORS                                            527
      Now let us consider a second request, this time asking about node 36's line 2.
From Fig. 8-6(c) we see that this line is cached at node 82.  At this point the hard-
ware could update directory entry 2 to say that the line is now at node 20 and then
send a message to node 82 instructing it to pass the line to node 20 and invalidate
its cache. Note that even a so-called ``shared-memory multiprocessor'' has a lot of
message passing going on under the hood.
      As a quick aside, let us calculate how much memory is being taken up by the
directories. Each node has 16 MB of RAM and 218 9-bit entries to keep track of
that RAM.  Thus the directory overhead is about 9 × 218 bits divided by 16 MB or
about 1.76%, which is generally acceptable (although it has to be high-speed mem-
ory, which increases its cost, of course).  Even with 32-byte cache lines the over-
head would only be 4%. With 128-byte cache lines, it would be under 1%.
      An obvious limitation of this design is that a line can be cached at only one
node. To allow lines to be cached at multiple nodes, we would need some way of
locating all of them, for example, to invalidate or update them on a write. On many
multicore processors, a directory entry therefore consists of a bit vector with one
bit per core. A ``1'' indicates that the cache line is present on the core, and a ``0''
that it is not. Moreover, each directory entry typically contains a few more bits. As
a result, the memory cost of the directory increases considerably.
Multicore Chips
      As chip manufacturing technology improves, transistors are getting smaller
and smaller and it is possible to put more and more of them on a chip.    This empir-
ical  observation  is  often  called  Moore's  Law,    after  Intel  co-founder  Gordon
Moore, who first noticed it.  In 1974, the Intel 8080 contained a little over 2000
transistors, while Xeon Nehalem-EX CPUs have over 2 billion transistors.
      An obvious question is: ``What do you do with all those transistors?'' As we
discussed in Sec. 1.3.1, one option is to add megabytes of cache to the chip. This
option is serious, and chips with 4­32 MB of on-chip cache are common.           But at
some point increasing the cache size may run the hit rate up only from 99% to
99.5%, which does not improve application performance much.
      The other option is to put two or more complete CPUs, usually called cores,
on the same chip (technically, on the same die).     Dual-core, quad-core, and octa-
core chips are already common; and you can even buy chips with hundreds of
cores. No doubt more cores are on their way.      Caches are still crucial and are now
spread across the chip.  For instance, the Intel Xeon 2651 has 12 physical hyper-
threaded cores, giving 24 virtual cores. Each of the 12 physical cores has 32 KB of
L1 instruction cache and 32 KB of L1 data cache. Each one also has 256 KB of L2
cache. Finally, the 12 cores share 30 MB of L3 cache.
      While the CPUs may or may not share caches (see, for example, Fig. 1-8), they
always share main memory, and this memory is consistent in the sense that there is
always a unique value for each memory word.       Special hardware circuitry makes



528                MULTIPLE PROCESSOR SYSTEMS                                   CHAP. 8
sure that if a word is present in two or more caches and one of the CPUs modifies
the word, it is automatically and atomically removed from all the caches in order to
maintain consistency. This process is known as snooping.
     The result of this design is that multicore chips are just very small multiproces-
sors.   In fact, multicore chips are sometimes called CMPs (Chip MultiProces-
sors).  From a software perspective, CMPs are not really that different from bus-
based multiprocessors or multiprocessors that use switching networks.           However,
there are some differences.  To start with, on a bus-based multiprocessor, each of
the CPUs has its own cache, as in Fig. 8-2(b) and also as in the AMD design of
Fig. 1-8(b). The shared-cache design of Fig. 1-8(a), which Intel uses in many of its
processors, does not occur in other multiprocessors.  A shared L2 or L3 cache can
affect performance. If one core needs a lot of cache memory and the others do not,
this design allows the cache hog to take whatever it needs.  On the other hand, the
shared cache also makes it possible for a greedy core to hurt the other cores.
     An area in which CMPs differ from their larger cousins is fault tolerance. Be-
cause the CPUs are so closely connected, failures in shared components may bring
down multiple CPUs at once, something unlikely in traditional multiprocessors.
     In addition to symmetric multicore chips, where all the cores are identical, an-
other common category of multicore chip is the System On a Chip (SoC). These
chips have one or more main CPUs, but also special-purpose cores, such as video
and audio decoders, cryptoprocessors, network interfaces, and more, leading to a
complete computer system on a chip.
Manycore Chips
     Multicore simply means ``more than one core,'' but when the number of cores
grows well beyond the reach of finger counting, we use another name.            Manycore
chips are multicores that contain tens, hundreds, or even thousands of cores. While
there is no hard threshold beyond which a multicore becomes a manycore, an easy
distinction is that you probably have a manycore if you no longer care about losing
one or two cores.
     Accelerator add-on cards like Intel's Xeon Phi have in excess of 60 x86 cores.
Other vendors have already crossed the 100-core barrier with different kinds of
cores. A thousand general-purpose cores may be on their way. It is not easy to im-
agine what to do with a thousand cores, much less how to program them.
     Another problem with really large numbers of cores is that the machinery
needed to keep their caches coherent becomes very complicated and very expen-
sive. Many engineers worry that cache coherence may not scale to many hundreds
of cores. Some even advocate that we should give it up altogether. They fear that
the cost of coherence protocols in hardware will be so high that all those shiny new
cores will not help performance much because the processor is too busy keeping
the caches in a consistent state. Worse, it would need to spend way too much mem-
ory on the (fast) directory to do so. This is known as the coherency wall.



SEC. 8.1                  MULTIPROCESSORS                                                529
Consider, for instance, our directory-based cache-coherency solution discussed
above. If each directory entry contains a bit vector to indicate which cores contain
a particular cache line, the directory entry for a CPU with 1024 cores will be at
least 128 bytes long. Since cache lines themselves are rarely larger than 128 bytes,
this leads to the awkward situation that the directory entry is larger than the cache-
line it tracks. Probably not what we want.
Some engineers argue that the only programming model that has proven to
scale to very large numbers of processors is that which employs message passing
and distributed memory--and that is what we should expect in future manycore
chips also. Experimental processors like Intel's 48-core SCC have already dropped
cache consistency and provided hardware support for faster message passing in-
stead. On the other hand, other processors still provide consistency even at large
core counts. Hybrid models are also possible. For instance, a 1024-core chip may
be partitioned in 64 islands with 16 cache-coherent cores each, while abandoning
cache coherence between the islands.
Thousands of cores are not even that special any more. The most common
manycores today, graphics processing units, are found in just about any computer
system that is not embedded and has a monitor.  A GPU is a processor with dedi-
cated memory and, literally, thousands of itty-bitty cores. Compared to gener-
al-purpose processors, GPUs spend more of their transistor budget on the circuits
that perform calculations and less on caches and control logic. They are very good
for many small computations done in parallel, like rendering polygons in graphics
applications. They are not so good at serial tasks. They are also hard to program.
While GPUs can be useful for operating systems (e.g., encryption or processing of
network traffic), it is not likely that much of the operating system itself will run on
the GPUs.
Other computing tasks are increasingly handled by the GPU, especially com-
putationally demanding ones that are common in scientific computing.  The term
used for general-purpose processing on GPUs is--you guessed it-- GPGPU.                  Un-
fortunately, programming GPUs efficiently is extremely difficult and requires spe-
cial programming languages such as OpenGL, or NVIDIA's proprietary CUDA.
An important difference between programming GPUs and programming gener-
al-purpose processors is that GPUs are essentially ``single instruction multiple
data'' machines, which means that a large number of cores execute exactly the
same instruction but on different pieces of data. This programming model is great
for data parallelism, but not always convenient for other programming styles (such
as task parallelism).
Heterogeneous Multicores
Some chips integrate a GPU and a number of general-purpose cores on the
same die. Similarly, many SoCs contain general-purpose cores in addition to one or
more special-purpose processors. Systems that integrate multiple different breeds



530             MULTIPLE PROCESSOR SYSTEMS                                   CHAP. 8
of processors in a single chip are collectively known as heterogeneous multicore
processors. An example of a heterogeneous multicore processor is the line of IXP
network processors originally introduced by Intel in 2000 and updated regularly
with the latest technology. The network processors typically contain a single gener-
al purpose control core (for instance, an ARM processor running Linux) and many
tens of highly specialized stream processors that are really good at processing net-
work packets and not much else. They are commonly used in network equipment,
such as routers and firewalls. To route network packets you probably do not need
floating-point operations much, so in most models the stream processors do not
have a floating-point unit at all. On the other hand, high-speed networking is high-
ly dependent on fast access to memory (to read packet data) and the stream proc-
essors have special hardware to make this possible.
     In the previous examples, the systems were clearly heterogeneous. The stream
processors and the control processors on the IXPs are completely different beasts
with different instruction sets. The same is true for the GPU and the general-pur-
pose cores. However, it is also possible to introduce heterogeneity while main-
taining the same instruction set. For instance, a CPU can have a small number of
``big'' cores, with deep pipelines and possibly high clock speeds, and a larger num-
ber of ``little'' cores that are simpler, less powerful, and perhaps run at lower fre-
quencies.  The  powerful  cores  are  needed  for    running  code  that  requires      fast
sequential processing while the little cores are useful for tasks that can be executed
efficiently in parallel. An example of a heterogeneous architecture along these lines
is ARM's big.LITTLE processor family.
Programming with Multiple Cores
     As has often happened in the past, the hardware is way ahead of the software.
While multicore chips are here now, our ability to write applications for them is
not. Current programming languages are poorly suited for writing highly parallel
programs and good compilers and debugging tools are scarce on the ground. Few
programmers have had any experience with parallel programming and most know
little about dividing work into multiple packages that can run in parallel.         Syn-
chronization, eliminating race conditions, and deadlock avoidance are such stuff as
really bad dreams are made of, but unfortunately performance suffers horribly if
they are not handled well. Semaphores are not the answer.
     Beyond these startup problems, it is far from obvious what kind of application
really needs hundreds, let alone thousands, of cores--especially in home environ-
ments. In large server farms, on the other hand, there is often plenty of work for
large numbers of cores. For instance, a popular server may easily use a different
core for each client request. Similarly, the cloud providers discussed in the previ-
ous chapter can soak up the cores to provide a large number of virtual machines to
rent out to clients looking for on-demand computing power.



SEC. 8.1                     MULTIPROCESSORS                                         531
8.1.2 Multiprocessor Operating System Types
Let us now turn from multiprocessor hardware to multiprocessor software, in
particular,  multiprocessor  operating   systems.  Various     approaches    are  possible.
Below we will study three of them.      Note that all of these are equally applicable to
multicore systems as well as systems with discrete CPUs.
Each CPU Has Its Own Operating System
The simplest possible way to organize a multiprocessor operating system is to
statically divide memory into as many partitions as there are CPUs and give each
CPU its own private memory and its own private copy of the operating system.              In
effect, the  n CPUs then operate as n independent computers.         One obvious opti-
mization is to allow all the CPUs to share the operating system code and make pri-
vate copies of only the operating system data structures, as shown in Fig. 8-7.
CPU 1             CPU 2      CPU 3       CPU 4           Memory            I/O
Has                    Has   Has         Has              1    2
                                                         Data  Data
private           private    private     private          3    4
OS                     OS    OS          OS              Data  Data
                                                          OS code
                                    Bus
Figure 8-7. Partitioning multiprocessor memory among four CPUs, but sharing a
single copy of the operating system code. The boxes marked Data are the operat-
ing system's private data for each CPU.
This scheme is still better than having n separate computers since it allows all
the machines to share a set of disks and other I/O devices, and it also allows the
memory to be shared flexibly. For example, even with static memory allocation,
one CPU can be given an extra-large portion of the memory so it can handle large
programs efficiently.  In addition, processes can efficiently communicate with one
another by allowing a producer to write data directly into memory and allowing a
consumer to fetch it from the place the producer wrote it. Still, from an operating
systems' perspective, having each CPU have its own operating system is as primi-
tive as it gets.
It is worth mentioning four aspects of this design that may not be obvious.
First, when a process makes a system call, the system call is caught and handled on
its own CPU using the data structures in that operating system's tables.
Second, since each operating system has its own tables, it also has its own set
of processes that it schedules by itself. There is no sharing of processes.       If a user
logs into CPU 1, all of his processes run on CPU 1.  As a consequence, it can hap-
pen that CPU 1 is idle while CPU 2 is loaded with work.



532                     MULTIPLE PROCESSOR SYSTEMS                       CHAP. 8
     Third, there is no sharing of physical pages.     It can happen that CPU 1 has
pages to spare while CPU 2 is paging continuously.     There is no way for CPU 2 to
borrow some pages from CPU 1 since the memory allocation is fixed.
     Fourth, and worst, if the operating system maintains a buffer cache of recently
used disk blocks, each operating system does this independently of the other ones.
Thus it can happen that a certain disk block is present and dirty in multiple buffer
caches at the same time, leading to inconsistent results. The only way to avoid this
problem is to eliminate the buffer caches.  Doing so is not hard, but it hurts per-
formance considerably.
     For these reasons, this model is rarely used in production systems any more,
although it was used in the early days of multiprocessors, when the goal was to
port existing operating systems to some new multiprocessor as fast as possible. In
research, the model is making a comeback, but with all sorts of twists. There is
something to be said for keeping the operating systems completely separate. If all
of the state for each processor is kept local to that processor, there is little to no
sharing to lead to consistency or locking problems. Conversely, if multiple proc-
essors have to access and modify the same process table, the locking becomes
complicated quickly (and crucial for performance). We will say more about this
when we discuss the symmetric multiprocessor model below.
Master-Slave Multiprocessors
     A second model is shown in Fig. 8-8.   Here, one copy of the operating system
and its tables is present on CPU 1 and not on any of the others. All system calls are
redirected to CPU 1 for processing there.   CPU 1 may also run user processes if
there is CPU time left over. This model is called master-slave since CPU 1 is the
master and all the others are slaves.
     CPU 1   CPU 2            CPU 3         CPU 4      Memory       I/O
     Master  Slave            Slave         Slave      User
     runs    runs user  runs user           runs user  processes
     OS      processes  processes           processes  OS
                                       Bus
             Figure 8-8.      A master-slave multiprocessor model.
     The master-slave model solves most of the problems of the first model. There
is a single data structure (e.g., one list or a set of prioritized lists) that keeps track
of ready processes. When a CPU goes idle, it asks the operating system on CPU 1
for a process to run and is assigned one. Thus it can never happen that one CPU is



SEC. 8.1                          MULTIPROCESSORS                                          533
idle while another is overloaded.    Similarly, pages can be allocated among all the
processes dynamically and there is only one buffer cache, so inconsistencies never
occur.
The problem with this model is that with many CPUs, the master will become
a bottleneck. After all, it must handle all system calls from all CPUs. If, say, 10%
of all time is spent handling system calls, then 10 CPUs will pretty much saturate
the master, and with 20 CPUs it will be completely overloaded. Thus this model is
simple and workable for small multiprocessors, but for large ones it fails.
Symmetric Multiprocessors
Our       third   model,  the  SMP   (Symmetric      MultiProcessor),      eliminates      this
asymmetry. There is one copy of the operating system in memory, but any CPU
can run it.     When a system call is made, the CPU on which the system call was
made traps to the kernel and processes the system call. The SMP model is illustrat-
ed in Fig. 8-9.
         CPU 1    CPU 2           CPU 3      CPU 4      Memory                    I/O
         Runs     Runs            Runs         Runs
users and         users and       users and  users and
shared OS         shared OS    shared OS     shared OS  OS
                                                                           Locks
                                     Bus
                          Figure 8-9. The SMP multiprocessor model.
This model balances processes and memory dynamically, since there is only
one set of operating system tables.      It also eliminates the master CPU bottleneck,
since there is no master, but it introduces its own problems. In particular, if two or
more CPUs are running operating system code at the same time, disaster may well
result.  Imagine  two  CPUs    simultaneously  picking  the          same  process     to  run  or
claiming the same free memory page. The simplest way around these problems is
to associate a mutex (i.e., lock) with the operating system, making the whole sys-
tem one big critical region. When a CPU wants to run operating system code, it
must first acquire the mutex.     If the mutex is locked, it just waits.   In this way, any
CPU can run the operating system, but only one at a time. This approach is some-
things called a big kernel lock.
This model works, but is almost as bad as the master-slave model. Again, sup-
pose that 10% of all run time is spent inside the operating system.        With 20 CPUs,
there will be long queues of CPUs waiting to get in. Fortunately, it is easy to im-
prove.   Many parts of the operating system are independent of one another. For



534               MULTIPLE PROCESSOR SYSTEMS                                    CHAP. 8
example, there is no problem with one CPU running the scheduler while another
CPU is handling a file-system call and a third one is processing a page fault.
     This observation leads to splitting the operating system up into multiple inde-
pendent critical regions that do not interact with one another. Each critical region is
protected by its own mutex, so only one CPU at a time can execute it.         In this way,
far more parallelism can be achieved.    However, it may well happen that some ta-
bles, such as the process table, are used by multiple critical regions. For example,
the process table is needed for scheduling, but also for the fork system call and also
for signal handling. Each table that may be used by multiple critical regions needs
its own mutex.    In this way, each critical region can be executed by only one CPU
at a time and each critical table can be accessed by only one CPU at a time.
     Most modern multiprocessors use this arrangement. The hard part about writ-
ing the operating system for such a machine is not that the actual code is so dif-
ferent from a regular operating system.  It is not.      The hard part is splitting it into
critical regions that can be executed concurrently by different CPUs without inter-
fering with one another, not even in subtle, indirect ways.  In addition, every table
used by two or more critical regions must be separately protected by a mutex and
all code using the table must use the mutex correctly.
     Furthermore, great care must be taken to avoid deadlocks.           If two critical re-
gions both need table A and table B, and one of them claims A first and the other
claims B first, sooner or later a deadlock will occur and nobody will know why.          In
theory, all the tables could be assigned integer values and all the critical regions
could be required to acquire tables in increasing order. This strategy avoids dead-
locks, but it requires the programmer to think very carefully about which tables
each critical region needs and to make the requests in the right order.
     As the code evolves over time, a critical region may need a new table it did not
previously need.  If the programmer is new and does not understand the full logic
of the system, then the temptation will be to just grab the mutex on the table at the
point it is needed and release it when it is no longer needed. However reasonable
this may appear, it may lead to deadlocks, which the user will perceive as the sys-
tem freezing. Getting it right is not easy and keeping it right over a period of years
in the face of changing programmers is very difficult.
8.1.3 Multiprocessor Synchronization
     The CPUs in a multiprocessor frequently need to synchronize.        We just saw the
case in which kernel critical regions and tables have to be protected by mutexes.
Let us now take a close look at how this synchronization actually works in a multi-
processor. It is far from trivial, as we will soon see.
     To start with, proper synchronization primitives are really needed. If a process
on a uniprocessor machine (just one CPU) makes a system call that requires ac-
cessing some critical kernel table, the kernel code can just disable interrupts before



SEC. 8.1                         MULTIPROCESSORS                                              535
touching the table.  It can then do its work knowing that it will be able to finish
without any other process sneaking in and touching the table before it is finished.
On a multiprocessor, disabling interrupts affects only the CPU doing the disable.
Other CPUs continue to run and can still touch the critical table.             As a conse-
quence, a proper mutex protocol must be used and respected by all CPUs to guar-
antee that mutual exclusion works.
      The heart of any practical mutex protocol is a special instruction that allows a
memory word to be inspected and set in one indivisible operation.              We saw how
TSL (Test and Set Lock) was used in Fig. 2-25 to implement critical regions.                  As
we discussed earlier, what this instruction does is read out a memory word and
store it in a register. Simultaneously, it writes a 1 (or some other nonzero value) in-
to the memory word.  Of course, it takes two bus cycles to perform the memory
read and memory write.  On a uniprocessor, as long as the instruction cannot be
broken off halfway, TSL always works as expected.
      Now think about what could happen on a multiprocessor.                   In Fig. 8-10 we see
the worst-case timing, in which memory word 1000, being used as a lock, is ini-
tially 0.  In step 1, CPU 1 reads out the word and gets a 0.           In step 2, before CPU 1
has a chance to rewrite the word to 1, CPU 2 gets in and also reads the word out as
a 0.  In step 3, CPU 1 writes a 1 into the word.   In step 4, CPU 2 also writes a 1
into the word.  Both CPUs got a 0 back from the TSL instruction, so both of them
now have access to the critical region and the mutual exclusion fails.
           CPU 1        Word              Memory                       CPU 2
                        1000 is
                        initially 0
                     1. CPU 1 reads a 0           2. CPU 2 reads a 0
                     3. CPU 1 writes a 1          4. CPU 2 writes a 1
                                                                               Bus
           Figure 8-10. The TSL instruction can fail if the bus cannot be locked. These four
           steps show a sequence of events where the failure is demonstrated.
      To prevent this problem, the TSL instruction must first lock the bus, preventing
other CPUs from accessing it, then do both memory accesses, then unlock the bus.
Typically, locking the bus is done by requesting the bus using the usual bus request
protocol, then asserting (i.e., setting to a logical 1 value) some special bus line until
both cycles have been completed.          As long as this special line is being asserted, no
other CPU will be granted bus access. This instruction can only be implemented on
a bus that has the necessary lines and (hardware) protocol for using them. Modern
buses all have these facilities, but on earlier ones that did not, it was not possible to



536                     MULTIPLE PROCESSOR SYSTEMS                              CHAP. 8
implement TSL correctly. This is why Peterson's protocol was invented: to synchro-
nize entirely in software (Peterson, 1981).
     If TSL is correctly implemented and used, it guarantees that mutual exclusion
can be made to work. However, this mutual exclusion method uses a spin lock be-
cause the requesting CPU just sits in a tight loop testing the lock as fast as it can.
Not only does it completely waste the time of the requesting CPU (or CPUs), but it
may also put a massive load on the bus or memory, seriously slowing down all
other CPUs trying to do their normal work.
     At first glance, it might appear that the presence of caching should eliminate
the problem of bus contention, but it does not. In theory, once the requesting CPU
has read the lock word, it should get a copy in its cache.   As long as no other CPU
attempts to use the lock, the requesting CPU should be able to run out of its cache.
When the CPU owning the lock writes a 0 to it to release it, the cache protocol
automatically invalidates all copies of it in remote caches, requiring the correct
value to be fetched again.
     The problem is that caches operate in blocks of 32 or 64 bytes. Usually, the
words surrounding the lock are needed by the CPU holding the lock.       Since the TSL
instruction is a write (because it modifies the lock), it needs exclusive access to the
cache block containing the lock. Therefore every TSL invalidates the block in the
lock holder's cache and fetches a private, exclusive copy for the requesting CPU.
As soon as the lock holder touches a word adjacent to the lock, the cache block is
moved to its machine. Consequently, the entire cache block containing the lock is
constantly being shuttled between the lock owner and the lock requester, generat-
ing even more bus traffic than individual reads on the lock word would have.
     If we could get rid of all the TSL-induced writes on the requesting side, we
could reduce the cache thrashing appreciably. This goal can be accomplished by
having the requesting CPU first do a pure read to see if the lock is free. Only if the
lock appears to be free does it do a TSL to actually acquire it. The result of this
small change is that most of the polls are now reads instead of writes.         If the CPU
holding the lock is only reading the variables in the same cache block, they can
each have a copy of the cache block in shared read-only mode, eliminating all the
cache-block transfers.
     When the lock is finally freed, the owner does a write, which requires exclu-
sive access, thus invalidating all copies in remote caches.  On the next read by the
requesting CPU, the cache block will be reloaded.  Note that if two or more CPUs
are contending for the same lock, it can happen that both see that it is free simul-
taneously, and both do a TSL simultaneously to acquire it. Only one of these will
succeed, so there is no race condition here because the real acquisition is done by
the TSL instruction, and it is atomic. Seeing that the lock is free and then trying to
grab it immediately with a TSL does not guarantee that you get it. Someone else
might win, but for the correctness of the algorithm, it does not matter who gets it.
Success on the pure read is merely a hint that this would be a good time to try to
acquire the lock, but it is not a guarantee that the acquisition will succeed.



SEC. 8.1                            MULTIPROCESSORS                                         537
Another way to reduce bus traffic is to use the well-known Ethernet binary
exponential backoff algorithm (Anderson, 1990).         Instead of continuously polling,
as in Fig. 2-25, a delay loop can be inserted between polls. Initially the delay is one
instruction.  If the lock is still busy, the delay is doubled to two instructions, then
four instructions, and so on up to some maximum.        A low maximum gives a fast
response when the lock is released, but wastes more bus cycles on cache thrashing.
A high maximum reduces cache thrashing at the expense of not noticing that the
lock is free so quickly. Binary exponential backoff can be used with or without the
pure reads preceding the TSL instruction.
An even better idea is to give each CPU wishing to acquire the mutex its own
private lock variable to test, as illustrated in Fig. 8-11 (Mellor-Crummey and Scott,
1991).  The variable should reside in an otherwise unused cache block to avoid
conflicts. The algorithm works by having a CPU that fails to acquire the lock allo-
cate a lock variable and attach itself to the end of a list of CPUs waiting for the
lock. When the current lock holder exits the critical region, it frees the private lock
that the first CPU on the list is testing (in its own cache). This CPU then enters the
critical region. When it is done, it frees the lock its successor is using, and so on.
Although the protocol is somewhat complicated (to avoid having two CPUs attach
themselves to the end of the list simultaneously), it is efficient and starvation free.
For all the details, readers should consult the paper.
                            CPU 3              3
                                                  CPU 3 spins on this (private) lock
CPU 2 spins on this (private) lock
                                                        CPU 4 spins on this (private) lock
                         2                              4
          Shared memory                                 When CPU 1 is finished with the
                                                        real lock, it releases it and also
                                    CPU 1               releases the private lock CPU 2
                                    holds the  1        is spinning on
                                    real lock
              Figure 8-11.          Use of multiple locks to avoid cache thrashing.
Spinning vs. Switching
So far we have assumed that a CPU needing a locked mutex just waits for it,
by polling continuously, polling intermittently, or attaching itself to a list of wait-
ing CPUs.     Sometimes, there is no alternative for the requesting CPU to just wait-
ing. For example, suppose that some CPU is idle and needs to access the shared



538                       MULTIPLE PROCESSOR SYSTEMS                             CHAP. 8
ready list to pick a process to run.   If the ready list is locked, the CPU cannot just
decide to suspend what it is doing and run another process, as doing that would re-
quire reading the ready list. It must wait until it can acquire the ready list.
     However, in other cases, there is a choice. For example, if some thread on a
CPU needs to access the file system buffer cache and it is currently locked, the
CPU can decide to switch to a different thread instead of waiting. The issue of
whether to spin or to do a thread switch has been a matter of much research, some
of which will be discussed below. Note that this issue does not occur on a uniproc-
essor because spinning does not make much sense when there is no other CPU to
release the lock. If a thread tries to acquire a lock and fails, it is always blocked to
give the lock owner a chance to run and release the lock.
     Assuming that spinning and doing a thread switch are both feasible options,
the trade-off is as follows. Spinning wastes CPU cycles directly.  Testing a lock re-
peatedly is not productive work. Switching, however, also wastes CPU cycles,
since the current thread's state must be saved, the lock on the ready list must be ac-
quired, a thread must be selected, its state must be loaded, and it must be started.
Furthermore, the CPU cache will contain all the wrong blocks, so many expensive
cache misses will occur as the new thread starts running. TLB faults are also like-
ly.  Eventually, a switch back to the original thread must take place, with more
cache misses following it. The cycles spent doing these two context switches plus
all the cache misses are wasted.
     If it is known that mutexes are generally held for, say, 50 sec and it takes 1
msec to switch from the current thread and 1 msec to switch back later, it is more
efficient just to spin on the mutex.   On the other hand, if the average mutex is held
for 10 msec, it is worth the trouble of making the two context switches. The trouble
is that critical regions can vary considerably in their duration, so which approach is
better?
     One design is to always spin. A second design is to always switch. But a third
design is to make a separate decision each time a locked mutex is encountered. At
the time the decision has to be made, it is not known whether it is better to spin or
switch, but for any given system, it is possible to make a trace of all activity and
analyze it later offline. Then it can be said in retrospect which decision was the
best one and how much time was wasted in the best case. This hindsight algorithm
then becomes a benchmark against which feasible algorithms can be measured.
     This problem has been studied by researchers for decades (Ousterhout, 1982).
Most work uses a model in which a thread failing to acquire a mutex spins for
some period of time.  If this threshold is exceeded, it switches.  In some cases the
threshold is fixed, typically the known overhead for switching to another thread
and then switching back.  In other cases it is dynamic, depending on the observed
history of the mutex being waited on.
     The best results are achieved when the system keeps track of the last few
observed spin times and assumes that this one will be similar to the previous ones.
For example, assuming a 1-msec context switch time again, a thread will spin for a



SEC. 8.1                      MULTIPROCESSORS                                           539
maximum of 2 msec, but observe how long it actually spun.   If it fails to acquire a
lock and sees that on the previous three runs it waited an average of 200 sec, it
should spin for 2 msec before switching. However, if it sees that it spun for the full
2 msec on each of the previous attempts, it should switch immediately and not spin
at all.
     Some modern processors, including the x86, offer special intructions to make
the waiting more efficient in terms of reducing power consumption. For instance,
the MONITOR/MWAIT instructions on x86 allow a program to block until some
other processor modifies the data in a previously defined memory area.         Specif-
ically, the MONITOR instruction defines an address range that should be monitored
for writes.  The MWAIT instruction then blocks the thread until someone writes to
the area. Effectively, the thread is spinning, but without burning many cycles need-
lessly.
8.1.4 Multiprocessor Scheduling
     Before looking at how scheduling is done on multiprocessors, it is necessary to
determine what is being scheduled.  Back in the old days, when all processes were
single threaded, processes were scheduled--there was nothing else schedulable.
All  modern  operating     systems  support  multithreaded  processes,  which  makes
scheduling more complicated.
     It matters whether the threads are kernel threads or user threads. If threading is
done by a user-space library and the kernel knows nothing about the threads, then
scheduling happens on a per-process basis as it always did. If the kernel does not
even know threads exist, it can hardly schedule them.
     With kernel threads, the picture is different. Here the kernel is aware of all the
threads and can pick and choose among the threads belonging to a process. In these
systems, the trend is for the kernel to pick a thread to run, with the process it be-
longs to having only a small role (or maybe none) in the thread-selection algo-
rithm.    Below we will talk about scheduling threads, but of course, in a system
with single-threaded processes or threads implemented in user space, it is the proc-
esses that are scheduled.
     Process vs. thread is not the only scheduling issue.   On a uniprocessor, sched-
uling is one dimensional. The only question that must be answered (repeatedly) is:
``Which thread should be run next?'' On a multiprocessor, scheduling has two
dimensions. The scheduler has to decide which thread to run and which CPU to
run it on. This extra dimension greatly complicates scheduling on multiprocessors.
     Another complicating factor is that in some systems, all of the threads are
unrelated, belonging to different processes and having nothing to do with one
another.  In others they come in groups, all belonging to the same application and
working together.  An example of the former situation is a server system in which
independent users start up independent processes. The threads of different proc-
esses are unrelated and each one can be scheduled without regard to the other ones.



540               MULTIPLE PROCESSOR SYSTEMS                             CHAP. 8
     An example of the latter situation occurs regularly in program development en-
vironments. Large systems often consist of some number of header files containing
macros, type definitions, and variable declarations that are used by the actual code
files. When a header file is changed, all the code files that include it must be re-
compiled. The program make is commonly used to manage development.            When
make is invoked, it starts the compilation of only those code files that must be re-
compiled on account of changes to the header or code files. Object files that are
still valid are not regenerated.
     The original version of make did its work sequentially, but newer versions de-
signed for multiprocessors can start up all the compilations at once. If 10 compila-
tions are needed, it does not make sense to schedule 9 of them to run immediately
and leave the last one until much later since the user will not perceive the work as
completed until the last one has finished.      In this case it makes sense to regard the
threads doing the compilations as a group and to take that into account when
scheduling them.
     Moroever sometimes it is useful to schedule threads that communicate exten-
sively, say in a producer-consumer fashion, not just at the same time, but also close
together in space. For instance, they may benefit from sharing caches. Likewise, in
NUMA architectures, it may help if they access memory that is close by.
Time Sharing
     Let us first address the case of scheduling independent threads; later we will
consider how to schedule related threads. The simplest scheduling algorithm for
dealing with unrelated threads is to have a single systemwide data structure for
ready threads, possibly just a list, but more likely a set of lists for threads at dif-
ferent priorities as depicted in Fig. 8-12(a).  Here the 16 CPUs are all currently
busy, and a prioritized set of 14 threads are waiting to run. The first CPU to finish
its current work (or have its thread block) is CPU 4, which then locks the schedul-
ing queues and selects the highest-priority thread, A, as shown in Fig. 8-12(b).
Next, CPU 12 goes idle and chooses thread B, as illustrated in Fig. 8-12(c).             As
long as the threads are completely unrelated, doing scheduling this way is a rea-
sonable choice and it is very simple to implement efficiently.
     Having a single scheduling data structure used by all CPUs timeshares the
CPUs, much as they would be in a uniprocessor system.  It also provides automatic
load balancing because it can never happen that one CPU is idle while others are
overloaded. Two disadvantages of this approach are the potential contention for the
scheduling data structure as the number of CPUs grows and the usual overhead in
doing a context switch when a thread blocks for I/O.
     It is also possible that a context switch happens when a thread's quantum ex-
pires.  On a multiprocessor, that has certain properties not present on a uniproc-
essor.  Suppose that the thread happens to hold a spin lock when its quantum ex-
pires. Other CPUs waiting on the spin lock just waste their time spinning until that



SEC. 8.1                                   MULTIPROCESSORS                                                                 541
0         1       2     3                      0            1       2     3                    0            1        2     3
4         5       6     7      CPU             A            5       6     7      CPU 12        A            5        6     7
8         9   10        11          CPU 4      8            9   10        11     goes idle     8            9   10         11
                                    goes idle
12        13  14        15                     12           13  14        15                   B            13  14         15
Priority                                       Priority                                        Priority
    7                A  B   C                            7             B  C                              7              C
    6                D  E                                6             D  E                              6              D  E
    5                F                                   5             F                                 5              F
    4                                                    4                                               4
    3                G  H   I                            3             G  H   I                          3              G  H   I
    2                J  K                                2             J  K                              2              J  K
    1                                                    1                                               1
    0                L  M   N                            0             L  M   N                          0              L  M   N
             (a)                                               (b)                                              (c)
                  Figure 8-12. Using a single data structure for scheduling a multiprocessor.
thread is scheduled again and releases the lock.                              On a uniprocessor, spin locks are
rarely used, so if a process is suspended while it holds a mutex, and another thread
starts and tries to acquire the mutex, it will be immediately blocked, so little time is
wasted.
To get around this anomaly, some systems use smart scheduling, in which a
thread acquiring a spin lock sets a processwide flag to show that it currently has a
spin lock (Zahorjan et al., 1991).             When it releases the lock, it clears the flag. The
scheduler then does not stop a thread holding a spin lock, but instead gives it a lit-
tle more time to complete its critical region and release the lock.
Another issue that plays a role in scheduling is the fact that while all CPUs are
equal, some CPUs are more equal.               In particular, when thread A has run for a long
time on CPU k, CPU k's cache will be full of                                  A's blocks.  If  A gets to run again
soon, it may perform better if it is run on CPU k, because k's cache may still con-
tain some of A's blocks. Having cache blocks preloaded will increase the cache hit
rate and thus the thread's speed.              In addition, the TLB may also contain the right
pages, reducing TLB faults.
Some multiprocessors take this effect into account and use what is called affin-
ity scheduling (Vaswani and Zahorjan, 1991).                                  The basic idea here is to make a
serious effort to have a thread run on the same CPU it ran on last time. One way to
create this affinity is to use a two-level scheduling algorithm.                               When a thread is
created, it is assigned to a CPU, for example based on which one has the smallest
load at that moment. This assignment of threads to CPUs is the top level of the al-
gorithm.      As a result of this policy, each CPU acquires its own collection of
threads.
The actual scheduling of the threads is the bottom level of the algorithm.                                                    It is
done by each CPU separately, using priorities or some other means.                                          By trying to



542                      MULTIPLE PROCESSOR SYSTEMS                                  CHAP. 8
keep a thread on the same CPU for its entire lifetime, cache affinity is maximized.
However, if a CPU has no threads to run, it takes one from another CPU rather than
go idle.
     Two-level scheduling has three benefits. First, it distributes the load roughly
evenly over the available CPUs.    Second, advantage is taken of cache affinity
where possible. Third, by giving each CPU its own ready list, contention for the
ready lists is minimized because attempts to use another CPU's ready list are rel-
atively infrequent.
Space Sharing
     The other general approach to multiprocessor scheduling can be used when
threads are related to one another in some way. Earlier we mentioned the example
of parallel make as one case.  It also often occurs that a single process has multiple
threads that work together. For example, if the threads of a process communicate a
lot, it is useful to have them running at the same time. Scheduling multiple threads
at the same time across multiple CPUs is called space sharing.
     The simplest space-sharing algorithm works like this. Assume that an entire
group of related threads is created at once.  At the time it is created, the scheduler
checks to see if there are as many free CPUs as there are threads. If there are, each
thread is given its own dedicated (i.e., nonmultiprogrammed) CPU and they all
start.  If there are not enough CPUs, none of the threads are started until enough
CPUs are available. Each thread holds onto its CPU until it terminates, at which
time the CPU is put back into the pool of available CPUs.        If a thread blocks on
I/O, it continues to hold the CPU, which is simply idle until the thread wakes up.
When the next batch of threads appears, the same algorithm is applied.
     At any instant of time, the set of CPUs is statically partitioned into some num-
ber of partitions, each one running the threads of one process.                      In Fig. 8-13, we
have partitions of sizes 4, 6, 8, and 12 CPUs, with 2 CPUs unassigned, for ex-
ample.    As time goes on, the number and size of the partitions will change as new
threads are created and old ones finish and terminate.
        8-CPU partition
                         0     1   2     3    4   5     6   7                        4-CPU partition
                         8     9   10    11   12  13    14  15
        6-CPU partition  16    17  18    19   20  21    22  23
                         24    25  26    27   28  29    30  31
                         Unassigned CPU                 12-CPU partition
          Figure 8-13.   A set of 32 CPUs split into four partitions, with two CPUs
          available.



SEC. 8.1                             MULTIPROCESSORS                                    543
Periodically, scheduling decisions have to be made.             In uniprocessor systems,
shortest job first is a well-known algorithm for batch scheduling. The analogous al-
gorithm for a multiprocessor is to choose the process needing the smallest number
of CPU cycles, that is, the thread whose CPU-count × run-time is the smallest of
the candidates. However, in practice, this information is rarely available, so the al-
gorithm is hard to carry out.    In fact, studies have shown that, in practice, beating
first-come, first-served is hard to do (Krueger et al., 1994).
In this simple partitioning model, a thread just asks for some number of CPUs
and either gets them all or has to wait until they are available. A different approach
is for threads to actively manage the degree of parallelism. One method for manag-
ing the parallelism is to have a central server that keeps track of which threads are
running and want to run and what their minimum and maximum CPU requirements
are (Tucker and Gupta, 1989).        Periodically, each application polls the central ser-
ver to ask how many CPUs it may use.         It then adjusts the number of threads up or
down to match what is available.
For example, a Web server can have 5, 10, 20, or any other number of threads
running in parallel.  If it currently has 10 threads and there is suddenly more de-
mand for CPUs and it is told to drop to five, when the next five threads finish their
current work, they are told to exit instead of being given new work. This scheme
allows the partition sizes to vary dynamically to match the current workload better
than the fixed system of Fig. 8-13.
Gang Scheduling
A clear advantage of space sharing is the elimination of multiprogramming,
which eliminates the context-switching overhead. However, an equally clear disad-
vantage is the time wasted when a CPU blocks and has nothing at all to do until it
becomes ready again. Consequently, people have looked for algorithms that at-
tempt to schedule in both time and space together, especially for threads that create
multiple threads, which usually need to communicate with one another.
To see the kind of problem that can occur when the threads of a process are in-
dependently  scheduled,  consider    a  system  with  threads   A0  and  A1  belonging      to
process   A and threads  B0 and      B1 belonging to process    B.  Threads  A0 and B0  are
timeshared on CPU 0; threads         A1 and  B1 are timeshared on CPU 1.     Threads        A0
and A1 need to communicate often. The communication pattern is that A0 sends A1
a message, with A1 then sending back a reply to A0, followed by another such se-
quence, common in client-server situations.     Suppose luck has it that     A0 and      B1
start first, as shown in Fig. 8-14.
In time slice 0, A0 sends A1 a request, but A1 does not get it until it runs in
time slice 1 starting at 100 msec.   It sends the reply immediately, but     A0  does not
get the reply until it runs again at 200 msec. The net result is one request-reply se-
quence every 200 msec. Not very good performance.



544                     MULTIPLE PROCESSOR SYSTEMS                                       CHAP. 8
                                       Thread A0 running
         CPU 0      A0             B0       A0            B0           A0       B0
                    Request     1           Request 2
                                   Reply 1                    Reply 2
         CPU 1      B1             A1       B1            A1           B1       A1
         Time   0       100            200      300           400          500      600
         Figure 8-14. Communication between two threads belonging to thread A that are
         running out of phase.
     The solution to this problem is gang scheduling, which is an outgrowth of co-
scheduling (Ousterhout, 1982). Gang scheduling has three parts:
     1.  Groups of related threads are scheduled as a unit, a gang.
     2.  All members of a gang run at once on different timeshared CPUs.
     3.  All gang members start and end their time slices together.
The trick that makes gang scheduling work is that all CPUs are scheduled syn-
chronously. Doing this means that time is divided into discrete quanta as we had in
Fig. 8-14.  At the start of each new quantum, all the CPUs are rescheduled, with a
new thread being started on each one.           At the start of the next quantum, another
scheduling event happens.       In between, no scheduling is done.              If a thread blocks,
its CPU stays idle until the end of the quantum.
     An example of how gang scheduling works is given in Fig. 8-15.                      Here we
have a multiprocessor with six CPUs being used by five processes, A through E,
with a total of 24 ready threads. During time slot 0, threads A0 through A6 are
scheduled and run.      During time slot 1, threads B0, B1, B2, C0, C1, and C2 are
scheduled and run. During time slot 2,      D's five threads and E0        get to run.   The re-
maining six threads belonging to thread E run in time slot 3.                   Then the cycle re-
peats, with slot 4 being the same as slot 0 and so on.
     The idea of gang scheduling is to have all the threads of a process run together,
at the same time, on different CPUs, so that if one of them sends a request to an-
other one, it will get the message almost immediately and be able to reply almost
immediately.    In Fig. 8-15, since all the A threads are running together, during one
quantum, they may send and receive a very large number of messages in one
quantum, thus eliminating the problem of Fig. 8-14.



SEC. 8.2                            MULTICOMPUTERS                                      545
                                                   CPU
                              0     1          2        3        4      5
                         0    A0    A1         A2       A3       A4     A5
                         1    B0    B1         B2       C0       C1     C2
                         2    D0    D1         D2       D3       D4     E0
                Time     3    E1    E2         E3       E4       E5     E6
                   slot  4    A0    A1         A2       A3       A4     A5
                         5    B0    B1         B2       C0       C1     C2
                         6    D0    D1         D2       D3       D4     E0
                         7    E1    E2         E3       E4       E5     E6
                                  Figure 8-15. Gang scheduling.
8.2 MULTICOMPUTERS
Multiprocessors are popular and attractive because they offer a simple commu-
nication model: all CPUs share a common memory. Processes can write messages
to memory that can then be read by other processes. Synchronization can be done
using mutexes, semaphores, monitors, and other well-established techniques. The
only fly in the ointment is that large multiprocessors are difficult to build and thus
expensive.  And very large ones are impossible to build at any price.       So something
else is needed if we are to scale up to large numbers of CPUs.
To get around these problems, much research has been done on multicomput-
ers, which are tightly coupled CPUs that do not share memory. Each one has its
own memory, as shown in Fig. 8-1(b).           These systems are also known by a variety
of other names, including cluster computers and COWS (Clusters Of Worksta-
tions).  Cloud computing services are always built on multicomputers because they
need to be large.
Multicomputers           are  easy  to  build  because      the  basic  component  is  just  a
stripped-down PC, without a keyboard, mouse, or monitor, but with a high-per-
formance network interface card. Of course, the secret to getting high performance
is to design the interconnection network and the interface card cleverly. This prob-
lem is completely analogous to building the shared memory in a multiprocessor
[e.g., see Fig. 8-1(b)]. However, the goal is to send messages on a microsecond
time scale, rather than access memory on a nanosecond time scale, so it is simpler,
cheaper, and easier to accomplish.
In the following sections, we will first take a brief look at multicomputer hard-
ware, especially the interconnection hardware. Then we will move onto the soft-
ware, starting with low-level communication software, then high-level communica-
tion software.  We will also look at a way shared memory can be achieved on sys-
tems that do not have it. Finally, we will examine scheduling and load balancing.



546                     MULTIPLE PROCESSOR SYSTEMS                                   CHAP. 8
8.2.1 Multicomputer Hardware
     The basic node of a multicomputer consists of a CPU, memory, a network in-
terface, and sometimes a hard disk. The node may be packaged in a standard PC
case, but the monitor, keyboard, and mouse are nearly always absent.                 Sometimes
this configuration is called a headless workstation because there is no user with a
head in front of it.  A workstation with a human user should logically be called a
``headed workstation,'' but for some reason it is not.  In some cases, the PC con-
tains a 2-way or 4-way multiprocessor board, possibly each with a dual-, quad- or
octa-core chip, instead of a single CPU, but for simplicity, we will assume that
each node has one CPU.  Often hundreds or even thousands of nodes are hooked
together to form a multicomputer. Below we will say a little about how this hard-
ware is organized.
Interconnection Technology
     Each node has a network interface card with one or two cables (or fibers) com-
ing out of it. These cables connect either to other nodes or to switches.            In a small
system, there may be one switch to which all the nodes are connected in the star
topology of Fig. 8-16(a). Modern switched Ethernets use this topology.
     (a)                      (b)                                     (c)
     (d)                      (e)                                     (f)
     Figure 8-16. Various interconnect topologies. (a) A single switch. (b) A ring.
     (c) A grid. (d) A double torus. (e) A cube. (f) A 4D hypercube.



SEC. 8.2                         MULTICOMPUTERS                                          547
As an alternative to the single-switch design, the nodes may form a ring, with
two wires coming out the network interface card, one into the node on the left and
one going into the node on the right, as shown in Fig. 8-16(b).  In this topology, no
switches are needed and none are shown.
The grid or mesh of Fig. 8-16(c) is a two-dimensional design that has been
used in many commercial systems. It is highly regular and easy to scale up to large
sizes.  It has a diameter, which is the longest path between any two nodes, and
which increases only as the square root of the number of nodes.   A variant on the
grid is the double torus of Fig. 8-16(d), which is a grid with the edges connected.
Not only is it more fault tolerant than the grid, but the diameter is also less because
the opposite corners can now communicate in only two hops.
The cube of Fig. 8-16(e) is a regular three-dimensional topology.         We have il-
lustrated a 2 × 2 × 2 cube, but in the most general case it could be a k × k × k
cube.   In Fig. 8-16(f) we have a four-dimensional cube built from two three-dimen-
sional cubes with the corresponding nodes connected.        We could make a five-
dimensional cube by cloning the structure of Fig. 8-16(f) and connecting the cor-
responding nodes to form a block of four cubes. To go to six dimensions, we could
replicate the block of four cubes and interconnect the corresponding nodes, and so
on. An n-dimensional cube formed this way is called a hypercube.
Many        parallel  computers  use  a    hypercube  topology   because  the  diameter
grows linearly with the dimensionality. Put in other words, the diameter is the base
2 logarithm of the number of nodes. For example, a 10-dimensional hypercube has
1024 nodes but a diameter of only 10, giving excellent delay properties. Note that
in contrast, 1024 nodes arranged as a 32 × 32 grid have a diameter of 62, more
than six times worse than the hypercube. The price paid for the smaller diameter is
that the fanout, and thus the number of links (and the cost), is much larger for the
hypercube.
Two kinds of switching schemes are used in multicomputers.         In the first one,
each message is first broken up (either by the user software or the network inter-
face) into a chunk of some maximum length called a packet.                The switching
scheme, called store-and-forward packet switching, consists of the packet being
injected into the first switch by the source node's network interface board, as
shown in Fig. 8-17(a).  The bits come in one at a time, and when the whole packet
has arrived at an input buffer, it is copied to the line leading to the next switch
along the path, as shown in Fig. 8-17(b).  When the packet arrives at the switch at-
tached to the destination node, as shown in Fig. 8-17(c), the packet is copied to that
node's network interface board and eventually to its RAM.
While store-and-forward packet switching is flexible and efficient, it does have
the problem of increasing latency (delay) through the interconnection network.
Suppose that the time to move a packet one hop in Fig. 8-17 is T nsec. Since the
packet must be copied four times to get it from CPU 1 to CPU 2 (to A, to C, to D,
and to the destination CPU), and no copy can begin until the previous one is fin-
ished, the latency through the interconnection network is 4T .    One way out is to



548                       MULTIPLE PROCESSOR SYSTEMS                                 CHAP.   8
        CPU 1                Four-port                   Input port
                             switch                             Output port
               A          B             A          B                      A       B
Entire         C          D             C          D                      C       D
packet
                                                                                     CPU 2
                                           Entire                                    Entire
                                           packet                                    packet
                  (a)                      (b)                               (c)
                       Figure 8-17.  Store-and-forward packet switching.
design a network in which a packet can be logically divided into smaller units.              As
soon as the first unit arrives at a switch, it can be forwarded, even before the tail
has arrived. Conceivably, the unit could be as small as 1 bit.
     The other switching regime, circuit switching, consists of the first switch first
establishing a path through all the switches to the destination switch. Once that
path has been set up, the bits are pumped all the way from the source to the desti-
nation nonstop as fast as possible. There is no intermediate buffering at the inter-
vening switches.       Circuit switching requires a setup phase, which takes some time,
but is faster once the setup has been completed. After the packet has been sent, the
path must be torn down again.        A variation on circuit switching, called wormhole
routing, breaks each packet up into subpackets and allows the first subpacket to
start flowing even before the full path has been built.
Network Interfaces
     All the nodes in a multicomputer have a plug-in board containing the node's
connection to the interconnection network that holds the multicomputer together.
The way these boards are built and how they connect to the main CPU and RAM
have substantial implications for the operating system.  We will now briefly look at
some of the issues here.     This material is based in part on the work of Bhoedjang
(2000).
     In virtually all multicomputers, the interface board contains substantial RAM
for holding outgoing and incoming packets. Usually, an outgoing packet has to be
copied to the interface board's RAM before it can be transmitted to the first switch.
The reason for this design is that many interconnection networks are synchronous,
so that once a packet transmission has started, the bits must continue flowing at a



SEC. 8.2                            MULTICOMPUTERS                                       549
constant rate.  If the packet is in the main RAM, this continuous flow out onto the
network cannot be guaranteed due to other traffic on the memory bus. Using a ded-
icated RAM on the interface board eliminates this problem.    This design is shown
in Fig. 8-18.
                Node 1                                        Node 2
                       Main RAM                               Main RAM
                User    1
          CPU                           3                               CPU
                OS               2                        4   5
                                           Switch
                       Main RAM                               Main RAM
          CPU                                                           CPU
                                        Optional
                                        on- board
                Node 3                     CPU     Interface  Node 4
                           Interface               board
                                 board
                                 RAM
          Figure 8-18. Position of the network interface boards in a multicomputer.
The same problem occurs with incoming packets. The bits arrive from the net-
work at a constant and often extremely high rate.             If the network interface board
cannot store them in real time as they arrive, data will be lost. Again here, trying to
go over the system bus (e.g., the PCI bus) to the main RAM is too risky. Since the
network board is typically plugged into the PCI bus, this is the only connection it
has to the main RAM, so competing for this bus with the disk and every other I/O
device is inevitable.   It is safer to store incoming packets in the interface board's
private RAM and then copy them to the main RAM later.
The interface board may have one or more DMA channels or even a complete
CPU (or maybe even multiple CPUs) on board. The DMA channels can copy pack-
ets between the interface board and the main RAM at high speed by requesting
block transfers on the system bus, thus transferring several words without having to
request the bus separately for each word. However, it is precisely this kind of block
transfer, which ties up the system bus for multiple bus cycles, that makes the inter-
face board RAM necessary in the first place.
Many interface boards have a CPU on them, possibly in addition to one or
more DMA channels. They are called network processors and are becoming in-
creasingly powerful (El Ferkouss et al., 2011).    This design means that the main
CPU can offload some work to the network board, such as handling reliable trans-
mission (if the underlying hardware can lose packets), multicasting (sending a
packet to more than one destination), compression/decompression, encryption/de-
cryption, and taking care of protection in a system that has multiple processes.



550                       MULTIPLE PROCESSOR SYSTEMS                    CHAP. 8
However, having two CPUs means that they must synchronize to avoid race condi-
tions, which adds extra overhead and means more work for the operating system.
      Copying data across layers is safe, but not necessarily efficient. For instance, a
brower requesting data from a remote web server will create a request in the brow-
ser's address space. That request is subsequently copied to the kernel so that TCP
and IP can handle it. Next, the data are copied to the memory of the network inter-
face. On the other end, the inverse happens: the data are copied from the network
card to a kernel buffer, and from a kernel buffer to the Web server. Quite a few cop-
ies, unfortunately. Each copy introduces overhead, not just the copying itself, but
also the pressure on the cache, TLB, etc. As a consequence, the latency over such
network connections is high.
      In the next section, we discuss techniques to reduce the overhead due to copy-
ing, cache pollution, and context switching as much as possible.
8.2.2 Low-Level Communication Software
      The enemy of high-performance communication in multicomputer systems is
excess copying of packets.    In the best case, there will be one copy from RAM to
the interface board at the source node, one copy from the source interface board to
the destination interface board (if no storing and forwarding along the path occurs),
and one copy from there to the destination RAM, a total of three copies. However,
in many systems it is even worse.  In particular, if the interface board is mapped
into kernel virtual address space and not user virtual address space, a user process
can send a packet only by issuing a system call that traps to the kernel. The kernels
may have to copy the packets to their own memory both on output and on input,
for example, to avoid page faults while transmitting over the network. Also, the re-
ceiving kernel probably does not know where to put incoming packets until it has
had a chance to examine them. These five copy steps are illustrated in Fig. 8-18.
      If copies to and from RAM are the bottleneck, the extra copies to and from the
kernel may double the end-to-end delay and cut the throughput in half.  To avoid
this performance hit, many multicomputers map the interface board directly into
user space and allow the user process to put the packets on the board directly, with-
out the kernel being involved. While this approach definitely helps performance, it
introduces two problems.
      First, what if several processes are running on the node and need network ac-
cess to send packets?  Which one gets the interface board in its address space?
Having a system call to map the board in and out of a virtual address space is ex-
pensive, but if only one process gets the board, how do the other ones send pack-
ets?  And what happens if the board is mapped into process          A's virtual address
space and a packet arrives for process B, especially if A and B have different own-
ers, neither of whom wants to put in any effort to help the other?
      One solution is to map the interface board into all processes that need it, but
then a mechanism is needed to avoid race conditions. For example, if A claims a



SEC. 8.2                     MULTICOMPUTERS                                              551
buffer on the interface board, and then, due to a time slice, B runs and claims the
same buffer, disaster results. Some kind of synchronization mechanism is needed,
but these mechanisms, such as mutexes, work only when the processes are as-
sumed to be cooperating.     In a shared environment with multiple users all in a
hurry to get their work done, one user might just lock the mutex associated with
the board and never release it. The conclusion here is that mapping the interface
board into user space really works well only when there is just one user process
running on each node unless special precautions are taken (e.g., different processes
get different portions of the interface RAM mapped into their address spaces).
The second problem is that the kernel may well need access to the intercon-
nection network itself, for example, to access the file system on a remote node.
Having the kernel share the interface board with any users is not a good idea.  Sup-
pose that while the board was mapped into user space, a kernel packet arrived.           Or
suppose that the user process sent a packet to a remote machine pretending to be
the kernel.   The conclusion is that the simplest design is to have two network inter-
face boards, one mapped into user space for application traffic and one mapped
into kernel space for use by the operating system. Many multicomputers do pre-
cisely this.
On the other hand, newer network interfaces are frequently multiqueue, which
means that they have more than one buffer to support multiple users efficiently. For
instance, the Intel I350 series of network cards has 8 send and 8 receive queues,
and is virtualizable to many virtual ports. Better still, the card supports core affin-
ity. Specifically, it has its own hashing logic to help steer each packet to a suitable
process. As it is faster to process all segments in the same TCP flow on the same
processor (where the caches are warm), the card can use the hashing logic to hash
the TCP flow fields (IP addresses and TCP port numbers) and add all segments
with the same hash on the same queue that is served by a specific core. This is also
useful for virtualization, as it allows us to give each virtual machine its own queue.
Node-to-Network Interface Communication
Another issue is how to get packets onto the interface board. The fastest way is
to use the DMA chip on the board to just copy them in from RAM.  The problem
with this approach is that DMA may use physical rather than virtual addresses and
runs independently of the CPU, unless an I/O MMU is present.     To start with, al-
though a user process certainly knows the virtual address of any packet it wants to
send, it generally does not know the physical address. Making a system call to do
the virtual-to-physical mapping is undesirable, since the point of putting the inter-
face board in user space in the first place was to avoid having to make a system call
for each packet to be sent.
In addition, if the operating system decides to replace a page while the DMA
chip is copying a packet from it, the wrong data will be transmitted. Worse yet, if
the operating system replaces a page while the DMA chip is copying an incoming



552  MULTIPLE PROCESSOR SYSTEMS                                        CHAP. 8
packet to it, not only will the incoming packet be lost, but also a page of innocent
memory will be ruined, probably with disastrous consequences shortly.
     These problems can be avoided by having system calls to pin and unpin pages
in memory, marking them as temporarily unpageable. However, having to make a
system call to pin the page containing each outgoing packet and then having to
make another call later to unpin it is expensive.  If packets are small, say, 64 bytes
or less, the overhead for pinning and unpinning every buffer is prohibitive. For
large packets, say, 1 KB or more, it may be tolerable.       For sizes in between, it de-
pends on the details of the hardware.  Besides introducing a performance hit, pin-
ning and unpinning pages adds to the software complexity.
Remote Direct Memory Access
     In some fields, high network latencies are simply not acceptable. For instance,
for certain applications in high-performance computing the computation time is
strongly dependent on the network latency. Likewise, high-frequency trading is all
about having computers perform transactions (buying and selling stock) at ex-
tremely high speeds--every microsecond counts. Whether or not it is wise to have
computer programs trade millions of dollars worth of stock in a millisecond, when
pretty much all software tends to be buggy, is an interesting question for dining
philosophers to consider when they are not busy grabbing their forks. But not for
this book. The point here is that if you manage to get the latency down, it is sure to
make you very popular with your boss.
     In these scenarios, it pays to reduce the amount of copying. For this reason,
some network interfaces support RDMA (Remote Direct Memory Access), a
technique that allows one machine to perform a direct memory access from one
computer to that of another. The RDMA does not involve either of the operating
system and the data is directly fetched from, or written to, application memory.
     RDMA sounds great, but it is not without its disadvantages. Just like normal
DMA, the operating system on the communicating nodes must pin the pages invol-
ved in the data exchange. Also, just placing data in a remote computer's memory
will not reduce the latency much if the other program is not aware of it. A suc-
cessful RDMA does not automatically come with an explicit notification. Instead, a
common solution is that a receiver polls on a byte in memory. When the transfer is
done, the sender modifies the byte to signal the receiver that there is new data.
While this solution works, it is not ideal and wastes CPU cycles.
     For really serious high-frequency trading, the network cards are custom built
using field-programmable gate arrays. They have wire-to-wire latency, from re-
ceiving the bits on the network card to transmitting a message to buy a few million
worth of something, in well under a microsecond.             Buying $1 million worth of
stock in 1 sec gives a performance of 1 terabuck/sec, which is nice if you can get
the ups and downs right, but is not for the faint of heart.  Operating systems do not
play much of a role in such extreme settings.



SEC. 8.2                  MULTICOMPUTERS                                             553
8.2.3 User-Level Communication Software
Processes on different CPUs on a multicomputer communicate by sending
messages to one another.  In the simplest form, this message passing is exposed to
the user processes.  In other words, the operating system provides a way to send
and receive messages, and library procedures make these underlying calls available
to user processes.   In a more sophisticated form, the actual message passing is hid-
den from users by making remote communication look like a procedure call.               We
will study both of these methods below.
Send and Receive
At the barest minimum, the communication services provided can be reduced
to two (library) calls, one for sending messages and one for receiving them. The
call for sending a message might be
send(dest, &mptr);
and the call for receiving a message might be
receive(addr, &mptr);
The former sends the message pointed to by mptr to a process identified by dest
and causes the called to be blocked until the message has been sent. The latter
causes the called to be blocked until a message arrives.  When one does, the mes-
sage is copied to the buffer pointed to by mptr and the called is unblocked. The
addr parameter specifies the address to which the receiver is listening. Many vari-
ants of these two procedures and their parameters are possible.
One issue is how addressing is done. Since multicomputers are static, with the
number of CPUs fixed, the easiest way to handle addressing is to make addr a two-
part address consisting of a CPU number and a process or port number on the ad-
dressed CPU.     In this way each CPU can manage its own addresses without poten-
tial conflicts.
Blocking versus Nonblocking Calls
The calls described above are blocking calls (sometimes called synchronous
calls).  When a process calls send, it specifies a destination and a buffer to send to
that destination. While the message is being sent, the sending process is blocked
(i.e., suspended).   The instruction following the call to send is not executed until
the message has been completely sent, as shown in Fig. 8-19(a).     Similarly, a call
to receive does not return control until a message has actually been received and
put in the message buffer pointed to by the parameter. The process remains sus-
pended in receive until a message arrives, even if it takes hours.  In some systems,



554                     MULTIPLE PROCESSOR SYSTEMS                                         CHAP. 8
the receiver can specify from whom it wishes to receive, in which case it remains
blocked until a message from that sender arrives.
     Sender running                      Sender blocked                   Sender running
                        Trap to kernel,                          Return from kernel,
                        sender blocked                                    sender released
                              Message being sent
                                                 (a)
                        Sender
                        blocked
     Sender running                                      Sender running
                        Trap             Return
                                                      Message being sent
                        Message
                        copied to a
                        kernel buffer
                                                 (b)
     Figure 8-19. (a) A blocking send call. (b) A nonblocking send call.
     An alternative to blocking calls is the use of nonblocking calls (sometimes
called asynchronous calls). If send is nonblocking, it returns control to the called
immediately, before the message is sent. The advantage of this scheme is that the
sending process can continue computing in parallel with the message transmission,
instead of having the CPU go idle (assuming no other process is runnable).                 The
choice between blocking and nonblocking primitives is normally made by the sys-
tem designers (i.e., either one primitive is available or the other), although in a few
systems both are available and users can choose their favorite.
     However, the performance advantage offered by nonblocking primitives is off-
set by a serious disadvantage: the sender cannot modify the message buffer until
the message has been sent. The consequences of the process overwriting the mes-
sage during transmission are too horrible to contemplate. Worse yet, the sending
process has no idea of when the transmission is done, so it never knows when it is
safe to reuse the buffer. It can hardly avoid touching it forever.
     There are three possible ways out. The first solution is to have the kernel copy
the message to an internal kernel buffer and then allow the process to continue, as
shown in Fig. 8-19(b).  From the sender's point of view, this scheme is the same as
a blocking call: as soon as it gets control back, it is free to reuse the buffer.          Of



SEC. 8.2                          MULTICOMPUTERS                                           555
course, the message will not yet have been sent, but the sender is not hindered by
this fact. The disadvantage of this method is that every outgoing message has to be
copied from user space to kernel space. With many network interfaces, the mes-
sage will have to be copied to a hardware transmission buffer later anyway, so the
first copy is essentially wasted. The extra copy can reduce the performance of the
system considerably.
The second solution is to interrupt (signal) the sender when the message has
been fully sent to inform it that the buffer is once again available.  No copy is re-
quired here, which saves time, but user-level interrupts make programming tricky,
difficult, and subject to race conditions, which makes them irreproducible and
nearly impossible to debug.
The third solution is to make the buffer copy on write, that is, to mark it as read
only until the message has been sent. If the buffer is reused before the message has
been sent, a copy is made.   The problem with this solution is that unless the buffer
is isolated on its own page, writes to nearby variables will also force a copy. Also,
extra administration is needed because the act of sending a message now implicitly
affects the read/write status of the page. Finally, sooner or later the page is likely to
be written again, triggering a copy that may no longer be necessary.
Thus the choices on the sending side are
1.        Blocking send (CPU idle during message transmission).
2.        Nonblocking send with copy (CPU time wasted for the extra copy).
3.        Nonblocking send with interrupt (makes programming difficult).
4.        Copy on write (extra copy probably needed eventually).
Under normal conditions, the first choice is the most convenient, especially if mul-
tiple threads are available, in which case while one thread is blocked trying to
send, other threads can continue working.  It also does not require any kernel buff-
ers to be managed.    Furthermore, as can be seen from comparing Fig. 8-19(a) to
Fig. 8-19(b), the message will usually be out the door faster if no copy is required.
For the record, we would like to point out that some authors use a different cri-
terion to distinguish synchronous from asynchronous primitives.        In the alternative
view, a call is synchronous only if the sender is blocked until the message has been
received and an acknowledgement sent back (Andrews, 1991).             In the world of
real-time communication, synchronous has yet another meaning, which can lead to
confusion, unfortunately.
Just as send can be blocking or nonblocking, so can receive.           A blocking call
just suspends the called until a message has arrived.  If multiple threads are avail-
able, this is a simple approach.  Alternatively, a nonblocking receive just tells the
kernel where the buffer is and returns control almost immediately.        An interrupt
can be used to signal that a message has arrived.      However, interrupts are difficult
to program and are also quite slow, so it may be preferable for the receiver to poll



556                      MULTIPLE PROCESSOR SYSTEMS                      CHAP. 8
for incoming messages using a procedure, poll, that tells whether any messages are
waiting.  If so, the called can call get  message, which returns the first arrived mes-
sage.  In some systems, the compiler can insert poll calls in the code at appropriate
places, although knowing how often to poll is tricky.
      Yet another option is a scheme in which the arrival of a message causes a new
thread to be created spontaneously in the receiving process' address space. Such a
thread is called a pop-up thread.         It runs a procedure specified in advance and
whose parameter is a pointer to the incoming message. After processing the mes-
sage, it simply exits and is automatically destroyed.
      A variant on this idea is to run the receiver code directly in the interrupt hand-
ler, without going to the trouble of creating a pop-up thread.  To make this scheme
even faster, the message itself contains the address of the handler, so when a mes-
sage arrives, the handler can be called in a few instructions. The big win here is
that no copying at all is needed.  The handler takes the message from the interface
board and processes it on the fly. This scheme is called active messages (Von
Eicken et al., 1992).    Since each message contains the address of the handler, ac-
tive messages work only when senders and receivers trust each other completely.
8.2.4 Remote Procedure Call
      Although the message-passing model provides a convenient way to structure a
multicomputer operating system, it suffers from one incurable flaw: the basic
paradigm around which all communication is built is input/output. The procedures
send and receive are fundamentally engaged in doing I/O, and many people believe
that I/O is the wrong programming model.
      This problem has long been known, but little was done about it until a paper by
Birrell and Nelson (1984) introduced a completely different way of attacking the
problem. Although the idea is refreshingly simple (once someone has thought of
it), the implications are often subtle.   In this section we will examine the concept,
its implementation, its strengths, and its weaknesses.
      In a nutshell, what Birrell and Nelson suggested was allowing programs to call
procedures located on other CPUs.         When a process on machine 1 calls a proce-
dure on machine 2, the calling process on 1 is suspended, and execution of the call-
ed procedure takes place on 2.     Information can be transported from the called to
the callee in the parameters and can come back in the procedure result.  No mes-
sage passing or I/O at all is visible to the programmer. This technique is known as
RPC (Remote Procedure Call) and has become the basis of a large amount of
multicomputer software. Traditionally the calling procedure is known as the client
and the called procedure is known as the server, and we will use those names here
too.
      The idea behind RPC is to make a remote procedure call look as much as pos-
sible like a local one.  In the simplest form, to call a remote procedure, the client
program must be bound with a small library procedure called the client stub that



SEC. 8.2                           MULTICOMPUTERS                                               557
represents the server procedure in the client's address space. Similarly, the server is
bound with a procedure called the server stub. These procedures hide the fact that
the procedure call from the client to the server is not local.
The actual steps in making an RPC are shown in Fig. 8-20. Step 1 is the client
calling the client stub. This call is a local procedure call, with the parameters
pushed onto the stack in the normal way. Step 2 is the client stub packing the pa-
rameters into a message and making a system call to send the message. Packing the
parameters is called marshalling.             Step 3 is the kernel sending the message from
the client machine to the server machine. Step 4 is the kernel passing the incoming
packet    to  the  server   stub   (which     would   normally   have      called     receive   earlier).
Finally, step 5 is the server stub calling the server procedure. The reply traces the
same path in the other direction.
                   Client CPU                                           Server CPU
                   1               Client             Server               5
                   Client          stub               stub
                                                                              Server
                               2                                        4
          Operating system                                                 Operating system
                                              3
                                                                        Network
          Figure 8-20.  Steps  in  making  a  remote  procedure  call.  The stubs  are  shaded
          gray.
The key item to note here is that the client procedure, written by the user, just
makes a normal (i.e., local) procedure call to the client stub, which has the same
name as the server procedure. Since the client procedure and client stub are in the
same address space, the parameters are passed in the usual way. Similarly, the ser-
ver procedure is called by a procedure in its address space with the parameters it
expects.    To the server procedure, nothing is unusual.         In this way, instead of doing
I/O using send and receive, remote communication is done by faking a normal pro-
cedure call.
Implementation Issues
Despite the conceptual elegance of RPC, there are a few snakes hiding under
the grass.    A big one is the use of pointer parameters.        Normally, passing a pointer
to a procedure is not a problem. The called procedure can use the pointer the same
way the caller can because the two procedures reside in the same virtual address



558          MULTIPLE PROCESSOR SYSTEMS                                         CHAP. 8
space. With RPC, passing pointers is impossible because the client and server are
in different address spaces.
     In some cases, tricks can be used to make it possible to pass pointers. Suppose
that the first parameter is a pointer to an integer, k.  The client stub can marshal k
and send it along to the server. The server stub then creates a pointer to k and
passes it to the server procedure, just as it expects. When the server procedure re-
turns control to the server stub, the latter sends k back to the client, where the new
k is copied over the old one, just in case the server changed it.   In effect, the stan-
dard calling sequence of call-by-reference has been replaced by copy restore. Un-
fortunately, this trick does not always work, for example, if the pointer points to a
graph or other complex data structure. For this reason, some restrictions must be
placed on parameters to procedures called remotely.
     A second problem is that in weakly typed languages, like C, it is perfectly legal
to write a procedure that computes the inner product of two vectors (arrays), with-
out specifying how large either one is. Each could be terminated by a special value
known only to the calling and called procedures. Under these circumstances, it is
essentially impossible for the client stub to marshal the parameters: it has no way
of determining how large they are.
     A third problem is that it is not always possible to deduce the types of the pa-
rameters, not even from a formal specification or the code itself.  An example is
printf, which may have any number of parameters (at least one), and they can be an
arbitrary mixture of integers, shorts, longs, characters, strings, floating-point num-
bers of various lengths, and other types. Trying to call printf as a remote procedure
would be practically impossible because C is so permissive. However, a rule saying
that RPC can be used provided that you do not program in C (or C++) would not
be popular.
     A fourth problem relates to the use of global variables. Normally, the calling
and called procedures may communicate using global variables, in addition to
communicating via parameters.       If the called procedure is now moved to a remote
machine, the code will fail because the global variables are no longer shared.
     These problems are not meant to suggest that RPC is hopeless.  In fact, it is
widely used, but some restrictions and care are needed to make it work well in
practice.
8.2.5 Distributed Shared Memory
     Although RPC has its attractions, many programmers still prefer a model of
shared memory and would like to use it, even on a multicomputer. Surprisingly
enough, it is possible to preserve the illusion of shared memory reasonably well,
even when it does not actually exist, using a technique called DSM (Distributed
Shared Memory) (Li, 1986; and Li and Hudak, 1989). Despite being an old topic,
research on it is still going strong (Cai and Strazdins, 2012; Choi and Jung, 2013;
and Ohnishi and Yoshida, 2011).     DSM is a useful technique to study as it shows



SEC. 8.2                          MULTICOMPUTERS                                                     559
many of the issues and complications in distributed systems. Moreover, the idea it-
self has been very influential.   With DSM, each page is located in one of the mem-
ories of Fig. 8-1(b).    Each machine has its own virtual memory and page tables.
When a CPU does a LOAD or STORE on a page it does not have, a trap to the oper-
ating system occurs. The operating system then locates the page and asks the CPU
currently holding it to unmap the page and send it over the interconnection net-
work. When it arrives, the page is mapped in and the faulting instruction restarted.
In effect, the operating system is just satisfying page faults from remote RAM in-
stead of from local disk. To the user, the machine looks as if it has shared memory.
The          difference  between  actual     shared  memory             and  DSM        is  illustrated      in
Fig. 8-21. In Fig. 8-21(a), we see a true multiprocessor with physical shared mem-
ory implemented by the hardware.          In Fig. 8-21(b), we see DSM, implemented by
the operating system.       In Fig. 8-21(c), we see yet another form of shared memory,
implemented by yet higher levels of software.        We will come back to this third
option later in the chapter, but for now we will concentrate on DSM.
Machine 1         Machine 2       Machine 1          Machine 2               Machine 1           Machine 2
Application       Application     Application        Application             Application         Application
Run-time          Run-time        Run-time           Run-time                Run-time                Run-time
system            system          system             system                  system                  system
Operating         Operating       Operating          Operating               Operating           Operating
system            system          system             system                  system                  system
Hardware          Hardware        Hardware           Hardware                Hardware            Hardware
Shared memory                     Shared memory                                Shared memory
             (a)                               (b)                                          (c)
          Figure 8-21. Various layers where shared memory can be implemented.               (a) The
          hardware. (b) The operating system. (c) User-level software.
Let us now look in some detail at how DSM works.                        In a DSM system, the ad-
dress space is divided up into pages, with the pages being spread over all the nodes
in the system. When a CPU references an address that is not local, a trap occurs,



560                       MULTIPLE PROCESSOR SYSTEMS                                   CHAP. 8
and the DSM software fetches the page containing the address and restarts the
faulting instruction, which now completes successfully. This concept is illustrated
in Fig. 8-22(a) for an address space with 16 pages and four nodes, each capable of
holding six pages.
               Globally shared virtual memory consisting of 16 pages
     0  1      2    3  4     5      6     7  8        9  10  11  12     13  14     15
     0  2      5          1     3      6          4      7   11         13  15
     9                    8     10                12     14                            Memory
        CPU 0                CPU 1                    CPU 2                 CPU 3
                                             (a)             Network
     0  2      5          1     3      6          4      7   11         13  15
     9  10                8                       12     14
        CPU 0                CPU 1                    CPU 2                 CPU 3
                                             (b)
     0  2      5          1     3      6          4      7   11         13  15
     9  10                8     10                12     14
        CPU 0                CPU 1                    CPU 2                 CPU 3
                                             (c)
        Figure 8-22. (a) Pages of the address space distributed among four machines.
        (b) Situation after CPU 0 references page 10 and the page is moved there.
        (c) Situation if page 10 is read only and replication is used.
     In this example, if CPU 0 references instructions or data in pages 0, 2, 5, or 9,
the references are done locally. References to other pages cause traps.                For ex-
ample, a reference to an address in page 10 will cause a trap to the DSM software,
which then moves page 10 from node 1 to node 0, as shown in Fig. 8-22(b).



SEC. 8.2                           MULTICOMPUTERS                                       561
Replication
One improvement to the basic system that can improve performance consid-
erably is to replicate pages that are read only, for example, program text, read-only
constants, or other read-only data structures. For example, if page 10 in Fig. 8-22 is
a section of program text, its use by CPU 0 can result in a copy being sent to CPU
0 without the original in CPU 1's memory being invalidated or disturbed, as shown
in Fig. 8-22(c).  In this way, CPUs 0 and 1 can both reference page 10 as often as
needed without causing traps to fetch missing memory.
Another possibility is to replicate not only read-only pages, but also all pages.
As long as reads are being done, there is effectively no difference between replicat-
ing a read-only page and replicating a read-write page. However, if a replicated
page is suddenly modified, special action has to be taken to prevent having multi-
ple, inconsistent copies in existence. How inconsistency is prevented will be dis-
cussed in the following sections.
False Sharing
DSM systems are similar to multiprocessors in certain key ways.     In both sys-
tems, when a nonlocal memory word is referenced, a chunk of memory containing
the word is fetched from its current location and put on the machine making the
reference (main memory or cache, respectively). An important design issue is how
big the chunk should be?  In multiprocessors, the cache block size is usually 32 or
64 bytes, to avoid tying up the bus with the transfer too long. In DSM systems, the
unit has to be a multiple of the page size (because the MMU works with pages),
but it can be 1, 2, 4, or more pages.  In effect, doing this simulates a larger page
size.
There are advantages and disadvantages to a larger page size for DSM.                   The
biggest advantage is that because the startup time for a network transfer is fairly
substantial, it does not really take much longer to transfer 4096 bytes than it does
to transfer 1024 bytes.   By transferring data in large units, when a large piece of
address space has to be moved, the number of transfers may often be reduced. This
property is especially important because many programs exhibit locality of refer-
ence, meaning that if a program has referenced one word on a page, it is likely to
reference other words on the same page in the immediate future.
On the other hand, the network will be tied up longer with a larger transfer,
blocking other faults caused by other processes. Also, too large an effective page
size introduces a new problem, called false sharing, illustrated in Fig. 8-23.  Here
we have a page containing two unrelated shared variables, A and B.  Processor 1
makes heavy use of A, reading and writing it. Similarly, process 2 uses B frequent-
ly. Under these circumstances, the page containing both variables will constantly
be traveling back and forth between the two machines.



562                   MULTIPLE    PROCESSOR SYSTEMS                                 CHAP.  8
          CPU 1                                CPU 2
                                                      A and B are unrelated
Shared    A                                    A      shared variables that just
page                                                  happen to be on the same page
          B                                    B
          Code using              Code using
          variable A              variable B
                                                      Network
          Figure 8-23. False sharing of a page containing two unrelated variables.
     The problem here is that although the variables are unrelated, they appear by
accident on the same page, so when a process uses one of them, it also gets the
other. The larger the effective page size, the more often false sharing will occur,
and conversely, the smaller the effective page size, the less often it will occur.
Nothing analogous to this phenomenon is present in ordinary virtual memory sys-
tems.
     Clever compilers that understand the problem and place variables in the ad-
dress space accordingly can help reduce false sharing and improve performance.
However, saying this is easier than doing it.  Furthermore, if the false sharing con-
sists of node 1 using one element of an array and node 2 using a different element
of the same array, there is little that even a clever compiler can do to eliminate the
problem.
Achieving Sequential Consistency
     If writable pages are not replicated, achieving consistency is not an issue.
There is exactly one copy of each writable page, and it is moved back and forth dy-
namically as needed. Since it is not always possible to see in advance which pages
are writable, in many DSM systems, when a process tries to read a remote page, a
local copy is made and both the local and remote copies are set up in their re-
spective MMUs as read only.  As long as all references are reads, everything is
fine.
     However, if any process attempts to write on a replicated page, a potential con-
sistency problem arises because changing one copy and leaving the others alone is
unacceptable. This situation is analogous to what happens in a multiprocessor
when one CPU attempts to modify a word that is present in multiple caches. The
solution there is for the CPU about to do the write to first put a signal on the bus
telling all other CPUs to discard their copy of the cache block. DSM systems typi-
cally work the same way. Before a shared page can be written, a message is sent to



SEC. 8.2                          MULTICOMPUTERS                                       563
all other CPUs holding a copy of the page telling them to unmap and discard the
page. After all of them have replied that the unmap has finished, the original CPU
can now do the write.
It is also possible to tolerate multiple copies of writable pages under carefully
restricted circumstances. One way is to allow a process to acquire a lock on a por-
tion of the virtual address space, and then perform multiple read and write opera-
tions on the locked memory.  At the time the lock is released, changes can be prop-
agated to other copies.    As long as only one CPU can lock a page at a given
moment, this scheme preserves consistency.
Alternatively, when a potentially writable page is actually written for the first
time, a clean copy is made and saved on the CPU doing the write. Locks on the
page can then be acquired, the page updated, and the locks released. Later, when a
process on a remote machine tries to acquire a lock on the page, the CPU that
wrote it earlier compares the current state of the page to the clean copy and builds
a message listing all the words that have changed. This list is then sent to the
acquiring CPU to update its copy instead of invalidating it (Keleher et al., 1994).
8.2.6 Multicomputer Scheduling
On a multiprocessor, all processes reside in the same memory. When a CPU
finishes its current task, it picks a process and runs it.  In principle, all processes
are potential candidates.  On a multicomputer the situation is quite different.      Each
node has its own memory and its own set of processes.       CPU 1 cannot suddenly
decide to run a process located on node 4 without first doing a fair amount of work
to go get it. This difference means that scheduling on multicomputers is easier but
allocation of processes to nodes is more important. Below we will study these is-
sues.
Multicomputer scheduling is somewhat similar to multiprocessor scheduling,
but not all of the former's algorithms apply to the latter. The simplest multiproces-
sor algorithm--maintaining a single central list of ready processes--does not work
however, since each process can only run on the CPU it is currently located on.
However, when a new process is created, a choice can be made where to place it,
for example to balance the load.
Since each node has its own processes, any local scheduling algorithm can be
used. However, it is also possible to use multiprocessor gang scheduling, since that
merely requires an initial agreement on which process to run in which time slot,
and some way to coordinate the start of the time slots.
8.2.7 Load Balancing
There is relatively little to say about multicomputer scheduling because once a
process has been assigned to a node, any local scheduling algorithm will do, unless
gang scheduling is being used. However, precisely because there is so little control



564                          MULTIPLE PROCESSOR SYSTEMS                      CHAP. 8
once a process has been assigned to a node, the decision about which process
should go on which node is important. This is in contrast to multiprocessor sys-
tems, in which all processes live in the same memory and can be scheduled on any
CPU at will. Consequently, it is worth looking at how processes can be assigned to
nodes in an effective way. The algorithms and heuristics for doing this assignment
are known as processor allocation algorithms.
     A large number of processor (i.e., node) allocation algorithms have been pro-
posed over the years. They differ in what they assume is known and what the goal
is.  Properties that might be known about a process include the CPU requirements,
memory usage, and amount of communication with every other process. Possible
goals include minimizing wasted CPU cycles due to lack of local work, minimiz-
ing total communication bandwidth, and ensuring fairness to users and processes.
Below we will examine a few algorithms to give an idea of what is possible.
A Graph-Theoretic Deterministic Algorithm
     A widely studied class of algorithms is for systems consisting of processes
with known CPU and memory requirements, and a known matrix giving the aver-
age amount of traffic between each pair of processes. If the number of processes is
greater than the number of CPUs, k, several processes will have to be assigned to
each CPU. The idea is to perform this assignment to minimize network traffic.
     The system can be represented as a weighted graph, with each vertex being a
process and each arc representing the flow of messages between two processes.
Mathematically, the problem then reduces to finding a way to partition (i.e., cut)
the graph into k disjoint subgraphs, subject to certain constraints (e.g., total CPU
and memory requirements below some limits for each subgraph).              For each solu-
tion that meets the constraints, arcs that are entirely within a single subgraph
represent intramachine communication and can be ignored. Arcs that go from one
subgraph to another represent network traffic.  The goal is then to find the parti-
tioning that minimizes the network traffic while meeting all the constraints. As an
example, Fig. 8-24 shows a system of nine processes, A through I , with each arc
labeled with the mean communication load between those two processes (e.g., in
Mbps).
     In Fig. 8-24(a), we have partitioned the graph with processes A, E, and G on
node 1, processes B, F, and H on node 2, and processes C, D, and I on node 3. The
total network traffic is the sum of the arcs intersected by the cuts (the dashed
lines), or 30 units. In Fig. 8-24(b) we have a different partitioning that has only 28
units of network traffic. Assuming that it meets all the memory and CPU con-
straints, this is a better choice because it requires less communication.
     Intuitively, what we are doing is looking for clusters that are tightly coupled
(high intracluster traffic flow) but which interact little with other clusters (low
intercluster traffic flow).  Some of the earliest papers discussing the problem are
Chow and Abraham (1982, Lo, (1984), and Stone and Bokhari (1978).



SEC. 8.2                                                  MULTICOMPUTERS                                                                         565
   Node 1                     Node 2             Node 3                              Node 1               Node  2                        Node 3
                                                                   Traffic
A     3                       B        2      C        3  D        between     A           3        B        2  C                        3       D
                                       8                           D and I                                   8
   2          2                  1                                                      2        2     1
6  E                             F                     5     4                 6           E           F                                 5          4
                              4                  5                                                  4                                 5
   3                                1                                                   3                 1
G          4                        H               2     I                    G              4           H                           2          I
                                                                   Process
                                 Figure       8-24. Two   ways of  allocating  nine processes    to three nodes.
A Sender-Initiated Distributed Heuristic Algorithm
   Now let us look at some distributed algorithms. One algorithm says that when
a process is created, it runs on the node that created it unless that node is overload-
ed. The metric for overloaded might involve too many processes, too big a total
working set, or some other metric.                                     If it is overloaded, the node selects another
node at random and asks it what its load is (using the same metric).                                                              If the probed
node's load is below some threshold value, the new process is sent there (Eager et
al., 1986).                   If not, another machine is chosen for probing. Probing does not go on
forever.   If no suitable host is found within N probes, the algorithm terminates and
the process runs on the originating machine. The idea is for heavily loaded nodes
to try to get rid of excess work, as shown in Fig. 8-25(a), which depicts send-
er-initiated load balancing.
              I'm overloaded        I'm full           Here, have a process    I have nothing to do             I'm free tonight
   Help !                                                                      Yawn                                                              help  ?
                                                       Take some work                                                                    Need
                                                                             I'm bored
                                    (a)                                                                (b)
           Figure 8-25.                   (a)    An overloaded node looking for a lightly loaded                node              to  hand
           off processes                  to.    (b) An empty node looking for work to do.



566                    MULTIPLE PROCESSOR SYSTEMS                 CHAP. 8
     Eager et al. constructed an analytical queueing model of this algorithm. Using
this model, it was established that the algorithm behaves well and is stable under a
wide range of parameters, including various threshold values, transfer costs, and
probe limits.
     Nevertheless, it should be observed that under conditions of heavy load, all
machines will constantly send probes to other machines in a futile attempt to find
one that is willing to accept more work. Few processes will be off-loaded, but con-
siderable overhead may be incurred in the attempt to do so.
A Receiver-Initiated Distributed Heuristic Algorithm
     A complementary algorithm to the one discussed above, which is initiated by
an overloaded sender, is one initiated by an underloaded receiver, as shown in
Fig. 8-25(b).  With this algorithm, whenever a process finishes, the system checks
to see if it has enough work.  If not, it picks some machine at random and asks it
for work.  If that machine has nothing to offer, a second, and then a third machine
is asked.  If no work is found with N probes, the node temporarily stops asking,
does any work it has queued up, and tries again when the next process finishes.         If
no work is available, the machine goes idle.   After some fixed time interval, it
begins probing again.
     An advantage of this algorithm is that it does not put extra load on the system
at critical times. The sender-initiated algorithm makes large numbers of probes
precisely when the system can least tolerate it--when it is heavily loaded. With the
receiver-initiated algorithm, when the system is heavily loaded, the chance of a
machine having insufficient work is small. However, when this does happen, it will
be easy to find work to take over. Of course, when there is little work to do, the re-
ceiver-initiated algorithm creates considerable probe traffic as all the unemployed
machines desperately hunt for work. However, it is far better to have the overhead
go up when the system is underloaded than when it is overloaded.
     It is also possible to combine both of these algorithms and have machines try
to get rid of work when they have too much, and try to acquire work when they do
not have enough. Furthermore, machines can perhaps improve on random polling
by keeping a history of past probes to determine if any machines are chronically
underloaded or overloaded. One of these can be tried first, depending on whether
the initiator is trying to get rid of work or acquire it.
8.3 DISTRIBUTED SYSTEMS
     Having    now  completed  our  study  of  multicores,   multiprocessors,           and
multicomputers we are now ready to turn to the last type of multiple processor sys-
tem, the distributed system.   These systems are similar to multicomputers in that



SEC. 8.3                        DISTRIBUTED SYSTEMS                                   567
each node has its own private memory, with no shared physical memory in the sys-
tem. However, distributed systems are even more loosely coupled than multicom-
puters.
To start with, each node of a multicomputer generally has a CPU, RAM, a net-
work interface, and possibly a disk for paging.    In contrast, each node in a distrib-
uted system is a complete computer, with a full complement of peripherals. Next,
the nodes of a multicomputer are normally in a single room, so they can communi-
cate by a dedicated high-speed network, whereas the nodes of a distributed system
may be spread around the world. Finally, all the nodes of a multicomputer run the
same operating system, share a single file system, and are under a common admin-
istration, whereas the nodes of a distributed system may each run a different oper-
ating system, each of which has its own file system, and be under a different
administration.   A typical example of a multicomputer is 1024 nodes in a single
room     at  a  company   or  university  working  on,  say,    pharmaceutical   modeling,
whereas a typical distributed system consists of thousands of machines loosely co-
operating over the Internet.    Figure 8-26 compares multiprocessors, multicom-
puters, and distributed systems on the points mentioned above.
         Item                 Multiprocessor       Multicomputer                 Distributed System
Node configuration            CPU                  CPU, RAM, net interface       Complete computer
Node peripherals              All shared           Shared exc. maybe disk        Full set per node
Location                      Same rack            Same room                     Possibly worldwide
Internode communication       Shared RAM           Dedicated interconnect        Traditional network
Operating systems             One, shared          Multiple, same                Possibly all different
File systems                  One, shared          One, shared                   Each node has own
Administration                One organization     One organization              Many organizations
                Figure 8-26. Comparison of three kinds of multiple CPU systems.
Multicomputers are clearly in the middle using these metrics.                    An interesting
question is: ``Are multicomputers more like multiprocessors or more like distrib-
uted systems?'' Oddly enough, the answer depends strongly on your perspective.
From a technical perspective, multiprocessors have shared memory and the other
two do not. This difference leads to different programming models and different
mindsets.       However,  from  an  applications   perspective,      multiprocessors  and
multicomputers are just big equipment racks in a machine room. Both are used for
solving computationally intensive problems, whereas a distributed system con-
necting computers all over the Internet is typically much more involved in commu-
nication than in computation and is used in a different way.
To some extent, loose coupling of the computers in a distributed system is both
a strength and a weakness.    It is a strength because the computers can be used for a
wide variety of applications, but it is also a weakness, because programming these
applications is difficult due to the lack of any common underlying model.



568                       MULTIPLE PROCESSOR SYSTEMS                      CHAP. 8
     Typical Internet applications include access to remote computers (using telnet,
ssh, and rlogin), access to remote information (using the World Wide Web and
FTP, the File Transfer Protocol), person-to-person communication (using email and
chat programs), and many emerging applications (e.g., e-commerce, telemedicine,
and distance learning). The trouble with all these applications is that each one has
to reinvent the wheel. For example, email, FTP, and the World Wide Web all basi-
cally move files from point A to point B, but each one has its own way of doing it,
complete with its own naming conventions, transfer protocols, replication techni-
ques, and everything else. Although many Web browsers hide these differences
from the average user, the underlying mechanisms are completely different. Hiding
them at the user-interface level is like having a person at a full-service travel agent
Website book a trip from New York to San Francisco, and only later learn whether
she has purchased a plane, train, or bus ticket.
     What distributed systems add to the underlying network is some common
paradigm (model) that provides a uniform way of looking at the whole system. The
intent of the distributed system is to turn a loosely connected bunch of machines
into a coherent system based on one concept. Sometimes the paradigm is simple
and sometimes it is more elaborate, but the idea is always to provide something
that unifies the system.
     A simple example of a unifying paradigm in a different context is found in
UNIX, where all I/O devices are made to look like files. Having keyboards, print-
ers, and serial lines all operated on the same way, with the same primitives, makes
it easier to deal with them than having them all conceptually different.
     One method by which a distributed system can achieve some measure of uni-
formity in the face of different underlying hardware and operating systems is to
have a layer of software on top of the operating system. The layer, called middle-
ware, is illustrated in Fig. 8-27. This layer provides certain data structures and op-
erations that allow processes and users on far-flung machines to interoperate in a
consistent way.
     In a sense, middleware is like the operating system of a distributed system.
That is why it is being discussed in a book on operating systems.         On the other
hand, it is not really an operating system, so the discussion will not go into much
detail. For a comprehensive, book-length treatment of distributed systems, see Dis-
tributed Systems (Tanenbaum and van Steen, 2007). In the remainder of this chap-
ter, we will look quickly at the hardware used in a distributed system (i.e., the un-
derlying computer network), then its communication software (the network proto-
cols). After that we will consider a variety of paradigms used in these systems.
8.3.1 Network Hardware
     Distributed systems are built on top of computer networks, so a brief introduc-
tion to the subject is in order. Networks come in two major varieties, LANs (Local
Area Networks), which cover a building or a campus, and WANs (Wide Area



SEC.  8.3                         DISTRIBUTED SYSTEMS                                              569
                    Common base for applications
      Application                 Application           Application                   Application
      Middleware                  Middleware            Middleware                    Middleware
           Windows                Linux                 Solaris                       Mac OS
           Pentium                Pentium               SPARC                         Macintosh
                                               Network
                    Figure 8-27.  Positioning of middleware in a distributed system.
Networks), which can be citywide, countrywide, or worldwide. The most impor-
tant kind of LAN is Ethernet, so we will examine that as an example LAN. As our
example WAN, we will look at the Internet, even though technically the Internet is
not one network, but a federation of thousands of separate networks. However, for
our purposes, it is sufficient to think of it as one WAN.
Ethernet
Classic Ethernet, which is described in IEEE Standard 802.3, consists of a co-
axial cable to which a number of computers are attached. The cable is called the
Ethernet, in reference to the luminiferous ether through which electromagnetic ra-
diation was once thought to propagate. (When the nineteenth-century British phys-
icist James Clerk Maxwell discovered that electromagnetic radiation could be de-
scribed by a wave equation, scientists assumed that space must be filled with some
ethereal medium in which the radiation was propagating. Only after the famous
Michelson-Morley experiment in 1887, which failed to detect the ether, did physi-
cists realize that radiation could propagate in a vacuum.)
In the very first version of Ethernet, a computer was attached to the cable by li-
terally drilling a hole halfway through the cable and screwing in a wire leading to
the computer. This was called a vampire tap, and is illustrated symbolically in
Fig. 8-28(a).  The taps were hard to get right, so before long, proper connectors
were used. Nevertheless, electrically, all the computers were connected as if the
cables on their network interface cards were soldered together.



570                       MULTIPLE PROCESSOR SYSTEMS                                 CHAP. 8
                                                Computer
        Computer
                                                                             Switch
                                                Ethernet
     Vampire tap          Ethernet
                  (a)                                          (b)
                  Figure 8-28. (a) Classic Ethernet. (b) Switched Ethernet.
     With many computers hooked up to the same cable, a protocol is needed to
prevent chaos.    To send a packet on an Ethernet, a computer first listens to the
cable to see if any other computer is currently transmitting.       If not, it just begins
transmitting a packet, which consists of a short header followed by a payload of 0
to 1500 bytes.    If the cable is in use, the computer simply waits until the current
transmission finishes, then it begins sending.
     If two computers start transmitting simultaneously, a collision results, which
both of them detect. Both respond by terminating their transmissions, waiting a
random amount of time between 0 and T           sec and then starting again.         If another
collision occurs, all colliding computers randomize the wait into the interval 0 to
2T sec, and then try again. On each further collision, the maximum wait interval
is doubled, reducing the chance of more collisions. This algorithm is known as
binary exponential backoff.  We saw it earlier to reduce polling overhead on
locks.
     An Ethernet has a maximum cable length and also a maximum number of
computers that can be connected to it.  To exceed either of these limits, a large
building or campus can be wired with multiple Ethernets, which are then con-
nected by devices called bridges.   A bridge is a device that allows traffic to pass
from one Ethernet to another when the source is on one side and the destination is
on the other.
     To avoid the problem of collisions, modern Ethernets use switches, as shown in
Fig. 8-28(b).   Each switch has some number of ports, to which can be attached a
computer, an Ethernet, or another switch. When a packet successfully avoids all
collisions and makes it to the switch, it is buffered there and sent out on the port
where the destination machine lives.    By giving each computer its own port, all
collisions can be eliminated, at the cost of bigger switches. Compromises, with just
a few computers per port, are also possible.    In Fig. 8-28(b), a classical Ethernet
with multiple computers connected to a cable by vampire taps is attached to one of
the ports of the switch.



SEC. 8.3                   DISTRIBUTED SYSTEMS                                           571
The Internet
The Internet evolved from the ARPANET, an experimental packet-switched
network funded by the U.S. Dept. of Defense Advanced Research Projects Agency.
It went live in December 1969 with three computers in California and one in Utah.
It was designed at the height of the Cold War to a be a highly fault-tolerant net-
work that would continue to relay military traffic even in the event of direct nuclear
hits on multiple parts of the network by automatically rerouting traffic around the
dead machines.
The ARPANET grew rapidly in the 1970s, eventually encompassing hundreds
of computers. Then a packet radio network, a satellite network, and eventually
thousands of Ethernets were attached to it, leading to the federation of networks we
now know as the Internet.
The Internet consists of two kinds of computers, hosts and routers.    Hosts are
PCs, notebooks, handhelds, servers, mainframes, and other computers owned by
individuals or companies that want to connect to the Internet.  Routers are spe-
cialized switching computers that accept incoming packets on one of many incom-
ing lines and send them on their way along one of many outgoing lines. A router is
similar to the switch of Fig. 8-28(b), but also differs from it in ways that will not
concern us here. Routers are connected together in large networks, with each router
having wires or fibers to many other routers and hosts. Large national or world-
wide router networks are operated by telephone companies and ISPs (Internet Ser-
vice Providers) for their customers.
Figure 8-29 shows a portion of the Internet.  At the top we have one of the
backbones, normally operated by a backbone operator.  It consists of a number of
routers connected by high-bandwidth fiber optics, with connections to backbones
operated by other (competing) telephone companies. Usually, no hosts connect di-
rectly to the backbone, other than maintenance and test machines run by the tele-
phone company.
Attached to the backbone routers by medium-speed fiber optic connections are
regional networks and routers at ISPs.  In turn, corporate Ethernets each have a
router on them and these are connected to regional network routers. Routers at
ISPs are connected to modem banks used by the ISP's customers.         In this way,
every host on the Internet has at least one path, and often many paths, to every
other host.
All traffic on the Internet is sent in the form of packets. Each packet carries its
destination address inside it, and this address is used for routing. When a packet
comes into a router, the router extracts the destination address and looks (part of) it
up in a table to find which outgoing line to send the packet on and thus to which
router. This procedure is repeated until the packet reaches the destination host.
The routing tables are highly dynamic and are updated continuously as routers and
links go down and come back up and as traffic conditions change.       The routing
algorithms have been intensively studied and modified over the years.



572                        MULTIPLE PROCESSOR SYSTEMS                            CHAP. 8
                                         Backbone         High-bandwidth fiber
     Regional     network                Medium-
                                     bandwidth                       Router at ISP
                                         fiber
                                                                                ADSL line
                                                                                to home PC
     Fiber or                            Router
     copper wire
                                                                                Home PC
Local router                                      Host
                           Ethernet
                           Figure 8-29.  A portion of the Internet.
8.3.2 Network Services and Protocols
     All computer networks provide certain services to their users (hosts and proc-
esses), which they implement using certain rules about legal message exchanges.
Below we will give a brief introduction to these topics.
Network Services
     Computer networks provide services to the hosts and processes using them.
Connection-oriented service is modeled after the telephone system.              To talk to
someone, you pick up the phone, dial the number, talk, and then hang up. Simi-
larly, to use a connection-oriented network service, the service user first establishes
a connection, uses the connection, and then releases the connection. The essential
aspect of a connection is that it acts like a tube: the sender pushes objects (bits) in
at one end, and the receiver takes them out in the same order at the other end.
     In contrast, connectionless service is modeled after the postal system. Each
message (letter) carries the full destination address, and each one is routed through
the system independent of all the others. Normally, when two messages are sent to
the same destination, the first one sent will be the first one to arrive. However, it is
possible that the first one sent can be delayed so that the second one arrives first.
With a connection-oriented service this is impossible.



SEC. 8.3          DISTRIBUTED SYSTEMS                                                     573
Each service can be characterized by a quality of service.  Some services are
reliable in the sense that they never lose data. Usually, a reliable service is imple-
mented by having the receiver confirm the receipt of each message by sending
back a special acknowledgement packet so the sender is sure that it arrived. The
acknowledgement process introduces overhead and delays, which are necessary to
detect packet loss, but which do slow things down.
A typical situation in which a reliable connection-oriented service is appro-
priate is file transfer. The owner of the file wants to be sure that all the bits arrive
correctly and in the same order they were sent. Very few file-transfer customers
would prefer a service that occasionally scrambles or loses a few bits, even if it is
much faster.
Reliable connection-oriented service has two relatively minor variants: mes-
sage sequences and byte streams.  In the former, the message boundaries are pre-
served.   When two 1-KB messages are sent, they arrive as two distinct 1-KB mes-
sages, never as one 2-KB message. In the latter, the connection is simply a stream
of bytes, with no message boundaries. When 2K bytes arrive at the receiver, there
is no way to tell if they were sent as one 2-KB message, two 1-KB messages, 2048
1-byte messages, or something else. If the pages of a book are sent over a network
to an imagesetter as separate messages, it might be important to preserve the mes-
sage boundaries.  On the other hand, with a terminal logging into a remote server
system, a byte stream from the terminal to the computer is all that is needed. There
are no message boundaries here.
For some applications, the delays introduced by acknowledgements are unac-
ceptable. One such application is digitized voice traffic.  It is preferable for tele-
phone users to hear a bit of noise on the line or a garbled word from time to time
than to introduce a delay to wait for acknowledgements.
Not all applications require connections. For example, to test the network, all
that is needed is a way to send a single packet that has a high probability of arrival,
but no guarantee. Unreliable (meaning not acknowledged) connectionless service
is often called datagram service, in analogy with telegram service, which also
does not provide an acknowledgement back to the sender.
In other situations, the convenience of not having to establish a connection to
send one short message is desired, but reliability is essential. The acknowledged
datagram service can be provided for these applications. It is like sending a regis-
tered letter and requesting a return receipt. When the receipt comes back, the send-
er is absolutely sure that the letter was delivered to the intended party and not lost
along the way.
Still another service is the request-reply service.         In this service the sender
transmits a single datagram containing a request; the reply contains the answer. For
example, a query to the local library asking where Uighur is spoken falls into this
category. Request-reply is commonly used to implement communication in the cli-
ent-server model: the client issues a request and the server responds to it.  Figure
8-30 summarizes the types of services discussed above.



574                       MULTIPLE PROCESSOR SYSTEMS                              CHAP. 8
                               Service                                   Example
                               Reliable message stream  Sequence of pages of a book
     Connection-oriented       Reliable byte stream     Remote login
                               Unreliable connection    Digitized voice
                               Unreliable datagram      Network test packets
     Connectionless            Acknowledged datagram    Registered mail
                               Request-reply            Database query
                   Figure 8-30. Six different types of network service.
Network Protocols
     All networks have highly specialized rules for what messages may be sent and
what responses may be returned in response to these messages. For example, under
certain circumstances (e.g., file transfer), when a message is sent from a source to a
destination, the destination is required to send an acknowledgement back indicat-
ing correct receipt of the message. Under other circumstances (e.g., digital tele-
phony), no such acknowledgement is expected. The set of rules by which particular
computers communicate is called a protocol.             Many protocols exist, including
router-router protocols, host-host protocols, and others. For a thorough treatment of
computer networks and their protocols, see Computer Networks, 5/e (Tanenbaum
and Wetherall, 2010).
     All modern networks use what is called a protocol stack to layer different pro-
tocols on top of one another.  At each layer, different issues are dealt with.    For ex-
ample, at the bottom level protocols define how to tell where in the bit stream a
packet begins and ends.   At a higher level, protocols deal with how to route packets
through complex networks from source to destination. And at a still higher level,
they make sure that all the packets in a multipacket message have arrived correctly
and in the proper order.
     Since most distributed systems use the Internet as a base, the key protocols
these systems use are the two major Internet protocols: IP and TCP.               IP (Internet
Protocol) is a datagram protocol in which a sender injects a datagram of up to 64
KB into the network and hopes that it arrives.          No guarantees are given.  The data-
gram may be fragmented into smaller packets as it passes through the Internet.
These packets travel independently, possibly along different routes.              When all the
pieces get to the destination, they are assembled in the correct order and delivered.
     Two versions of IP are currently in use, v4 and v6.  At the moment, v4 still
dominates, so we will describe that here, but v6 is up and coming. Each v4 packet
starts with a 40-byte header that contains a 32-bit source address and a 32-bit desti-
nation address among other fields. These are called IP addresses and form the
basis of Internet routing. They are conventionally written as four decimal numbers



SEC. 8.3                         DISTRIBUTED SYSTEMS                                    575
in the range 0­255 separated by dots, as in 192.31.231.65.        When a packet arrives
at a router, the router extracts the IP destination address and uses that for routing.
Since IP datagrams are not acknowledged, IP alone is not sufficient for reliable
communication in the Internet.   To provide reliable communication, another proto-
col, TCP (Transmission Control Protocol), is usually layered on top of IP.              TCP
uses IP to provide connection-oriented streams.  To use TCP, a process first estab-
lishes a connection to a remote process. The process required is specified by the IP
address of a machine and a port number on that machine, to which processes inter-
ested in receiving incoming connections listen. Once that has been done, it just
pumps bytes into the connection and they are guaranteed to come out the other end
undamaged and in the correct order. The TCP implementation achieves this guar-
antee by using sequence numbers, checksums, and retransmissions of incorrectly
received packets. All of this is transparent to the sending and receiving processes.
They just see reliable interprocess communication, just like a UNIX pipe.
To see how all these protocols interact, consider the simplest case of a very
small message that does not need to be fragmented at any level. The host is on an
Ethernet connected to the Internet.  What happens exactly?        The user process gen-
erates the message and makes a system call to send it on a previously established
TCP connection. The kernel protocol stack adds a TCP header and then an IP
header to the front. Then it goes to the Ethernet driver, which adds an Ethernet
header directing the packet to the router on the Ethernet. This router then injects
the packet into the Internet, as depicted in Fig. 8-31.
                                     Internet
          Host           Router
                    Message          Ethernet 1
                                     header
                                                         IP  TCP  Message
          Ethernet
                                                         Headers
                    Figure 8-31. Accumulation of packet headers.
To establish a connection with a remote host (or even to send it a datagram), it
is necessary to know its IP address. Since managing lists of 32-bit IP addresses is
inconvenient for people, a scheme called DNS (Domain Name System) was in-
vented as a database that maps ASCII names for hosts onto their IP addresses.
Thus it is possible to use the DNS name star.cs.vu.nl instead of the corresponding
IP address 130.37.24.6.      DNS names are commonly known because Internet email



576                                               MULTIPLE PROCESSOR SYSTEMS                                                                                                         CHAP. 8
addresses are of the form user-name@DNS-host-name.                                                                                             This naming system al-
lows the mail program on the sending host to look up the destination host's IP ad-
dress in the DNS database, establish a TCP connection to the mail daemon process
there, and send the message as a file. The user-name is sent along to identify which
mailbox to put the message in.
8.3.3 Document-Based Middleware
     Now that we have some background on networks and protocols, we can start
looking at different middleware layers that can overlay the basic network to pro-
duce a consistent paradigm for applications and users.                                                                                     We will start with a simple
but well-known example: the World Wide Web.                                                                                     The Web was invented by Tim
Berners-Lee at CERN, the European Nuclear Physics Research Center, in 1989 and
since then has spread like wildfire all over the world.
     The original paradigm behind the Web was quite simple: every computer can
hold one or more documents, called Web pages.                                                                                   Each Web page contains text,
images, icons, sounds, movies, and the like, as well as hyperlinks (pointers) to
other Web pages. When a user requests a Web page using a program called a Web
browser, the page is displayed on the screen. Clicking on a link causes the current
page to be replaced on the screen by the page pointed to. Although many bells and
whistles have recently been grafted onto the Web, the underlying paradigm is still
clearly present: the Web is a great big directed graph of documents that can point
to other documents, as shown in Fig. 8-32.
                                                                                      UniversityofNorthSouth
                                                                                      School of
                                                                                      Humanities
                                                                                      School of
                                                                                      Sciences
                                                                                      School of Social
                                                                                      Sciences
                 Northern University                                                      Science                                                                  Social sciences
                 Geography                                                                Astronomy                                                                Anthropology
                 History                                                                  Biology                                                                  Psychology
                 Languages                                                                Chemistry                                                                Sociology
                                                                                          Physics
                 Main page                                                                Main page                                                                Main page
Geography Dept   History Dept.        Languages   Dept.  Astronomy  Dept.  Biology Dept.                      Chemistry  Dept.  Physics Dept.  Anthropology Dept.  Psychology Dept.  Sociology Dept
Big countries    Ancient times        English            Galaxies          Arachnids                          Acids             Electrons      African tribes      Freud             Class struggle
Small countries  Medieval times       French             Nebulas           Mammals                            Bases             Mesons         Australian tribes   Rats              Gender struggle
Rich countries   Modern times         Dutch              Planets           Protozoa                           Esters            Neutrons       New Guinean                           Generic struggle
Poor countries   Future times         Frisian            Quasars           Worms                              Proteins          Neutrinos      tribes
                                      Spanish            Stars                                                                  Protons                            Social sciences
Humanities                                                                                                                                                                           Social sciences
                 Humanities                                                Sciences                           Sciences                         Social sciences
                                      Humanities         Sciences                                                               Sciences
                                      Figure 8-32. The Web is a big directed graph of documents.
     Each Web page has a unique address, called a URL (Uniform Resource Loca-
tor), of the form protocol://DNS-name/file-name.                                                                                The protocol is most commonly
http (HyperText Transfer Protocol), but ftp and others also exist. Then comes the
DNS name of the host containing the file.                                                            Finally, there is a local file name telling
which file is needed. Thus a URL uniquely specifies a single file worldwide



SEC. 8.3                       DISTRIBUTED SYSTEMS                                         577
The way the whole system hangs together is as follows. The Web is fundamen-
tally a client-server system, with the user being the client and the Website being the
server. When the user provides the browser with a URL, either by typing it in or
clicking on a hyperlink on the current page, the browser takes certain steps to fetch
the requested Web page.        As a simple example, suppose the URL provided is
http://www.minix3.org/getting-started/index.html.      The browser then takes the fol-
lowing steps to get the page.
1.        The browser asks DNS for the IP address of www.minix3.org.
2.        DNS replies with 66.147.238.215.
3.        The browser makes a TCP connection to port 80 on 66.147.238.215.
4.        It then sends a request asking for the file getting-started/index.html.
5.        The www.minix3.org server sends the file getting-started/index.html.
6.        The browser displays all the text in getting-started/index.html.
7.        Manwhile, the browser fetches and displays all images on the page.
8.        The TCP connection is released.
To a first approximation, that is the basis of the Web and how it works. Many
other features have since been added to the basic Web, including style sheets, dy-
namic Web pages that are generated on the fly, Web pages that contain small pro-
grams or scripts that execute on the client machine, and more, but they are outside
the scope of this discussion.
8.3.4 File-System-Based Middleware
The basic idea behind the Web is to make a distributed system look like a giant
collection of hyperlinked documents.  A second approach is to make a distributed
system look like a great big file system. In this section we will look at some of the
issues involved in designing a worldwide file system.
Using a file-system model for a distributed system means that there is a single
global file system, with users all over the world able to read and write files for
which they have authorization. Communication is achieved by having one process
write data into a file and having other ones read them back. Many of the standard
file-system issues arise here, but also some new ones related to distribution.
Transfer Model
The first issue is the choice between the upload/download model and the
remote-access model.  In the former, shown in Fig. 8-33(a), a process accesses a
file by first copying it from the remote server where it lives. If the file is only to be



578                            MULTIPLE PROCESSOR SYSTEMS                         CHAP. 8
read, the file is then read locally, for high performance.    If the file is to be written,
it is written locally.  When the process is done with it, the updated file is put back
on the server. With the remote-access model, the file stays on the server and the cli-
ent sends commands there to get work done there, as shown in Fig. 8-33(b).
       1. Client fetches file
Client                         Server         Old file      Client                Server
                                              New file              Request
                                                                    Reply
2. Accesses are         3. When client is                                         File stays
     done on the               done, file is                                      on server
     client                    returned to server
                        (a)                                         (b)
        Figure 8-33. (a) The upload/download model. (b) The remote-access model.
     The advantages of the upload/download model are its simplicity, and the fact
that transferring entire files at once is more efficient than transferring them in small
pieces. The disadvantages are that there must be enough storage for the entire file
locally, moving the entire file is wasteful if only parts of it are needed, and consis-
tency problems arise if there are multiple concurrent users.
The Directory Hierarchy
     Files are only part of the story. The other part is the directory system.    All dis-
tributed file systems support directories containing multiple files. The next design
issue is whether all clients have the same view of the directory hierarchy.       As an
example of what we mean, consider Fig. 8-34.            In Fig. 8-34(a) we show two file
servers, each holding three directories and some files.       In Fig. 8-34(b) we have a
system in which all clients (and other machines) have the same view of the distrib-
uted file system.  If the path /D/E/x is valid on one machine, it is valid on all of
them.
     In contrast, in Fig. 8-34(c), different machines can have different views of the
file system.  To repeat the preceding example, the path /D/E/x might well be valid
on client 1 but not on client 2.              In systems that manage multiple file servers by re-
mote mounting, Fig. 8-34(c) is the norm.                It is flexible and straightforward to im-
plement, but it has the disadvantage of not making the entire system behave like a
single old-fashioned timesharing system.                In a timesharing system, the file system
looks the same to any process, as in the model of Fig. 8-34(b).                   This property
makes a system easier to program and understand.



SEC. 8.3                DISTRIBUTED SYSTEMS                                                     579
File server 1              Client 1                         Client 1
          A                            Root                 Root
B              C        A                       D        A                                D
                     B     C                 E     F  B     C                          E     F
File server 2              Client 2                         Client 2
          D                            Root                 Root
E              F        A                       D        A
                     B     C                 E     F  B     C
          (a)
                                       (b)                  D
                                                         E                          F
                                                               (c)
   Figure 8-34. (a) Two file servers. The squares are directories and the circles are
   files. (b) A system in which all clients have the same view of the file system.
   (c) A system in which different clients have different views of the file system.
A closely related question is whether or not there is a global root directory,
which all machines recognize as the root. One way to have a global root directory
is to have the root contain one entry for each server and nothing else. Under these
circumstances, paths take the form /server/path, which has its own disadvantages,
but at least is the same everywhere in the system.
Naming Transparency
The principal problem with this form of naming is that it is not fully transpar-
ent. Two forms of transparency are relevant in this context and are worth distin-
guishing. The first one, location transparency, means that the path name gives no
hint as to where the file is located.  A path like /server1/dir1/dir2/x tells everyone



580                       MULTIPLE PROCESSOR SYSTEMS                         CHAP. 8
that x is located on server 1, but it does not tell where that server is located. The
server is free to move anywhere it wants to in the network without the path name
having to be changed. Thus this system has location transparency.
     However, suppose that file x is extremely large and space is tight on server 1.
Furthermore, suppose that there is plenty of room on server 2.        The system might
well like to move x to server 2 automatically. Unfortunately, when the first compo-
nent of all path names is the server, the system cannot move the file to the other
server automatically, even if dir1 and dir2 exist on both servers. The problem is
that moving the file automatically changes its path name from /server1/dir1/dir2/x
to /server2/dir1/dir2/x.  Programs that have the former string built into them will
cease to work if the path changes.  A system in which files can be moved without
their names changing is said to have location independence. A distributed system
that embeds machine or server names in path names clearly is not location inde-
pendent. One based on remote mounting is not, either, since it is not possible to
move a file from one file group (the unit of mounting) to another and still be able
to use the old path name. Location independence is not easy to achieve, but it is a
desirable property to have in a distributed system.
     To summarize what we said earlier, there are three common approaches to file
and directory naming in a distributed system:
     1.  Machine + path naming, such as /machine/path or machine:path.
     2.  Mounting remote file systems onto the local file hierarchy.
     3.  A single name space that looks the same on all machines.
The first two are easy to implement, especially as a way to connect existing sys-
tems that were not designed for distributed use. The latter is difficult and requires
careful design, but makes life easier for programmers and users.
Semantics of File Sharing
     When two or more users share the same file, it is necessary to define the
semantics of reading and writing precisely to avoid problems.      In single-processor
systems the semantics normally state that when a read system call follows a write
system call, the read returns the value just written, as shown in Fig. 8-35(a). Simi-
larly, when two writes happen in quick succession, followed by a read, the value
read is the value stored by the last write. In effect, the system enforces an ordering
on all system calls, and all processors see the same ordering.     We will refer to this
model as sequential consistency.
     In a distributed system, sequential consistency can be achieved easily as long
as there is only one file server and clients do not cache files. All reads and writes
go directly to the file server, which processes them strictly sequentially.
     In practice, however, the performance of a distributed system in which all file
requests must go to a single server is frequently poor. This problem is often solved



SEC. 8.3                              DISTRIBUTED        SYSTEMS                                 581
                                                                  Client    1
                                                             A     a     b
                                                                a     b     c
   Single processor                   2. Write "c"                                    1. Read "ab"
1. Write "c"
                            Original
                            file                                                   File server
A            a     b
          a     b     c                                                            a  b
                         B
2. Read gets "abc"                                                             3.  Read gets "ab"
             (a)                                                Client      2
                                                             B     a     b
                                                                   (b)
          Figure 8-35.      (a) Sequential consistency.  (b) In a distributed system with cach-
          ing, reading a file may return an obsolete value.
by allowing clients to maintain local copies of heavily used files in their private
caches. However, if client 1 modifies a cached file locally and shortly thereafter
client 2 reads the file from the server, the second client will get an obsolete file, as
illustrated in Fig. 8-35(b).
One way out of this difficulty is to propagate all changes to cached files back
to the server immediately. Although conceptually simple, this approach is inef-
ficient.  An alternative solution is to relax the semantics of file sharing. Instead of
requiring a read to see the effects of all previous writes, one can have a new rule
that says: ``Changes to an open file are initially visible only to the process that
made them. Only when the file is closed are the changes visible to other proc-
esses.'' The adoption of such a rule does not change what happens in Fig. 8-35(b),
but it does redefine the actual behavior (B getting the original value of the file) as
being the correct one. When client 1 closes the file, it sends a copy back to the ser-
ver, so that subsequent reads get the new value, as required. Effectively, this is the



582                     MULTIPLE PROCESSOR SYSTEMS                          CHAP. 8
upload/download model shown in Fig. 8-33.          This semantic rule is widely imple-
mented and is known as session semantics.
     Using session semantics raises the question of what happens if two or more cli-
ents are simultaneously caching and modifying the same file. One solution is to say
that as each file is closed in turn, its value is sent back to the server, so the final re-
sult depends on who closes last.  A less pleasant, but slightly easier to implement,
alternative is to say that the final result is one of the candidates, but leave the
choice of which one unspecified.
     An alternative approach to session semantics is to use the upload/download
model, but to automatically lock a file that has been downloaded. Attempts by
other clients to download the file will be held up until the first client has returned
it.  If there is a heavy demand for a file, the server could send messages to the cli-
ent holding the file, asking it to hurry up, but that may or may not help. All in all,
getting the semantics of shared files right is a tricky business with no elegant and
efficient solutions.
8.3.5 Object-Based Middleware
     Now let us take a look at a third paradigm. Instead of saying that everything is
a document or everything is a file, we say that everything is an object.    An object
is a collection of variables that are bundled together with a set of access proce-
dures, called methods.  Processes are not permitted to access the variables directly.
Instead, they are required to invoke the methods.
     Some programming languages, such as C++ and Java, are object oriented, but
these are language-level objects rather than run-time objects. One well-known sys-
tem based on run-time objects is CORBA (Common Object Request Broker
Architecture) (Vinoski, 1997).    CORBA is a client-server system, in which client
processes on client machines can invoke operations on objects located on (possibly
remote) server machines.  CORBA was designed for a heterogeneous system run-
ning a variety of hardware platforms and operating systems and programmed in a
variety of languages.   To make it possible for a client on one platform to invoke a
server on a different platform, ORBs (Object Request Brokers) are interposed be-
tween client and server to allow them to match up. The ORBs play an important
role in CORBA, even providing the system with its name.
     Each CORBA object is defined by an interface definition in a language called
IDL  (Interface       Definition  Language),  which   tells  what  methods  the  object
exports and what parameter types each one expects. The IDL specification can be
compiled into a client stub procedure and stored in a library.     If a client process
knows in advance that it will need to access a certain object, it is linked with the
object's client stub code. The IDL specification can also be compiled into a skele-
ton procedure that is used on the server side.     If it is not known in advance which
CORBA objects a process needs to use, dynamic invocation is also possible, but
how that works is beyond the scope of our treatment.



SEC. 8.3                       DISTRIBUTED SYSTEMS                                     583
When a CORBA object is created, a reference to it is also created and returned
to the creating process. This reference is how the process identifies the object for
subsequent invocations of its methods. The reference can be passed to other proc-
esses or stored in an object directory.
To invoke a method on an object, a client process must first acquire a reference
to the object. The reference can come either directly from the creating process or,
more likely, by looking it up by name or by function in some kind of directory.
Once the object reference is available, the client process marshals the parameters to
the method calls into a convenient structure and then contacts the client ORB.         In
turn, the client ORB sends a message to the server ORB, which actually invokes
the method on the object. The whole mechanism is similar to RPC.
The function of the ORBs is to hide all the low-level distribution and commu-
nication details from the client and server code.       In particular, the ORBs hide from
the client the location of the server, whether the server is a binary program or a
script, what hardware and operating system the server runs on, whether the object
is currently active, and how the two ORBs communicate (e.g., TCP/IP, RPC, shar-
ed memory, etc.).
In the first version of CORBA, the protocol between the client ORB and the
server ORB was not specified. As a result, every ORB vendor used a different pro-
tocol and no two of them could talk to each other. In version 2.0, the protocol was
specified.  For communication over the Internet, the protocol is called IIOP (Inter-
net InterOrb Protocol).
To make it possible to use objects that were not written for CORBA with
CORBA systems, every object can be equipped with an object adapter. This is a
wrapper that handles chores such as registering the object, generating object refer-
ences, and activating the object if it is invoked when it is not active. The arrange-
ment of all these CORBA parts is shown in Fig. 8-36.
                   Client      Client    stub      Skeleton  Server
Client                                                                                 Server
code                                                                                   code
                                         Object
                   Client ORB            adapter             Server ORB
            Operating system                                 Operating system
                                         IIOP protocol
                                                                         Network
        Figure 8-36. The main elements of a distributed system based on CORBA.    The
        CORBA parts are shown in gray.



584                       MULTIPLE PROCESSOR SYSTEMS                  CHAP. 8
     A serious problem with CORBA is that every object is located on only one ser-
ver, which means the performance will be terrible for objects that are heavily used
on client machines around the world.            In practice, CORBA functions acceptably
only in small-scale systems, such as to connect processes on one computer, one
LAN, or within a single company.
8.3.6 Coordination-Based Middleware
     Our last paradigm for a distributed system is called coordination-based mid-
dleware.       We will discuss it by looking at the Linda system, an academic research
project that started the whole field.
     Linda is a novel system for communication and synchronization developed at
Yale University by David Gelernter and his student Nick Carriero (Carriero and
Gelernter, 1986; Carriero and Gelernter, 1989; and Gelernter, 1985).  In Linda, in-
dependent processes communicate via an abstract tuple space.         The tuple space is
global to the entire system, and processes on any machine can insert tuples into the
tuple space or remove tuples from the tuple space without regard to how or where
they are stored.  To the user, the tuple space looks like a big, global shared memo-
ry, as we have seen in various forms before, as in Fig. 8-21(c).
     A tuple is like a structure in C or Java.  It consists of one or more fields, each
of which is a value of some type supported by the base language (Linda is imple-
mented by adding a library to an existing language, such as C).      For C-Linda, field
types include integers, long integers, and floating-point numbers, as well as com-
posite types such as arrays (including strings) and structures (but not other tuples).
Unlike objects, tuples are pure data; they do not have any associated methods. Fig-
ure 8-37 shows three tuples as examples.
("abc", 2, 5)
("matrix-1", 1, 6, 3.14)
("family", "is-sister", "Stephany", "Roberta")
                                   Figure 8-37. Three Linda tuples.
     Four operations are provided on tuples. The first one, out, puts a tuple into the
tuple space. For example,
     out("abc", 2, 5);
puts the tuple ("abc", 2, 5) into the tuple space. The fields of out are normally con-
stants, variables, or expressions, as in
     out("matrix-1", i, j, 3.14);
which outputs a tuple with four fields, the second and third of which are deter-
mined by the current values of the variables i and j.



SEC. 8.3               DISTRIBUTED SYSTEMS                                               585
Tuples are retrieved from the tuple space by the in primitive. They are ad-
dressed by content rather than by name or address.  The fields of in can be expres-
sions or formal parameters. Consider, for example,
in("abc", 2, ?i);
This operation ``searches'' the tuple space for a tuple consisting of the string
``abc'', the integer 2, and a third field containing any integer (assuming that i is an
integer).  If found, the tuple is removed from the tuple space and the variable i is
assigned the value of the third field. The matching and removal are atomic, so if
two processes execute the same in operation simultaneously, only one of them will
succeed, unless two or more matching tuples are present. The tuple space may even
contain multiple copies of the same tuple.
The matching algorithm used by in is straightforward. The fields of the in
primitive, called the template, are (conceptually) compared to the corresponding
fields of every tuple in the tuple space. A match occurs if the following three con-
ditions are all met:
1.         The template and the tuple have the same number of fields.
2.         The types of the corresponding fields are equal.
3.         Each constant or variable in the template matches its tuple field.
Formal parameters, indicated by a question mark followed by a variable name or
type, do not participate in the matching (except for type checking), although those
containing a variable name are assigned after a successful match.
If no matching tuple is present, the calling process is suspended until another
process inserts the needed tuple, at which time the called is automatically revived
and given the new tuple. The fact that processes block and unblock automatically
means that if one process is about to output a tuple and another is about to input it,
it does not matter which goes first. The only difference is that if the in is done be-
fore the out, there will be a slight delay until the tuple is available for removal.
The fact that processes block when a needed tuple is not present can be put to
many uses. For example, it can be used to implement semaphores. To create or do
an up on semaphore S, a process can execute
out("semaphore S");
To do a down, it does
in("semaphore S");
The state of semaphore S is determined by the number of (``semaphore S'') tuples
in the tuple space. If none exist, any attempt to get one will block until some other
process supplies one.
In addition to out and in, Linda also has a primitive operation read, which is
the same as in except that it does not remove the tuple from the tuple space. There



586                 MULTIPLE PROCESSOR SYSTEMS                                 CHAP. 8
is also a primitive eval, which causes its parameters to be evaluated in parallel and
the resulting tuple to be put in the tuple space. This mechanism can be used to per-
form an arbitrary computation. This is how parallel processes are created in Linda.
Publish/Subscribe
     Our next example of a coordination-based model was inspired by Linda and is
called publish/subscribe (Oki et al., 1993).    It consists of a number of processes
connected by a broadcast network. Each process can be a producer of information,
a consumer of information, or both.
     When an information producer has a new piece of information (e.g., a new
stock price), it broadcasts the information as a tuple on the network. This action is
called publishing.  Each tuple contains a hierarchical subject line containing mul-
tiple fields separated by periods. Processes that are interested in certain infor-
mation can subscribe to certain subjects, including the use of wildcards in the sub-
ject line. Subscription is done by telling a tuple daemon process on the same ma-
chine that monitors published tuples what subjects to look for.
     Publish/subscribe is implemented as illustrated in Fig. 8-38.    When a process
has a tuple to publish, it broadcasts it out onto the local LAN.      The tuple daemon
on each machine copies all broadcasted tuples into its RAM.           It then inspects the
subject line to see which processes are interested in it, forwarding a copy to each
one that is. Tuples can also be broadcast over a wide area network or the Internet
by having one machine on each LAN act as an information router, collecting all
published tuples and then forwarding them to other LANs for rebroadcasting. This
forwarding can also be done intelligently, forwarding a tuple to a remote LAN only
if that remote LAN has at least one subscriber who wants the tuple. Doing this re-
quires having the information routers exchange information about subscribers.
                            Producer
                                                WAN
LAN
Consumer            Daemon
                            Information router
                    Figure 8-38. The publish/subscribe architecture.
     Various kinds of semantics can be implemented, including reliable delivery
and guaranteed delivery, even in the presence of crashes.             In the latter case, it is



SEC. 8.3               DISTRIBUTED SYSTEMS                                             587
necessary to store old tuples in case they are needed later. One way to store them is
to hook up a database system to the system and have it subscribe to all tuples. This
can be done by wrapping the database system in an adapter, to allow an existing
database to work with the publish/subscribe model. As tuples come by, the adapter
captures all of them and puts them in the database.
The publish/subscribe model fully decouples producers from consumers, as
does Linda. However, sometimes it is useful to know who else is out there.  This
information can be acquired by publishing a tuple that basically asks: ``Who out
there is interested in x?'' Responses come back in the form of tuples that say: ``I
am interested in x.''
8.4 RESEARCH ON MULTIPLE PROCESSOR SYSTEMS
Few topics in operating systems research are as popular as multicores, multi-
processors, and distributed systems. Besides the direct problems of mapping oper-
ating system functionality on a system consisting of multiple processing cores,
there are many open research problems related to synchronization and consistency,
and the way to make such systems faster and more reliable.
Some research efforts have aimed at designing new operating systems from
scratch specifically for multicore hardware. For instance, the Corey operating sys-
tem addresses the performance problems caused by data structure sharing across
multiple cores (Boyd-Wickizer et al., 2008).      By carefully arranging kernel data
structures in such a way that no sharing is needed, many of the performance bottle-
necks disappear. Similarly, Barrelfish (Baumann et al., 2009) is a new operating
system motivated by the rapid growth in the number of cores on the one hand, and
the growth in hardware diversity on the other. It models the operating system after
distributed systems with message passing instead of shared memory as the commu-
nication model. Other operating systems aim at scalability and performance. Fos
(Wentzlaff et al., 2010) is an operating system that was designed to scale from the
small (multicore CPUs) to the very large (clouds). Meanwhile, NewtOS (Hruby et
al., 2012; and Hruby et al., 2013) is a new multiserver operating system that aims
for both dependability (with a modular design and many isolated components
based originally on Minix 3) and performance (which has traditionally been the
weak point of such modular multiserver systems).
Multicore is not just for new designs. In Boyd-Wickizer et al. (2010), the re-
searchers study and remove the bottlenecks they encounter when scaling Linux to a
48-core machine. They show that such systems, if designed carefully, can be made
to scale quite well. Clements et al. (2013) investigate the fundamental principle
that govern whether or not an API can be implemented in a scalable fashion. They
show that whenever interface operations commute, a scalable implementation of
that interface exists. With this knowledge, operating system designers can build
more scalable operating systems.



588                    MULTIPLE PROCESSOR SYSTEMS                        CHAP. 8
     Much systems research in recent years has also gone into making large appli-
cations scale to multicore and multiprocessor environments. One example is the
scalable database engine described by Salomie et al. (2011). Again, the solution is
to achieve scalability by replicating the database rather than trying to hide the par-
allel nature of the hardware.
     Debugging parallel applications is very hard, and race conditions are hard to
reproduce. Viennot et al. (2013) show how replay can help to debug software on
multicore systems. Lachaize et al. provide a memory profiler for multicore sys-
tems, and Kasikci et al. (2012) present work not just on detecting race conditions
in software, but even on how to tell good races from bad ones.
     Finally, there is a lot of work on reducing power consumption in multiproces-
sors. Chen et al. (2013) propose the use of power containers to provide fine-
grained power and energy management.
8.5 SUMMARY
     Computer systems can be made faster and more reliable by using multiple
CPUs.     Four organizations for multi-CPU systems are multiprocessors, multicom-
puters, virtual machines, and distributed systems. Each of these has its own proper-
ties and issues.
     A multiprocessor consists of two or more CPUs that share a common RAM.
Often these CPUs themselves consists of multiple cores. The cores and CPUs can
be interconnected by a bus, a crossbar switch, or a multistage switching network.
Various operating system configurations are possible, including giving each CPU
its own operating system, having one master operating system with the rest being
slaves, or having a symmetric multiprocessor, in which there is one copy of the op-
erating system that any CPU can run. In the latter case, locks are needed to provide
synchronization. When a lock is not available, a CPU can spin or do a context
switch. Various scheduling algorithms are possible, including time sharing, space
sharing, and gang scheduling.
     Multicomputers also have two or more CPUs, but these CPUs each have their
own private memory. They do not share any common RAM, so all communication
uses message passing.  In some cases, the network interface board has its own
CPU, in which case the communication between the main CPU and the inter-
face-board CPU has to be carefully organized to avoid race conditions. User-level
communication on multicomputers often uses remote procedure calls, but distrib-
uted shared memory can also be used. Load balancing of processes is an issue here,
and  the  various  algorithms  used  for  it  include  sender-initiated  algorithms,    re-
ceiver-initiated algorithms, and bidding algorithms.
     Distributed systems are loosely coupled systems each of whose nodes is a
complete computer with a complete set of peripherals and its own operating sys-
tem. Often these systems are spread over a large geographical area. Middleware is



SEC. 8.5                                SUMMARY                                            589
often put on top of the operating system to provide a uniform layer for applications
to   interact  with.  The  various    kinds    include  document-based,       file-based,          ob-
ject-based,    and  coordination-based  middleware.         Some  examples    are  the  World
Wide Web, CORBA, and Linda.
                                      PROBLEMS
1.   Can the USENET newsgroup system or the SETI@home project be considered distrib-
     uted systems?    (SETI@home uses several million idle personal computers to analyze
     radio telescope data to search for extraterrestrial intelligence.) If so, how do they relate
     to the categories described in Fig. 8-1?
2.   What happens if three CPUs in a multiprocessor attempt to access exactly the same
     word of memory at exactly the same instant?
3.   If a CPU issues one memory request every instruction and the computer runs at 200
     MIPS, about how many CPUs will it take to saturate a 400-MHz bus?             Assume that a
     memory reference requires one bus cycle. Now repeat this problem for a system in
     which caching is used and the caches have a 90% hit rate. Finally, what cache hit rate
     would be needed to allow 32 CPUs to share the bus without overloading it?
4.   Suppose that the wire between switch 2A and switch 3B in the omega network of
     Fig. 8-5 breaks. Who is cut off from whom?
5.   How is signal handling done in the model of Fig. 8-7?
6.   When a system call is made in the model of Fig. 8-8, a problem has to be solved im-
     mediately after the trap that does not occur in the model of Fig. 8-7. What is the nature
     of this problem and how might it be solved?
7.   Rewrite the enter  region code of Fig. 2-22 using the pure read to reduce thrashing
     induced by the TSL instruction.
8.   Multicore CPUs are beginning to appear in conventional desktop machines and laptop
     computers. Desktops with tens or hundreds of cores are not far off. One possible way
     to harness this power is to parallelize standard desktop applications such as the word
     processor or the web browser. Another possible way to harness the power is to paral-
     lelize the services offered by the operating system -- e.g., TCP processing -- and com-
     monly-used library services -- e.g., secure http library functions). Which approach ap-
     pears the most promising? Why?
9.   Are critical regions on code sections really necessary in an SMP operating system to
     avoid race conditions or will mutexes on data structures do the job as well?
10.  When the TSL instruction is used for multiprocessor synchronization, the cache block
     containing the mutex will get shuttled back and forth between the CPU holding the
     lock and the CPU requesting it if both of them keep touching the block.       To reduce bus
     traffic, the requesting CPU executes one TSL every 50 bus cycles, but the CPU holding
     the lock always touches the cache block between TSL instructions.        If a cache block



590                      MULTIPLE PROCESSOR SYSTEMS                                  CHAP. 8
     consists of 16 32-bit words, each of which requires one bus cycle to transfer, and the
     bus runs at 400 MHz, what fraction of the bus bandwidth is eaten up by moving the
     cache block back and forth?
11.  In the text, it was suggested that a binary exponential backoff algorithm be used be-
     tween uses of TSL to poll a lock.  It was also suggested to have a maximum delay be-
     tween polls. Would the algorithm work correctly if there were no maximum delay?
12.  Suppose that the TSL instruction was not available for synchronizing a multiprocessor.
     Instead, another instruction, SWP, was provided that atomically swapped the contents
     of a register with a word in memory. Could that be used to provide multiprocessor syn-
     chronization? If so, how could it be used? If not, why does it not work?
13.  In this problem you are to compute how much of a bus load a spin lock puts on the bus.
     Imagine that each instruction executed by a CPU takes 5 nsec. After an instruction has
     completed, any bus cycles needed, for example, for TSL are carried out. Each bus cycle
     takes an additional 10 nsec above and beyond the instruction execution time. If a proc-
     ess is attempting to enter a critical region using a TSL loop, what fraction of the bus
     bandwidth does it consume?   Assume that normal caching is working so that fetching
     an instruction inside the loop consumes no bus cycles.
14.  Affinity scheduling reduces cache misses. Does it also reduce TLB misses?                What
     about page faults?
15.  For each of the topologies of Fig. 8-16, what is the diameter of the interconnection net-
     work? Count all hops (host-router and router-router) equally for this problem.
16.  Consider the double-torus topology of Fig. 8-16(d) but expanded to size k × k.           What
     is the diameter of the network? (Hint: Consider odd k and even k differently.)
17.  The bisection bandwidth of an interconnection network is often used as a measure of
     its capacity.  It is computed by removing a minimal number of links that splits the net-
     work into two equal-size units. The capacity of the removed links is then added up.        If
     there are many ways to make the split, the one with the minimum bandwidth is the
     bisection bandwidth. For an interconnection network consisting of an 8 × 8 × 8 cube,
     what is the bisection bandwidth if each link is 1 Gbps?
18.  Consider a multicomputer in which the network interface is in user mode, so only three
     copies are needed from source RAM to destination RAM.    Assume that moving a
     32-bit word to or from the network interface board takes 20 nsec and that the network
     itself operates at 1 Gbps. What would the delay be for a 64-byte packet being sent from
     source to destination if we could ignore the copying time? What is it with the copying
     time?  Now consider the case where two extra copies are needed, to the kernel on the
     sending side and from the kernel on the receiving side. What is the delay in this case?
19.  Repeat the previous problem for both the three-copy case and the five-copy case, but
     this time compute the bandwidth rather than the delay.
20.  When transferring data from RAM to a network interface, pinning a page can be used,
     but suppose that system calls to pin and unpin pages each take 1 sec. Copying takes 5
     bytes/nsec using DMA but 20 nsec per byte using programmed I/O.           How big does a
     packet have to be before pinning the page and using DMA is worth it?



CHAP. 8                                    PROBLEMS                                         591
21.  When a procedure is scooped up from one machine and placed on another to be called
     by RPC, some problems can occur.        In the text, we pointed out four of these: pointers,
     unknown array sizes, unknown parameter types, and global variables.            An issue not
     discussed is what happens if the (remote) procedure executes a system call. What prob-
     lems might that cause and what might be done to handle them?
22.  In a DSM system, when a page fault occurs, the needed page has to be located. List
     two possible ways to find the page.
23.  Consider the processor allocation of Fig. 8-24.   Suppose that process H  is moved from
     node 2 to node 3. What is the total weight of the external traffic now?
24.  Some multicomputers allow running processes to be migrated from one node to anoth-
     er. Is it sufficient to stop a process, freeze its memory image, and just ship that off to a
     different node? Name two hard problems that have to be solved to make this work.
25.  Why is there a limit to cable length on an Ethernet network?
26.  In Fig. 8-27, the third and fourth layers are labeled Middleware and Application on all
     four machines. In what sense are they all the same across platforms, and in what sense
     are they different?
27.  Figure 8-30 lists six different types of service. For each of the following applications,
     which service type is most appropriate?
     (a) Video on demand over the Internet.
     (b) Downloading a Web page.
28.  DNS    names  have   a  hierarchical  structure,  such  as  sales.general-widget.com.         or
     cs.uni.edu One way to maintain the DNS database would be as one centralized data-
     base, but that is not done because it would get too many requests/sec.    Propose a way
     that the DNS database could be maintained in practice.
29.  In the discussion of how URLs are processed by a browser, it was stated that con-
     nections are made to port 80. Why?
30.  Migrating virtual machines may be easier than migrating processes, but migration can
     still be difficult. What problems can arise when migrating a virtual machine?
31.  When a browser fetches a Web page, it first makes a TCP connection to get the text on
     the page (in the HTML language).         Then it closes the connection and examines the
     page.  If there are figures or icons, it then makes a separate TCP connection to fetch
     each one. Suggest two alternative designs to improve performance here.
32.  When session semantics are used, it is always true that changes to a file are immediate-
     ly visible to the process making the change and never visible to processes on other ma-
     chines. However, it is an open question as to whether or not they should be immediate-
     ly visible to other processes on the same machine. Give an argument each way.
33.  When multiple processes need access to data, in what way is object-based access better
     than shared memory?
34.  When a Linda in operation is done to locate a tuple, searching the entire tuple space
     linearly is very inefficient. Design a way to organize the tuple space that will speed up
     searches on all in operations.



592          MULTIPLE PROCESSOR SYSTEMS                                           CHAP. 8
35.  Copying buffers takes time. Write a C program to find out how much time it takes on a
     system to which you have access. Use the clock or times functions to determine how
     long it takes to copy a large array. Test with different array sizes to separate copying
     time from overhead time.
36.  Write C functions that could be used as client and server stubs to make an RPC call to
     the standard printf function, and a main program to test the functions. The client and
     server should communicate by means of a data structure that could be transmitted over
     a network. You may impose reasonable limits on the length of the format string and the
     number, types, and sizes of the variables your client stub will accept.
37.  Write a program that implements the sender-initiated and receiver-initiated load bal-
     ancing algorithms described in Sec. 8.2.  The algorithms should take as input a list of
     newly created jobs specified as (creating processor, start time, required CPU time)
     where the creating processor is the number of the CPU that created the job, the
     start time is the time at which the job was created, and the required CPU time is the
     amount of CPU time the job needs to complete (specified in seconds). Assume a node
     is overloaded when it has one job and a second job is created. Assume a node is
     underloaded when it has no jobs. Print the number of probe messages sent by both al-
     gorithms under heavy and light workloads. Also print the maximum and minimum
     number of probes sent by any host and received by any host. To create the workloads,
     write two workload generators. The first should simulate a heavy workload, generat-
     ing, on average, N jobs every AJL seconds, where AJL is the average job length and N
     is the number of processors. Job lengths can be a mix of long and short jobs, but the
     average job length must be AJL. The jobs should be randomly created (placed) across
     all processors. The second generator should simulate a light load, randomly generating
     N/3 jobs every AJL seconds. Play with other parameter settings for the workload gener-
     ators and see how it affects the number of probe messages.
38.  One of the simplest ways to implement a publish/subscribe system is via a centralized
     broker that receives published articles and distributes them to the appropriate sub-
     scribers. Write a multithreaded application that emulates a broker-based pub/sub sys-
     tem. Publisher and subscriber threads may communicate with the broker via (shared)
     memory. Each message should start with a length field followed by that many charac-
     ters. Publishers send messages to the broker where the first line of the message con-
     tains a hierarchical subject line separated by dots followed by one or more lines that
     comprise the published article. Subscribers send a message to the broker with a single
     line containing a hierarchical interest line separated by dots expressing the articles they
     are interested in. The interest line may contain the wildcard symbol ``*''.  The broker
     must respond by sending all (past) articles that match the subscriber's interest. Articles
     in the message are separated by the line ``BEGIN NEW ARTICLE.'' The subscriber
     should print each message it receives along with its subscriber identity (i.e., its interest
     line).  The subscriber should continue to receive any new articles that are posted and
     match its interests. Publisher and subscriber threads can be created dynamically from
     the terminal by typing ``P'' or ``S'' (for publisher or subscriber) followed by the hierar-
     chical subject/interest line. Publishers will then prompt for the article. Typing a single
     line containing ``.'' will signal the end of the article. (This project can also be imple-
     mented using processes communicating via TCP.)
