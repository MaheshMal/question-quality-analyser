Implementation of File Operations


                                                  Chapte                                   r  14
Implementation of
File Operations
A s we saw in Chapter 13, a file processing activity is implemented through
      modules of the file system and the input-output control system (IOCS).
      The file system modules provide file-naming freedom, sharing and pro-
tection of files, and reliability. Operations on files are implemented by the IOCS.
The IOCS has two primary concerns--efficient implementation of a file pro-
cessing activity in a process and high throughput of I/O devices. To address these
concerns, the IOCS is organized into two layers called the access method and the
physical IOCS layers. An access method module organizes reading and writing of
file data to efficiently implement a file processing activity in a process. It invokes
the physical IOCS to actually perform reading and writing of data. The physical
IOCS performs input-output at the I/O device level and carries out scheduling
policies to enhance throughput of an I/O device.
We first discuss the characteristics of I/O devices, and arrangements that
provide high reliability, fast access and high data transfer rates of disks. We then
discuss how I/O operations are performed at the level of I/O devices, what facil-
ities are offered by the physical IOCS to simplify I/O operations, and how disk
scheduling provides high disk throughput. Finally, we discuss how the techniques
of buffering, blocking, and caching of data speed up a file processing activity.
14.1  LAYERS OF THE INPUT-OUTPUT CONTROL SYSTEM                                         ·
The schematic of Figure 14.1 shows how the input-output control system (IOCS)
implements file operations. Processes Pi and Pj are engaged in file processing
activities and have already opened some files. When one of these processes makes
a request to read or write data from a file, the file system passes on the request to
the IOCS. Recall from Section 13.1 that the IOCS holds some file data in memory
areas called buffers, the file cache, or the disk cache to speed up file processing
activities. For a read operation, the IOCS checks whether the data required by
the process is present in memory. If so, the process can access the data straight-
away; otherwise, the IOCS issues one or more I/O operations to load the data
into a file buffer or the disk cache, and the process has to wait until this I/O
                                                                                              543



544  Part 4  File Systems
                                 Access methods
                                                                          Physical IOCS
             Process
             Pi
                                                                                 I/O
                                                                                 operations
             Process                             File buffers,                   Disk        Disk blocks
             PJ                               File cache, or                     scheduling  contain  1 records
                                                 Disk cache
             Figure 14.1         Implementation of file operations by the IOCS.
                                              Process
                                          File system layer
                                          Access method layer                    Layers of
                                                                                 the IOCS
                                          Physical IOCS layer
                                              Kernel
             Figure        14.2  Layers of the file system and the IOCS.
             operation completes. Since many processes perform I/O operations concurrently,
             the I/O operations are scheduled by a disk scheduling algorithm, which aims to
             provide high throughput of the disk. Thus the IOCS implements I/O operations
             in a manner that provides efficiency of file processing activities in processes and
             high throughput of I/O devices.
             The IOCS is structured into two layers called the access method and the
             physical IOCS. The access method layer provides efficient file processing and the
             physical IOCS layer provides high device throughput. This structure of the IOCS
             separates process-level concerns in efficient implementation of file operations
             from device-level concerns.
             Figure 14.2 shows the hierarchy of file system and IOCS layers. The number of
             IOCS layers and their interfaces vary across operating systems. In older operating
             systems, the physical IOCS was typically a part of the kernel; however, modern
             operating systems put it outside the kernel to enhance extensibility and reliability
             of the OS. We will assume that the physical IOCS is invoked through system calls,
             and it invokes other functionalities of the kernel also through system calls.



                                         Chapter 14      Implementation of File Operations  545
Table 14.1      Mechanisms and Policies in File System and IOCS Layers
Physical IOCS
                    ·  Mechanisms: I/O initiation, providing I/O operation
                       status, I/O completion processing, error recovery.
                    ·  Policy: Optimization of I/O device performance
                       through a disk scheduler and a disk cache.
Access methods
                    ·  Mechanisms: File open and close, read and write.
                    ·  Policy: Optimization of file access performance
                       through buffering and blocking of file data and use
                       of a file cache.
File System
                    ·  Mechanisms: Allocation of disk blocks, directory
                       maintenance, setting and checking of file protection
                       information.
                    ·  Policies: Disk space allocation for access efficiency,
                       sharing and protection of files.
Table 14.1 summarizes significant mechanisms and policies implemented by
IOCS layers in a conventional two-layer IOCS design. The physical IOCS layer
implements device-level I/O. Its policy modules determine the order in which I/O
operations should be performed to achieve high device throughput. These mod-
ules invoke physical IOCS mechanisms to perform I/O operations. The access
method layer has policy modules that ensure efficient file processing and mecha-
nisms that implement file-level I/O by using physical IOCS policy modules. The
file system layer implements sharing and protection of files, using the modules of
the access method.
Note that Table 14.1 lists only those mechanisms that can be meaningfully
accessed from a higher layer. Other mechanisms, which are "private" to a layer,
are not listed here. For example, mechanisms for buffering and blocking of file
data, and for managing a file cache exist in the access method layer. However,
they are available only to access method policy modules; they are not accessed
directly from the file system layer. Similarly, the physical IOCS has mechanisms
for managing the disk cache, which cannot be accessed from outside the physical
IOCS layer.
14.2  OVERVIEW OF I/O ORGANIZATION                                                          ·
Section 2.2.4 contained an overview of I/O organization. Three modes of per-
forming I/O operations--programmed mode, interrupt mode, and direct memory
access (DMA) mode--were summarized in Table 2.1. We focus on the DMA
mode of I/O operations. Figure 2.1 showed how I/O devices are connected to
device controllers, which are in turn connected to the DMA controller. Each
device controller has a unique numeric id. Similarly, each device connected



546  Part 4  File Systems
                   to it has a unique numeric device id. A device address is a pair of the form
                   (controller_id, device_id).
                          An I/O operation involves the following details:
                      ·   Operation to be performed--read, write, etc.
                      ·   Address of the I/O device.
                      ·   Number of bytes of data to be transferred.
                      ·   Addresses of areas in memory and on the I/O       device that are to participate
                          in the data transfer.
                          When an I/O operation is performed in the DMA mode, the CPU initiates
                   the I/O operation, but it is not involved in data transfer between an I/O device
                   and memory. To facilitate this mode of I/O, an I/O operation is initiated by
                   executing an I/O instruction. The CPU, the DMA controller, the device controller,
                   and the I/O device participate to realize an I/O instruction. The I/O instruction
                   points to a set of I/O commands that specify the individual tasks involved in
                   the data transfer. Implementation of an I/O command requires participation of
                   the DMA controller, the device controller, and the I/O device, but does not require
                   participation of the CPU. This way, the CPU is free to execute other instructions
                   while the I/O operation is in progress.
                          Typically, I/O commands are stored in memory and the address of the
                   memory area containing I/O commands is used as an operand in the I/O instruc-
                   tion (in some computers, the address is picked up from a standard memory
                   location when the I/O instruction is executed). When the I/O instruction is
                   executed, the CPU passes this address to the DMA controller. The DMA con-
                   troller now realizes the I/O commands. The next example provides details of this
                   arrangement.
·
     Example 14.1  I/O Operations
                   The I/O operation to read the data recorded in a disk block with the id (track_id,
                   block_id) is performed by executing the following I/O instruction:
                                 I/O-init (controller_id, device_id), I/O_command_addr
                   where I/O_command_addr is the start address of the memory area containing
                   the following two I/O commands:
                      1.  Position disk heads on track track_id
                      2.  Read  record  record_id     into  the  memory  area  with  the  start  address
                          memory_addr
                   ·
                          The arrangement called third party DMA works as follows: Device con-
                   trollers are connected to the DMA controller as shown in Figure 2.1. When
                   an I/O instruction is executed, the DMA controller passes details of the I/O
                   commands to the device controller of the I/O device. The device delivers the



                                                Chapter 14   Implementation of File Operations  547
data to the device controller. Transfer of data between the device controller
and memory is organized as follows: The device controller sends a DMA-
request signal when it is ready to perform a data transfer. On seeing this signal,
the DMA controller obtains control of the bus, puts address of the memory
location that is to participate in the data transfer on the bus, and sends a
DMA-acknowledgment signal to the device controller. The device controller now
transfers the data to or from memory. At the end of data transfer, the DMA
controller raises an I/O completion interrupt with the address of the device as
the interrupt code. The interrupt servicing routine analyzes the interrupt code
to find which device has completed its I/O operation, and takes appropriate
actions.
   Since the CPU continues to execute instructions while an I/O operation is
in progress, the CPU and the DMA controller are in competition for use of the
bus. The technique of cycle stealing ensures that both can use the bus without
facing large delays. The CPU defers to the DMA controller for use of the bus at
some specific points in its instruction cycle, typically when it is about to read an
instruction or its data from memory. When the DMA wishes to transfer data to
or from memory, it waits until the CPU reaches one of these points. It then steals
a memory cycle from the CPU to implement its data transfer.
   First party DMA is more efficient than third party DMA. In this arrange-
ment, the device controller and the DMA controller are rolled into one unit. The
combined unit obtains control of the bus when it is ready for a data transfer. This
technique is called bus mastering. It achieves higher data transfer rates than third
party DMA.
14.3      I/O DEVICES                                                                           ·
I/O devices operate under a variety of principles, such as electromechanical signal
generation and electromagnetic or optical data recording. I/O devices work with
different I/O media, serve different purposes, and organize and access data in
different ways, so they can be classified through the following criteria:
·  Purpose: Input, print and storage devices
·  Nature of access: Sequential and random-access devices
·  Data transfer mode: Character and block mode devices
The information written (or read) in one I/O command is said to form a record.
A sequential-access device uses its I/O medium in a sequential manner; hence an
operation is always performed on a record that adjoins the record accessed in the
previous operation. Access to any other record requires additional commands
to skip over intervening records. A random-access device can perform a read or
write operation on a record located in any part of the I/O medium. A keyboard,
a mouse, a network and a tape drive are sequential-access devices. Disks can be
accessed in both sequential and random manner.
   A unit of I/O medium is called an I/O volume; thus, a tape cartridge and a
disk can be called a tape volume and a disk volume, respectively. I/O volumes



548  Part 4  File Systems
             for some I/O devices are detachable, e.g., floppy disks, compact disks (CDs), or
             digital audiotape (DAT) cartridges; while those for other I/O devices like hard
             disks are permanently fixed in the device.
             Data Transfer Modes  The data transfer mode of a device depends on its speed
             of data transfer. A slow I/O device operates in the character mode; i.e., it transfers
             one character at a time between memory and the device. The device contains a
             buffer register that can store one character. The device controller raises an inter-
             rupt after an input device reads a character into the buffer or an output device
             writes a character from the buffer. Device controllers of such devices can be con-
             nected directly to the bus. The keyboard, mouse, and printer are character mode
             devices.
             A device capable of a high data transfer rate operates in the block mode of
             data transfer. It is connected to a DMA controller. Tapes and disk drives are
             block mode devices. A block mode device needs to read or write data at a specific
             speed. Two kinds of problems would arise if a data transfer is delayed because
             of contention for the bus: Data would be lost during a read operation if the bus
             were unable to accept data from an I/O device at the required rate for transfer to
             memory. A write operation would fail if the bus were unable to deliver data to
             the I/O device at the required rate.
             To prevent problems due to contention for the bus, data is not transferred
             over the bus during the operation; instead, it is transferred between an I/O device
             and a buffer. During an input operation, the data delivered by the I/O device is
             stored in a buffer in the DMA controller, which we will call the DMA buffer. It is
             transferred from the DMA buffer to memory after the I/O operation completes.
             To perform an output operation, data to be written onto the I/O device is first
             transferred from memory to the DMA buffer. During the I/O operation, it is
             transferred from the DMA buffer to the I/O device.
             Data transfer between the CPU and an I/O device can also be realized by
             using memory-mapped I/O. In this approach, a set of memory addresses are
             reserved for an I/O device. These addresses are mapped into some of the reg-
             isters of the I/O device such that when the CPU writes some data into a memory
             location with one of the reserved addresses, the data is actually written into the
             corresponding register of the I/O device. Similarly, when the CPU executes an
             instruction that reads data from a memory location with one of the reserved
             addresses, the data actually gets read from the corresponding register of the I/O
             device. This way the transfer of data takes place without a DMA yet it does
             not load the CPU much. Memory-mapped I/O is implemented as follows: An
             I/O device listens on the bus on which memory is connected. When one of its
             reserved addresses appears on the bus, it simply transfers data between the bus
             and the register corresponding to the reserved address. Memory-mapped I/O
             is popular on the PCs because a special I/O bus is not needed, and the CPU
             does not have to provide any special instructions for initiating I/O operations
             and for checking the status of I/O devices, which reduces the cost of the CPU.
             However, more hardware is needed on the memory bus to decode the reserved
             addresses.



                                                              Chapter 14          Implementation of File  Operations  549
Access Time and Transfer Time        We use the following notation while discussing
I/O operations.
tio  I/O time, i.e., time interval between the execution of an
     instruction to initiate an I/O operation and completion of
     the I/O operation.
ta   access time, i.e., time interval between the issue of a read or
     write command and the start of data transfer.
tx   transfer time, i.e., time taken to transfer the data from/to an
     I/O device during a read or write operation. It is the time
     between start of transfer of the first byte to end of transfer of
     the last byte.
The I/O time for a record is the sum of its access time and transfer time, i.e.,
                                     tio = ta + tx                                (14.1)
Figure 14.3 illustrates the factors influencing tio. The access time in a sequential
device is a constant because the device can only read or skip a record on either
side of its current position. The access time in a random-access device varies
because it can read or write any record in an I/O volume, so it must reposition
either the read/write head or the I/O medium before commencing a read or write
operation.
Error Detection and Correction       Errors might arise during recording or reading
of data or transferring it between an I/O medium and memory. To facilitate
detection and correction of such errors, data being recorded or transmitted is
viewed as a bit stream, i.e., as a stream of 1s and 0s, and special codes are used to
represent the bit stream. We discuss some of these codes in the following.
Error detection is performed through recording of redundancy information
with data. This information, which we will call error detection information, is
derived from the data by using a standard technique. When data is read off an
I/O medium, this information is also read off the medium. Now, error detection
information is computed again from the read data, using the same technique,
and it is compared with the error detection information read off the medium. A
mismatch indicates some recording errors. Error correction is performed anal-
ogously, except that more powerful algorithms are used to generate the error
correction information. This information can both detect an error and indicate
                                                tio
                            Device readied for               Data transfer
                                data transfer                in progress
                                     ta                       tx
                 Read/write command            Data transfer      Data transfer
                 is issued                           starts                 ends
Figure 14.3  Access and transfer times in an I/O operation.



550  Part 4  File  Systems
                   how it can be corrected. Recording and reading of redundant information causes
                   an overhead. Error correction incurs more overhead than error detection.
                   Figure 14.4 describes two approaches to error detection and correction. In
                   the parity bits approach, np parity bits are computed for nd bits of data. The
                   parity bits are put in fixed locations in a record. They are indistinguishable from
                   data, except to the error detection/correction algorithm. In the cyclic redundancy
                   check (CRC) approach, an nc bit number called the CRC is recorded in the CRC
                   field of a record. A key difference between the two approaches is that np depends
                   on nd , while nc is independent of nd .
                   Both approaches use modulo-2 arithmetic. This arithmetic is analogous to
                   binary arithmetic, except that it ignores carries or borrows generated in any bit
                   position. This property makes it very fast. A modulo-2 addition is represented as
                   an exclusive-OR operation . It uses the following rules: 0  0 = 0, 1  0 = 1,
                   0  1 = 1, and 1  1 = 0.
                   A popular variant of the parity bits approach used in RAMs and older
                   magnetic tapes associates a single parity bit with a byte of data. As described
                   in Figure 14.4, it is generated from all bits of a byte by using the  oper-
                   ation. It can detect a single error in a byte, but fails if two errors occur. It
                   also cannot correct any errors. The error detection overhead is 1 parity bit for
                   8 bits of data, i.e., 12.5 percent. A Hamming code can detect up to two errors in
                   a record and can correct a single error. The correct technical name of the code is
                   (nd + np, nd ) Hamming code. Comparison of the parity bit values in a record read
                   off the medium with parity values computed from the read data by applying the
                   rules of the code indicates which bit is in error. The value in this bit is inverted to
                   correct the error. Figure 14.4 gives the rules for determining the number of parity
                   bits and computing their values. A (12, 8) Hamming code can perform error detec-
                   tion and correction for 1 byte. It uses 12 ­ 8, i.e., 4, parity bits. Thus, the overhead is
                   50 percent. The overhead decreases with the number of data bits; e.g., 8 parity
                   bits are adequate for 30 bytes of data.
                   The CRC is computed from data that is to be transmitted or recorded, and it
                   is put into the CRC field of a record. It can indicate whether one or more errors
                   have occurred in any byte of data, or if bytes have been swapped or reordered.
                   When a record is read, a CRC is computed from its data field and compared with
                   the number in its CRC field. An error exists if the two do not match. A practical
                   value of nc is 16 or 32 bits, irrespective of the value of nd . With nc < nd , error
                   detection is not foolproof because two bit streams, say s1 and s2, could generate
                   the same CRC. If one of them is transformed into the other due to errors, the
                   errors   cannot  be   detected  using     CRC.    The     probability   of  this  happening   is  1    .
                                                                                                                     2nc
                                                             1
                   Hence,   reliability  of  CRC   is  1  -  2nc  .  For  a  16-bit  CRC,  the  reliability  is  99.9985
                   percent. For a 32-bit CRC, reliability is 99.9999 percent.
                   14.3.1 Magnetic Tapes
                   The I/O medium in a tape or cartridge is a strip of magnetic material on which
                   information is recorded in the form of 1s and 0s, using principles of electromag-
                   netic recording. The recording on a tape is multitrack; each track records a bit



                                                             Chapter 14  Implementation of File Operations  551
              Data and parity                                Data          CRC
              nd + np bits               Parity bit          nd bits       nc bits
              Parity bits approach                           CRC approach
Calculating a parity bit
A parity bit is computed from a collection of data bits by modulo-2 arithmetic, i.e., by
using the exclusive OR operator . For example, the parity bit for 4 data bits bi, bj , bk
and bl is computed as follows: p = bi  bj  bk  bl  c1, where c1 is a constant which
is 1 for odd parity and 0 for even parity.
Hamming code
Step 1: Determine the number of parity bits as the smallest value of np which satisfies
nd + np + 1  2np . Fix parity bit positions as powers of 2, i.e., positions b1, b2, b4, b8, . . . ,
in a record, where bits are numbered as b1, b2 . . . from the start of the record.
Step 2: Compute the parity bit occupying the 2nth position from the following bits,
excepting itself: For each value of c2, take 2n consecutive bits starting on bit position
2n + c2 × 2n+1, where c2 has values 1, 2, 3, . . . , etc. Thus, parity bit b1 is computed
from b3, b5, . . . ; b2 is computed from b3, b6, b7, b10, b11 . . . ; and b4 is computed from
b5, b6, b7, b12, b13, b14, b15, . . . .
Step 3: When a record is received or read, compute parity bits and compare them with
the parity bit values in the record. Form a binary number e1, e2, e4, . . . as follows: ei is 1
if the received and computed values of parity bit bi are different; otherwise, it is 0. No
error has occurred if this number is zero. If a single error exists, this number indicates
the position of the bit which is in error.
Example: If 5-bit data 10110 is to be transmitted or recorded, 4 parity bits are used.
They occupy positions b1, b2, b4, and b8. The record contains 011001100, where the
parity bits have been underlined. If the record is read as 011001101, the error word is
1001, indicating that the error has occurred in position 9.
Cyclic redundancy check (CRC)
Step 1: A bit stream is looked upon as a binary polynomial, i.e., a polynomial each of
whose coefficients is either a 0 or a 1. For example, a bit stream 1101 is looked upon as
a binary polynomial 1 × x3 + 1 × x2 + 0 × x1 + 1 × x0, i.e., x3 + x2 + 1. Here a + is
interpreted as modulo-2 addition, i.e., an exclusive-OR operation .
Step 2: The data in a received record is augmented by adding nc zeroes at its end. The
polynomial obtained from the augmented data is divided by a predefined polynomial of
degree nc + 1. The remainder of this division is a polynomial of degree nc. Coefficients
in this polynomial form the CRC. For example, the CRC for data 11100101 using a
predefined 5-bit polynomial 11011 is 0100.
Step 3: When a record is received, the receiver computes the CRC from the data part
of the record and compares it with the CRC part of the record. A mismatch indicates
error(s). Alternatively, the receiver computes the CRC from the entire record. An error
exists if the computed CRC is not 0.
Figure  14.4  Approaches to error detection and correction.



552  Part 4  File Systems
             of a byte or a parity bit. A read­write head is positioned on each track. Tape
             drives are sequential-access devices. The operations that can be performed on
             these devices are: read/write a specified number of bytes, skip, and rewind.
             Because of the sequential nature, tapes and DAT cartridges are popularly used for
             archival purposes, which involve reading or writing of all records on the medium.
             In older tape technologies, adjoining records on a tape are separated by
             an interrecord gap. This gap provides for the start­stop motion of the medium
             between the reading or writing of successive records. The access time (ta) during
             a read or write operation is caused by both the need to achieve uniform-velocity
             motion of the I/O medium before the data transfer can be initiated and the need
             to position the next record under the read­write head. Total I/O time for a record
             of size s bytes is given by the formula
                                                           s
                                                 tio = ta + d × v
             where         d  recording density
                           v  velocity of the I/O medium.
             Interrecord gaps cause heavy penalties--they lead to poor use of the record-
             ing medium and slow down file processing activities. Despite the drawback of
             poor utilization of the recording medium, in the 1990s tapes offered a cost per
             megabyte that was one-tenth of that offered by disks. However, tapes lost this
             edge in the subsequent decade because disk technology made rapid progress and
             large disks became both practical and cheap. To regain the cost advantage, a
             streaming tape technology was developed.
             A streaming tape contains a single record that is stored without a break
             irrespective of its size. Hence interrecord gaps do not exist even when a large
             volume of data is recorded on a tape. A streaming tape device contains a buffer.
             A write operation is started after putting some data in the buffer. The device writes
             the data from the buffer onto the tape. To keep the streaming tape operating at
             full speed, it is important to put new data into the buffer at a speed that matches
             the writing speed of the tape. The tape drive stops writing when it finds that the
             buffer is empty. When new data is put into the buffer, the tape drive resumes the
             write operation. To avoid creating an interrecord gap, the tape is first moved back
             and then moved forward again so that it can gather recording velocity by the time
             the head passes over the last bit it has written. It now resumes writing. Effectively,
             resumption of writing consumes a few milliseconds.
             The streaming tape provides a high data transfer rate if the buffer is not
             allowed to become empty at any time. However, if the tape stops frequently, the
             effective writing speed can drop to a much smaller value. The physical IOCS has
             to ensure that this does not happen. The stop­start­resume operation of the tape
             also requires precise positioning and alignment, which makes streaming tapes
             expensive.
             14.3.2 Magnetic Disks
             The essential storage element of a magnetic disk is a flat circular object called a
             platter, which rotates on its axis. The circular surfaces of a platter are covered with



                                                  Chapter 14      Implementation of File Operations  553
magnetic material. A single read­write head records on and reads from a surface,
so a byte is recorded serially along a circular track on the disk surface. The read­
write head can move radially over the platter. For each position of the head, the
recorded information forms a separate circular track. Parity information is not
used in a disk; a CRC is written with each record to support error detection.
A start-of-track position is marked on each track, and records of a track are
given serial numbers with respect to this mark. The disk can access any record
whose address is specified by the pair (track number, record number). The access
time for a disk record is given by
                                    ta = ts + tr                               (14.2)
where        ts  seek time, i.e., time to position the head on the required track
             tr  rotational latency, i.e., time to access desired record on the track
The seek time is the time required for the mechanical motion of the head. Rota-
tional latency arises because an I/O operation can start only when the required
record is about to start passing under the head. The average rotational latency
is the time taken for half a disk revolution. Representative values of the average
rotational latency are 3­4 ms, seek times are in the range of 5­15 ms, and data
transfer rates are of the order of tens of megabytes per second.
Variations in disk organization have been motivated by the desire to reduce
the access time of a disk, increase its capacity and data transfer rate, and reduce
its price. The cheapest disk is a floppy disk which is slow and has a small capacity.
A hard disk has a higher capacity; still higher capacities are obtained mainly
through mounting of many platters on the same spindle. One read­write head
is provided for each circular surface of a platter--that is one above and one
below each platter. All heads in the disk pack are mounted on a single access
arm, which is called the actuator, and so at any moment all heads are located
on identically positioned tracks of different surfaces. The set of such identi-
cally positioned tracks outlines a cylinder (see Figure 14.5), a form that can be
exploited for data organization. All the tracks in a cylinder are accessible from
the same position of the access arm; thus, cylinders make several disk tracks
accessible without requiring any movement of the disk heads, and so I/O opera-
tions on records situated in the same cylinder can be performed without incurring
seek times.
A hard disk can be looked upon as consisting of a set of concentric cylinders,
from the innermost to the outermost. A record's address can thus be specified
by the triple (cylinder number, surface number, record number). The necessary
commands for operation of a disk device are seek (cylinder number, surface
number) and read/write a specified record.
Disk capacity can be increased by increasing the number of platters. How-
ever, more platters require more disk heads, which in turn require a heavier
actuator and impose more mechanical stresses. Hence disks tend to have only
a few platters. When a very large capacity is desired, applications use multiple
disk drives. (In Section 14.3.5, we discuss how arrangements using multiple disk



554  Part 4  File Systems
                                                Read-Write heads
                                 Platter
                                                                     Access arm
                                 Cylinder
                                         Track
             Figure 14.5   A disk pack.
             drives can also be exploited to provide high data transfer rates and high reliabil-
             ity.) Seek times can be reduced by using higher rotational speeds, but high speeds
             increase the cost of mechanical components, and so fast disks tend to have smaller
             platters to compensate. PCs and desktop computers tend to use cheaper disks.
             These disks have large platters, which provide large capacity, and comparatively
             low rotational speeds. In contrast, servers tend to use costlier disks that are smaller
             and rotate faster.
             To optimize use of disk surface, tracks are organized into sectors. A sector
             is a standard-sized "slot" in a track for a disk record. The sector size is chosen
             to ensure minimum wastage of recording capacity due to interrecord gaps on the
             surface. Sectoring can be made a part of the disk hardware (hard sectoring), or
             could be implemented by the software (soft sectoring).
             14.3.3 Data Staggering Techniques
             Recall from Section 14.3 that the data read off an I/O device during a read opera-
             tion is stored in the DMA buffer, from where the DMA transfers it to memory as
             a single block. But while this transfer is under way, the disk continues to revolve
             and one or more following sectors may pass under the head by the time the trans-
             fer is completed. Hence if a read operation on the next consecutive sector is issued
             immediately after the previous one, the required sector may have passed under the
             head by the time the DMA can initiate the read operation. Such a read operation
             can be performed only in the next disk revolution. Analogously, during a write
             operation, recording of the data is initiated only after data is transferred from
             memory to the DMA buffer, so recording in the next sector cannot take place
             in the same revolution if the sector passes under the read­write head before the
             data transfer is completed. A similar problem is caused by head switching time,
             which is the time taken to switch operation between heads positioned on differ-
             ent platters. By this time a few sectors of the next platter have passed under the
             read­write head. The seek time to move the head to the next cylinder also causes
             a similar problem. All these problems adversely affect the throughput of a disk.



                                                   Chapter 14  Implementation of File Operations  555
The techniques of sector interleaving, head skewing, and cylinder skewing
address the problems caused by data transfer time, head switch time, and seek
time, respectively. These techniques, collectively called data staggering techniques,
ensure that the next consecutively numbered sector will not pass under the read­
write head before the head will be in a position to perform a read/write operation
on it, so that the operation can be performed in the current revolution of the disk.
Sector interleaving staggers sectors along a track in such a way that consecutively
numbered sectors are separated by a few other sectors. This arrangement permits
the I/O operation for a sector to be completed by the time the sector with the next
consecutive address passes under the head. Head skewing staggers the "start of
track" positions on different platters of a cylinder so that the times when the last
sector of a track and the first sector of the next track pass under their respective
heads are separated by the head switch time. Cylinder skewing analogously stag-
gers the "start of track" positions on consecutive cylinders to allow for the seek
time after reading the last sector on a cylinder.
Figure 14.6 illustrates how the techniques of sector interleaving, head skew-
ing, and cylinder skewing reduce rotational delays through data staggering. It is
assumed that the disk has five sectors in a track and uses ten platters, so a cylinder
has 50 sectors in it. For each data staggering technique, the left and right parts
of the figure show operation of the disk without and with data staggering. The
first line in each part shows which sectors pass under the read-write heads of the
disk at different times. The next few lines show what activities involved in an I/O
operation are in progress as the disk rotates--they constitute a timing diagram
for the I/O operation.
Figure 14.6(a) illustrates sector interleaving. We assume the disk head is
positioned immediately before the first sector on the first cylinder where a file
is stored, so the command to read the first sector does not incur any seek or
rotational latency. Reading of the sector into the DMA buffer completes a little
before time t1, and the transfer of this data to memory by the DMA controller
completes a little after time t1. The command to read the next sector is issued
immediately after reading of the previous sector completes, i.e., a little after time
t1. By that time the head is positioned somewhere over sector 2, so sector 2
cannot be read immediately. A rotational delay now occurs that lasts until sector
2 passes under the head in the next revolution of the disk, i.e., until time t6. The
right part of the figure shows the arrangement of sectors when sector interleaving
is employed; sectors 1 and 2 are separated by sector 4 on the track. When the
command to read sector 2 is issued, the read­write head is located almost at the
end of sector 4. Now, the rotational delay lasts only until time t2, when sector 2
starts passing under the head.
Figure 14.6(b) illustrates head skewing. Here, we show the arrangement of
sectors in the first two tracks allocated to a file. The read command on sector
5, which is the last sector on the first track, is issued at time t10. The reading
of this sector and transfer of the data to memory completes before time t11,
so the read command for sector 6 is issued some time before t11. However, it
involves head switching because t11 is located on a different track; head switch-
ing is not completed before time t11 when sector 6 starts passing under the head.



556  Part 4  File  Systems
                                                        Without data staggering                           With data staggering
                            (a) Sector interleaving
                                  Sectors of         1      2      3      4      5      1      2     1        4      2      5      3      1      4
                                  first track
                                  Seek
                                  Rotational
                                  latency
                                  Read
                                  Memory
                                  transfer
                                                  0     t1     t2     t3     t4     t5     t6     0       t1     t2     t3     t4     t5     t6
                            (b)  Head skewing
                                  Sectors of         5      1      2      3      4      5      1     5        1      2      3      4      5      1
                                  first two       10        6      7      8      9  10         6       9  10         6      7      8      9  10
                                  tracks
                                  Seek and
                                  head
                                  switching
                                  Rotational
                                  latency
                                  Read
                                  Memory
                                  transfer
                                                  t10   t11    t12    t13    t14    t15    t16    t10     t11    t12    t13    t14    t15    t16
                            (c) Cylinder skewing
                                  Sectors of      50    46     47     48     49     50     46        50   46     47     48     49     50     46
                                 first tracks of  55    51     52     53     54     55     51        53   54     55     51     52     53     54
                                  2 cylinders
                                  Seek
                                  Rotational
                                  latency
                                  Read
                                  Memory
                                  transfer
                                                  t20   t21    t22    t23    t24    t25    t26    t20     t21    t22    t23    t24    t25    t26
                   Figure   14.6  Effect of data staggering: (a) sector interleaving; (b) head skewing; and
                   (c) cylinder skewing.



                                                                Chapter 14          Implementation of File Operations  557
So reading of sector 6 cannot be commenced immediately; it has to wait until
sector 6 starts passing under the head in the next revolution of the disk at time t16.
This rotational delay is reduced by staggering the recording on the second track
by one sector position, as shown in the right half of the figure. Now, the reading
of sector 6 can commence at time t12, thus incurring a much smaller rotational
delay. Figure 14.6(c) illustrates cylinder skewing. Here, we show the arrange-
ment of sectors in the first track of the first two cylinders allocated to a file. The
seek operation for reading sector 51 results in movement of the read­write head
by one cylinder. The seek operation completes a little before t23; however, sector
51 has passed under the read­write head by that time, hence a rotational delay is
incurred until sector 51 passes under the head in the next revolution at time t26.
As shown in the right half of the figure, data staggering by two sector positions
enables sector 51 to be read starting at time t23.
Sector interleaving had a dramatic impact on the throughput of older disks.
Modern disks have controllers that transfer data to and from memory at very
high rates, so that sector interleaving is not needed. However, we discuss sector
interleaving because it provides an insight into optimizing the peak disk through-
put through data staggering. Head and cylinder skewing are still used to optimize
the peak disk throughput.
Figure 14.7 illustrates sector interleaving. The interleaving factor (Fint) is the
number of sectors that separate consecutively numbered sectors on the same disk
track. Part (b) of Figure 14.7 illustrates the arrangement when Fint = 2, i.e., con-
secutively numbered sectors have two other sectors between them. Interleaving
is uniform, that is, each pair of consecutively numbered sectors are separated by
the same number of sectors, if either n - 1 or n + 1 is a multiple of Fint + 1,
where n is the number of sectors on a track. The arrangement in the figure, where
there are 8 sectors to a track, is uniform, whereas interleaving with Fint = 1 or 3
would not be uniform (see the second column in Table 14.2--some consecutive
sectors are separated by more than Fint sectors). As we shall see in Example 14.2,
a performance penalty is incurred when interleaving is not uniform.
Let tst be the time taken to transfer one sector's data between the DMA
controller and memory, and let tsect be the time taken for one sector to pass under
the disk head. Optimal performance is obtained if tst = Fint × tsect, since I/O on
the next sector can be started immediately after the DMA finishes transferring
                            8     1                          6             1
              7                           2             3                        4
              6                           3             8                        7
                            5     4                          5             2
              (a)                                       (b)
Figure  14.7  Sectors in a  disk  track:  (a)  without  interleaving; (b)  with  interleaving  factor  =  2.



558  Part 4  File Systems
                   the previous sector's data. If tst > Fint × tsect, the next sector would pass under
                   the head before the DMA finishes data transfer for the previous sector. Hence the
                   next sector can be accessed only in the next revolution of the disk. tst < Fint ×tsect
                   implies that the disk would be idle for some time before the next sector is accessed
                   in the same revolution. Disk throughput suffers in both these cases. Analogously,
                   throughput would suffer when other data staggering techniques are employed if
                   data is staggered by too little or too much. The following example illustrates the
                   variation of peak disk throughput with the sector interleaving factor.
·
     Example 14.2  Sector Interleaving
                   A disk completes one revolution in 8 ms and has 8 sectors on a track, each
                   containing 1000 bytes. The values of tst and tsect satisfy the relation tsect <
                   tst < 2 × tsect. To obtain the peak disk throughput for a value of Fint, we read
                   the sectors in the order 1, . . . , 8 over and over again and observe the number
                   of bytes transferred in one second. Figure 14.8 shows variation of peak disk
                   throughput for different values of Fint.
                      Table 14.2 shows the arrangement of sectors for different values of Fint
                   and the corresponding disk throughput represented in units of kB/s where
                   1 kB/s is 1000 bytes per second. Interleaving with Fint = 1 or 3 is not uniform.
                   For Fint = 1, the arrangement of sectors on the track is 1, 5, 2, 6, 3, 7, 4, 8.
                   After reading sector 1, sector 2 cannot be read in the same revolution. Hence
                   the disk takes 10 ms to read sector 2. Similarly, sectors 3 and 4 require 10 ms.
                   Sectors 4 and 5 are separated by 2 sectors. Hence they can be read in the same
                   revolution of the disk; the disk takes only 3 ms to read sector 5 after sector 4
                   has been read. Reading of sectors 6, 7, and 8 requires 10 ms each, while reading
                   of sector 1 requires 9 ms.
                      Figure 14.8 shows the variation of throughput with different values of Fint.
                   Fint = 2 is adequate to satisfy tst  Fint × tsect, and so the throughput increases
                   sharply. Values of Fint > 2 are counterproductive since the disk spends some
                   idle time before the next sector passes under the head. Hence the throughput
                   dips for Fint > 2.
                   ·
                                               350
                                 Peak          250
                                 disk
                                 throughput    150
                                               50
                                                    0      1            2  3                  4
                                                                           Interleaving       factor
                   Figure  14.8  Variation of  throughput  with sector  interleaving factor.



                                                              Chapter 14       Implementation of  File  Operations  559
Table 14.2           Sector Arrangement and Performance in Sector
Interleaving
            Arrangement of                                        Average      Peak throughput
Fint        sectors             tio for sectors (ms)              tio (ms)     (kB/s)
0       1, 2, 3, 4, 5, 6, 7, 8  9, 9, 9, 9, 9, 9, 9, 9            9            111.1
1       1, 5, 2, 6, 3, 7, 4, 8  9, 3, 10, 10, 10, 10, 10, 10      9            111.1
2       1, 4, 7, 2, 5, 8, 3, 6  3, 3, 3, 3, 3, 3, 3, 3            3            333.3
3       1, 3, 5, 7, 2, 4, 6, 8  9, 5, 5, 5, 4, 4, 4, 4            5            200.0
4       1, 6, 3, 8, 5, 2, 7, 4  5, 5, 5, 5, 5, 5, 5, 5            5            200.0
14.3.4 Disk Attachment Technologies
EIDE and SCSI Interfaces        Enhanced integrated device electronics (EIDE) and
small computer system interconnect (SCSI) are the leading disk interfaces for
attaching disks to computers. Disks attached this way have come to be called
host-attached storage. Integrated device electronics (IDE, also called advanced
technology attachment, or ATA) was the predecessor of EIDE. Before EIDE
was developed, the different features of IDE and SCSI made each of them ideal
for specific applications. For example, IDE was considered to provide excellent
performance for sequential I/O while SCSI was considered to be superior for
random I/O. Accordingly, IDE disks were used in the low-cost PC and desk-
top environment while SCSI disks were used in the server environment. With
EIDE, the gap in random-access performance has narrowed considerably. Both
retain their traditional niche areas, but EIDE and SCSI now compete in some
application segments, such as backup storage media. Both kinds of disks provide
a large buffer of a few megabytes.
   IDE disks primarily worked with programmed I/O modes, though they sup-
ported a DMA mode as well. EIDE supports new DMA modes including the
first party, i.e., bus mastering, DMA mode; the ultra ATA mode of EIDE sup-
ports transfer rates of 33.3 MB per second, which is 8 times faster than the IDE
data transfer rate. EIDE disks use larger platters, rotate relatively slowly, and are
cheap. Up to two disks can be connected to EIDE; however, only one of them
can operate at a time.
   SCSI supports several DMA modes; the fastest of these provides a data
transfer rate of 80 MB per second. SCSI permits up to 7 disks to be con-
nected  to    it.  SCSI  is  called    an  interface,   but   technically      it  is  an  I/O  bus
because it permits simultaneous operation of many disks connected to it. SCSI
disks are smaller, rotate faster, and are more expensive. Accordingly, they pro-
vide smaller seek times and higher data transfer rates. A SCSI disk supports
scatter/gather I/O wherein it can transfer data from a disk block into non-
contiguous areas of memory or collect data from noncontiguous areas and
write   them  into   a   disk   block  (see  Section    12.2.4).     It  also  provides    several



560  Part 4  File Systems
             functionalities that were traditionally performed by the IOCS, including the
             following:
             ·  Disk scheduling: A SCSI disk accepts several I/O requests concurrently and
                stores them into a queue of requests. It uses its knowledge of the current
                position of disk heads and the rotational position of the platters to select an
                I/O operation that involves the minimum delay due to seek and rotational
                latency. This feature is described in Section 14.7.
             ·  Bad block recovery: A SCSI disk detects bad disk blocks and assigns substitute
                disk blocks for them. It maintains a table showing addresses of bad blocks
                and their substitutes. If an I/O command is directed toward a bad disk block,
                the disk automatically redirects it at the substitute block. This feature speeds
                up I/O operations by performing bad block management in the device rather
                than in the access method layer of IOCS.
             ·  Prefetching of data: A SCSI disk contains a buffer. At every I/O opera-
                tion, it reads the next few disk blocks into the buffer. This action speeds
                up subsequent read operations during processing of a sequential file.
             Network-Attached Storage and Storage Area Networks      Host attachment of
             disks suffers from poor scalability because disk sizes are limited by prevailing
             technologies and the number of disks that can be attached to a host is limited
             by the interface. Therefore, organizations have to constantly replace disks or
             add more servers to meet their requirements for more storage. This problem is
             addressed by facilitating use of remote disks through a network. This approach
             enables the storage capacity to be increased incrementally and seamlessly, and
             storage to be shared by applications operating on many servers.
                A network-attached storage (NAS) is a disk or a redundant array of inexpen-
             sive disks (RAID), which is discussed in the next section, attached directly to a
             local area network (LAN) [see Figure 14.9(a)]. NAS is an inexpensive method
             of providing large disk capacities, because it employs the hardware and software
             existing in a LAN environment. Functionalities such as a file server or a dis-
             tributed file system (see Chapter 20) can be provided by using the NAS. However,
             use of NAS faces some difficulties in practice: LANs use protocols that optimize
             application-to-application data transfers whereas the file server or distributed file
             system requires use of a file-based protocol like the Sun NFS protocol discussed
             in Section 20.6.1, or Microsoft's common interface file system (CIFS) protocol.
             The load created by the file-based protocol slows down networking applications.
                A storage area network (SAN) is an alternative arrangement that avoids slow-
             down of networking applications. A SAN is a network composed of disks that
             provides a high bandwidth [see Figure 14.9(b)]. The network could be a dedicated
             fiber channel that uses the SCSI protocol, or an Internet protocol (IP) network
             that uses the iSCSI protocol. Several servers can be connected to a SAN; each
             server can access the entire storage. This feature facilitates formation of high-
             performance clusters of computer systems (see Section 16.2). Data integrity and
             availability is provided through the redundancy of disks and servers connected
             to the SAN.



                                                          Chapter 14            Implementation of File Operations  561
                          Clients                                      Clients
Network-                  Local area                                   Local area
attached           network (LAN)                                network (LAN)
storage
                                                                       Server                 Server
                                                                Storage area
                                                                network (SAN)
(a)                                                 (b)
Figure  14.9  (a) Network-attached storage; (b) storage area network.
     New technologies that employ the iSCSI protocol over an IP network to
combine the features of the NAS and SAN technologies are emerging. These
technologies support both block-accessed SAN devices and file-accessed NAS
devices without incurring the cost of a fiber channel.
14.3.5 RAID
Computer users constantly clamor for disks with larger capacity, faster access to
data, higher data transfer rate and higher reliability. All these issues are addressed
through arrangements involving multiple disks. The redundant array of inexpen-
sive disks (RAID) technology was originally employed for providing large disk
capacities at a low cost through use of several inexpensive disks. However, the
recent trend is to enhance disk capacities through network-attached storage and
storage area networks (see Section 14.3.4). Hence today's RAID technology is
used for providing fast access, high data transfer rates, and high reliability; it is
more appropriately called redundant array of independent disks.
     The   RAID    technology         spreads  the  data  involved     in  an      I/O    operation
across    several  disks  and  performs        I/O  operations  on     these       disks  in  paral-
lel. This feature can provide either fast access or a high data transfer rate,
depending on the arrangement employed. High reliability is achieved by record-
ing redundant information; however, the redundancy employed in a RAID is
qualitatively different from that employed in conventional disks: A conven-
tional disk provides reliability only by writing a CRC at the end of every
record (see Section 14.3), whereas redundancy techniques in a RAID employ
extra disks to store redundant information so that data can be recovered even
when some disks fail. Access to redundant information does not cost addi-
tional I/O time because both data and redundant information can be accessed in
parallel.



562  Part 4  File Systems
             Recording in a RAID is performed as follows: A disk strip is a unit of data on
             a disk, which can be a sector, a disk block, or a disk track. Identically positioned
             disk strips on different disks form a disk stripe. A file is allocated an integral
             number of disk stripes. The data residing in the strips of the same stripe can be
             read or written simultaneously because they exist on different disks. If the disk
             array contains n disks, theoretically the data transfer rate could be n times that
             of a single disk. Practical values of data transfer rates depend on overhead and
             on any factors that may limit the parallelism of I/O operations while processing
             a file.
             Several RAID organizations using different redundancy techniques and disk
             striping arrangements have been proposed. These organizations are called RAID
             levels. Table 14.3 summarizes the properties of various RAID levels. RAID levels
             0 + 1 and 1 + 0, which are hybrid organizations based on RAID levels 0 and 1,
             and RAID level 5 are the most popular RAID organizations.
             RAID Level 0   Level 0 employs only disk striping; it is not really a RAID organi-
             zation because it does not involve redundant recording of data. It provides high
             data transfer rates, particularly if each disk is under a separate disk controller.
             However, it suffers from low reliability. Data becomes inaccessible even if a single
             disk is inoperative. Also, lack of redundancy implies that data is lost if a disk
             fails, and so reliability still has to be achieved by means other than the RAID
             organization.
             RAID Level 1   Level 1 RAID organization writes identical information on two
             disks; it is called disk mirroring. When a process writes or updates a record in a
             file, one copy of the record is written on each disk. This way, RAID 1 incurs 100
             percent overhead; however, one copy of a record is guaranteed to be accessible
             even if a single fault occurs. During a read, the RAID simply reads the copy
             that can be accessed earlier. High data transfer rates can be achieved during read
             operations because both disks could operate in parallel when no errors arise.
             Hybrid organizations that use the features of RAID levels 0 and 1 are often
             used in practice to obtain both high data transfer rates as in RAID level 0 and
             high reliability as in RAID level 1. RAID 0 +1 employs disk striping as in RAID
             0, and mirrors each stripe as in RAID 1. RAID 1 + 0 first mirrors each disk
             and then performs striping. These organizations provide different kinds of fault
             tolerance: In RAID 0+1, a single error in a copy of a stripe makes the entire copy
             inaccessible, so errors in both copies of a stripe would make the stripe inaccessible.
             In RAID 1 + 0, an error on one disk would be tolerated by accessing its mirror
             disk. A stripe would become inaccessible only if both a disk and its mirror disk
             have errors.
             RAID Level 2   This RAID organization uses bit striping, i.e., it stores each bit of
             data or redundancy information on a different disk. When data is to be written,
             the ith data strip contains the ith bit of each byte and a parity strip contains one
             of the parity bits computed from corresponding bits in all strips of the stripe. An
             error correcting code is used to compute and store redundancy information for
             each byte (see Section 14.3). Thus, 8 disks are used to record the bits of a byte,



                                                                        Chapter 14   Implementation of File Operations                       563
Table 14.3  RAID Levels
Level       Technique                                                   Description
Level 0     Disk striping                                               Data is interleaved on several disks. During an I/O oper-
                                                                        ation, the disks are accessed in parallel. Potentially, this
                    ...                                                 organization can provide an n-fold increase in data transfer
                                                                        rates when n disks are used.
Level 1     Disk mirroring                                              Identical data is recorded on two disks. During reading
                                                                        of data, the copy that is accessible faster is used. One of
                                                                        the copies is accessible even after a failure occurs. Read
            Disk 1  Disk 2                                              operations can be performed in parallel if errors do not
                                                                        arise.
Level 2     Error correction codes                                      Redundancy information is recorded to detect and cor-
                                                                        rect errors. Each bit of data or redundancy information is
                    ...     ...                              ...        stored on a different disk and is read or written in parallel.
            D            D                                P          P  Provides high data transfer rates.
Level 3     Bit-interleaved parity                                      Analogous to level 2, except that it uses a single parity disk
                                                                        for error correction. An error that occurs while reading
                    ...                                                 data from a disk is detected by its device controller. The
            D            D                             P                parity bit is used to recover lost data.
Level 4     Block-interleaved parity                                    Writes a block of data, i.e., consecutive bytes of data, into
                                                                        a strip and computes a single parity strip for strips of
                    ...                                                 a stripe. Provides high data transfer rates for large read
            D            D                             P                operations. Small read operations have low data transfer
                                                                        rates; however, many such operations can be performed in
                                                                        parallel.
Level 5     Block-interleaved                                           Analogous to level 4, except that the parity information
            distributed parity                                          is distributed across all disk drives. Prevents the parity
                    ...                                                 disk from becoming an I/O bottleneck as in level 4. Also
                                                                        provides better read performance than level 4.
Level 6     P + Q redundancy                                            Analogous to RAID level 5, except that it uses two inde-
                                                                        pendent    distributed  parity                   schemes.  Supports  recovery
                    ...                                      ...        from failure of two disks.
            D            D                             P          P
Note: D and P indicate disks that contain only data and only parity information, respectively.  indicates a strip. · Indicates bits of a byte that
are stored on different disks, and their parity bits.             indicates a strip containing only parity information.



564  Part 4  File Systems
             and a few more disks are used to record redundancy information. For example,
             the (12, 8) Hamming code, which is adequate for recovery from a single failure,
             would require 4 redundancy bits. The RAID 2 arrangement employing this code
             would consist of 8 data disks and 4 disks containing redundancy information,
             each storing 1 bit of data or parity information. This RAID arrangement can
             read/write data 8 times faster than a single disk. However, it is expensive because
             several disks are needed to store redundancy information, hence it is not practical.
             RAID Level 3    Level 3 employs disk striping with a bit-interleaved parity scheme;
             i.e., it employs bit interleaving--it writes the bits of a byte on different disks--and
             employs a single parity bit per byte. The data strips of a stripe are stored on 8 data
             disks and the parity strip is stored on the parity disk. Thus, RAID level 3 employs
             a significantly smaller amount of redundant information than RAID level 2. A
             read operation is performed as follows: The disk controller checks whether an
             error exists within a strip. If so, it ignores the entire strip and recovers the data in
             the strip using the parity strip--the value of a data bit is the modulo-2 difference
             between the parity bit and the modulo-2 sum of corresponding bits of other strips
             in the stripe.
             All data disks participate in an I/O operation. This feature provides high
             data transfer rates. However, it also implies that only one I/O operation can be in
             progress at any time. Another drawback of RAID level 3 is that parity compu-
             tation can be a significant drain of the CPU power. Hence parity computation is
             off-loaded to the RAID itself.
             RAID Level 4    Level 4 is analogous to level 3 except that it employs block-
             interleaved parity. Each strip accommodates a block of data, i.e., a few consecutive
             bytes of data. If an I/O operation involves a large amount of data, it will involve
             all data disks as in RAID level 3, hence RAID level 4 can provide high data
             transfer rates for large I/O operations. A fault-free read operation whose data fits
             into one block will involve only a single data disk, so small I/O operations have
             small data transfer rates; however, several such I/O operations can be performed
             in parallel.
             A write operation involves computation of parity information based on data
             recorded in all strips of a stripe. This can be achieved by first reading data con-
             tained in all strips of a stripe, replacing the data in some of the strips with new
             data that is to be written, computing the new parity information, and writing
             the new data and parity information on all disks. However, this procedure limits
             parallelism because all disks are involved in the write operation even when new
             data is to be written into a single block blocki of stripe stripei. Hence, the parity
             information is computed by a simpler method that involves the exclusive OR of
             three items--the old information in the parity block, the old data in block blocki,
             and the new data to be written in block blocki. This way, only the disk(s) contain-
             ing the block(s) to be written into and the parity block are involved in the write
             operation, and so several small fault-free read operations involving other disks
             can be performed in parallel with the write operation.
             RAID Level 5    Level 5 uses block level parity as in level 4, but distributes the
             parity information across all disks in the RAID. This technique permits small



                                                Chapter 14   Implementation of File Operations  565
write operations that involve a single data block to be performed in parallel if their
parity information is located on different disks. Small fault-free read operations
can be performed in parallel as in RAID level 4. Hence this organization is
particularly suitable for small I/O operations performed at a high rate. Larger
operations cannot be performed in parallel; however, the organization provides
high data transfer rates for such operations. It also provides higher peak disk
throughput for read operations than level 4 because one more disk can participate
in read operations.
RAID   Level  6  This  organization  uses  two  independent  distributed  parity
schemes. These schemes support recovery from failure of two disks. Peak disk
throughput is slightly higher than in level 5 because of the existence of one more
disk.
14.3.6 Optical Disks
Data is recorded on an optical disk by creating changes in reflectivity of the disk,
and it is read by a laser and a photosensitive assembly that picks up changes in
reflectivity of the surface under the disk head. A compact disc (CD) is an optical
disk. The disk writer stores a 1 by causing a change in reflectivity compared
with the data bit in the preceding position, and stores a 0 by retaining the same
reflectivity as the preceding bit.
Recording on a CD can be performed by various means. Mass-produced
prerecorded CDs that contain music are produced by mechanical means. They
are called stamped CDs. Recording can also be performed by using a laser beam.
A laser-recorded CD contains three layers: a polycarbonate layer, a polymer dye,
and a reflective metallic layer. When a strong laser beam is directed at a spot
on the CD, it heats the dye and creates a permanent mark on the disk called
a pit, which has a lower reflectivity. This is why the recording process is called
"burning" a CD. Data is recorded in a shallow spiral groove on a CD that extends
from the inside diameter of the disk to its outside diameter. A CD contains
22,188 spiral revolutions, which are about 1.6 microns apart. Each revolution
is called a track. Speed control and absolute time information are prerecorded
on a CD.
A CD contains several regions reserved for use by a CD recorder. The power
calibration area is used to calibrate the power of the writing laser. The program
memory area stores track information for all sessions in the CD. It is followed
by lead-in, program, and lead-out areas for each session. A lead-in area is a table
of contents of a session. It indicates the number of tracks, track start and stop
points, and the length of the session. The program area contains data tracks of
the session. The lead-out area indicates end of a session.
Two features of a CD are important from an operating system viewpoint--
recording of data and creation of a file system. Data is recorded in the form
of sectors on a track. A CD-ROM intended for computer use contains sec-
tors of 2 KB. It has a capacity of about 650 MB. A DVD (digital versatile disk),
on the other hand, has a capacity of about 5 GB. Data is recorded on either



566  Part 4  File Systems
                                                                                           root
                           Primary volume                                         A        B        C
                           descriptor (PVD)
                                                                               D
                                                                                     u  v        w  x
                                             Path table
                                                                            z
             Figure        14.10 Primary     volume descriptor  of  a  CD.
             type of disk by using the encoding method called CIRC (cross-interleaved Reed­
             Solomon code). CIRC encodes a unit of data that is 24 bytes, i.e., 192 bits, in
             size, to produce an encoded unit that is 588 bits in size. Apart from data, this
             unit contains information concerning relative and absolute timing, placement of
             tracks and indices; synchronization data, and error prevention and correction
             data. To make error correction reliable, data is scrambled while encoding. This
             way, if a few bytes of consecutively recorded data are lost, a large number of bytes
             may lose only 1 bit each. This data can be recovered by using the error correction
             information.
             ISO Standard 9660 defines a common logical format for files and directories
             on a CD. It defines basic requirements for data interchange and also provides
             for optional extensions to Windows, Unix, and Macintosh environments. The
             Rockridge extension allows Unix-specific long filenames, multilevel directories,
             access privileges, and file types. The universal disk format (UDF) is also designed
             for a common logical, i.e., cross-platform, file system. UDF can coexist with ISO
             9660, and many CD drives can write information in either format.
             Figure 14.10 shows how a file system is implemented on a CD volume. The
             primary volume descriptor (PVD) is recorded in logical sector 16. It indicates
             the position of the root directory, and the position of a path table. Each entry in
             the path table contains information about the location of a directory. Its use to
             locate a required directory avoids searches through intermediate directories in a
             path name; in a Unix system, for example, it avoids searches through directories
             root and A for a pathname ~A/D/z.
             14.4          DEVICE-LEVEL I/O                                                            ·
             Four functions are involved in implementing I/O at the level of an I/O device--
             initiating an I/O operation, performing read/write operations, checking the status
             of an I/O device, and handling interrupts raised by devices. The first three of these
             functions are performed through I/O instructions and I/O commands described in
             Section 14.2. Table 14.4 describes features in the computer system that support



                                                          Chapter 14  Implementation of File Operations  567
Table 14.4         Computer System Features Supporting Functions
in Device-Level I/O
Function             Description of computer system feature supporting it
Initiating an        The I/O instruction I/O-init (cu, d), command_address initiates
I/O operation        an I/O operation (see Example 14.1). The I/O-init instruction
                     sets a condition code to indicate whether the I/O operation has
                     been initiated successfully.
Performing           Device-specific I/O commands implement tasks like positioning
read/write           of read­write heads over a record and reading of a record.
Checking             The I/O instruction I/O-status (cu, d) obtains status information
device status        for an I/O device. The information indicates whether the device
                     is busy, free, or in an error state, and cause of the error, if any.
Handling             The interrupt hardware implements the interrupt action
interrupts           described in Section 2.2. The CPU is switched to the physical
                     IOCS when an I/O completion interrupt occurs.
these functions. We assume that I/O operations are performed in the DMA mode
(see Section 2.2.4). In Section 14.4.1, we discuss details of device-level I/O and in
Section 14.5, we discuss the facilities provided by the physical IOCS to simplify
device-level I/O.
14.4.1 I/O Programming
We use the term I/O programming to describe all actions involved in performing
an I/O operation. To understand two key aspects of I/O programming--namely,
I/O initiation and I/O completion processing--we consider the program of
Figure 14.11, which is an assembly language version of the following program in
a higher-level language:
                          read  a,  b;
                          ...
                          result    :=  a          +  b;
The program uses a bare machine, i.e., a computer system that does not have any
software layers between the program and the machine's hardware. The program
uses the flag IO_FLAG to indicate whether the I/O operation is in progress. It sets
the IO_FLAG to 1, initiates an I/O operation and loops until the I/O operation
completes before performing its computations.
I/O Initiation  When the I/O-init instruction of Figure 14.11 is executed, the
CPU sends the device address to the DMA controller. The DMA controller
finds whether the device is available for the I/O operation, and informs the CPU
accordingly; the CPU sets an appropriate condition code in the condition code
field (also called the flags field) of the PSW. If the device is available, the DMA also
starts the I/O operation by accessing and decoding the first I/O command. The



568        Part 4  File Systems
                   SET         IO_FLAG,    `1'      To   indicate   that     I/O    is  in   progress
RETRY:             IO_init     (cu, d), COMMANDS    Read   a,  b
                   BC          cc1, IN_PROGRESS     Branch     if  I/O  initiation       is     successful
                   BC          cc2, RETRY           Loop   if  the  device      is  busy
                   BC          cc3, ERROR           Error.     Inform   system      administrator
IN_PROGRESS:       COMP        IO_FLAG,    `1'      Check     whether   I/O     is  still    in  progress
                   BC          EQ,  IN_PROGRESS     Loop   if  I/O  is  in      progress
                   { Perform result      :=  a+b;}
COMMANDS:          {I/O commands}
                   ···
IO_INTRPT:         SET         IO_FLAG,    `0'      Interrupt      processing:      I/O     is   complete
                   ···
Figure 14.11 I/O programming.
                         I/O-init instruction is now complete. The I/O operation, if initiated, will proceed
                         in parallel with the CPU's execution of instructions.
                               In the next few instructions, the program examines the condition code set
                         by the I/O-init instruction to handle any exceptional situations that might have
                         occurred when the I/O-init instruction was executed. The instruction BC cc1,
                         IN_PROGRESS is a conditional branch instruction. Condition code cc1 would
                         have been set if I/O initiation was successful. In that event the I/O operation
                         would have already started, and so execution of the program is diverted to the
                         instruction with the label IN_PROGRESS. Condition code cc2 indicates that the
                         device was busy, so the program would retry the I/O instruction until I/O initiation
                         succeeds. Condition code cc3 indicates that an I/O error occurred, so the program
                         would report the error to the system administrator. These details are not shown
                         in Figure 14.11.
                         I/O   Completion    Processing  The   program  cannot      perform     the  computation
                         result     :=     a+b; until the I/O operation completes. However, the program's
                         execution cannot be suspended because it is executing on a bare machine. The
                         program addresses this problem by using the flag IO_flag to indicate whether
                         the I/O operation has completed. To start with, it sets the value of IO_FLAG to 1
                         to indicate that I/O is in progress. After starting the I/O operation, it enters a loop
                         at IN_PROGRESS where it repeatedly checks this flag. This is a busy wait--see
                         Section 6.5.1.
                               When an I/O interrupt occurs indicating the end of the I/O operation, con-
                         trol is transferred to the instruction with the label IO_INTRPT by the interrupt
                         action (see Section 2.2). This is the start of the I/O interrupt servicing routine,
                         which changes IO_FLAG to 0 and returns. This action ends the busy wait at
                         IN_PROGRESS.



                                                 Chapter 14      Implementation of File Operations  569
14.5      THE PHYSICAL IOCS                                                                         ·
The purpose of physical IOCS is to simplify the code of user processes by hiding
the complexity of I/O operations and to ensure high system performance. It is
achieved through the following three functions:
·  Handling device-level I/O: The physical IOCS provides an interface for device-
   level I/O that eliminates the complexity of I/O programming discussed earlier
   in Section 14.4.1.
·  Synchronizing a process with completion of an I/O operation: This synchro-
   nization avoids the busy wait following I/O initiation in Figure 14.11 and
   releases the CPU for use by other processes.
·  I/O    scheduling:  The  physical  IOCS  schedules  the  I/O  operations  to         be
   performed on a device in a suitable order to provide high device throughput.
Handling Device-Level I/O   While requesting initiation of an I/O operation, a pro-
cess needs to specify only the device address and details of the I/O operation. The
physical IOCS initiates an I/O operation immediately if the I/O device is available;
otherwise, it notes the request for I/O initiation and initiates it sometime later. In
either case, control is returned to the process that made the I/O request. When
an interrupt arises, the physical IOCS notes which I/O operation has completed,
and initiates another operation on the I/O device, if one is pending.
Synchronizing a Process with Completion of an I/O Operation            The physical
IOCS provides an "await I/O completion" functionality to block a process until
an I/O operation completes. Its parameters are the address of the I/O device and
details of the I/O operation. When a process invokes this functionality, the phys-
ical IOCS checks whether the I/O operation has already completed. If it has not,
it requests the kernel to block the process. This action avoids the busy wait of
Figure 14.11. The state of the process is changed to ready when the I/O operation
completes.
I/O Scheduling  The throughput of an I/O device can be computed as the num-
ber of bytes of data transferred per unit time, or the number of I/O operations
performed per unit time. Throughput can be optimized by minimizing the access
times suffered during I/O operations. In disk devices it can be achieved by reduc-
ing the rotational latency and mechanical motion of disk heads by performing
I/O operations in a suitable order. This function is called I/O scheduling. It is
performed automatically by the physical IOCS; it is not explicitly invoked by a
process.
14.5.1 Logical Devices
A logical device is an abstraction that is employed for a variety of useful purposes.
In the simplest case, a logical device is merely a name for a physical I/O device.
Use of a logical device in the code of a process solves a practical difficulty--the
address of a physical device that a process will use is not known when its code is



570  Part 4  File Systems
             written. While creating a process that uses a logical device, the kernel assigns a
             physical device to the logical device. When the process performs an operation on
             the logical device, the physical IOCS implements the operation on the physical
             device assigned to the logical device.
                A logical device can also be a virtual device as described in Section 1.3.2.
             In this case, the kernel has to map the logical device into a part of a physical
             device. Many logical disks may be mapped into a physical disk in this manner;
             the I/O operations directed at the logical disks would all be performed on the
             same physical disk.
             14.5.2 Physical IOCS Data Structures
             The physical IOCS uses the following data structures (see Figure 14.12):
             ·  Physical device table (PDT)
             ·  Logical device table (LDT)
             ·  I/O control block (IOCB)
             ·  I/O queue (IOQ)
                The physical device table (PDT) is a systemwide data structure. Each entry in
             it contains information about one I/O device. The IOQ pointer field of an entry
             points to the queue of I/O operations that are to be performed on the device. Each
             entry in the queue is a pointer to an I/O control block (IOCB), which contains
             information concerning one I/O operation. The current operation field points to
             the I/O control block that contains information concerning the I/O operation
                                  Logical Physical
                                  device      device            Device   Device      IOQ  Current
                                  name        address           address  type    pointer  operation
                                  std_out
                                                                         Disk
                                  std_err
                                  Logical device                         Physical device
                                  table (LDT)                            table (PDT)
                                  of process Pi
                                                       I/O Queue (IOQ)
                                  Kernel space
                                  User space           Logical
                                                       device   I/O      Status
                                                       name     details        flag
                                                       I/O control block (IOCB)
             Figure        14.12  Data structures of the physical IOCS.



                                                Chapter 14     Implementation of File Operations  571
that has been initiated on the device. This information is useful in processing
completion of the I/O operation.
   The logical device table (LDT) is a per-process data structure. There is one
copy of the LDT for every process in the system; this copy is accessible from
the process control block (PCB) of the process. The LDT contains one entry
for each logical device used by the process. The field physical device address in
the entry contains information concerning the current assignment, if any, for the
logical device. Note that many logical devices, possibly belonging to different user
processes, may be assigned the same physical device such as a disk.
   An I/O control block (IOCB) contains all information pertaining to an I/O
operation. The important fields in an IOCB are logical device name, I/O details,
and status flag. The I/O details field contains the address of the first I/O com-
mand. The status flag indicates whether an I/O operation is "in progress" or
"completed"; it is the equivalent of IO_FLAG in Figure 14.11.
   The I/O queue (IOQ) is a list of all I/O operations pending on a physical device.
Each entry of the IOQ contains a pointer to an I/O control block. Information
in the IOQ is used for I/O scheduling.
   The PDT is formed at system boot time by obtaining details of all devices
connected to the system. The size of the LDT is specified at boot time. An LDT
is formed when a process is created. An I/O control block is allocated when
an I/O operation is to be initiated. The IOQ is shown as an array of pointers in
Figure 14.12. However, it is more practical to organize it as a linked list of IOCBs.
   The PDT, LDT, and IOQ data structures are found within the kernel, whereas
a process creates an IOCB in its own address space, initializes its fields, and uses
it as a parameter in a call on a physical IOCS module. The IOCB's presence in
the address space of the process permits the process to check the status of an I/O
operation without having to invoke the kernel.
14.5.3 Organization of Physical IOCS
Figure 14.13 shows organization of the physical IOCS. Modules above the dashed
line execute with the CPU in the user mode, while those below this line execute
with the CPU in the kernel mode. The physical IOCS is activated in one of two
ways:
·  Through  calls  on  the  physical    IOCS    library  modules     start-io          or
   await-io by a process, with an I/O control block as a parameter
·  Through occurrence of an I/O completion interrupt
When a process invokes start-io, start-io invokes the I/O initiator through
a system call. The I/O initiator obtains the address of the physical device on which
the I/O operation is to be performed, enters the I/O operation in the IOQ of the
physical device and passes control to the I/O scheduler. The I/O scheduler invokes
the I/O initiator to start the I/O operation immediately if no other I/O operations
exist in the IOQ of the device. Control is then passed to the process scheduler,
which returns it to the process that had requested the I/O operation.
   When the await-io module of the physical IOCS is invoked, it determines
the status of the I/O operation from the status flag of the I/O control block. If



572  Part 4  File Systems
                                                    Process
                                                   Start-io           Await-io
                                                   Obtain
                                       IOQ         physical device
                                                   address
                                  I/O  I/O          I/O               I/O                 Process
                           interrupt   completion  scheduler          initiator           scheduler
                                       handler
                                       Error                                     Data
                                       recovery                                  Control
             Figure        14.13  Organization of the physical IOCS.
             the I/O operation is complete, control is immediately returned to the process;
             otherwise, the await-io module makes a system call to block the process. At
             an I/O completion interrupt from a device, an error recovery routine is invoked
             if an I/O error has occurred; otherwise, the status flag in the I/O control block
             describing the current operation on the device is set to "completed," the ECB-
             PCB arrangement of Example 5.4 is used to activate a process (if any) awaiting
             completion of the I/O operation, and the I/O scheduler is invoked. It selects one
             of the I/O operations pending on the device and hands it over to the I/O initiator.
             The I/O initiator initiates the I/O operation and passes control to the process
             scheduler.
             14.5.4 Implementation of Physical IOCS
             Recall from Section 13.1 that the compiler replaces the file processing statements
             in a program with calls on the file system operations open, read, and close.
             As seen in Section 13.8, the file system operation read makes a call on the IOCS
             library module seq-read. seq-read contains code that contributes to efficient
             processing of a file (more about it later in this chapter). This code makes a call on
             the physical IOCS library module start-io to perform device-level I/O. The
             linker links all these modules of the file system, IOCS, and the physical IOCS
             with the compiled program.
             A process representing execution of the linked program makes a call on the
             file system operation open to open a file named alpha. open constructs a
             file control block (FCB) for alpha, i.e., fcbalpha, in the open files table (OFT)
             and returns internal idalpha, which is the offset of the FCB in the OFT (see
             Section 13.9.1). The following actions take place when the process wishes to read
             a record of alpha (see Figure 14.14):
             1. The process calls the file system module read, which invokes the IOCS
             module seq-read with internal idalpha as a parameter.



                                                               Chapter 14  Implementation of File  Operations  573
        ···                        ···                    ···
                                                          <Load>  <reg>,
        seq _read                  start_io                       Ad(<OPN>)
        (internal  idalpha)        (Ad(<OPN>))            <SI>    <int_code>
        ···                        ···
                                                          ···
        File system module         IOCS module                 Physical IOCS library
                read                    seq _read              module start-io
Figure  14.14   Invocation of the  physical IOCS library  module start-io in a process.
2.  When seq-read decides to read a record of alpha, it uses internal idalpha
    to access fcbalpha, obtains the address of fmtalpha and finds the address of
    the disk block that contains the desired record. It now forms an I/O control
    block for the I/O operation and calls start-io with the address of the
    I/O control block as a parameter. The I/O control block is named OPN in
    Figure 14.14.
3.  start-io loads the address of the I/O control block in a general-purpose
    register and executes an SI instruction with an appropriate code to invoke
    the physical IOCS.
I/O Initiation  When invoked through a system call, the physical IOCS obtains
the address of the IOCB from the general-purpose register and performs the
following actions:
1. Sets the status flag field of the IOCB to "in progress."
2. Enters the address of the I/O control block in the IOQ of the physical device.
3. Initiates the I/O operation, if the I/O device is not busy.
4. Returns control to the process.
    To enter the I/O control block address in the correct IOQ, the physical IOCS
extracts the logical device id from the IOCB, and accesses the logical device table
(LDT) of the process to obtain the address of the physical device assigned to the
logical device. It then obtains the address of the IOQ for the physical device from
its entry in the physical device table (PDT) and adds the IOCB address at the
end of the IOQ. The I/O operation can be initiated immediately if there are no
other entries in the IOQ. If other entries exist, presumably one of the previous
I/O operations is in progress, so the I/O operation cannot be initiated now.
    I/O initiation is performed as described in Section 14.4.1. The status flag field
of the I/O control block is used in a manner analogous to the use of IO_FLAG in
Figure 14.11. Address of the I/O control block is stored in the current operation
field of the device's entry in the physical device table.
I/O Completion Handling            The I/O completion handler is implicitly invoked at
the occurrence of an I/O completion interrupt. The interrupt hardware provides
the address of the physical device raising the I/O interrupt. The I/O completion
handler queries the device to obtain an I/O status code describing the cause of
the interrupt. It now performs the following actions: If the I/O operation was



574  Part 4  File Systems
             unsuccessful, it consults the device type field of the PDT entry and invokes an
             appropriate I/O error recovery routine with the address of the I/O control block
             as a parameter. Otherwise, it sets the status flag of the I/O control block to
             "completed" and removes the address of the I/O control block from the IOQ of
             the device. If any I/O operations are pending on the device, it initiates one of
             them through the I/O scheduler (see Section 14.7), and puts the address of its I/O
             control block in the current operation field of the PDT entry. If the process that
             had issued the just-completed I/O operation is blocked awaiting completion of
             the I/O operation, it changes the state of the process to ready. The arrangement
             used for this purpose is described in the following.
             Awaiting      Completion  of  an  I/O  Operation  A   process  invokes  this  function
             through the physical IOCS library call await-io (<IOCB_address>) where
             the I/O control block describes the awaited I/O operation. The physical IOCS
             merely tests the status flag in the I/O control block, and returns control to the
             process if the flag value is "completed." If not, the physical IOCS library rou-
             tine makes a "block me" system call to block itself on the event "successful I/O
             completion." The kernel creates an event control block (ECB) for the I/O com-
             pletion event and enters it in the list of event control blocks. This event control
             block contains the id of the process waiting for completion of the I/O operation.
             When the I/O completion event occurs, the I/O completion handler locates its
             event control block, extracts the id of the process, and marks an appropriate
             change in its state. This arrangement ensures that the process would be acti-
             vated at the completion of the I/O operation and would return from the call on
             the physical IOCS library routine. (See Example 5.4 for an explanation of this
             arrangement.)
             14.6          DEVICE DRIVERS                                                           ·
             In the physical IOCS design described in previous sections, the physical IOCS
             handles I/O initiation, I/O completion and error recovery for all classes of I/O
             devices within the system. Consequently, addition of a new class of I/O devices
             requires changes to the physical IOCS, which can be both complex and expensive
             because the physical IOCS may be a part of the kernel. Modern operating sys-
             tems overcome this problem through a different arrangement. The physical IOCS
             provides only generic support for I/O operations, and invokes a specialized device
             driver (DD) module for handling device-level details for a specific class of devices.
             Thus device drivers are not part of the physical IOCS. This arrangement enables
             new classes of I/O devices to be added to the system without having to modify the
             physical IOCS. Device drivers are loaded by the system boot procedure depend-
             ing on the classes of I/O devices connected to the computer. Alternatively, device
             drivers can be loaded whenever needed during operation of the OS. This feature
             is particularly useful for providing a plug-and-play capability.
             Figure 14.15 illustrates how device drivers are used by the physical IOCS.
             The entry of a device in the physical device table (PDT) shows the name of its



                                                              Chapter    14  Implementation  of  File  Operations  575
               Device   DD             IOQ
               address  name           pointer
                        Tape_DD                                          Tape_DD
                        Disk_DD
                  Physical device table         Table of
                        (PDT)                   entry points
               IOQ                                            IO_init:   Disk_DD
                                                              Int_proc:
Figure  14.15  Use of device drivers.
device driver in the DD name field. The Disk_DD, the device driver for the sys-
tem disk, has been loaded at system boot time. The Tape_DD would be loaded
on demand, so it is shown as a dashed box. A device driver contains function-
alities of the four physical IOCS modules shown in Figure 14.13, namely, I/O
scheduler, I/O initiator, I/O completion handler, and error recovery. A table of
entry points located at the start of its code contains start addresses of these
functionalities.
When the physical IOCS is invoked for initiating an I/O operation, it locates
the PDT entry of the device and performs the generic function of entering details
of the I/O operation into the IOQ of the device. It now consults the DD name
field of the PDT entry, obtains the identity of the device driver and loads the
device driver in memory if it is not already in memory. It now obtains the address
of the entry point for I/O initiator in the device driver by following the standard
conventions and passes control to it. The device driver performs I/O initiation
processing and returns control to the physical IOCS, which passes control to
the process scheduler. When the physical IOCS is invoked implicitly at an I/O
interrupt, it performs similar actions to identify the device driver entry point for
handling interrupts and passes control to it. After servicing the interrupt, the
device driver returns control to the physical IOCS, which passes it to the process
scheduler.
Device-Level Optimization        One important optimization is disk scheduling to
ensure good throughput, which is discussed in the next section. Another opti-
mization is reducing the number of seek operations in a disk. This optimization
can be performed in various ways. One simple way is to read several adjoining
disk blocks when a read operation is to be performed. It amounts to buffer-
ing of data, which is useful in sequential files. Device drivers for RAID units
reduce the number of seek operations by combining several I/O operations into a
single one.
A device driver can also support a novel or nonstandard I/O device. A good
example of the former is a RAM disk, which is simply a virtual disk maintained in
the RAM of a computer system: An area in RAM is reserved for use as a disk. All
read and write operations directed at the disk are actually performed on relevant
parts of the RAM. Operation of the RAM disk is extremely fast. However, data



576  Part 4  File Systems
             stored in it is lost if the system crashes or if the RAM disk is abolished. For this
             reason, only scratch files of compilers and processes are typically created in a
             RAM disk. Files intended for storage of data over a period of time are stored on
             conventional disk devices.
             14.7          DISK SCHEDULING                                                           ·
             The seek time of a disk block depends on its position relative to the current posi-
             tion of the disk heads. Consequently, the total seek time involved in performing a
             set of I/O operations depends on the order in which the operations are performed.
             The throughput of a disk defined as the number of I/O operations performed per
             second, also depends on the order in which I/O operations are performed. Hence
             the physical IOCS and device drivers for disks employ a disk scheduling policy
             to perform disk I/O operations in a suitable order. We shall discuss the following
             disk scheduling policies before describing disk scheduling in modern systems:
             ·  First-come, first-served (FCFS) scheduling: Select the I/O operation that was
                requested earliest.
             ·  Shortest seek time first (SSTF) scheduling: Select the I/O operation whose
                seek time from the current position of disk heads is the shortest.
             ·  SCAN scheduling: This policy moves the disk heads from one end of the
                platter to the other, servicing I/O operations for blocks on each track or
                cylinder before moving on to the next one. It is called a scan. When the
                disk heads reach the other end of the platter, they are moved in the reverse
                direction and newly arrived requests are processed in a reverse scan. A variant
                called look scheduling reverses the direction of disk heads when no more I/O
                operations can be serviced in the current direction. It is also called the elevator
                algorithm.
             ·  Circular SCAN or CSCAN scheduling: This policy performs a scan as in
                SCAN scheduling. However, it never performs a reverse scan; instead, it
                moves the heads back to that end of the platter from where they started
                and initiates another scan. The circular look variant (we will call it C-look
                scheduling) moves the heads only as far as needed to service the last I/O
                operation in a scan before starting another scan.
             The FCFS disk scheduling policy is easy to implement but does not guarantee
             good disk throughput. To implement the shortest seek time first (SSTF) policy,
             the physical IOCS uses a model of the disk to compute the seek time of the
             disk block involved in an I/O operation given the current position of the disk
             heads. However, the SSTF policy is analogous to the shortest request next (SRN)
             scheduling policy, so while it achieves good disk throughput, it may starve some
             I/O requests. SSTF and the various scan policies can be efficiently implemented
             if the IOQs are maintained in sorted order by track number.
                Example 14.3 describes the operation of various disk scheduling policies for
             a set of five I/O operations. The look policy completes all I/O operations of this



                                Chapter 14            Implementation of File            Operations        577
example in the shortest amount of time. However, none of these policies is a clear
winner in practice because the pattern of disk accesses cannot be predicted.
                                                                                                          ·
Disk Scheduling Policies                                                                Example     14.3
Figure 14.16 summarizes the performance of the FCFS, SSTF, Look, and C-
Look disk scheduling policies for five I/O operations on a hypothetical disk
having 200 tracks. The requests are made at different instants of time. It is
assumed that the previous I/O operation completes when the system clock
reads 160 ms. The time required for the disk heads to move from track1
to track2 is assumed to be a linear function of the difference between their
positions:
            thm = tconst + | track1 - track2 | × tpt
where tconst is a constant, tpt is the per-track head movement time and thm
is the total head movement time. We assume the rotational latency and data
transfer times to be negligible, tconst = 0 ms and tpt = 1 ms. A practical value
of tconst is 2 ms. Also, the formula for thm is not linear in practice.
Figure 14.16 shows the following details for each decision: time at which
the decision is made, pending requests and head position at that time, the
scheduled I/O operation, and its seek time. The last column shows the total
seek time for each policy. The plots in the lower half of the figure show the
disk head movement for each policy. Note that the total seek times in different
scheduling policies vary greatly. SSTF is better than FCFS; however look has
the smallest total seek time in this example. It is better than C-Look because
it can reverse the direction of disk-head traversal after completing the I/O
operation on track 100, and service the operations on tracks 75, 40, and 12,
whereas C-Look starts a new scan with the operation on track 12.
                                                                                     ·
Scheduling in the disk itself can surpass scheduling in the physical IOCS
because the disk uses a more precise model that considers the seek time as well
as the rotational latency of a disk block. Hence it can make fine distinctions
between two I/O commands that would appear equivalent to the physical IOCS.
As an example, consider I/O commands that concern disk blocks that are +n
and -n tracks away from the current position of the disk heads. Both commands
have equal seek times; the physical IOCS would have to make a random choice
between them. However, given the current rotational position of the platters and
the position of the required disk block or sector, the disk may find that the block
that is +n tracks away may already be passing under the heads by the time the
heads are positioned on that track. It would mean that the disk block can be read
only in the next revolution of the disk. The disk block that is -n tracks away, on
the other hand, might pass under the heads sometime after the heads have been
positioned on that track. Hence its rotational latency would be smaller than that
of the disk block that is +n tracks away. Such finer distinctions can contribute to
higher throughput of the disk.



578  Part 4  File  Systems
                                   tconst and tpt                      =      0 ms and 1 ms, respectively
                                   Current head position               =      Track 65
                                   Direction of last movement          =      Toward higher numbered tracks
                                   Current clock time                  =      160 ms
                   Requested I/O operations:
                                                Serial number          1      2       3          4     5
                                                Track number           12     85      40     100    75
                                                Time of arrival        65     80   110       120    175
                   Scheduling details:
                                                                           Scheduling decisions                               Seek
                   Policy              Details               1             2              3         4          5          time
                   FCFS        Time of decision              160           213          286         331        391
                               Pending requests         1, 2, 3, 4     2, 3, 4, 5     3, 4, 5       4, 5           5
                               Head position                 65            12             85        40         100
                               Selected request                 1             2              3         4           5
                               Seek time                     53            73             45        60         25         256
                   SSTF        Time of decision              160           180          190         215        275
                               Pending requests         1, 2, 3, 4     1, 3, 4, 5     1, 3, 4       1, 3           1
                               Head position                 65            85             75        100        40
                               Selected request                 2             5              4         3           1
                               Seek time                     20            10             25        60         28         143
                   Look        Time of decision              160           180          195         220        255
                               Pending requests         1, 2, 3, 4     1, 3, 4, 5     1, 3, 5       1, 3           1
                               Head position                 65            85           100         75         40
                               Selected request                 2             4              5         3           1
                               Seek time                     20            15             25        35         28         123
                   C-Look      Time of decision              160           180          195         283        311
                               Pending requests         1, 2, 3, 4     1, 3, 4, 5     1, 3, 5       3, 5           5
                               Head position                 65            85           100         12         40
                               Selected request                 2             4              1         3           5
                               Seek time                     20            15             88        28         35         186
                                           100          100                        100                        100
                                   85                  85                     85                          85
                                           75          75                               75                                75
                        65                         65                         65                          65
                   Track                                        40                           40                           40
                   no.                 40
                               12                                  12                           12                    12
                   Time   160              416     160          303              160      283             160             346
                                   FCFS                         SSTF                         Look                     C-Look
                   Figure 14.16    Disk    scheduling summary using        the FCFS, SSTF, Look,          and C-Look policies.



                                                Chapter 14  Implementation of File Operations  579
Scheduling in SCSI Disks  A SCSI disk can accept up to 32 commands concur-
rently from the physical IOCS. The physical IOCS associates a tag with each I/O
command to indicate how it wants the disk to handle it. The disk stores the com-
mands in a command table and uses their tags while making scheduling decisions.
This feature is called tagged command queuing.
The tag in a command can be of three kinds--simple queue tag, ordered
queue tag, and head-of-queue tag. A simple queue tag in a command indicates
that the command can be reordered to optimize disk throughput. A command
with an ordered queue tag indicates that all commands that were entered in the
queue earlier should be scheduled before it is scheduled. Such a command should
be issued periodically to ensure that I/O operations do not starve, i.e., do not
remain indefinitely in the command table. A command with a head-of-queue tag
should be performed immediately by the disk; i.e., it should be performed ahead
of any other command. This feature may be used to ensure that file data are
written to the disk before metadata (see the discussion of journaling file systems
in Section 13.12).
Scheduling in the disk also has its drawbacks. Since the disk treats all I/O
operations uniformly, it might interfere with file-level optimizations performed
by access method modules. Consider processing of a sequential file through a
few buffers, which we discuss later in Section 14.8. When the file is opened, the
access method layer issues commands to read the first few records of the file in its
buffers. To exploit the advantages of buffering, these read commands should be
performed in the order in which they are issued. However, the disk might reorder
them on the basis of their seek and rotational latencies. Hence a later record of
the file may be read in while a process waits to access an earlier record! When a
disk is used for both paging and user files, the OS may wish to perform paging
operations at a higher priority. Scheduling in the disk may interfere with this
requirement.
These drawbacks of disk scheduling lead to the obvious question--should
disk scheduling be performed in the disk, in the physical IOCS or in both? Use
of a more precise model to compute seek and rotational latencies indicates that
scheduling should be performed in the disk. Command ordering requirements
to support file-level access optimization imply that scheduling should also be
performed in the physical IOCS. An OS designer has to use the tagged command
queuing features to ensure that these schedulers work harmoniously.
14.8  BUFFERING OF RECORDS                                                                     ·
To process the records in a sequential file using the physical IOCS, a process
initiates a read operation on a record by invoking the start-io module and
immediately invokes the await-io module to check whether the read operation
has completed. The await-io module blocks the process until the I/O oper-
ation completes (see Section 14.5.4). Thus the process suffers a wait time for
each record, which affects its performance. An access method for sequential files
reduces the wait times faced by a process through the technique of buffering of



580  Part 4  File Systems
             records, which tries to overlap the I/O and CPU activities in the process. It is
             achieved through two means:
             · Prefetching an input record into an I/O buffer, or
             · Postwriting an output record from an I/O buffer
             where an I/O buffer, or simply a buffer, is a memory area that is temporarily used
             to hold the data involved in an I/O operation.
             In prefetching, the I/O operation to read the next record into a buffer is
             started sometime before the record is actually needed by the process--it may
             be started while the process is engaged in processing the previous record. This
             arrangement overlaps a part of the time spent in reading the next record with
             processing of the previous record, which reduces the wait time for the next record.
             In postwriting, the record to be written is simply copied into a buffer when the
             process issues a write operation and the process is allowed to continue. Actual
             writing is performed from the buffer sometime later. It can overlap with (a part
             of) processing of the next record.
             We use the following notation while discussing the technique of buffering:
             tio           I/O time per record [see Eq. (14.1)]
             tc            copying time per record (i.e., the amount of CPU time required
                           to copy a record from one memory area to another)
             tp            processing time per record (i.e., the amount of CPU time con-
                           sumed by the process in processing a record)
             tw            wait time per record (i.e., the amount of time for which the process
                           has to wait before the next record is available for processing)
             tee           effective elapsed time per record (i.e., the interval between the
                           time when a process wishes to start processing a record and the
                           time when the processing of the record is completed)
             tw and tee are analogously defined for an output file.
             Consider a program that reads and processes 100 records from a sequential
             file F. We consider three versions of the program named Unbuf_P, Single_buf_P,
             and Multi_buf_P that use zero, one, and n buffers, n > 1, respectively. We assume
             tio = 75 ms, tp = 50 ms and tc = 5 ms.
             Figure 14.17 illustrates the operation and performance of processes that rep-
             resent executions of Unbuf_P, Single_buf_P, and Multi_buf_P. For convenience,
             we assume a process to have the same name as the program it executes. Each
             column of the figure shows the code of a program, illustrates the steps involved in
             reading and processing a record and shows a timing chart depicting performance
             of the process executing it. The statements "start an I/O operation" and "await
             I/O completion" in the programs are translated into calls on the physical IOCS
             modules start-io and await-io with appropriate operands. The start I/O
             statement reads the next record of F, if any, into a memory area. If there are no
             more records in F, the end_of_file condition is set when an await I/O statement
             is executed. Unbuf_P uses a single area of memory named Rec_area to read and
             process a record of file F [see Figure 14.17(a)]. It issues a read operation and



                                                                  Chapter 14  Implementation of File Operations      581
Programs
Program Unbuf_P                          Program Single_buf_P                           Program Multi_buf_P
start an I/O operation for            start an I/O operation for                    for i := 1 to n
     read (F, Rec_area);                             read (F, Buffer);                   start an I/O operation
await I/O completion;                 await I/O completion;                                   for read (F, Bufi);
while (not end_of_file(F ))           while (not end_of_file(F ))                   await I/O completion on
begin                                 begin                                                   Buf 1;
     process Rec_area;                     copy Buffer into Rec_area;               k := 1;
     start an I/O operation for            start an I/O operation for               while (not end_of_file(F ))
          read (F, Rec_area);                        read (F, Buffer);                   copy Bufk into Rec_area;
     await I/O completion;                 process Rec_area;                             start an I/O operation for
end                                        await I/O completion;                              read (F, Buf k);
                                      end                                                process Rec_area;
                                                                                         k := (k mod n) +1;
                                                                                         await I/O completion on
                                                                                              Buf k;
                                                                                    end
I/O, Copying, and Processing activites (UP:Unbuf_P, SP:Single_buf_P, MP:Multi_buf_P)
        Rec_area                         Buffer                               Buf1
                     UP
        Rec_area                         Buffer  Rec_area                     Buf2
        Rec_area                                                              Buf1  Rec_area
                                         Buffer                                                  MP
                     UP                                       SP                    Rec_area
        Rec_area                                 Rec_area
                                         Buffer  Rec_area
                                                                              Buf1
                                         Buffer                               Buf1  Rec_area
                                                              SP                    Rec_area     MP
                                                 Rec_area
Timing Diagrams (I: I/O operation, C: Copying, P: Processing)
                                                                        I
                                                                        C
                                                                        Buf1        75   150
                                 I                                      I
                                 C                                      C
                                 Buffer          75  155                Buf2        75   150
     I                           C                                      C
     P                           P                                      P
Rec_area       75    125  200    Rec_area        75  130 160   210      Rec_area    80  130 155  205
(a)                              (b)                                    (c)
Figure  14.17  Unbuffered and buffered file processing. (Note: the end_of_file condition is set
when the statement await I/O completion is executed for an operation that tried to read past
the end of a file.)



582  Part 4  File Systems
             awaits its completion before processing the record in Rec_area itself. The timing
             diagram shows that I/O is performed on Rec_area from 0 to 75 ms, and CPU
             processing of the record held in Rec_area occurs between 75 ms and 125 ms.
             Hence tw = tio and tee = tio + tp. This sequence of operations repeats 100 times,
             hence the elapsed time of the process is 100 × (75 + 50) ms = 12.5 seconds.
             Figure 14.17(b) illustrates operation of Single_buf_P, which uses a single
             buffer area named Buffer. The process issues a read operation to read the first
             record into Buffer and awaits its completion. It now enters the main loop of the
             program, which repeats the following four-step procedure 99 times:
             1. Copy the record from Buffer into Rec_area.
             2. Initiate an I/O operation on Buffer.
             3. Process the record held in Rec_area.
             4. Await end of I/O operation on Buffer.
             As shown in the timing diagram of Figure 14.17(b), the process faces an
             I/O wait in Step 1 until the read operation on Buffer completes. It now per-
             forms Steps 2­4. Hence after copying the record into Rec_area, it initiates a read
             operation for the second record and starts processing the first record. These two
             activities proceed in parallel, thus overlapping processing of the first record with
             I/O for the second record. We depict this parallelism by drawing a rectangular
             box to enclose these two actions in the activities part of Figure 14.17(b). Step 1,
             i.e., copying of the next record from Buffer to Rec_area, is performed only after
             both, reading of the next record and processing of the current record, complete.
             It is once again followed by processing of a record and reading of the next record
             in parallel. Hence, the wait time before processing each of records 2­99 is
                           tw = (tio - tp) + tc, if tio > tp                              (14.3)
                           = tc,                               if tio  tp
             and so buffering is more effective when tio  tp.
             For records 2­99, effective elapsed time per record (tee) is given by
                           tee = tw + tp
                           = tc + max (tio, tp)                                           (14.4)
             Thus the process goes through three distinct phases--the start-up phase when
             the first record is read, the steady state when a record is copied and processed
             while the next record is read in parallel, and the final phase when the last record
             is copied and processed. Accordingly, the total elapsed time of the process is
             given by
             Total elapsed time = tio + (number of records - 1) × tee + (tc + tp)         (14.5)
             From Eqs. (14.4) and (14.5), tee is 80 ms and total elapsed time of the process is
             75 + 99 × 80 + 55 ms = 8.05 seconds. If tio had been 50 ms, the total elapsed time
             of the process would have been 5.55 seconds.
             Figure 14.17(c) illustrates operation of the process Multi_buf_P, which
             uses buffer areas named Buf 1, Buf 2, . . . , Buf n. At the start of file processing,



                                                   Chapter 14  Implementation of File Operations  583
Multi_buf_P initiates I/O operations on all n buffers. Inside the file processing
loop, it uses the buffers in turn, following the four steps of the program loop
for processing a record in a buffer. The statement k := (k mod n) +1; ensures
that the buffers are used in a cyclic manner. The process waits for I/O to com-
plete on the next buffer, copies the record from the buffer into Rec_area, invokes
start-io for reading the next record in the buffer, and then processes the record
in Rec_area.
Presence of multiple buffers causes one significant difference between oper-
ations of Multi_buf_P and Single_buf_P. Consider processing of the first two
records by Multi_buf_P [see Figure 14.17(c)]. When I/O on Buf 1 completes,
Multi_buf_P would copy the first record from Buf 1 into Rec_area and start pro-
cessing it. A read operation on Buf 2 would have been requested earlier, so the
physical IOCS would initiate this read operation when the I/O on Buf 1 completes.
Hence this operation would overlap with the copying out of the first record from
Buf 1. In Figure 14.17(c), we depict this parallelism as follows: The dashed rect-
angular box around copying and processing of the record from Buf 1 is meant
to indicate that these actions are performed sequentially. The rectangular box
enclosing this box and the I/O operation on Buf 2 indicates that these two activi-
ties are performed in parallel. Accordingly, the effective elapsed time per record
is given by
                         tw = tio - tp  if tio > tc + tp       (14.6)
                         = tc,          if tio  tc + tp
                         tee = max(tio, tc + tp)               (14.7)
From Eq. (14.7), tee = 75 ms. The total elapsed time, which is governed by
Eq. (14.5), is 75 + 99 × 75 + 55 ms = 7.555 seconds, which is marginally better
than Single_Buf_P's elapsed time of 8.05 seconds.
The ratio of the elapsed times of Unbuf_P and Multi_buf_P is the speedup
factor due to use of multiple buffers. Considering the steady state, the speedup
factor is approximately                 tio + tp
                         max (tio, tc + tp)
From Eq. (14.7), it can be seen that its best value is obtained when tio = tc + tp.
This value has the upper bound of 2.
Consider the operation of Multi_Buf_P when more than one buffer is used.
Figure 14.18 illustrates a typical situation during execution of Multi_Buf_P. The
CPU has recently copied the record from Buf i-1 into Rec_area and started an
I/O operation on Buf i-1. Thus, I/O operations have been initiated on all n buffers.
Some of the I/O operations, specifically, those on Bufi, . . . , Bufj-1, are already
complete. I/O is currently in progress for Bufj , while Bufj+1, . . ., Bufn, Buf1, . . . ,
Bufi-1 are currently in the queue for I/O initiation. Thus (j - i) buffers are full at
the moment, I/O is in progress for one buffer, and (n - j + i - 1) buffers are in the
queue for I/O.
The value of (j - i) depends on the values of tio and tp. If tio < tp, i.e., if
the I/O operation for a record requires less time than its processing, we can see



584  Part 4  File Systems
                                               n-1          n  1
                                                                    2
                                     I/O    j
                                                                          i-1
                                                                       i
                                                               i+1
                                                                               Empty buffer
                                                                               Full buffer
                   Figure  14.18  Use of buffers in Buf_P.
                   that buffers Bufi+1, . . . , Bufn, Buf1, . . . , Bufi-2 will be full, and Bufi-1 will be
                   either under I/O or full when the CPU is processing the record copied out of
                   Buf i-1. If tio > tp, the steady-state situation will be that Bufi is under I/O when
                   the CPU is processing the record copied out of Bufi-1 and buffers Bufi+1, . . . ,
                   Bufn, Buf1, . . . , Bufi-1 are empty.
                      Use of multiple buffers is irrelevant if a process manipulates each record
                   individually. However, it makes a significant difference if a process manipulates
                   many records together. Using n buffers helps in such a case because many buffers
                   may be full when the process needs a few records together. The next example
                   illustrates this point.
·
     Example 14.4  Use of Multiple Buffers
                   Each line of a program written in language L is stored in a record of file F. The
                   compiler of L used to compile this program needs to read an entire statement
                   into memory before starting its processing. A statement may contain up to
                   l lines. The I/O wait for the compiler can be eliminated only if the following
                   conditions hold:
                      1. tio  tpl, and
                      2. l  n
                   where tpl is the average processing time for each line of a statement. Condition
                   1 ensures that in the steady state, all buffers will be full when the compiler
                   finishes processing one statement. Condition 2 ensures that at least l buffers
                   are full when the compiler finishes processing a statement. Hence the compiler
                   will not face I/O waits. It would face I/O waits if l > n, e.g., if l = 3 and it used
                   two buffers.
                   ·
                   14.9    BLOCKING OF RECORDS                                                               ·
                   In unbuffered processing of a file by a process, the time spent in performing
                   I/O operations may dominate the elapsed time of the process. Even in buffered



                                                                      Chapter 14  Implementation of File Operations  585
processing     of a file, tw     >  0  if tio  >  tp,     or  tio  >  tc + tp  [see Eqs. (14.3) and
(14.6)]. Thus both unbuffered and buffered processing of files would benefit from
a reduction in tio. The technique of blocking of records reduces the effective I/O
time per record by reading or writing many records in a single I/O operation.
From Eq. (14.1), tio = ta + tx. Hence, a program that processes two records from
a file that does not employ blocking would incur the total I/O time of 2 ×(ta +tx).
If blocking is employed and a process reads or writes two records in a single I/O
operation, the total I/O time would reduce to ta + 2 × tx.
Logical and Physical Records           When several records are read or written together,
it is necessary to differentiate between how file data is accessed and processed in
a process, and how it is written on an I/O device. A logical record is the unit of
file data for accessing and processing in a process. A physical record, also called a
block, is the unit of data for transfer to or from an I/O device. The blocking factor
of a file is the number of logical records in one physical record. A file is said to
employ blocking of records if the blocking factor is greater than 1. Figure 14.19
shows a file that uses a blocking factor of 3. Note that when blocking is employed,
interrecord gaps on the I/O media separate physical records, i.e., blocks, rather
than logical records.
Deblocking Actions     A read operation on a file containing blocked records trans-
fers m logical records to memory, where m is the blocking factor. Actions for
extracting a logical record from a block for use in a process are collectively called
deblocking actions.
Figure 14.20 shows a program that manipulates a file with blocked records in
an unbuffered manner. The main loop of the program reads one physical record
in each iteration. It contains an inner loop that extracts logical records from a
physical record and processes them. Thus, an I/O operation is initiated only after
                    Interrecord                                            Interrecord
                       gap                                                 gap
                                    Logical      Logical      Logical
                                    record        record           record
                                       Physical record (i.e., block)
Figure  14.19  A file with blocking factor = 3.
               start an I/O operation for read (F, Rec_area);
               await I/O completion;
               while (not end_of_file(F ))
                    for i := 1 to m
                       { extract i th record in Rec_area and process it }
                    start an I/O operation for read (F, Rec_area);
                    await I/O completion;
               end
Figure  14.20  Processing of a file with blocked records in an unbuffered manner.



586  Part 4  File Systems
                   m records are processed. A similar logic can be incorporated into the programs of
                   Figures 14.17(b), (c) to achieve buffered processing of a file containing blocked
                   records.
                   Choice of Blocking Factor         Generalizing on the previous discussion,we can say
                   that if slr and spr represent the size of a logical and a physical record, respectively,
                   spr  =   m × slr.    The    I/O  time  per  physical     record,     (tio )pr ,  and   the  I/O      time  per
                   logical record, (tio)lr, are given by
                                                          (tio)pr = ta + m × tx                                         (14.8)
                                                          (tio)lr  =   ta   +  tx                                       (14.9)
                                                                       m
                        Thus blocking reduces the effective I/O time per logical record, which would
                   benefit   both    buffered  and   unbuffered        processing       of  a  file.  If  tx  <  tp,    with  an
                   appropriate choice of m it is possible to reduce (tio)lr such that (tio)lr  tp. Once
                   it is achieved, from Eqs. (14.3) and (14.6) it follows that buffering can be used
                   to reduce the wait time per record to tc. The next example illustrates how (tio)lr
                   varies with the blocking factor.
·
     Example 14.5  Blocking of Records
                   Table 14.5 shows the variation of (tio)lr with m for a disk device with ta = 10 ms,
                   transfer rate of 800 kB/s, where 1 kB/s = 1000 bytes per second, and slr = 200
                                                                                   200
                   bytes.  tx,  the  transfer  time  per  logical  record,     is       ms,  i.e.,  0.25  ms.  (tio)pr  and
                                                                                   800
                   (tio)lr are computed according to Eqs. (14.8) and (14.9). If tp = 3 ms, m  4
                   makes (tio)lr < tp.
                   ·
                        The value of m is bounded on the lower side by the desire to make (tio)lr  tp.
                   On the higher side, it is bounded by the memory commitment for file buffers, and
                   the size of a disk track or sector. A practical value of the blocking factor is the
                   smallest value of m that makes (tio)lr  tp. The next example illustrates processing
                   of a file employing both blocking and buffering of records.
                            Table 14.5         Variation       of  (tio)lr  with Blocking Factor
                                Blocking                           ta          m × tx          (tio)pr         (tio)lr
                             factor (m)        Block size          ms              ms          ms              ms
                                     1               200           10          0.25            10.25           10.25
                                     2               400           10          0.50            10.50           5.25
                                     3               600           10          0.75            10.75           3.58
                                     4               800           10          1.00            11.00           2.75



                                                                  Chapter 14      Implementation of File  Operations        587
                                                                      Records            Records
       Buf1                                                           from               from
                                                    Rec_area          Buf1               Buf2
       Buf2                                                       1   2  3     4      1  2  3  4
              0  11        2223            3435               0   11              23              35
                 I/O       activities                             Processing activity
Figure 14.21     Buffered  processing  of  blocked  records with  blocking factor = 4 and
two buffers.
                                                                                                                            ·
   Buffered Processing of a File Containing Blocked Records                                               Example     14.6
   Figure 14.21 shows the timing chart of processing the blocked file of Exam-
   ple 14.5 with a blocking factor of 4, using two buffers Buf 1 and Buf 2. We
   assume tc to be negligible. When the file is opened at time 0 second, read oper-
   ations are initiated on Buf 1 and Buf 2. The operation on Buf 1 completes at
   t = 11 ms. The process copies out one logical record from Buf 1 at a time and
   processes it. tp = 3 ms, so processing of the four records of Buf 1 consumes 12
   ms. This processing overlaps with the read operation on Buf 2, which consumes
   11 ms. Hence the next physical record of the file has been read into Buf 2 before
   processing of records in Buf 1 is completed. The process starts processing of
   the logical records copied from Buf 2 at t = 23 ms. Thus, it does not suffer any
   waits after the start-up phase.
                                                                                                      ·
14.10  ACCESS METHODS                                                                                                       ·
As mentioned in Section 13.3.4, an access method provides support for efficient
processing of a class of files that use a specific file organization. For the funda-
mental file organizations discussed in Section 13.3, the IOCS may provide access
methods for the following kinds of processing:
·  Unbuffered processing of sequential-access files
·  Buffered processing of sequential-access files
·  Processing of direct-access files
·  Unbuffered processing of index sequential-access files
·  Buffered processing of index sequential-access files
Access methods for buffered processing of sequential-access and index sequential-
access files incorporate the buffering technique illustrated in Figure 14.17(c).
These access methods also optionally perform blocking of records, using the
technique shown in Figure 14.20.
   We assume that each access method module provides three entry points with
the following parameters:
1. AM-open (<internal_id>)
2. AM-close (<internal_id>)
3. AM-read/write (<internal_id>, <record_info>, <I/O_area addr>)



588  Part 4  File  Systems
                   AM-open (internal idalpha);                       1.  Allocate buffers, set their addresses in
                                                                         fcbalpha.
                                                                     2.  Issue start-io calls on all buffers.
                   AM-read (internal idalpha,                        1.  Issue await-io for the I/O operation
                   < Rec_area      address >);                           on the next buffer.
                                                                     2.  If   blocking  is     employed,  perform
                                                                         deblocking actions. Copy record into
                                                                         Rec_area. If necessary, issue start-io
                                                                         for the next block.
                                                                     3.  Otherwise, copy record into Rec_area
                                                                         and  issue  start-io       for   the  next
                                                                         record.
                   AM-close (internal idalpha);                      1.  Release all buffers.
                            Application program                              Access method actions
                   Figure   14.22  Actions of an access method  for  buffered reading of a file.
                   Modules of the file system and IOCS invoke these functionalities to implement file
                   processing. AM-open is invoked after a file control block has been created for the
                   file, using information from the directory entry of the file. Similarly, AM-close is
                   invoked by iocs-close. AM-read/write are invoked by a file system mod-
                   ule; the entry point AM-read is actually the start of the IOCS library module
                   seq-read of Figure 14.14.
                   Figure 14.22 shows actions of the access method for buffered processing of
                   a sequential-access file alpha. AM-open issues read operations on all buffers.
                   AM-read uses the information in fcbalpha, including fmtalpha, to form a pair
                   (record id, byte id) in Steps 2 and 3 for the next physical record in the file. A few
                   actions of the access method would be different if alpha were an output file.
                   AM-write would be invoked to perform write operations. In steps 2 and 3, it
                   would invoke a module of the file system to allocate more disk space to alpha
                   and enter its address into fmtalpha.
                   14.11    DISK AND FILE CACHES                                                                     ·
                   A generic technique of speeding up access to file data is to use a memory hierarchy
                   consisting of a part of memory and files stored on a disk. Recall from the principles
                   of memory hierarchies discussed in Section 2.2.3 that memory would contain
                   some parts of the file data stored on the disk; other parts would be loaded in
                   memory when required. In essence, memory would function as a cache between
                   files on the disk and processes. Both physical IOCS and access methods use this



                                                Chapter 14  Implementation of File Operations  589
principle. The physical IOCS uses a disk cache, which treats all files stored on
a disk uniformly and holds some data of some files in memory at any time. An
access method, on the other hand, uses a file cache, which focuses on keeping
some part of the data in a specific file in memory. The access method maintains
a separate file cache for each file.
   The unit of data kept in a disk or file cache is typically a few consecutive
disk blocks; for simplicity we assume it to be a single disk block. We will call the
memory area used to store a unit of data a buffer. The cache is thus a collection
of buffers managed in the software. Each buffer has two parts--the header part
indicates what data is contained in it, and the data part actually contains data.
The header contains the following information:
·  Address of the disk blocks from where data has been loaded in the buffer
·  A dirty flag
·  Information needed for performing replacement of data in the buffer, such as
   the time of last reference made to it
   When a process issues a read operation, it specifies the offset of the required
data in the file. The IOCS determines the address of the disk block that contains
the required data and searches the cache to check whether contents of that disk
block are present in a buffer. If so, the required data is copied from the buffer into
the address space of the process. Otherwise, an I/O operation is initiated to load
the data from the disk block into a buffer in the cache and it is copied into the
address space of the process when the I/O operation completes. When a process
performs a write operation, the IOCS checks whether contents of the disk block
that contains old values of the data are present in a buffer. If so, it copies the
values to be written from address space of the process into the buffer and sets the
dirty flag of the buffer to true. Otherwise, it copies the disk block address and
values of the data to be written into a new buffer and sets its dirty flag to true. In
either case, contents of the buffer would be written on the disk by the procedure
described in the following.
   To facilitate speedy search in the cache, the buffer headers are stored in an
efficient data structure such as a hash table. For example, the hash-with-chaining
organization used in the inverted page table of the virtual memory handler could
be adapted for use in the cache (see Figure 12.10 in Section 12.2.3.1). In this orga-
nization, the address of a disk block whose data is contained in a buffer is hashed
to obtain an entry number in the hash table. All buffers that contain disk blocks
whose addresses hash into the same entry of the hash table are entered into a linked
list, called a chain, and the hash table entry is made to point to the chain. To check
whether data from a disk block is present in the cache, the address of the disk
block is hashed to obtain an entry number in the hash table, and the chain pointed
to by this entry is searched to check whether a copy of the disk block is contained
in one of the buffers. If it is not present in the cache, it is loaded in a free buffer in
the cache and the buffer is added to the chain. If the cache is full, a policy such as
LRU replacement is employed to decide which buffer should be used to load the
required data. If the dirty flag of the buffer is true, its contents would be written in
the disk block whose address is contained in its header before new data is loaded in



590  Part 4  File Systems
             the buffer. Such an arrangement used in the Unix buffer cache is described later in
             Section 14.13.1.2.
             Loading of whole disk blocks, which are a few KB in size, in the cache captures
             spatial locality because data that adjoins previously accessed data would exist in
             the cache. This effect is analogous to blocking of records discussed previously in
             Section 14.9. Studies mentioned in Section 14.13.1.2 indicate that disk cache hit
             ratios of 0.9 or more can be obtained by committing a small amount of memory
             to the disk cache. A file cache can exploit temporal locality further by preloading
             the next few disk blocks of a sequential-access file in the cache, which is analogous
             to buffering of records discussed in Section 14.8.
             Use of a cache has some drawbacks too. An I/O operation involves two copy
             operations, one between the disk and the cache and the other between the cache
             and the address space of the process that initiated the I/O operation. Use of a
             cache also leads to poor reliability of the file system because modified data exists
             in a buffer in the cache until it is written to the disk. This data will be lost in the
             event of a crash.
             File Cache           A file cache is implemented in an access method and aims to provide
             efficient access to data stored in a file. As shown in Figure 14.23(a), the access
             method invokes the cache manager, which checks whether the required data is
             available in the file cache. It invokes the physical IOCS only if the file cache does
             not already contain the required data. The key advantage of a file cache over a
             disk cache is that the cache manager can employ file-level techniques to speed up
             accesses to file data. Such a technique exploits properties of a file's organization
             to speed up data accesses, e.g., it can perform prefetching of data for sequential-
             access files. However, a key disadvantage is that a separate file cache has to be
             implemented for each file, so the IOCS has to decide how much memory to
             commit to each individual file cache.
             Disk Cache           The disk cache is implemented in the physical IOCS or device driver
             of a disk. Its purpose is to speed up accesses to data stored on the disk. As shown
             in Figure 14.23(b), a request for an I/O operation is passed to the I/O scheduler
             only if the required data is not present in the disk cache. The key advantage of a
                           File                                       File
                           system                                     system
                           Access    Cache                   File     Access
                           method    manager                 cache    method
                                                                                Cache      Disk
                           Physical                                   Physical  manager    cache
                           IOCS                                       IOCS      I/O
                                                                                scheduler
                           (a)                                        (b)
             Figure        14.23  (a) File cache; (b)  disk  caches.



                                                                Chapter 14  Implementation of File Operations  591
disk cache over a file cache is that it does not differentiate between files stored on
a disk, so its use benefits all file processing activities in the system. It also does
not have to determine cache size for each file individually. However, the hit ratio
in the disk cache is sensitive to the file access behavior of processes. For example,
if a process reads a large sequential file very rapidly, its data might occupy most
of the buffers in the cache, which will degrade accesses to data in other files. Disk
caches implemented in modern operating systems also incorporate some features
of file caches to enhance hit ratios. Hence a disk cache may prefetch a few disk
blocks in a sequential-access file to reduce wait times in processes.
14.12   UNIFIED DISK CACHE                                                                                     ·
Apart from disk or file caches, the OS also maintains, implicitly or explicitly,
another cache called the page cache in the virtual memory handler. Use of several
caches may increase the number of copy operations that have to be performed
to access data stored on a disk. The time and memory overhead introduced by
multiple copy operations motivates use of a unified disk cache.
     Figure 14.24(a) is a schematic diagram showing use of the disk cache and the
page cache. The page cache contains all code and data pages of processes that
are presently in memory, including pages of any memory-mapped files. A new
page is loaded into the page cache when a page fault occurs. Since the page size
is typically a few disk blocks, this operation involves reading a few blocks from
a program file or a swap file. This is file I/O. Hence the disk blocks get read into
the disk cache, and they have to be copied into the page cache. When a modified
page is to be removed from memory, it is first copied into the disk cache. From
there, it is written to the disk sometime in the future. Thus, two copy operations
are  involved   in  each  page-in  and         page-out  operation--one     copy  operation
between a disk and the disk cache, and another between the disk cache and the
Demand paging,                  Process                                        Process
memory-mapped                            File                                             File
        files                      accesses                                       accesses
        Page cache        File system           Demand paging,                 File system
                                                memory-mapped
                                                         files
                    Disk cache                                        Unified
                                                                disk cache
                Disk                                            Disk
(a)                                             (b)
Figure  14.24  Disk caching: (a) separate disk  and page caches; (b) unified disk cache.



592  Part 4  File Systems
             page cache. After a page-in operation, two copies of the page would be present
             in memory until either of the copies is overwritten.
             Multiple copy operations on pages and duplicate copies of pages cause per-
             formance problems. The amount of memory to be committed to each cache is
             also a difficult design decision; it can affect performance of the system because
             undercommitment of memory to the page cache could lead to either a reduced
             degree of multiprogramming or thrashing, while undercommitment to the disk
             cache would slow down file processing activities because of frequent accesses to
             the disk. Merging the two caches would solve these problems: duplicate copies
             and multiple copy operations would be eliminated, and portions of the cache
             committed to the two uses could be varied to adapt to changes in the system
             workload.
             A unified disk cache is a single cache used for both paging and file I/O.
             Figure 14.24(b) is a schematic diagram of the unified disk cache. The file sys-
             tem looks upon files as paged objects on the disk. It decomposes the byte offset
             provided in a read or write statement into a page number and an offset into a
             page. It passes the page number to the unified disk cache to ensure that the page
             is loaded in memory and uses the offset into the page to copy the data between
             the unified disk cache and the address space of a process. Page I/O continues to
             be handled as in conventional systems because the unified disk cache is really a
             page cache.
             The unified disk cache was introduced in the Sun OS 4.0. Later it was imple-
             mented in the Unix System 5 version 4. The Linux 2.4 kernel and its later versions
             also use a unified disk cache.
             14.13         CASE STUDIES                                                            ·
             14.13.1 Unix
             Unix supports two types of devices--block devices and character devices. Block
             devices are random-access devices that are capable of reading or writing blocks
             of data, such as various kinds of disks, while character devices are serial-access
             devices such as keyboards, printers and mice. A block device can also be used
             as a serial device. Unix files are simply sequences of characters, and so are I/O
             devices, so Unix treats I/O devices as files. Thus a device has a file name, has an
             entry in the directory hierarchy, and is accessed by using the same calls as files,
             viz. open, close, read and write.
             The Unix IOCS consists of two main components--device drivers and a
             buffer cache. These are described in the following sections.
             14.13.1.1 Device Drivers
             A Unix device driver is structured into two parts called the top half and the bottom
             half. The top half consists of routines that initiate I/O operations on a device in
             response to open, close, read, or write calls issued by a process, while the bottom
             half consists of the interrupt handler for the device class serviced by the driver.
             Thus the top half corresponds to the I/O scheduler and I/O initiator modules in



                                                         Chapter 14  Implementation of File Operations  593
Figure 14.13, while the bottom half corresponds to the I/O completion handler
and error recovery modules.
A device driver has an interface consisting of a set of predefined entry points
into the device driver routines. Some of these are:
1. <ddname>_init : Device driver initialization routine
2. <ddname>_read/write : Routines to read or write a character
3. <ddname>_int : Interrupt handler routine
The <ddname>_init routine is called at system boot time. It initializes various
flags used by the device driver. It also checks for the presence of various devices,
sets flags to indicate their presence, and may allocate buffers to them. Character
I/O is performed by invoking the <ddname>_read and <ddname>_write rou-
tines. The device driver has to provide a strategy routine for block data transfers,
which is roughly equivalent to the I/O scheduler shown in Figure 14.13. A call
on the strategy routine takes the address of an I/O control block as a parameter.
The strategy routine adds this I/O control block to an IOQ, and initiates the I/O
operation if possible. If immediate initiation is not possible, the I/O operation is
initiated subsequently when an I/O completion interrupt occurs.
14.13.1.2 Buffer Cache
The buffer cache is a disk cache as described in Section 14.12. It is organized
as a pool of buffers, where each buffer is the same size as a disk block. Each
buffer has a header containing three items of information: A (device address, disk
block address) pair gives the address of the disk block that is present in the buffer,
a status flag indicates whether I/O is in progress for the buffer, and a busy flag
indicates whether some process is currently accessing the contents of the buffer.
A hash table is used to speed up the search for a required disk block (see
Figure 14.25). The hash table consists of a number of buckets, where each bucket
points to a list of buffers. When a disk block with address aaa is loaded into a
buffer with the address bbb, aaa is hashed with function h to compute a bucket
number e = h(aaa) in the hash table. The buffer is now entered in the list of
buffers in the eth bucket. Thus, the list contains all buffers that hold disk blocks
whose addresses hash into the eth bucket.
                          Hash table       Buffers
               Bucket #1              9    25        13
               Bucket #2              6    18
               Bucket #3              11   3             Free list
                                                         pointer
               Bucket #4              4
Figure  14.25  Unix buffer cache.



594  Part 4  File Systems
                          The following procedure is used when a process Pi performs a read operation
                   on some file alpha:
                      1.  Form the pair (device address, disk block address) for the byte required by Pi.
                      2.  Hash disk block address to obtain a bucket number. Search the buffers in the
                          bucket to check whether a buffer has a matching pair in its header.
                      3.  If there is no buffer with a matching header, allocate a free buffer, put the
                          (device address, disk block address) information in its header, enter the buffer
                          in the list of the appropriate bucket, set its status flag to "I/O in progress,"
                          queue the buffer for I/O, and put Pi to sleep on completion of I/O.
                      4.  If a buffer with matching header exists, return to Pi with its address if flags
                          indicate that the I/O operation on the buffer is complete and the buffer is
                          "not busy." Otherwise, put Pi to sleep on completion of a read operation on
                          the buffer or buffer "not busy" condition.
                      5.  If free buffers exist, check whether the next disk block allocated to alpha is
                          already present in a buffer. If not, allocate a free buffer to it and queue it for
                          a read operation.
                   This procedure does not allocate buffers on a per-process basis, so processes
                   that concurrently access a file can share the file data present in a buffer. This
                   arrangement facilitates Unix file sharing semantics (see Section 13.14.1). At the
                   same time, prefetching of data is performed on a per-process basis by initiating
                   an I/O for the next disk block of the file (see Step 5), which provides buffering on
                   a per-process basis. The benefits of blocking of records are inherent in the fact
                   that a complete disk block is read/written when any byte in it is accessed.
                          Buffers in the buffer pool are reused on an LRU basis as follows: All buffers
                   are entered in a free list. A buffer is moved to the end of the list whenever its
                   contents are referenced. Thus the least recently used buffers move toward the
                   head of the free list. In Step 3, the buffer at the head of the free list is allocated
                   unless it contains some modified data that is yet to be written into the disk block.
                   In that case, a write operation for the buffer is queued and the next buffer in the
                   list is allocated.
·
     Example 14.7  Unix Buffer Cache
                   Figure 14.25 illustrates the Unix buffer cache. Disk blocks 9, 25 and 13 hash
                   into the first entry of the hash table; hence they are entered in the linked list
                   starting on this entry. Similarly 6, 18 and 11, 3 form the linked lists starting
                   on the second and third entries of the hash table. All buffers are also entered
                   in the free list. If a process accesses some data residing in disk block 18, the
                   buffer containing block 18 is moved to the end of the free list. If the process
                   now accesses data in disk block 21, the first buffer in the free list, i.e., the buffer
                   containing block 13, is allocated if its contents have not been modified since
                   it was loaded. The buffer is added to an appropriate list in the hash table after
                   block 21 is loaded in it. It is also moved to the end of the free list.
                   ·



                                                    Chapter 14  Implementation of File Operations  595
    The effectiveness of the Unix buffer cache has been extensively studied. A
1989 study reported that a 60 MB cache on an HP system provided a hit ratio of
0.99 and a 16 MB cache on another system provided a hit ratio of 0.9. Thus a
comparatively small memory commitment to the buffer cache can provide a high
hit ratio.
14.13.2 Linux
The organization of Linux IOCS is analogous to that of Unix IOCS. Thus, block-
and character-type I/O devices are supported by individual device drivers, devices
are treated like files, and a buffer cache is used to speed up file processing. However,
many IOCS specifics are different. We list some of them before discussing details
of disk scheduling in Linux 2.6.
1.  Linux kernel modules--which include device drivers--are dynamically load-
    able, so a device driver has to be registered with the kernel when loaded and
    deregistered before being removed from memory.
2.  For devices, the vnode data structure of the virtual file system (VFS) (see
    Section 13.13) contains pointers to device-specific functions for the file
    operations open, close, read, and write.
3.  Each buffer in the disk cache has a buffer header that is allocated in a slab
    of the slab allocator (see Section 11.11).
4.  Dirty buffers in the disk cache are written to the cache when the cache is too
    full, when a buffer has been in the cache for a long time, or when a file directs
    the file system to write out its buffers in the interest of reliability.
    I/O scheduling in Linux 2.6 uses some innovations to improve I/O scheduling
performance. A read operation needs to be issued to the disk when a process makes
a read call and the required data does not already exist in the buffer cache. The
process would get blocked until the read operation is completed. On the other
hand, when a process makes a write call, the data to be written is copied into
a buffer and the actual write operation takes place sometime later. Hence the
process issuing a write call does not get blocked; it can go on to issue more write
calls. Therefore, to provide better response times to processes, the IOCS performs
read operations at a higher priority than write operations.
    The I/O scheduler maintains a list of pending I/O operations and schedules
from this list. When a process makes a read or a write call, the IOCS checks
whether the same operation on some adjoining data is pending. If this check suc-
ceeds, it combines the new operation with the pending operation, which reduces
the number of disk operations and the movement of disk heads, thereby improving
disk throughput.
    Linux 2.6 provides four I/O schedulers. The system administrator can choose
the one that best suits the workload in a specific installation. The no-op scheduler
is simply an FCFS scheduler. The deadline scheduler uses Look scheduling as its
basis but also incorporates a feature to avoid large delays. It implements Look
scheduling by maintaining a scheduling list of requests sorted by track numbers
and selecting a request based on the current position of disk heads. However,



596  Part 4  File Systems
             Look scheduling faces a problem when a process performs a large number of
             write operations in one part of the disk--I/O operations in other parts of the disk
             would be delayed. If a delayed operation is a read, it would cause considerable
             delays in the requesting process. To prevent such delays, the scheduler assigns
             a deadline of 0.5 second to a read operation and a deadline of 5 seconds to a
             write operation, and maintains two queues--one for read requests and one for
             write requests--according to deadlines. It normally schedules requests from the
             scheduling list; however, if the deadline of a request at the head of the read or write
             queue expires, it schedules this request, and a couple of more requests from its
             queue, out of sequence before resuming normal scheduling. The completely fair
             queuing scheduler maintains a separate queue of I/O requests for each process and
             performs round robin between these queues. This approach avoids large delays
             for processes.
             A process that performs synchronous I/O is blocked until its I/O operation
             completes. Such a process typically issues the next I/O operation immediately
             after waking up. When Look scheduling is used, the disk heads would most
             probably have passed over the track that contains the data involved in the next
             I/O operation, so the next I/O operation of the process would get serviced only in
             the next scan of the disk. This causes delays in the process and may cause more
             movement of the disk heads. The anticipatory scheduler addresses this problem.
             After completing an I/O operation, it waits a few milliseconds before issuing the
             next I/O operation. This way, if the process that was activated when the previous
             I/O operation completed issues another I/O operation in close proximity to the
             previous operation that operation may also be serviced in the same scan of the
             disk.
             14.13.3 File Processing in Windows
             The schematic of Figure 14.26 shows the file processing arrangement used in
             Windows. The cache manager performs file caching. The I/O manager provides
             generic services that can be used to implement subsystem-specific I/O operations
             through a set of device drivers, and also performs management of I/O buffers.
             As described in Section 4.8.4, subsystem DLLs linked to a user application invoke
             functions in the I/O manager to obtain subsystem-specific I/O behavior. The VM
             manager was described in Section 12.8.4.
             The file cache is organized as a set of cache blocks, each of size 256 KB.
             The part of a file held in a cache block is called a view. A virtual address control
             block (VACB) describes each view; it contains the virtual address associated with
             the view, the offset of its first byte in the file, and the number of read or write
             operations currently accessing the view. Presence of the virtual address and file
             offset information in the VACB helps in implementing file sharing semantics--it
             ensures that processes making concurrent accesses to a file would see the result of
             the latest update operation irrespective of whether the file was memory-mapped
             or accessed directly. The cache manager sets up a VACB index array for a file
             when the file is opened. For a sequential file, the index array would contain only
             one pointer that points to the VACB covering the current offset into the file. For



                                                       Chapter 14   Implementation of File Operations  597
                                                       User
                                                       application
                                                       DLL
               Cache         I/O request       File
        manager                                system
                                               driver
        Data          Cache                                         Driver
        transfer  flushing                                          stacks
                             Data              Disk
                  VM         transfer          Driver
        manager
                             Page
                             loading                   I/O manager
Figure  14.26     File processing in Windows.
a random file, the VACB index array would contain pointers to VACBs that cover
several recent accesses made to the file.
An I/O operation is performed by a layered device driver. It is represented
as a linked list of device drivers called a driver stack. When a thread requests an
I/O operation, the I/O manager constructs an I/O request packet (IRP) for it and
passes it to the first device driver in the appropriate driver stack. The device driver
may perform the complete I/O operation itself, write a status code in the IRP,
and pass it back to the I/O manager. Alternatively, it may decide on additional
actions required to complete the I/O operation, write their information in the
IRP, and pass the IRP to the next device driver in the stack, and so on, until
the I/O operation actually gets implemented. This model permits device drivers
to be added to provide additional features in the I/O subsystem. For example,
a device driver could be added between the file system driver, which we discuss
in the following, and the disk driver to perform disk mirroring. Such a driver is
called a filter driver. Drivers such as the disk driver are called function drivers.
They contain functionalities for initialization, scheduling, and initiation of I/O
operations; interrupt servicing; and dynamic addition of new devices to facilitate
the plug-and-play capability.
A file system is also implemented as a file system driver (FSD). It invokes
other drivers that implement the functionalities of the access method and the
device drivers. This arrangement permits many file systems to coexist in the host.
The I/O manager thus provides the functionalities of a virtual file system (see
Section 13.13). When a subsystem DLL linked to a thread requests a file oper-
ation, the I/O manager invokes an appropriate file system driver to handle the
request. The request typically contains the byte offset of the file data involved
in the I/O operation. The file system driver consults the file map table for the
concerned file, which is accessible from the file's base file record in the master file
table (MFT), to convert the byte offset within the file into a byte offset within



598  Part 4  File Systems
             a data block on a device and invokes the device driver for it. If the concerned
             device is a RAID, the device driver is actually a volume manager, which manages
             the RAID. It converts the byte offset within a data block into one or more units
             containing a disk number, sector number, and a byte offset and passes the request
             to the disk driver. Windows supports striped volumes, which are level 0 RAID
             systems, mirrored volumes, which are level 1 RAID systems, and level 5 RAID
             systems in this manner. It supports spanned volumes described in section 13.14.4
             analogously.
             When a thread makes a request to read from a file, the I/O manager passes
             this request to the file system driver, which passes it to the cache manager. The
             cache manager consults the VACB index array for the file and determines whether
             the required bytes of the file are a part of some view in the cache. If not, it allocates
             a cache block, creates a view that covers the required bytes from the file in the
             cache block, and constructs a VACB for it. This operation involves reading the
             relevant part of the file into the cache block. The cache manager now copies
             the required data from the cache block into the caller's address space. Converse
             actions are performed at a write request. If a page fault arises while copying data
             to or from the caller's address space, the virtual memory manager invokes the
             disk driver through the file system to read the required page into the memory.
             This operation is performed in a noncached manner. Thus, a file system must
             support both cached and noncached file I/O. To facilitate efficient manipulation
             of metadata, the file system driver uses kernel-level read/write operations, which
             access the data directly in the cache instead of first copying it to/from the logical
             address space of the file system driver.
             The cache manager keeps information about the last few I/O requests on
             a file. If it can detect a pattern from them, such as sequential accesses to the
             file, it prefetches the next few data blocks according to this pattern. It also
             accepts hints from user applications concerning the nature of file processing
             activities and uses them for the same purpose. File updates take place in an
             asynchronous manner. The data to be written into a file is reflected into the view
             of the file held in the cache manager. Once every second, the lazy writer, which
             is a system thread created by the cache manager, queues one-quarter of the dirty
             pages in the cache for writing on a disk and nudges the virtual memory manager
             to write out the data.
             Recall that an OS typically finds out the devices connected to it at boot time
             and builds its device data structures accordingly. This arrangement is restrictive,
             as it requires rebooting of the system when a device is to be connected to it or
             disconnected from it. Windows supports a plug-and-play (PnP) capability which
             permits devices to be connected and disconnected to the system dynamically. It is
             achieved by coordinating the operation of I/O hardware, the operating system and
             the concerned device driver. The hardware cooperates with the boot software to
             construct the initial list of devices connected to the system, and also coordinates
             with the PnP manager when devices are added or disconnected. The PnP manager
             loads a device driver for a new device if necessary, determines the resources such
             as specific interrupt numbers that may be required for its operation, and ensures
             the absence of conflicts by assigning or reassigning required resources. It now



                                                              Chapter 14  Implementation of File Operations  599
initializes the device driver for the new device and reinitializes any other devices
that may have been affected by the reorganization.
     Windows Vista has a new feature that addresses a problem area in disk
scheduling: Disk scheduling treats all I/O operations uniformly while trying to
improve the throughput of a disk. Thus, occasionally I/O operations of low-
priority processes may be favored over other I/O operations, which would cause
delays in high-priority processes and degrade responsiveness of applications that
spawned them. The Vista feature called prioritized I/O provides a method of
striking the correct balance between throughput and responsiveness. Using this
feature, an application can specify a priority for its I/O operations. It can make
a system call to lower its I/O priority to background so that its I/O operations
would not have priority over those of nonbackground processes, and can revert
to its original priority through another system call when desired. A device driver
checks the priority of the process that issued an I/O operation and sets flags in
an IRP to indicate whether the I/O operation should be treated at a low priority.
14.14     SUMMARY                                                                                                     ·
During a file processing activity, the file system            I/O operation has completed, which enables the
implements sharing and protection of files, while             kernel to switch the CPU to another process. It
the input output control system (IOCS) actually               uses disk scheduling to perform the I/O operations
implements  file  operations.     The   IOCS   is  itself     directed at a disk in an order that would reduce the
structured into two layers called access methods              movement of read­write heads of the disk, which
and physical IOCS that ensure good performance                increases the throughput of the disk and reduces
of a file processing activity, and good throughput            the average wait time of I/O operations.
of I/O devices, respectively. In this chapter, we stud-       An access method improves the performance of
ied the techniques employed by the access methods             a file processing activity within a process through
and the physical IOCS.                                        the techniques of buffering and blocking of records.
     Good throughput of an I/O device is achieved             A buffer is a memory area used to temporarily hold
through joint actions of the I/O device and the               data that has been read off a device or that is to be
IOCS.  The  I/O   device  is  designed   such      that  it   written on it. For an input file, the technique of
is reliable, and I/O operations incur short access            buffering tries to prefetch the data so that it would
time--which is the time spent in positioning the              be available to a process without having to perform
I/O media or the read­write heads prior to data               an I/O operation, which reduces or eliminates the
transfer--and  achieve    high    data   transfer  rates.     wait time. For an output file, it copies the data into
Data staggering techniques, disk attachment tech-             the buffer and lets the process continue its oper-
nologies, and redundant arrays of inexpensive disks           ation; the actual writing is performed later. The
(RAID) are relevant in this context.                          technique of blocking reads more data off a device
     Even with fast access and high data trans-               in a single I/O operation than required by a process
fer rates of I/O devices, a process performing an             at a time; it reduces the number of I/O operations
I/O operation incurs considerable wait time until             to be performed.
the I/O operation completes. The physical IOCS                Caching is the technique of keeping some of
provides  two  basic  capabilities   to  enhance     sys-     the file data in memory, so that it can be accessed
tem  performance:     It  blocks  a  process  until      its  without     having  to  perform  an  I/O       operation.



600           Part 4      File Systems
Caching reduces the number of I/O operations                              The virtual memory handler also uses a cache
performed to access data stored in files, thereby                    called a page cache, which contains pages of pro-
improving performance of file processing activi-                     cesses, to improve virtual memory performance.
ties in processes and also improving performance                     However, since the swap areas of processes are
of the system. The physical IOCS implements a                        implemented on a disk, use of the page cache
disk cache to reduce the number of I/O opera-                        and  the  disk  cache  involves        copying    of   pages
tions performed for accessing the files stored on                    between the two caches, which consumes CPU time
a disk. An access method implements a file cache                     and ties up memory because of multiple copies of a
to reduce the number of I/O operations performed                     page. Operating systems therefore use a unified disk
during the processing of an individual file in a                     cache to reduce copying and eliminate the need for
process.                                                             multiple copies of pages.
TEST  YOUR CONCEPTS                                                                                                             ·
14.1  Classify each of the following statements as true                        ii. Increase the capacity of a disk
      or false:                                                                iii. None of (i)­(ii)
      a. When parity bits are used for reliable record-                   b.   Data staggering techniques are used to
              ing/reading of data, an error correction code                    i. Reduce the number of disk operations
              requires more parity bits than an error detec-                   while a file is processed
              tion code.                                                       ii. Reduce disk head movement between disk
          b. Restricting the disk space allocation for a file                  blocks having adjoining addresses
              to a cylinder group--which is a group of con-                    iii. Reduce  rotational      delays   while      disk
              secutive cylinders on a disk--reduces disk                       blocks       having    adjoining   addresses     are
              head movement while the file is processed.                       accessed.
          c. RAID level 4, which uses block-interleaved                        iv. Improve  effectiveness   of   buffering      and
              parity,   provides     parallelism  between     small            blocking of file records
              I/O operations.                                             c.   Disk scheduling
      d. Blocking of records speeds up processing of                           i. Reduces the number of I/O operations
              sequential files.                                                performed on a disk
          e. Buffering of records speeds up processing of                      ii. Reduces the average disk head movement
              direct-access files.                                             per I/O operation
          f.  The  SCAN        disk  scheduling    policy   suffers            iii. Aims at speeding up processing of a file
              from starvation.                                            d.   A program executes a read statement on a
      g. The physical IOCS provides a method to                                file alpha 100 times during its execution;
              avoid the busy wait condition while a process                    however only 50 I/O operations are actually
              awaits completion of its I/O operation.                          performed to read data from the file alpha.
      h. If tx     <     tp, it is possible to reduce tw      to tc            This is possible if
              through appropriate buffering and blocking.                      i. The   access      method  used    for    the  file
          i. Using a blocking factor of m reduces the                          alpha        employs         buffering    without
              effective   I/O  time  per  logical     record  by  a            blocking
              factor of m.                                                     ii. The  access      method  does    not    employ
14.2  Select       the   correct    alternative   in  each  of  the            blocking and the physical IOCS does not
      following questions:                                                     employ a disk cache
      a. A disk cache is used to                                               iii. Either the access method employs block-
              i. Reduce the number of I/O operations on                        ing or the physical IOCS employs a disk
              a disk                                                           cache



                                                            Chapter 14  Implementation of File Operations        601
EXERCISES                                                                                                                  ·
14.1  Explain how (and whether) buffering and block-               algorithm on the conventional disk described in
      ing of records is beneficial for the following kinds         Example 14.3?
      of files:                                             14.8   A process manipulates a sequential-access file.
      a. A sequential-access file                                  The I/O and processing times for each record in
      b. An index sequential-access file                           the file are as follows:
      c. A direct-access file                                           Access time of device       = 10 ms
14.2  An update file is one that is read and modified                   Transfer time per record    = 6 ms
      during processing--a program reads a record,                      Largest number of records = 5 records
      modifies it in memory, and writes it back into                    required together
      the file.                                                         Processing time per record = 10 ms
      a. Which I/O device is best suited for recording
      an update file?                                                   a. If two buffers are used, find the value of the
      b. Is buffering and blocking of records useful                    smallest blocking factor that can minimize
      for an update file?                                               the wait time per record.
      Justify your answers.                                             b. If two buffers and a blocking factor of 5
14.3  Discuss how the throughput of a disk device                       are used, what is the minimum number of
      can be optimized in a file system that performs                   records that are present in memory at any
      noncontiguous allocation of disk blocks to files.                 time? (Assume that a process initiates an I/O
      (Hint: Think of organization of blocks in the free                operation on a buffer after processing the last
      list, data staggering, and cylinder groups.)                      record in it--see Figure 14.20.)
14.4  A sectored disk has the following characteristics:    14.9   A sequential file is recorded by using blocking.
      Time for 1 revolution = 8 ms                                 A process manipulates it by using two buffers.
      tsect                        = 1 ms                          The I/O and processing times are as follows:
      tst                          = 3 ms                               Access time (average)       = 20 ms
      Sector size                  = 1024 bytes                         Transfer time per record    = 5 ms
      Plot the peak disk throughput against the sector                  Largest number of records = 5 records
      interleaving factor (Fint).                                       required together
14.5  Comment on the effectiveness of (a) a disk cache                  Processing time per record = 10 ms
      and (b) a RAM disk for speeding up processing                Determine optimal values of the blocking fac-
      of sequential-access and direct-access files.                tor and the number of buffers. What changes, if
14.6  Requests for I/O operations on the following                 any, would you make in your design if the largest
      tracks are pending at time = 160 ms.                         number of records that the process is likely to
                 7, 45, 98, 70, 68, 180                            require together is (i) 3 records, (ii) 8 records?
                                                                   (Hint: See Example 14.4.)
      If the requests are made in the above order, con-     14.10  One buffer is used in processing the file info of
      struct a table analogous to Table 14.16 for the              Exercise 13.6. Calculate the elapsed time of the
      disk of Example 14.3.                                        process if the copying time per record is 0.5 ms.
14.7  A biased disk is a hypothetical disk device whose            Explain your calculations.
      seek time for track n is a linear function in n (for  14.11  Classify the following statement as true or false:
      example, seek time = 0.1 × n). {seqi} is the set             "By judicious selection of the blocking factor
      of I/O operations requested over a certain period            and the number of buffers, it is always possible
      of time. Is the order in which I/O operations are            to reduce the wait time to tc."
      scheduled on a biased disk by the SSTF algo-          14.12  A process is expected to open a file before access-
      rithm identical to the order in which the same               ing it. If it tries to access a file without opening,
      I/O operations would be scheduled by a SCAN                  the file system performs an open before



602         Part 4    File Systems
         implementing the access. A system program-                         with the special I/O technique. In Section 14.8,
         mers' handbook warns all programmers to open                       the speedup factor due to buffering was shown
         a file before accessing it or suffer a performance                 to have an upper bound of 2. Develop a for-
         penalty. Explain the nature and causes of the                      mula for speedup factor when a process does
         performance penalty.                                               not use buffers while processing a file containing
14.13    How do different disk scheduling algorithms                        blocked records. Can the value of this speedup
         influence the effectiveness of I/O buffering?                      factor exceed 2? If so, give an example.
14.14    A process manipulates an input file using many              14.18  Develop a formula for speedup factor when a
         buffers. Which of the following statements are                     process uses two buffers while processing a file
         accurate? Explain your reasoning.                                  containing blocked records and tp  tx.
         a. "Of all the disk scheduling algorithms, FCFS             14.19  Describe the implications of a file or disk cache
            disk scheduling is likely to provide the best                   for file system reliability. Unix supports a sys-
            elapsed time performance for the process."                      tem call flush () that forces the kernel to write
         b. "Data     staggering      is  effective  only  during           buffered output onto the disk. Can a program-
            reading of the first few records in the file; it is             mer use flush () to improve the reliability of his
            not effective during reading of other records                   files?
            in the file."                                            14.20  The lseek command of Unix indicates the off-
14.15    A  magnetic       tape  has  a   recording  density     of         set of the next byte in a sequential-access file
         80 bits/cm along a track. The tape moves at                        to be read or written. When a process wishes
         a velocity of 2 meters per second while read-                      to perform a read or write operation, it issues an
         ing/writing data. The inter-record gap is 0.5 cm                   lseek command. This command is followed by
         wide, and the access time of the tape is 5 ms. A                   an actual read or write command.
         sequential file containing 5000 records, each of                   a. What are the advantages of using the lseek
         size 400 bytes, is stored on this magnetic tape.                       command?
         Calculate the length of the magnetic tape occu-                    b. What is the sequence of actions the file system
         pied by the file and the total I/O time required                       and the IOCS should execute when a process
         to read the file if the file is recorded (a) without                   issues an lseek command.
         blocking and (b) with a blocking factor of 4.               14.21  Show that division of the binary polynomial
14.16    A process uses many buffers while manipulating                     formed from nd + nc bits in a record, where
         a file containing blocked records. A system fail-                  nd  is    the  number  of  data     bits  and  nc  is  the
         ure occurs during its execution. Is it possible to                 number of CRC bits, by the CRC polynomial
         resume execution of the process from the point                     will yield a 0 remainder. (Hint: A term of xi,
         of failure?                                                        i = 1, . . . , nd - 1, in the polynomial for the nd
14.17    The speedup factor resulting from the use of a                     bits of data is the term of xi+nc in the polyno-
         special I/O technique is the ratio of the elapsed                  mial for the nd + nc bits in the record. Also note
         time of a process without blocking or buffering                    that modulo-2 addition and subtraction produce
         of records to the elapsed time of the same process                 identical results.)
BIBLIOGRAPHY                                                                                                                       ·
Tanenbaum (1990) describes I/O hardware. Ruemmler                    disk drives. Lumb et al. (2000) discusses how back-
and Wilkes (1994) presents a disk drive model that can               ground activities like disk reorganization can be per-
be used for performance analysis and tuning. Teorey                  formed during mechanical positioning of disk heads
and Pinkerton (1972) and Hofri (1980) compare var-                   for   servicing  foreground   activities,  and   the  effect  of
ious  disk  scheduling     algorithms,    while      Worthington     disk  scheduling      algorithms  on  effectiveness       of  this
et  al.  (1994)  discusses  disk      scheduling     for   modern    approach.



                                                             Chapter 14  Implementation of File Operations           603
    Chen and Patterson (1990) and Chen et al. (1994)         6.   Chau, A., and A. W. Fu (2000): "A gracefully
describe RAID organizations, while Wilkes et al. (1996)           degradable declustered RAID architecture with
and Yu et al. (2000) discuss enhancements to RAID sys-            near optimal maximal read and write
tems. Alvarez et al. (1996) discusses how multiple failures       parallelism," Cluster Computing, 5 (1), 97­105.
can be tolerated in a RAID architecture, while Chau          7.   Chen, P. M., and D. Patterson (1990):
and Fu (2000) discusses a new layout method to evenly             "Maximizing performance in a striped disk
distribute  parity  information  for  declustered  RAID.          array," Proceedings of 17th Annual International
Gibson et al. (1997) discusses file servers for network-          Symposium on Computer Architecture,
attached disks. Nagle et al. (1999) discusses integration         May 1990.
of user-level networking with network-attached storage       8.   Chen, P. M., E. K. Lee, G. A. Gibson,
(NAS). Curtis Preston (2002) discusses NAS and stor-              R. H. Katz, and D. A. Patterson (1994):
age area networks (SANs), while Clark (2003) is devoted           "RAID--high performance, reliable secondary
to the SAN technology. Toigo (2000) discusses modern              storage," Computing Surveys, 26 (2), 145­186.
disks and future storage technologies.                       9.   Clark, T. (2003): Designing Storage Area
    Disk    caching  is   discussed     in  Smith  (1985).        Networks: A Practical Reference for Implementing
Braunstein et al. (1989) discusses how file accesses are          Fibre Channel and IP SANS, 2nd ed., Addison
speeded up when virtual memory hardware is used to                Wesley Professional.
look up the file buffer cache.                               10.  Curtis Preston, W. (2002): Using SANs and NAS,
    McKusick et al. (1996) discusses the Berkeley fast            O'Reilly, Sebastopol,Calif.
file system for Unix 4.4BSD. Bach (1986) and Vahalia         11.  Custer, H. (1994): Inside the Windows NT File
(1996) discuss other Unix file systems. Ruemmler and              System, Microsoft Press, Redmond,Wash.
Wilkes (1993) presents performance studies concerning        12.  Gibson, G. A., D. Nagle, K. Amiri, F. W. Chang,
various characteristics of disk accesses made in the Unix         E. M. Feinberg, H. Gobioff, C. Lee, B. Ozceri,
file system. Beck et al. (2002) and Bovet and Cesati              E. Riedel, D. Rochberg, and J. Zelenka (1997):
(2005) discuss the I/O schedulers of Linux. Love (2004,           "File server scaling with network-attached secure
2005) describes the I/O schedulers in Linux 2.6. Custer           disks," Measurement and Modeling of Computer
(1994) describes the Windows NT file system, while                Systems, 272­284.
Russinovich and Solomon (2005) discusses NTFS for            13.  Hofri, M. (1980): "Disk scheduling: FCFS vs.
Windows.                                                          SSTF revisited," Communications of the ACM,
                                                                  23 (11), 645­53.
1.  Alvarez, G. A., W. A. Burkhard, F. Cristian              14.  Iyer, S., and P. Druschel (2001): "Anticipatory
    (1996): "Tolerating multiple failures in RAID                 scheduling: a disk scheduling framework to
    architectures with optimal storage and uniform                overcome deceptive idleness in synchronous I/O,"
    declustering," Proceedings of the 24th Annual                 Proceedings of the 18th ACM Symposium on
    International Symposium on Computer                           Operating Systems Principles.
    Architecture, 62­72.                                     15.  Lampson, B. (1981): "Atomic transactions," in
2.  Bach, M. J. (1986): The design of the Unix                    Distributed Systems--Architecture and
    operating system, Prentice-Hall, Englewood                    Implementation: An Advanced Course, Goos, G.
    Cliffs, N.J.                                                  and J. Hartmanis (eds.), Springer Verlag, Berlin,
3.  Beck, M., H. Bohme, M. Dziadzka, U. Kunitz,                   246­265.
    R. Magnus, C. Schroter, and D. Verworner                 16.  Love, R. (2004): "I/O schedulers," Linux Journal,
    (2002): Linux Kernel Programming, Pearson                     118.
    Education, New York.                                     17.  Love, R. (2005): Linux Kernel Development, 2nd
4.  Bovet, D. P., and M. Cesati (2005): Understanding             ed., Novell Press.
    the Linux Kernel, 3rd ed., O'Reilly, Sebastopol,         18.  Lumb, C. R., J. Schindler, G. R. Ganger, and
    Calif.                                                        D. F. Nagle (2000): "Towards higher disk head
5.  Braunstein, A., M. Riley, and J. Wilkes (1989):               utilization: extracting free bandwidth from busy
    "Improving the efficiency of Unix buffer caches,"             disk drives," Proceedings of the 4th Symposium on
    ACM Symposium on OS Principles, 71­82.                        Operating Systems Design and Implementation.



604  Part 4          File Systems
19.  McKusick, M. K., K. Bostic, M. J. Karels, and     26.  Teorey, T. J., and T. B. Pinkerton (1972):
     J. S. Quarterman (1996): The Design and                "A comparative analysis of disk scheduling
     Implementation of the 4.4BSD Operating System,         policies," Communications of the ACM, 15 (3),
     Addison Wesley, Reading, Mass.                         177­184.
20.  Nagle D., G. Ganger, J. Butler, G. Gibson, and    27.  Toigo, J. (2000): "Avoiding a data crunch,"
     C. Sabol (1999): "Network support for                  Scientific American, 282 (5), 58­74.
     network-attached storage," Proceedings of Hot     28.  Vahalia, U. (1996): Unix Internals--The New
     Interconnects.                                         Frontiers, Prentice Hall, Englewood Cliffs, N. J.
21.  Ruemmler, C., and J. Wilkes (1993): "Unix disk    29.  Wilkes, J., R. Golding, C. Staelin, and T. Sullivan
     access patterns," Proceedings of the Winter 1993       (1996): "The HP autoRAID hierarchical storage
     USENIX Conference, 405­420.                            system," ACM Transactions on Computer
22.  Ruemmler, C., and J. Wilkes (1994): "An                Systems, 14 (1), 108­136.
     introduction to disk drive modeling," IEEE        30.  Worthington, B. L., G. R. Ganger, and Y. N. Patt
     Computer, 27 (3), 17­29.                               (1994): "Scheduling algorithms for modern disk
23.  Russinovich, M. E., and D. A. Solomon (2005):          drives," Proceedings of the 1994 ACM Sigmetrics
     Microsoft Windows Internals, 4th ed., Microsoft        Conference on Measurement and Modeling of
     Press, Redmond, Wash.                                  Computer Systems, 241­251.
24.  Smith, A. J. (1985): "Disk cache-miss ratio       31.  Yu, X., B. Gum, Y. Chen, R. Y. Wang, K. Li,
     analysis and design considerations," ACM               A. Krishnamurthy, and T. E. Anderson (2000):
     Transactions on Computer Systems, 3 (3),               "Trading capacity for performance in a disk
     161­203.                                               array," Proceedings of the 2000 Symposium on
25.  Tanenbaum, A. S. (1990): Structured Computer           Operating Systems Design and Implementation,
     Organization, 3rd ed., Prentice Hall, Englewood        243­258.
     Cliffs, N. J.
