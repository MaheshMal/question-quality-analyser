Virtual Memory


12   Chapter
     Virtual Memory
     V irtual memory is a part of the memory hierarchy that consists of mem-
             ory and a disk. In accordance with the principle of memory hierarchies
             described in Chapter 2, only some portions of the address space of a
     process--that is, of its code and data--exist in memory at any time; other por-
     tions of its address space reside on disk and are loaded into memory when needed
     during operation of the process. The kernel employs virtual memory to reduce
     the memory commitment to a process so that it can service a large number of
     processes concurrently, and to handle processes whose address space is larger
     than the size of memory.
     Virtual memory is implemented through the noncontiguous memory allo-
     cation  model  described  earlier  in  Chapter  11  and  comprises  both  hardware
     components and a software component called a virtual memory manager. The
     hardware components speed up address translation and help the virtual mem-
     ory manager perform its tasks more effectively. The virtual memory manager
     decides which portions of a process address space should be in memory at
     any time.
     Performance of virtual memory depends on the rate at which portions of a
     process address space have to be loaded in memory from a disk and removed
     from memory to make space for new portions. According to the empirical law of
     locality of reference, a process is likely to access recently referenced portions of
     its address space again. The virtual memory manager ensures good performance
     of virtual memory by allocating an adequate amount of memory to a process
     and employing a replacement algorithm to remove a portion that has not been
     referenced recently.
     We start by discussing locality of reference and its importance for perfor-
     mance of a virtual memory. The techniques employed by the virtual memory
     manager to ensure good performance are then discussed.
     12.1    VIRTUAL MEMORY BASICS                                                          Â·
     Users always want more from a computer system--more resources and more
     services. The need for more resources is satisfied either by obtaining more effi-
     cient use of existing resources, or by creating an illusion that more resources exist
     in the system. A virtual memory is what its name indicates--it is an illusion of
410



                                                                       Chapter 12        Virtual Memory  411
a memory that is larger than the real memory, i.e., RAM, of the computer sys-
tem. As we pointed out in Section 1.1, this illusion is a part of a user's abstract
view of memory. A user or his application program sees only the virtual memory.
The kernel implements the illusion through a combination of hardware and soft-
ware means. We refer to real memory simply as memory. We refer to the software
component of virtual memory as a virtual memory manager.
The illusion of memory larger than the system's memory crops up any time a
process whose size exceeds the size of memory is initiated. The process is able to
operate because it is kept in its entirety on a disk and only its required portions are
loaded in memory at any time. The basis of virtual memory is the noncontiguous
memory allocation model described earlier in Section 11.7. The address space of
each process is assumed to consist of portions called components. The portions
can be loaded into nonadjacent areas of memory. The address of each operand or
instruction in the code of a process is a logical address of the form (compi, bytei).
The memory management unit (MMU) translates it into the address in memory
where the operand or instruction actually resides.
Use of the noncontiguous memory allocation model reduces memory frag-
mentation, since a free area of memory can be reused even if it is not large enough
to hold the entire address space of a process. More user processes can be accom-
modated in memory this way, which benefits both users and the OS. The kernel
carries this idea further--even processes that can fit in memory are not loaded
fully into memory. This strategy reduces the amount of memory that is allocated
to each process, thus further increasing the number of processes that can be in
operation at the same time.
Figure 12.1 shows a schematic diagram of a virtual memory. The logical
address space of the process shown consists of five components. Three of these
components are presently in memory. Information about the memory areas where
these components exist is maintained in a data structure of the virtual memory
manager. This information is used by the MMU during address translation. When
an instruction in the process refers to a data item or instruction that is not in
memory, the component containing it is loaded from the disk. Occasionally, the
virtual memory manager removes some components from memory to make room
for other components.
                                           Memory
              Process                                Loading/    Disk
                                                     removal of
                                                     components
              Logical        Memory
              address        allocation
              space          information   Physical
                                           address
                                           space
Figure  12.1  Overview of virtual memory.



412  Part 3  Memory Management
             The arrangement shown in Figure 12.1 is a memory hierarchy as discussed in
             Section 2.2.3 and illustrated in Figure 2.4. The hierarchy consists of the system's
             memory and a disk. Memory is fast, but small in size. The disk is slow, but has
             a much larger capacity. The MMU and the virtual memory manager together
             manage the memory hierarchy, so that the current instruction in a process finds
             its operands in memory.
             We are now ready to define virtual memory.
             Definition 12.1 Virtual Memory  A memory hierarchy, consisting of a com-
             puter system's memory and a disk, that enables a process to operate with only
             some portions of its address space in memory.
             Demand Loading of Process Components       The virtual memory manager loads
             only one component of a process address space in memory to begin with--the
             component that contains the start address of the process, that is, address of the
             instruction with which its execution begins. It loads other components of the pro-
             cess only when they are needed. This technique is called demand loading. To keep
             the memory commitment to a process low, the virtual memory manager removes
             components of the process from memory from time to time. These components
             would be loaded back in memory when needed again.
             Performance of a process in virtual memory depends on the rate at which
             its components have to be loaded into memory. The virtual memory manager
             exploits the law of locality of reference to achieve a low rate of loading of process
             components. We discuss this law in Section 12.2.1.1.
             Table 12.1         Comparison of Paging and Segmentation
             Issue                    Comparison
             Concept                  A page is a fixed-size portion of a process address space
                                      that is identified by the virtual memory hardware. A
                                      segment is a logical entity in a program, e.g., a
                                      function, a data structure, or an object. Segments are
                                      identified by the programmer.
             Size of components       All pages are of the same size. Segments may be of
                                      different sizes.
             External fragmentation   Not found in paging because memory is divided into
                                      page frames whose size equals the size of pages.
                                      It occurs in segmentation because a free area of
                                      memory may be too small to accommodate a segment.
             Internal fragmentation   Occurs in the last page of a process in paging. Does not
                                      occur in segmentation because a segment is allocated a
                                      memory area whose size equals the size of the segment.
             Sharing                  Sharing of pages is feasible subject to the constraints
                                      on sharing of code pages described later in
                                      Section 12.6. Sharing of segments is freely possible.



                                                                            Chapter 12  Virtual Memory  413
Paging    and   Segmentation  In     Chapter  11,    we  discussed      how  these      two
approaches to implementation of virtual memory differ in the manner in which
the boundaries and sizes of address space components are determined. Table 12.1
compares the two approaches. In paging, each component of an address space is
called a page. All pages have identical size, which is a power of two. Page size is
defined by the computer hardware and demarcation of pages in the address space
of a process is performed implicitly by it. In segmentation, each component of
an address space is called a segment. A programmer declares some significant
logical entities (e.g., data structures or objects) in a process as segments. Thus
identification of components is performed by the programmer, and segments can
have different sizes. This fundamental difference leads to different implications
for efficient use of memory and for sharing of programs or data. Some systems
use a hybrid segmentation-with-paging approach to obtain advantages of both
the approaches.
12.2      DEMAND PAGING                                                                                 Â·
As discussed earlier in Section 11.8, a process is considered to consist of pages,
numbered from 0 onward. Each page is of size s bytes, where s is a power of 2. The
memory of the computer system is considered to consist of page frames, where a
page frame is a memory area that has the same size as a page. Page frames are
numbered from 0 to #frames-1 where #frames is the number of page frames of
memory. Accordingly, the physical address space consists of addresses from 0 to
#frames Ã s - 1. At any moment, a page frame may be free, or it may contain a
page of some process. Each logical address used in a process is considered to be a
pair (pi, bi), where pi is a page number and bi is the byte number in pi, 0  bi < s.
The  effective  memory      address  of  a  logical  address  (pi, bi)  is  computed    as
follows:
      Effective memory address of logical address (pi, bi)                      (12.1)
          = start address of the page frame containing page pi + bi
The size of a page is a power of 2, and so calculation of the effective address is
performed through bit concatenation, which is much faster than addition (see
Section 11.8 for details).
     Figure 12.2 is a schematic diagram of a virtual memory using paging in which
page size is assumed to be 1 KB, where 1 KB = 1024 bytes. Three processes P1, P2
and P3, have some of their pages in memory. The memory contains 8 page frames
numbered from 0 to 7. Memory allocation information for a process is stored in a
page table. Each entry in the page table contains memory allocation information
for one page of a process. It contains the page frame number where a page resides.
Process P2 has its pages 1 and 2 in memory. They occupy page frames 5 and 7
respectively. Process P1 has its pages 0 and 2 in page frames 4 and 1, while process
P3 has its pages 1, 3 and 4 in page frames 0, 2 and 3, respectively. The free frames
list contains a list of free page frames. Currently only page frame 6 is free.



414  Part 3  Memory Management
                                                                    0      4
                                                                    1         Page table
                                                                    2      1        of P1
                           Memory                                   3
                                                    MMU                       Page
             0                                      pi  bi                    frame #
             1                           1                          0
             2                                      2   480  2      1      5  Page table
             3                                                      2      7        of P2
                                                                    3
             4                                      7   480  3      4
             5             Add Â·Â· 2 480             qi  bi
             6                                                      0
             7                                                      1      0               6
                                                                    2         Page table
                                         4                          3      2        of P3  Free frames
                                                                    4      3               list
                                                                    5
             Figure  12.2  Address translation  in  virtual memory  using  paging.
             Process P2 is currently executing the instruction `Add Â·Â· 2528', so the MMU
             uses P2's page table for address translation. The MMU views the operand address
             2528 as the pair (2, 480) because 2528 = 2 Ã 1024 + 480. It now accesses the entry
             for page 2 in P2's page table. This entry contains frame number 7, so the MMU
             forms the effective address 7 Ã 1024 + 480 according to Eq. (12.1), and uses it to
             make a memory access. In effect, byte 480 in page frame 7 is accessed.
             12.2.1 Demand Paging Preliminaries
             If an instruction of P2 in Figure 12.2 refers to a byte in page 3, the virtual memory
             manager will load page 3 in memory and put its frame number in entry 3 of P2's
             page table. These actions constitute demand loading of pages, or simply demand
             paging.
             To implement demand paging, a copy of the entire logical address space of a
             process is maintained on a disk. The disk area used to store this copy is called the
             swap space of a process. While initiating a process, the virtual memory manager
             allocates the swap space for the process and copies its code and data into the swap
             space. During operation of the process, the virtual memory manager is alerted
             when the process wishes to use some data item or instruction that is located in a
             page that is not present in memory. It now loads the page from the swap space into
             memory. This operation is called a page-in operation. When the virtual memory
             manager decides to remove a page from memory, the page is copied back into the
             swap space of the process to which it belongs if the page was modified since the
             last time it was loaded in memory. This operation is called a page-out operation.
             This way the swap space of a process contains an up-to-date copy of every page
             of the process that is not present in memory. A page replacement operation is
             one that loads a page into a page frame that previously contained another page.
             It may involve a page-out operation if the previous page was modified while it
             occupied the page frame, and involves a page-in operation to load the new page.



                                                                                                     Chapter 12  Virtual  Memory  415
In this section we describe the data structures used by the virtual memory
manager, and the manner in which the virtual memory manager performs the
page-in, page-out, and page replacement operations. We then discuss how the
effective memory access time for a process depends on the overhead of the virtual
memory manager and the time consumed by the page-in, page-out, and page
replacement operations.
Page Table         The page table for a process facilitates implementation of address
translation, demand loading, and page replacement operations. Figure 12.3
shows the format of a page table entry. The valid bit field contains a boolean
value to indicate whether the page exists in memory. We use the convention that
1 indicates "resident in memory" and 0 indicates "not resident in memory." The
page frame # field, which was described earlier, facilitates address translation. The
misc info field is divided into four subfields. Information in the prot info field is
used for protecting contents of the page against interference. It indicates whether
the process can read or write data in the page or execute instructions in it. ref info
contains information concerning references made to the page while it is in mem-
ory. As discussed later, this information is used for page replacement decisions.
The modified bit indicates whether the page has been modified, i.e., whether it is
dirty. It is used to decide whether a page-out operation is needed while replacing
the page. The other info field contains information such as the address of the disk
block in the swap space where a copy of the page is maintained.
Page Faults and Demand Loading of Pages                      Table 12.2 summarizes steps in
address translation by the MMU. While performing address translation for a
logical address (pi, bi), the MMU checks the valid bit of the page table entry of pi
                                                       Misc info
                             Valid    Page      Prot   Ref   Modi-  Other
                             bit    frame #     info   info  fied   info
        Field                Description
        Valid bit            Indicates whether the page described by the entry currently exists
                             in memory. This bit is also called the presence bit.
        Page frame #         Indicates which page frame of memory is occupied by the page.
        Prot info            Indicates how the process may use contents of the page--whether
                             read, write, or execute.
        Ref info             Information concerning references made to the page while it is in
                             memory.
        Modified             Indicates whether the page has been modified while in memory,
                             i.e., whether it is dirty. This field is a single bit called the dirty
                             bit.
        Other info           Other useful information concerning the page, e.g., its position in
                             the swap space.
Figure  12.3 Fields   in  a  page table entry.



416  Part 3  Memory Management
             Table 12.2         Steps in      Address Translation by the MMU
             Step                                Description
             1. Obtain page number               A logical address is viewed as a pair (pi, bi), where bi
             and byte number in                  consists of the lower order nb bits of the address, and pi
             page                                consists of the higher order np bits (see Section 11.8).
             2. Look up page table               pi is used to index the page table. A page fault is raised
                                                 if the valid bit of the page table entry contains a 0, i.e.,
                                                 if the page in not present in memory.
             3. Form effective memory            The page frame # field of the page table entry contains
             address                             a frame number represented as an nf -bit number. It is
                                                 concatenated with bi to obtain the effective memory
                                                 address of the byte.
                                                        Swap space
                                                           of P2                         5
                                                                                               VM
                                                                                               Manager
                   6
                   Page-in                                                   Page table
                   operation                        3      Page                          7
                                                           fault             updating
                                Memory                                 Page table
                      0                             pi     bi             of P2
                      1                       1                        0
                      2                             3      682         1  1  5              4
                      3                                           2    2  1  7
                                                                       3  0
                      4                                                4
                      5         Sub Â·Â· 3 682        qi     bi
                      6                                              Valid Page Misc           6
                      7                             MMU              bit  frame info           Free frames
                                                                             #                 list
             Figure 12.4 Demand loading       of a  page.
             (see Step 2 in Table 12.2). If the bit indicates that pi is not present in memory, the
             MMU raises an interrupt called a missing page interrupt or a page fault, which is a
             program interrupt (see Section 2.2.5). The interrupt servicing routine for program
             interrupts finds that the interrupt was caused by a page fault, so it invokes the
             virtual memory manager with the page number that caused the page fault, i.e., pi,
             as a parameter. The virtual memory manager now loads page pi in memory and
             updates its page table entry. Thus, the MMU and the virtual memory manager
             interact to decide when a page of a process should be loaded in memory.
             Figure 12.4 is an overview of the virtual memory manager's actions in demand
             loading of a page. The broken arrows indicate actions of the MMU, whereas



                                                         Chapter 12                       Virtual Memory  417
firm arrows indicate accesses to the data structures, memory, and the disk by
the virtual memory manager when a page fault occurs. The numbers in circles
indicate the steps in address translation, raising, and handling of the page fault--
Steps 1Â­3 were described earlier in Table 12.2. Process P2 of Figure 12.2 is in
operation. While translating the logical address (3, 682), the MMU raises a page
fault because the valid bit of page 3's entry is 0. When the virtual memory manager
gains control, it knows that a reference to page 3 caused the page fault. The Misc
info field of the page table entry of page 3 contains the address of the disk block
in P2's swap space that contains page 3. The virtual memory manager obtains
this address. It now consults the free frames list and finds that page frame 6 is
currently free, so it allocates this page frame to page 3 and starts an I/O operation
to load page 3 in page frame 6. When the I/O operation completes, the virtual
memory manager updates page 3's entry in the page table by setting the valid
bit to 1 and putting 6 in the page frame # field. Execution of the instruction
"Sub Â·Â· (3, 682)", which had caused the page fault, is now resumed. The logical
address (3, 682) is translated to the effective address of byte number 682 in page
frame 6, i.e., 6 Ã 1024 + 682.
Page-in, Page-out, and Page Replacement Operations  Figure 12.4 showed how
a page-in operation is performed for a required page when a page fault occurs
in a process and a free page frame is available in memory. If no page frame
is free, the virtual memory manager performs a page replacement operation to
replace one of the pages existing in memory with the page whose reference caused
the page fault. It is performed as follows: The virtual memory manager uses a
page replacement algorithm to select one of the pages currently in memory for
replacement, accesses the page table entry of the selected page to mark it as "not
present" in memory, and initiates a page-out operation for it if the modified bit
of its page table entry indicates that it is a dirty page. In the next step, the virtual
memory manager initiates a page-in operation to load the required page into the
page frame that was occupied by the selected page. After the page-in operation
completes, it updates the page table entry of the page to record the frame number
of the page frame, marks the page as "present," and makes provision to resume
operation of the process. The process now reexecutes its current instruction. This
time, the address translation for the logical address in the current instruction
completes without a page fault.
The page-in and page-out operations required to implement demand paging
constitute page I/O; we use the term page traffic to describe movement of pages in
and out of memory. Note that page I/O is distinct from I/O operations performed
by processes, which we will call program I/O. The state of a process that encounters
a page fault is changed to blocked until the required page is loaded in memory,
and so its performance suffers because of a page fault. The kernel can switch the
CPU to another process to safeguard system performance.
Effective Memory Access Time     The effective memory access time for a process
in demand paging is the average memory access time experienced by the process.
It depends on two factors: time consumed by the MMU in performing address
translation, and the average time consumed by the virtual memory manager in



418  Part 3  Memory Management
             handling a page fault. We use the following notation to compute the effective
             memory access time:
             pr1      probability that a page exists in memory
             tmem     memory access time
             tpfh     time overhead of page fault handling
             pr1 is called the memory hit ratio. tpfh is a few orders of magnitude larger than
             tmem because it involves disk I/O--one disk I/O operation is required if only a
             page-in operation is sufficient, and two disk I/O operations are required if a page
             replacement is necessary.
             A process's page table exists in memory when the process is in operation.
             Hence, accessing an operand with the logical address (pi, bi) consumes two mem-
             ory cycles if page pi exists in memory--one to access the page table entry of pi
             for address translation, and the other to access the operand in memory using the
             effective memory address of (pi, bi). If the page is not present in memory, a page
             fault is raised after referencing the page table entry of pi, i.e., after one memory
             cycle. Now the required page is loaded in memory and its page table entry is
             updated to record the frame number where it is loaded. When operation of the
             process is resumed, it requires two more memory references--one to access the
             page table and the other to actually access the operand. Accordingly, the effective
             memory access time is as follows:
             Effective memory access time = pr1 Ã 2 Ã tmem                              (12.2)
                                                + (1 - pr1) Ã (tmem + tpfh + 2 Ã tmem)
             The effective memory access time can be improved by reducing the number
             of page faults. One way of achieving it is to load pages before they are needed by
             a process. The Windows operating system performs such loading speculatively--
             when a page fault occurs, it loads the required page and also a few adjoining
             pages of the process. This action improves the average memory access time if a
             preloaded page is referenced by the process. The Linux operating system permits
             a process to specify which pages should be preloaded. A programmer may use
             this facility to improve the effective memory access time.
             12.2.1.1 Page Replacement
             Page replacement becomes necessary when a page fault occurs and there are
             no free page frames in memory. However, another page fault would arise if the
             replaced page is referenced again. Hence it is important to replace a page that
             is not likely to be referenced in the immediate future. But how does the virtual
             memory manager know which page is not likely to be referenced in the immediate
             future?
             The empirical law of locality of reference states that logical addresses used by
             a process in any short interval of time during its operation tend to be bunched
             together in certain portions of its logical address space. Processes exhibit this
             behavior for two reasons. Execution of instructions in a process is mostly sequen-
             tial in nature, because only 10Â­20 percent of instructions executed by a process



                               Chapter 12                                               Virtual Memory        419
are branch instructions. Processes also tend to perform similar operations on
several elements of nonscalar data such as arrays. Due to the combined effect of
these two reasons, instruction and data references made by a process tend to be
in close proximity to previous instruction and data references made by it.
We define the current locality of a process as the set of pages referenced in
its previous few instructions. Thus, the law of locality indicates that the logical
address used in an instruction is likely to refer to a page that is in the current
locality of the process. As mentioned in Section 2.2.3, the computer exploits the
law of locality to ensure high hit ratios in the cache. The virtual memory manager
can exploit the law of locality to achieve an analogous effect--fewer page faults
would arise if it ensures that pages that are in the current locality of a process are
present in memory.
Note that locality of reference does not imply an absence of page faults. Let
the proximity region of a logical address ai contain all logical addresses that are in
close proximity to ai. Page faults can occur for two reasons: First, the proximity
region of a logical address may not fit into a page; in this case, the next address
may lie in an adjoining page that is not included in the current locality of the
process. Second, an instruction or data referenced by a process may not be in
the proximity of previous references. We call this situation a shift in locality of a
process. It typically occurs when a process makes a transition from one action in
its logic to another. The next example illustrates the locality of a process.
                                                                                                              Â·
Current Locality of a Process                                                              Example      12.1
In Figure 12.5, bullets indicate the last few logical addresses used during opera-
tion of a process Pi. Dashed boxes show the proximity regions of these logical
addresses. Note that the proximity region of a logical address may extend
beyond a page boundary. Proximity regions of logical addresses may also
overlap; we show the cumulative proximity regions in Figure 12.5; e.g., the
proximity regions of logical addresses referenced in page 4 cumulatively cover
the entire page 4 and parts of pages 3 and 5. Thus, proximity regions are located
in pages 0, 1, 3, 4, 5, 6, and 7; however, the current locality of Pi is the set of
pages whose numbers are marked with the  marks in Figure 12.5, i.e., the set
of pages {0, 1, 4, 6}.
                                                                                        Â·
The law of locality helps to decide which page should be replaced when a
page fault occurs. Let us assume that the number of page frames allocated to a
process Pi is a constant. Hence whenever a page fault occurs during operation
of Pi, one of Pi's own pages existing in memory must be replaced. Let t1 and t2
be the periods of time for which pages p1 and p2 have not been referenced during
the operation of Pi. Let t1 > t2, implying that some byte of page p2 has been
referenced or executed (as an instruction) more recently than any byte of page p1.
Hence page p2 is more likely to be a part of the current locality of the process
than page p1; that is, a byte of page p2 is more likely to be referenced or executed
than a byte of page p1. We use this argument to choose page p1 for replacement



420  Part 3  Memory  Management
                                             Page  Logical address
                                             no    space of process Pi            Logical
                                             0*              Â·                    address
                                 A page                                           accessed
                                 in current  1*           Â·
                                 locality                                         Proximity
                                             2                                    region of
                                                                                  logical
                                             3                                    address
                                             4*    Â·Â·  Â·     Â·Â·
                                             5
                                             6*                 Â·
                                                   Â·
                                             7
                                             8
             Figure 12.5 Proximity regions   of previous references and  current  locality of  a  process.
             when a page fault occurs. If many pages of Pi exist in memory, we can rank
             them according to the times of their last references and replace the page that has
             been least recently referenced. This page replacement policy is called LRU page
             replacement.
             12.2.1.2 Memory Allocation to a Process
             Figure 12.6 shows how the page fault rate of a process should vary with the
             amount of memory allocated to it. The page fault rate is large when a small
             amount of memory is allocated to the process; however, it drops when more
             memory is allocated to the process. This page fault characteristic of a process is
             desired because it enables the virtual memory manager to take corrective action
             when it finds that a process has a high page fault rate--it can bring about a
             reduction in the page fault rate by increasing the memory allocated to the process.
             As we shall discuss in Section 12.4, the LRU page replacement policy possesses
             a page fault characteristic that is similar to the curve of Figure 12.6 because it
             replaces a page that is less likely to be in the current locality of the process than
             other pages of the process that are in memory.
                     How much memory should the virtual memory manager allocate to a pro-
             cess? Two opposite factors influence this decision. From Figure 12.6, we see that
             an overcommitment of memory to a process implies a low page fault rate for the
             process; hence it ensures good process performance. However, a smaller number
             of processes would fit in memory, which could cause CPU idling and poor system
             performance. An undercommitment of memory to a process causes a high page
             fault rate, which would lead to poor performance of the process. The desirable
             operating zone marked in Figure 12.6 avoids the regions of overcommitment and
             undercommitment of memory.
                     The main problem in deciding how much memory to allocate to a process
             is that the page fault characteristic, i.e., the slope of the curve and the page



                                                                                    Chapter 12  Virtual Memory  421
                        Region of low      Region of moderate      Region of high
                        memory allocation  memory allocation   memory allocation
                        and high           and moderate            and low
                        page fault rate    page fault rate         page fault rate
              Page                         Desirable
              fault                        operating
              rate                         zone
                     0  No. of page frames allocated to a process
Figure  12.6  Desirable variation of page fault rate with memory allocation.
fault rate in Figure 12.6, varies among processes. Even for the same process, the
page fault characteristic may be different when it operates with different data.
Consequently, the amount of memory to be allocated to a process has to be
determined dynamically by considering the actual page fault characteristic of the
process. This issue is discussed in Section 12.5.
Thrashing     Consider a process that is operating in the region of low memory
allocation and high page fault rate in Figure 12.6. Due to the high page fault rate,
this process spends a lot of its time in the blocked state. Such a process is not in a
position to use the CPU effectively. It also causes high overhead due to high page
fault rate and process switching caused by page faults. If all processes in the system
operate in the region of high page fault rates, the CPU would be engaged in per-
forming page traffic and process switching most of the time. CPU efficiency would
be low and system performance, measured either in terms of average response time
or throughput, would be poor. This situation is called thrashing.
Definition 12.2 Thrashing  A condition in which high page traffic and low
CPU efficiency coincide.
Note that low CPU efficiency can occur because of other causes as well, e.g.,
if too few processes exist in memory or all processes in memory perform I/O
operations frequently. The thrashing situation is different in that all processes
make poor progress because of high page fault rates.
From Figure 12.6, we can infer that the cause of thrashing is an under-
commitment of memory to each process. The cure is to increase the memory
allocation for each process. This may have to be achieved by removing some
processes from memory--that is, by reducing the degree of multiprogramming.
A process may individually experience a high page fault rate without the system
thrashing. The same analysis now applies to the process--it must suffer from an



422  Part 3  Memory Management
             undercommitment of memory, so the cure is to increase the amount of memory
             allocated to it.
             12.2.1.3 Optimal Page Size
             The size of a page is defined by computer hardware. It determines the number of
             bits required to represent the byte number in a page. Page size also determines
             1. Memory wastage due to internal fragmentation
             2. Size of the page table for a process
             3. Page fault rates when a fixed amount of memory is allocated to a process
             Consider a process Pi of size z bytes. A page size of s bytes implies that the
             process has n pages, where n =  z/s  is the value of z/s rounded upward. Average
             internal fragmentation is s/2 bytes because the last page would be half empty
             on the average. The number of entries in the page table is n. Thus internal frag-
             mentation varies directly with the page size, while page table size varies inversely
             with it.
             Interestingly, page fault rate also varies with page size if a fixed amount of
             memory is allocated to Pi. This can be explained as follows: The number of pages
             of Pi in memory varies inversely with the page size. Hence twice as many pages of
             Pi would exist in memory if the page size were made s/2. Now let the proximity
             region of an instruction or data byte as defined in Section 12.2 be small compared
             with s/2, so that it can be assumed to fit within the page that contains the byte.
             When the page size is s/2, memory contains twice as many proximity regions
             of recent logical addresses as when the page size is s bytes. From the page fault
             characteristic of Figure 12.6, page fault rates would be smaller for smaller page
             sizes.
             We can compute the page size that minimizes the total of memory penalty due
             to internal fragmentation and memory commitment to page tables. If s             zand
             each page table entry occupies 1 byte of memory, the optimal value of s is            2z.
             Thus, the optimal page size is only 400 bytes for a process size of 80 KB, and it
             is 800 bytes for a process of 320 KB. However, computers tend to use larger page
             sizes (e.g., Pentium and MIPS use page sizes of 4 KB or more, Sun Ultrasparc
             uses page sizes of 8 KB or more and the PowerPC uses a page size of 4 KB) for
             the following reasons:
             1. Page table entries tend to occupy more than 1 byte.
             2. Hardware costs are high for smaller page sizes. For example, the cost of
             address translation increases if a larger number of bits is used to represent a
             page number.
             3. Disks, which are used as paging devices, tend to operate less efficiently for
             smaller disk block sizes.
             The decision to use larger page sizes than the optimal value implies somewhat
             higher page fault rates for a process. This fact represents a tradeoff between the
             hardware cost and efficient operation of a process.



                                                                                Chapter 12  Virtual  Memory  423
12.2.2 Paging Hardware
Figure 12.7 illustrates address translation in a multiprogrammed system. Page
tables for many processes are present in memory. The MMU contains a special
register called the page-table address register (PTAR) to point to the start of a page
table. For a logical address (pi, bi), the MMU computes <PTAR> + pi Ã lPT_entry
to obtain the address of the page table entry of page pi, where lPT_entry is the
length of a page table entry and <PTAR> denotes the contents of the PTAR.
The PTAR has to be loaded with the correct address when a process is scheduled.
To facilitate this, the kernel can store the address of the page table of a process in
its process control block (PCB).
Table 12.3 summarizes the functions performed by the paging hardware. We
describe the techniques used in implementing these functions, and name a few
modern computer systems that use them.
12.2.2.1 Memory Protection
A memory protection violation interrupt should be raised if a process tries to
access a nonexistent page, or exceeds its access privileges while accessing a page.
The MMU provides a special register called the page-table size register (PTSR)
to detect violations of the first kind. The kernel records the number of pages
                                                                    Page table
                   Memory         MMU                               of P1
                                                PTAR
             Add                            +             2
                   Â·Â·         1
                                            pi  bi              fi  Page table
                                            ?                1      of P2
                                                       3
                                  PTSR
                              1             fi  bi
                           4                                        Page table
                                                                    of P3
                   Memory protection exception
Figure 12.7  Address translation in a multiprogrammed  system.
Table 12.3         Functions of the Paging Hardware
Function                      Description
Memory protection             Ensure that a process can access only those memory
                              areas that are allocated to it.
Efficient address             Provide an arrangement to perform address translation
translation                   efficiently.
Page replacement support      Collect information concerning references made to
                              pages. The virtual memory manager uses this
                              information to decide which page to replace when a
                              page fault occurs.



424  Part 3  Memory Management
             contained in a process in its process control block (PCB) and loads this number
             from the PCB in the PTSR when the process is scheduled. A memory protection
             violation is raised if the page number in a logical address is not smaller than
             contents of PTSR; this check is analogous to the one using the size register in the
             memory protection scheme of Chapter 2.
             The access privileges of a process to a page are stored in the prot info field of
             the page's entry in the page table. During address translation, the MMU checks
             the kind of access being made to the page against this information and raises a
             memory protection violation if the two are not compatible. The information in
             the prot info field can be bit-encoded for efficient access--each bit in the field
             corresponds to one kind of access to the page (e.g., read, write, etc.); it is set "on"
             only if the process possesses the corresponding access privilege to the page.
             12.2.2.2 Address Translation and Page Fault Generation
             The MMU follows the steps of Table 12.2 to perform address translation. For a
             logical address (pi, bi), it accesses the page table entry of pi by using pi Ã lPT_entry
             as an offset into the page table, where lPT_entry is the length of a page table entry.
             lPT_entry is typically a power of 2, so pi Ã lPT_entry can be computed efficiently by
             shifting the value of pi by a few bits.
             Address Translation Buffers        A reference to the page table during address trans-
             lation consumes one memory cycle because the page table is stored in memory.
             The translation look-aside buffer (TLB) is a small and fast associative memory that
             is used to eliminate the reference to the page table, thus speeding up address trans-
             lation. The TLB contains entries of the form (page #, page frame #, protection
             info) for a few recently accessed pages of a program that are in memory. During
             address translation of a logical address (pi, bi), the TLB hardware searches for an
             entry of page pi. If an entry is found, the page frame # from the entry is used to
             complete address translation for the logical address (pi, bi). Figure 12.8 illustrates
                                                                           Page     Page     Prot
                                                                           #        frame #  info
                                                                                    2  7     Translation
                                                                                    1  5     look-aside
                                                                2                            buffer (TLB)
                           Memory                      MMU              3  TLB
                     0                                 pi  bi              hit
                     1                   1                                    0
                     2                                 2   480          2     1     15
                     3                                                        2     17       Page table
                     4                                                        3                    of P2
                                                       7   480          3     4
                     5     Add Â·Â· 2 480                qi  bi
                     6
                     7                                                     Valid    Page     Prot
                                                4                          bit      frame    info
                                                                                       #
             Figure  12.8  Address translation  using  the translation  look-aside  buffer and the page table.



                                                                                       Chapter  12  Virtual Memory  425
              Address (pi, bi)
        Y      pi's entry                          Page fault
               in TLB?
               N                                   Page-out          N
               pi in                               needed?               Load
                                N
               memory?                             Y                     page pi
                                   Raise           Remove page pi
               Y                   page fault      from memory           Update PT
               Enter (pi, fi)                                            entry of pi
               in TLB                              Update PT entry,
                                                   erase TLB entry       Invoke scheduler
              Form physical                        of pj
              address using fi
                  MMU actions                      Virtual memory        manager actions
Figure  12.9   Summary of address translation  of  (pi, bi) (note: PT =  page table).
operation of the TLB. The arrows marked 2          and 3            indicate TLB lookup. The
TLB contains entries for pages 1 and 2 of process P2. If pi is either 1 or 2, the TLB
lookup scores a hit, so the MMU takes the page frame number from the TLB
and completes address translation. A TLB miss occurs if pi is some other page,
hence the MMU accesses the page table and completes the address translation if
page pi is present in memory; otherwise, it generates a page fault, which activates
the virtual memory manager to load pi in memory.
Figure 12.9 summarizes the MMU and software actions in address trans-
lation and page fault handling for a logical address (pi, bi). MMU actions
concerning use of the TLB and the page table are as described earlier. The vir-
tual memory manager is activated by a page fault. If an empty page frame is
not available to load page pi, it initiates a page-out operation for some page pj
to free the page frame, say page frame fj, occupied by it. pj's page table entry
is updated to indicate that it is no longer present in memory. If pj has an entry
in the TLB, the virtual memory manager erases it by executing an "erase TLB
entry" instruction. This action is essential for preventing incorrect address trans-
lation at pj's next reference. A page-in operation is now performed to load pi in
page frame fj, and pi's page table entry is updated when the page-in operation
is completed. Execution of the instruction that caused the page fault is repeated
when the process is scheduled again. This time pi does not have an entry in the
TLB but it exists in memory, and so the MMU uses information in the page table
to complete the address translation. An entry for pi has to be made in the TLB
at this time.
New entries in the TLB can be made either by the hardware or by the virtual
memory manager. Hardware handling of the TLB is more efficient; the hard-
ware can make a new entry in the TLB whenever it has to complete address



426  Part 3  Memory Management
             translation through a reference to the page table. When the TLB is managed
             by the virtual memory manager, the MMU raises a "missing TLB entry" inter-
             rupt whenever it cannot find an entry for the required page in the TLB, and the
             virtual memory manager executes several instructions to make the TLB entry.
             In this approach, the MMU performs address translation exclusively through
             the TLB, and the page table is used only by the virtual memory manager. This
             arrangement provides flexibility because the virtual memory manager can use dif-
             ferent organizations of the page table to conserve memory (see Section 12.2.3).
             The PowerPC and Intel 80x86 architectures use hardware-managed TLBs, while
             the MIPS, Sparc, Alpha, and PA-RISC architectures use software-managed
             TLBs.
             A few features are common to both the approaches. A replacement algorithm
             is used to decide which TLB entry should be overwritten when a new entry is to be
             made. Use of the TLB can undermine protection if the MMU performs address
             translation through TLB entries that were made while some other process was
             in operation. This issue is analogous to the protection issue in a cache discussed
             earlier in Section 2.2.3. Hence the solutions are also analogous. Each TLB entry
             can contain the id of the process that was in operation when the entry was made--
             that is, each TLB entry can have the form (process id, page #, page frame #,
             protection info)--so that the MMU can avoid using it when some other process
             is in operation. Alternatively, the kernel must flush the TLB while performing
             process switching.
             We use the following notation to compute the effective memory access time
             when a TLB is used:
             pr1    probability that a page exists in memory
             pr2    probability that a page entry exists in TLB
             tmem   memory access time
             tTLB   access time of TLB
             tpfh   time overhead of page fault handling
             As mentioned earlier in Section 12.2.1, pr1 is the memory hit ratio and tmem is a
             few orders of magnitude smaller than tpfh. Typically tTLB is at least an order of
             magnitude smaller than tmem. pr2 is called the TLB hit ratio.
             When the TLB is not used, the effective memory access time is as given by
             Eq. (12.2). The page table is accessed only if the page being referenced does not
             have an entry in the TLB. Accordingly, a page reference consumes (tTLB + tmem)
             time if the page has an entry in the TLB, and (tTLB + 2 Ã tmem) time if it does
             not have a TLB entry but exists in memory. The probability of the latter situation
             is (pr1- pr2). When the TLB is used, pr2 is the probability that an entry for the
             required page exists in the TLB. The probability that a page table reference is both
             necessary and sufficient for address translation is (pr1- pr2). The time consumed
             by each such reference is (tTLB + 2 Ã tmem) since an unsuccessful TLB search
             would precede the page table lookup. The probability of a page fault is (1 - pr1).
             It occurs after the TLB and the page table have been looked up, and it requires
             (tpfh + tTLB + 2 Ã tmem) time if we assume that the TLB entry is made for the



                                                                  Chapter 12          Virtual Memory  427
page while the effective memory address is being calculated. Hence the effective
memory access time is
Effective memory access time =
pr2 Ã (tTLB + tmem) + (pr1 - pr2) Ã (tTLB + 2 Ã tmem)             (12.3)
            + (1 - pr1) Ã (tTLB + tmem + tpfh + tTLB + 2 Ã tmem)
To provide efficient memory access during operation of the kernel, most
computers provide wired TLB entries for kernel pages. These entries are never
touched by replacement algorithms.
Superpages  Sizes of computer memories and processes have grown rapidly since
the 1990s. TLB sizes have not kept pace with this increase because TLBs are
expensive as a result of their associative nature; their sizes have grown from about
eight in the 1960s to only about a thousand in 2005. Hence TLB reach, which is
the product of the number of entries in a TLB and the page size, has increased
marginally, but its ratio to memory size has shrunk by a factor of over 1000.
Consequently, TLB hit ratios are poor, and average memory access times are
high [see Eq. (12.3)]. Processor caches have also become larger than the TLB
reach, which affects performance of a cache that is searched by physical addresses
because access to contents of the cache may be slowed down by TLB misses and
lookups through the page table. A generic way of countering these problems is
to use a larger page size, so that the TLB reach becomes larger. However, it leads
to larger internal fragmentation and more page I/O. In the absence of a generic
solution, techniques were developed to address specific problems created by the
low TLB reach. Searching the cache by logical addresses took the TLB out of
the path from the CPU to the cache, which avoided a slowdown of cache lookup
due to limited TLB reach. However, poor TLB hit ratios continued to degrade
virtual memory performance.
Superpages were evolved as a generic solution to the problems caused by
low TLB reach. A superpage is like a page of a process, except that its size is a
power-of-2 multiple of the size of a page, and its start address in both the logical
and physical address spaces is aligned on a multiple of its own size. This feature
increases the TLB reach without increasing the size of the TLB, and helps to
obtain a larger TLB hit ratio. Most modern architectures permit a few standard
superpage sizes and provide an additional field in a TLB entry to indicate the size
of superpage that can be accessed through the entry.
The virtual memory manager exploits the superpages technique by adapting
the size and number of superpages in a process to its execution characteristics. It
may combine some pages of a process into a superpage of an appropriate size if
the pages are accessed frequently and satisfy the requirement of contiguity and
address alignment in the logical address space. This action is called a promotion.
The virtual memory manager may have to move the individual pages in memory
during promotion to ensure contiguity and address alignment in memory. A
promotion increases the TLB reach, and releases some of the TLB entries that
were assigned to individual pages of the new superpage.



428  Part 3  Memory Management
                If the virtual memory manager finds that some pages in a superpage are not
             accessed frequently, it may decide to disband the superpage into individual pages.
             This action, called demotion, frees some memory that can be used to load other
             pages. Thus, it has the potential to reduce page fault frequency.
             12.2.2.3 Support for Page Replacement
             The virtual memory manager needs two kinds of information for minimizing
             page faults and the number of page-in and page-out operations during page
             replacement:
             1. The time when a page was last used.
             2. Whether a page is dirty, i.e., whether a write operation has been performed
                on any byte in the page. (A page is clean if it is not dirty.)
                The time of last use indicates how recently a page was used by a process; it
             is useful in selecting a candidate for page replacement. However, it is expensive
             to provide a sufficient number of bits in a page table entry for this purpose, so
             most computers provide a single bit called the reference bit. The modified bit in
             a page table entry is used to indicate whether a page is clean or dirty. If a page
             is clean, its copy in the swap space of the process is still current, so no page-out
             operation is needed; the page being loaded can simply overwrite such a page in
             memory. For a dirty page, a page-out operation must be performed because its
             copy in the swap space is stale. A page-in operation for the new page to be loaded
             can be started only after the page-out operation is completed.
             12.2.3 Practical Page Table Organizations
             A process with a large address space requires a large page table. Hence the virtual
             memory manager has to commit a large amount of memory for each page table.
             For example, in a computer system using 32-bit logical addresses and a page size
             of 4 KB, a process can have 1 million pages. If the size of a page table entry is
             4 bytes, the page table has a size of 4 MB. Thus, the virtual memory manager might
             tie up a few hundred megabytes of memory for storing page tables of processes!
             The memory requirements would be even larger when 64-bit logical addresses are
             used. Two approaches are followed to reduce the size of memory committed to
             page tables:
             Â·  Inverted page table: The inverted page table (IPT) has one entry for each page
                frame in memory that indicates which page, if any, occupies the page frame;
                the table got this name because the information in it is the "inverse" of the
                information in a page table. The size of an inverted page table is governed by
                the size of memory, so it is independent of the number and sizes of processes.
                However, information about a page cannot be accessed directly as in a page
                table; it has to be searched for in the IPT.
             Â·  Multilevel page table: The page table of a process is itself paged; the entire
                page table therefore does not need to exist in memory at any time. A higher-
                level page table is used to access pages of the page table. If the higher-level
                page table is large, it could itself be paged, and so on. In this organization,



                                                                                                Chapter 12  Virtual Memory  429
the page table entry of a page has to be accessed through relevant entries of
the higher-level page tables.
In both approaches, the TLB is used to reduce the number of memory
references needed to perform address translation.
12.2.3.1 Inverted Page Tables
Figure 12.10(a) illustrates address translation using an inverted page table (IPT).
Each entry of the inverted page table is an ordered pair consisting of a process id
and a page number. Thus a pair (R, pi) in the fith entry indicates that page frame
fi is occupied by page pi of a process R. While scheduling a process, the scheduler
copies the id of the process from its PCB into a register of the MMU. Let this
id be P. The MMU performs address translation for a logical address (pi, bi) in
process P, using the following steps:
1. Separate the components pi and bi of the logical address.
2. Using the process id P, form the pair (P, pi).
3. Search for the pair (P, pi) in the IPT. Raise a page fault if the pair does not
exist in the IPT.
        (a)                                                      MMU
                                                             Page
                                                                fault
               Memory                       P    Process
                                                     id                     Page       Ref
                                                 pi      bi                  id        info
                               1         2                   3
                                            P    pi
                                                                       fi  (P, pi)
                                                 fi      bi  4
               Add     Â·Â·
                                         5
                                                                           Inverted page table
        (b)                pi                                Page      Ref
                    P                                        id        info  Pointer
                       h                             fk      (Q, pk)
                                  v  fk              fi      (P, pi)
                                                     fl      (R, pl )               -
                                     Hash table              Inverted page table
Figure  12.10  Inverted page table: (a) concept; (b) implementation using a hash table.



430  Part 3  Memory Management
                      4. If the pair (P, pi) exists in entry fi of the IPT, copy the page frame number fi
                      for use in address translation.
                      5. Calculate the effective memory address using fi and bi.
                   These steps are shown as the circled numbers 1 to 5 in Figure 12.10(a).
                      The search for (P, pi) in Step 3 should be conducted efficiently, otherwise it
                   would slow down address translation. Accordingly, a hash table is used to speed
                   up the search in the inverted page table. Figure 12.10(b) shows an arrangement
                   called hash-with-chaining, which operates as follows: Each entry of the inverted
                   page table contains an additional field pointer, which points to another entry in the
                   same table. To hash a pair (P, pi), we first concatenate the bit strings representing
                   P and pi to obtain a larger bit string. We now interpret this bit string as an integer
                   number x, and apply the following hash function h to it:
                                               h(x)    =  remainder  (  x    )
                                                                          a
                   where a is the size of the hash table, which is typically some prime number. h(x),
                   which is in the range 0, . . . , a - 1, is an entry number in the hash table. Let v
                   designate its value. Hashing of many process idÂ­page id pairs may produce the
                   same value v, because the total number of pages of all processes in memory is
                   much larger than the size of the hash table. Entries of all these pairs in the inverted
                   page table are chained together by the pointer field.
                      The inverted page table is constructed and maintained by the virtual memory
                   manager as follows: When page pi of process P is loaded in page frame fi in
                   memory, the virtual memory manager stores the pair (P, pi) in the fith entry of
                   the inverted page table. It now hashes this pair to obtain an entry number, say
                   v, and adds the fith entry of the inverted page table in the chain starting on the
                   vth entry of the hash table as follows: It copies the value found in the vth entry
                   of the hash table into the pointer field of the fith entry of the inverted page table,
                   and enters fi into the vth entry of the hash table. When this page is removed from
                   memory, the virtual memory manager deletes its entry from the chain starting
                   on the vth entry of the hash table. In Figure 12.10(b), the pages (R, pl ), (P, pi)
                   and (Q, pk) were loaded into page frames fl , fi and fk, respectively, and they all
                   happened to hash into the vth entry of the hash table. Example 12.2 describes
                   how the MMU uses the inverted page table during address translation.
Â·
     Example 12.2  Search in the Inverted Page Table
                   The logical address (pi, bi) is to be translated by using the inverted page table
                   of Figure 12.10(b). The pair (P, pi) is hashed to obtain an entry number v in
                   the hash table. The chain starting on this entry is searched. The pair (P, pi)
                   does not match with the pair (Q, pk) found in the page id field of the first entry
                   of the chain. Therefore, the MMU uses the pointer field of this entry to locate
                   the next entry in the chain. The pair in this entry matches (P, pi), so the MMU
                   uses the entry number of this entry, i.e., fi, as the frame number to form the
                   physical address (fi, bi).
                   Â·



                                                                    Chapter 12          Virtual Memory  431
The average number of comparisons required to locate the entry of a pair
(P, pi) in the inverted page table depends on the average length of the chain starting
on an entry of the hash table. Increasing the size of the hash table, a, reduces the
average length of the chain. A value of a > 2 Ã #frames ensures that the average
number of entries in a linked list is less than 2. The inverted page table contains
exactly #frames entries in it. Note that the inverted page table does not contain
any information about pages that are not present in memory; a conventional page
table would have to be maintained on disk to contain their information. Inverted
page tables have been used in the IBM RS 6000 and AS 400 systems, and in the
PowerPC and PA-RISC architectures. They have also been used in Solaris OSs
for Sparc architectures.
12.2.3.2 Multilevel Page Tables
The memory requirement of the page table of a process is reduced by paging
the page table itself and loading its pages on demand just like pages of pro-
cesses. This approach requires a two-tiered addressing arrangement in which a
higher-level page table contains entries that hold information about pages of
the page table and the page table contains information concerning pages of the
process. The information in each of these tables is similar to the information
contained in a conventional page table. Figure 12.11 illustrates the concept of a
two-level page table. Memory now contains two kinds of pages--pages of pro-
cesses and pages of page tables of processes, which we shall call PT pages. Only
three PT pages of a process P are currently in memory. For address translation
of a logical address (pi, bi) in process P, page pi of process P should exist in
memory and the PT page that contains the entry for page pi should also exist
in memory.
As mentioned in Section 12.2, the page number and byte number in a logical
address (pi, bi) are represented in np and nb bits. The size of each page table entry
is a power of 2, so the number of page table entries that fit in one PT page is also a
power of 2. If the size of a table entry is 2e bytes, the number of page table entries
in one PT page is 2nb /2e, i.e., 2nb-e. Therefore, the page number pi in the logical
address itself consists of two parts--id of the PT page that contains the page table
entry of pi, and an entry number within the PT page. As shown in Figure 12.11,
we call these two parts p1i and p2i , respectively. From the preceding discussion, pi2
is contained in the lower order nb - e bits of pi. Since the binary representation
of pi contains np bits, pi1 is contained in np - (nb - e) higher-order bits.
Figure 12.11 illustrates address translation for a logical address (pi, bi). It
consists of the following steps:
1. The address (pi, bi) is regrouped into three fields
                          np - (nb - e)- nb - e -       nb       -
                                  pi1  p2i              bi
The contents of these fields are pi1, p2i and bi, respectively.



432  Part 3  Memory Management
                                                                              Pages of
                                                       Pages of the           process P
                                                   page table of P
                                                   (i.e., PT pages of P)
                                Higher-level
                                page table
                                pi1  x
                                     x
                                     x
                                                   p2i                                   Page pi
                                                                          bi             Byte with
                                                                                         address (pi ,bi)
                                     p1i      pi2  bi
                                          pi
             Figure 12.11  Two-level page table organization.
             2.  The PT page with the number p1i contains the page table entry for pi. The
                 MMU checks whether this page exists in memory and raises a page fault if it
                 does not. The page fault is serviced by the virtual memory manager to load
                 the PT page in memory.
             3.  pi2 is the entry number for pi in the PT page. The MMU uses information in
                 this entry to check whether page pi exists in memory and raises a page fault
                 if it does not. The virtual memory manager services the page fault and loads
                 page pi in memory.
             4.  The contents of pi's page table entry are used to perform address translation.
                 Thus, address translation requires two memory accesses--one to access the
             higher-level page table and another to access the page table of process P. It can be
             speeded up through the TLB by making two kinds of entries--entries of the form
             (P, p1i , frame number, protection info) help to eliminate accesses to the higher-
             level page table of process P and entries of the form (P, pi1, pi2, frame number,
             protection info) help to eliminate accesses to the page table of P.
                 When the size of the higher-level page table in a two-level page table organiza-
             tion is very large, the higher-level page table can itself be paged. This arrangement



                                              Chapter 12                                     Virtual Memory  433
results in a three-level page table structure. Address translation using three-level
page tables is performed by an obvious extension of address translation in two-
level page tables. A logical address (pi, bi) is split into four components pi1, p2i , pi3,
and bi, and the first three components are used to address the three levels of the
page table. Thus address translation requires up to three memory accesses. In
computer systems using 64-bit addresses, even the highest-level page table in a
three-level page table organization may become too large. Four-level page tables
are used to overcome this problem.
The Intel 80386 architecture used two-level page tables. Three and four-level
page tables have been used in the Sun Sparc and Motorola 68030 architectures,
respectively.
12.2.4 I/O Operations in a Paged Environment
A process makes a system call for performing I/O operations. Two of its param-
eters are the number of bytes to be transferred and the logical address of the
data area, which is the area of memory that participates in the data transfer. The
call activates the I/O handler in the kernel. The I/O subsystem does not contain
an MMU; it uses physical addresses to implement data transfer to and from
the memory. Consequently, the I/O handler has to perform a few preparatory
actions before initiating the I/O operation. The first of these is to replace the
logical address of the data area with its physical address, using information from
the page table of the process. It has to perform some more actions to address two
more issues discussed in the following.
The data area in an I/O operation may span several pages of the process. A
page fault while accessing a page of the data area would disrupt the I/O oper-
ation, so all these pages must remain in memory while I/O is being performed.
The I/O handler satisfies this requirement by loading all pages of the data area
into memory and putting an I/O fix on each page to instruct the virtual memory
manager that these pages should not be replaced until the I/O fix is removed at
the end of the I/O operation. It now starts the I/O operation. A simple way to
implement I/O fixing of pages is to add an I/O fix bit in the misc info field of each
page table entry.
Since the I/O subsystem operates without an MMU, it expects the data area
to occupy a contiguous area of memory. However, the process is paged, hence
pages of the data area may not have contiguous physical addresses. This situa-
tion can be addressed in two ways. Most I/O subsystems provide a scatter/gather
feature, which can deposit parts of an I/O operation's data in noncontiguous
areas of memory. For example, the first few bytes from an I/O record can be
read into a page frame located in one part of memory and the remaining bytes
can be read into another page frame located in a different part of memory.
Analogously, a "gather write" can draw the data of the I/O operation from
noncontiguous memory areas and write it into one record on an I/O device.
Example 12.3 illustrates how a scatter-read operation is used to implement an
I/O operation that spans two pages in a process. If an I/O subsystem does not
provide the scatter/gather feature, the I/O handler can handle the situation in



434  Part 3  Memory Management
                   two ways. It can either instruct the virtual memory manager to put pages con-
                   taining the data area contiguously in memory, or it can first read the data into
                   a kernel area that has contiguous physical addresses and then copy it to the
                   data area in the process. Analogous provisions can be made to support a write
                   operation.
Â·
     Example 12.3  I/O Operations in Virtual Memory
                   Page i2 of a process Pi contains a system call "perf_io (alpha, read, 828,
                   (i1, 520))," where alpha is a file, 828 is the count of data bytes to be read,
                   and (i1, 520) is the logical address of the start of the data area. Figure 12.12
                   illustrates how the I/O operation is implemented. The page size is 1 KB, and so
                   the data area is situated in pages i1 and i1 + 1 of the process. Before initiating
                   the I/O operation, the I/O handler invokes the virtual memory manager to
                   load pages i1 and i1 + 1 into memory. They are loaded into page frames 14
                   and 10 of memory. The I/O handler puts an I/O fix on these pages by setting
                   the I/O fix bits in the misc info field of their page table entries. These pages are
                   not replaced until the I/O fix is removed at the end of the I/O operation. The
                   I/O handler now generates a scatter-read operation to read the first 504 bytes
                   starting at byte number 520 in page frame 14, and the remaining 324 bytes
                   starting at byte number 0 in page frame 10. It removes the I/O fix on pages 14
                   and 10 when the I/O operation completes.
                   Â·
                                  Logical address space           frame                Memory
                                                                      #                                         scatter-read
                                                                         8    Perf_io  (read, 828, (i1,  520))  504, (14,520),
                           i2     Perf_io (read, 828, (i1, 520))                                                324, (10,0)
                                                                      10
                           i1
                           i1+1
                                                                      14
                                                 Valid            Page        Misc
                                                 bit              frame #     info
                                           i1    1                14        I/O fix
                                           i1+1  1                10        I/O fix
                                           i2    1                8
                                                                  Page table
                   Figure  12.12  An  I/O  operation in virtual memory.



                                                                            Chapter 12   Virtual Memory  435
12.3   THE VIRTUAL MEMORY MANAGER                                                                        Â·
The virtual memory manager uses two data structures--the page table, whose
entry format is shown in Figure 12.3, and the free frames list. The ref info and
modified fields in a page table entry are typically set by the paging hardware. All
other fields are set by the virtual memory manager itself. Table 12.4 summarizes
the functions of the virtual memory manager. We discuss the first four func-
tions in this section. Other functions--page replacement, allocation of memory
to processes, and implementation of page sharing--are discussed in the next few
sections.
Management of the Logical Address Space of a Process                The virtual memory
manager manages the logical address space of a process through the following
subfunctions:
1. Organize a copy of the instructions and data of the process in its swap space.
2. Maintain the page table.
3. Perform page-in and page-out operations.
4. Perform process initiation.
As mentioned earlier in Section 12.2, a copy of the entire logical address space
of a process is maintained in the swap space of the process. When a reference to
a page leads to a page fault, the page is loaded from the swap space by using a
page-in operation. When a dirty page is to be removed from memory, a page-out
operation is performed to copy it from memory into a disk block in the swap
Table 12.4     Functions     of the Virtual Memory Manager
Function                     Description
Manage logical address       Set up the swap space of a process. Organize its logical
space                        address space in memory through page-in and page-out
                             operations, and maintain its page table.
Manage memory                Keep track of occupied and free page frames in memory.
Implement memory             Maintain the information needed for memory
protection                   protection.
Collect page reference       Paging hardware provides information concerning page
information                  references. This information is maintained in
                             appropriate data structures for use by the page
                             replacement algorithm.
Perform page replacement     Perform replacement of a page when a page fault arises
                             and all page frames in memory, or all page frames
                             allocated to a process, are occupied.
Allocate physical memory     Decide how much memory should be allocated to a
                             process and revise this decision from time to time to suit
                             the needs of the process and the OS.
Implement page sharing       Arrange sharing of pages be processes.



436  Part 3  Memory Management
                   space. Thus the copy of a page in the swap space is current if that page is not in
                   memory, or it is in memory but it has not been modified since it was last loaded.
                   For other pages the copy in the swap space is stale (i.e., outdated), whereas that
                   in memory is current.
                   One issue in swap space management is size of the swap space of a process.
                   Most virtual memory implementations permit the logical address space of a pro-
                   cess to grow dynamically during its operation. This can happen for a variety of
                   reasons. The size of stack or PCD data areas may grow (see Section 11.4.2), or the
                   process may dynamically link more modules or may perform memory mapping
                   of files (see Section 12.7). An obvious approach to handling dynamic growth of
                   address spaces is to allocate swap space dynamically and noncontiguously. How-
                   ever, this approach faces the problem that the virtual memory manager may run
                   out of swap space during operation of a process.
                   To initiate a process, only the page containing its start address, i.e., address
                   of its first instruction, need managers to be loaded in memory. Other pages are
                   brought in on demand. Details of the page table and the page-in and page-out
                   operations have been described earlier in Section 12.2.
                   Management of Memory   The free frames list is maintained at all times. A page
                   frame is taken off the free list to load a new page, and a frame is added to it when
                   a page-out operation is performed. All page frames allocated to a process are
                   added to the free list when the process terminates.
                   Protection  During process creation, the virtual memory manager constructs its
                   page table and puts information concerning the start address of the page table
                   and its size in the PCB of the process. The virtual memory manager records access
                   privileges of the process for a page in the prot info field of its page table entry.
                   During dispatching of the process, the kernel loads the page-table start address of
                   the process and its page-table size into registers of the MMU. During translation
                   of a logical address (pi, bi), the MMU ensures that the entry of page pi exists in
                   the page table and contains appropriate access privileges in the prot info field.
                   Collection of Information for Page Replacement       The ref info field of the page
                   table entry of a page indicates when the page was last referenced, and the modified
                   field indicates whether it has been modified since it was last loaded in memory.
                   Page reference information is useful only so long as a page remains in memory;
                   it is reinitialized the next time a page-in operation is performed for the page.
                   Most computers provide a single bit in the ref info field to collect page reference
                   information. This information is not adequate to select the best candidate for
                   page replacement. Hence the virtual memory manager may periodically reset the
                   bit used to store this information. We discuss this aspect in Section 12.4.1.
Â·
     Example 12.4  Page Replacement
                   The memory of a computer consists of eight page frames. A process P1 consists
                   of five pages numbered 0 to 4. Only pages 1, 2, and 3 are in memory at the
                   moment; they occupy page frames 2, 7, and 4, respectively. Remaining page



                                                                                     Chapter  12  Virtual  Memory  437
          Memory                                       Memory
     0                                            0
     1                                            1
     2                    Valid  Page     Misc    2                     Valid  Page     Misc
     3                    bit    frame #  info    3                     bit    frame #  info
     4                 0  0                       4                0    0
     5                 1  1      2        t4, m   5                1    0
     6                 2  1      7        t11     6                2    1      7        t11
                       3  1      4        t9                       3    1      4        t9
                                                                                        t12
     7                 4  0                       7                4    1      2
                          Page table of P1                              Page table of P1
     (a)                                          (b)
   Figure 12.13  Data  structures of the virtual  memory manager:  (a)  before and (b) after  a page
   replacement.
   frames have been allocated to other processes and no free page frames are left
   in the system.
        Figure 12.13(a) illustrates the situation in the system at time instant t1+1,
   i.e., a little after t11. Only the page table of P1 is shown in the figure since
   process P1 has been scheduled. Contents of the ref info and modified fields
   are shown in the misc info field. Pages 1, 2, and 3 were last referenced at time
   instants t4, t11, and t9, respectively. Page 1 was modified sometime after it
   was last loaded. Hence the misc info field of its page table entry contains the
   information t4, m.
        At time instant t12, process P1 gives rise to a page fault for page 4. Since all
   page frames in memory are occupied, the virtual memory manager decides to
   replace page 1 of the process. The mark m in the misc info field of page 1's page
   table entry indicates that it was modified since it was last loaded, so a page-out
   operation is necessary. The page frame # field of the page table entry of page
   1 indicates that the page exists in page frame 2. The virtual memory manager
   performs a page-out operation to write the contents of page frame 2 into the
   swap area reserved for page 1 of P1, and modifies the valid bit in the page table
   entry of page 1 to indicate that it is not present in memory. A page-in operation
   is now initiated for page 4 of P1. At the end of the operation, the page table
   entry of page 4 is modified to indicate that it exists in memory in page frame 2.
   Execution of P1 is resumed. It now makes a reference to page 4, and so the
   page reference information of page 4 indicates that it was last referenced at t12.
   Figure 12.13(b) indicates the page table of P1 at time instant t1+2.
                                                                                                  Â·
12.3.1 Overview of Operation of the Virtual Memory Manager
The  virtual     memory   manager         makes   two  important        decisions       during    its
operation:
Â·    When a page fault occurs during operation of some process proci, it decides
     which page should be replaced.



438  Part 3  Memory Management
                                                               Virtual memory
                                                               data structures
                                                               Page tables
                                             Page                        ..     Memory
                                         replacement                            allocation
                                             policy                             policy
                                                               Free frames
                                                                         list
                     Paging         Page-in          Page-out
                     mechanisms
                                                                                Data flow
                                             Paging hardware                    Control flow
             Figure  12.14 Modules  of the virtual memory      manager.
             Â· Periodically it decides how much memory, i.e., how many page frames, should
                be allocated to each process.
                As discussed in later sections, these decisions are taken independently of one
             another. When a page fault occurs, the virtual memory manager merely replaces
             a page of the same process if all page frames allocated to the process are occupied.
             When it decides to increase or decrease memory committed for a process, it merely
             specifies the new number of page frames that should be allocated to each process.
                Figure 12.14 depicts the arrangement of policy and mechanism modules of
             the virtual memory manager. The page replacement policy uses the page refer-
             ence information available in the virtual memory manager's data structures, and
             updates the page tables to reflect its decisions. It is implemented using page-in
             and page-out operations as mechanisms. The page-in and page-out mechanisms
             interact with the paging hardware to implement their functionalities. The pag-
             ing hardware updates page reference information maintained in virtual memory
             manager's tables. The memory allocation policy uses the information in the page
             tables and the free frames list to periodically decide whether and how to vary the
             memory allocated to each process. We use the following notation for the memory
             allocated to each process:
             alloci        Number of page frames allocated to process proci
             We omit the subscript of alloci when only one process is under consideration.
             12.4    PAGE REPLACEMENT POLICIES                                                     Â·
             As discussed earlier in Section 12.2.1.1, a page replacement policy should replace
             a page that is not likely to be referenced in the immediate future. We evaluate
             the following three page replacement policies to see how well they fulfill this
             requirement.
             Â·  Optimal page replacement policy
             Â·  First-in, first-out (FIFO) page replacement policy
             Â·  Least recently used (LRU) page replacement policy



                                                                              Chapter 12  Virtual Memory        439
For our analysis of these page replacement policies, we rely on the concept of
page reference strings. A page reference string of a process is a trace of the pages
accessed by the process during its operation. It can be constructed by monitoring
the operation of a process, and forming a sequence of page numbers that appear in
logical addresses generated by it. The page reference string of a process depends
on the data input to it, so use of different data would lead to a different page
reference string for a process.
For convenience we associate a reference time string t1, t2, t3, . . . with each
page reference string. This way, the kth page reference in a page reference string
is assumed to have occurred at time instant tk. (In effect, we assume a logical
clock that runs only when a process is in the running state and gets advanced only
when the process refers to a logical address.) Example 12.5 illustrates the page
reference string and the associated reference time string for a process.
                                                                                                                Â·
Page Reference String                                                                        Example      12.5
A computer supports instructions that are 4 bytes in length, and uses a page
size of 1 KB. It executes the following nonsense program in which the symbols
A and B are in pages 2 and 5, respectively:
                                 START       2040
                                 READ        B
          LOOP                   MOVER       AREG,  A
                                 SUB         AREG,  B
                                 BC          LT,   LOOP
                                 ...
                                 STOP
          A                      DS          2500
          B                      DS          1
                                 END
The page reference string and the reference time string for the process are as
follows:
          Page reference string        1, 5, 1, 2, 2, 5, 2, 1, . . .
          Reference time string        t1, t2, t3, t4, t5, t6, t7, t8, . . .
The logical address of the first instruction is 2040, and so it lies in page 1.
The first page reference in the string is therefore 1. It occurs at time instant
t1. B, the operand of the instruction is situated in page 5, and so the second
page reference in the string is 5, at time t2. The next instruction is located in
page 1 and refers to A, which is located in page 2, and thus the next two page
references are to pages 1 and 2. The next two instructions are located in page 2,
and the instruction with the label LOOP is located in page 1. Therefore, if the
value of B input to the READ statement is greater than the value of A, the next
four page references would be to pages 2, 5, 2 and 1, respectively; otherwise,
the next four page references would be to pages 2, 5, 2 and 2, respectively.
                                                                                          Â·



440  Part 3  Memory Management
                   Optimal   Page  Replacement    Optimal  page  replacement  means       making       page
                   replacement decisions in such a manner that the total number of page faults
                   during operation of a process is the minimum possible; i.e., no other sequence
                   of page replacement decisions can lead to a smaller number of page faults. To
                   achieve optimal page replacement, at each page fault, the page replacement pol-
                   icy would have to consider all alternative page replacement decisions, analyze
                   their implications for future page faults, and select the best alternative. Of course,
                   such a policy is infeasible in reality: the virtual memory manager does not have
                   knowledge of the future behavior of a process. As an analytical tool, however, this
                   policy provides a useful comparison in hindsight for the performance of other
                   page replacement policies (see Example 12.6, below, and Exercise 12.5).
                   Although optimal page replacement might seem to require excessive analysis,
                   Belady (1966) showed that it is equivalent to the following simple rule: At a page
                   fault, replace the page whose next reference is farthest in the page reference string.
                   FIFO Page Replacement          At every page fault, the FIFO page replacement policy
                   replaces the page that was loaded into memory earlier than any other page of
                   the process. To facilitate FIFO page replacement, the virtual memory manager
                   records the time of loading of a page in the ref info field of its page table entry.
                   When a page fault occurs, this information is used to determine pearliest, the page
                   that was loaded earlier than any other page of the process. This is the page that
                   will be replaced with the page whose reference led to the page fault.
                   LRU Page Replacement   The LRU policy uses the law of locality of reference as
                   the basis for its replacement decisions. Its operation can be described as follows:
                   At every page fault the least recently used (LRU) page is replaced by the required
                   page. The page table entry of a page records the time when the page was last
                   referenced. This information is initialized when a page is loaded, and it is updated
                   every time the page is referenced. When a page fault occurs, this information is
                   used to locate the page pLRU whose last reference is earlier than that of every
                   other page. This page is replaced with the page whose reference led to the page
                   fault.
                   Analysis of Page Replacement Policies   Example 12.6 illustrates operation of
                   the optimal, FIFO, and LRU page replacement policies.
Â·
     Example 12.6  Operation of Page Replacement Policies
                   A page reference string and the reference time string for a process P are as
                   follows:
                           Page reference string  0, 1, 0, 2, 0, 1, 2, . . .                (12.4)
                           Reference time string  t1, t2, t3, t4, t5, t6, t7, . . .         (12.5)
                   Figure 12.15 illustrates operation of the optimal, FIFO and LRU page replace-
                   ment policies for this page reference string with alloc = 2. For convenience, we
                   show only two fields of the page table, valid bit and ref info. In the interval t0



                                                                                 Chapter   12  Virtual  Memory  441
                       Optimal                   FIFO                        LRU
Time     Page     Valid Ref  Replace-     Valid  Ref   Replace-     Valid Ref    Replace-
instant  ref      bit  info  ment         bit    info  ment         bit  info     ment
               0  1                    0  1      t1              0  1        t1
t1       0     1  0             -      1  0            -         1  0             -
               2  0                    2  0                      2  0
               0  1                    0  1      t1              0  1        t1
                                                 t2                          t2
t2       1     1  1             -      1  1            -         1  1             -
               2  0                    2  0                      2  0
               0  1                    0  1      t1              0  1        t1
t3       0     1  1             -      1  1      t2    -         1  1        t2   -
               2  0                    2  0                      2  0
               0  1          Replace   0  0            Replace   0  1        t3  Replace
t4       2     1  0          1 by 2    1  1      t2    0 by 2    1  0             1 by 2
               2  1                    2  1      t4              2  1        t4
               0  1                    0  1      t5    Replace   0  1        t5
t5       0     1  0             -      1  0            1 by 0    1  0             -
               2  1                    2  1      t4              2  1        t4
               0  0          Replace   0  1      t5    Replace   0  1        t5  Replace
t6       1     1  1          0 by 1    1  1      t6    2 by 1    1  1        t6   2 by 1
               2  1                    2  0                      2  0
               0  0                    0  0            Replace   0  0            Replace
t7       2     1  1             -      1  1      t6    0 by 2    1  1        t6   0 by 2
               2  1                    2  1      t7              2  1        t7
Figure 12.15  Comparison of page replacement     policies with   alloc = 2.
to t3 (inclusive), only two distinct pages are referenced: pages 0 and 1. They
can both be accommodated in memory at the same time because alloc = 2. t4
is the first time instant when a page fault leads to page replacement.
The left column shows the results for optimal page replacement. Page
reference information is not shown in the page table since information con-
cerning past references is not needed for optimal page replacement. When the
page fault occurs at time instant t4, page 1 is replaced because its next reference
is farther in the page reference string than that of page 0. At time t6 page 1
replaces page 0 because page 0's next reference is farther than that of page 2.
The middle column of Figure 12.15 shows the results for the FIFO replace-
ment policy. When the page fault occurs at time t4, the ref info field shows that
page 0 was loaded earlier than page 1, and so page 0 is replaced by page 2.
The last column of Figure 12.15 shows the results for the LRU replace-
ment policy. The ref info field of the page table indicates when a page was last
referenced. At time t4, page 1 is replaced by page 2 because the last reference
of page 1 is earlier than the last reference of page 0.



442  Part 3  Memory Management
                The total number of page faults occurring under the optimal, FIFO, and
             LRU policies are 4, 6, and 5, respectively. By definition, no other policy has
             fewer page faults than the optimal page replacement policy.
             Â·
                When we analyze why the LRU policy performed better than the FIFO pol-
             icy in Example 12.6, we find that the FIFO policy removed page 0 at time t4 but
             LRU did not do so because it had been referenced later than page 1. This decision
             is consistent with the law of locality of reference, which indicates that because
             page 0 was referenced more recently than page 1, it has a higher probability of
             being referenced again than page 1. The LRU policy performed better because
             page 0 was indeed referenced earlier after time t4 than page 1 was. However, the
             LRU page replacement policy may not perform better than the FIFO policy if
             references in a page reference string do not follow the law of locality of reference.
             For example, for alloc = 3, the LRU and FIFO policies would perform identi-
             cally for the page reference string 0, 1, 2, 3, 0, 1, 2, 3, while the LRU policy would
             perform worse than the FIFO policy for the string 0, 1, 2, 0, 3, 1. However, such
             situations are not encountered frequently.
                To achieve the desirable page fault characteristic of Figure 12.6, a page
             replacement policy must possess the stack property (also called the inclusion
             property). It is defined by using the following notation:
                {pi }kn         Set of pages existing in memory at time instant tk+ if alloci = n
                                all through the operation of process proci (tk+ implies a time
                                after time instant tk but before tk+1).
                Definition 12.3 Stack Property        A page replacement policy possesses the
                stack property if
                                   {pi}nk  {pi}km     for all n, m such that n < m.
                Figure 12.16 illustrates {pi}nk for different values of n for a page replace-
             ment policy. We find that {pi}kn  {pi}nk+1 for n = 1, . . . , 4. Hence the algorithm
             possesses the stack property.
                To understand how the stack property ensures the desirable page fault char-
             acteristic of Figure 12.6, consider two runs of process proci, one with alloci = n
             all through the execution, and another with alloci = m, such that n < m. If a
                                   4               4  4    4             4
                                                   5  5    5             5
                                                      3    3             3
                                                           1             1
                                                                         2
                                   n=1      n=2       n=3  n=4           n=5
             Figure 12.16  {pi}kn for different n for a page replacement policy processing the stack property.



                                                                                Chapter 12             Virtual Memory        443
page replacement policy exhibits the stack property, then at identical points dur-
ing these operations of proci (i.e., at identical time instants) all pages that were in
memory when alloci = n would also be in memory when alloci = m. In addition,
memory also contains m - n other pages of the process. If any of these pages are
referenced in the next few page references of proci, page faults occur if alloci = n,
but  not  if alloci  =  m.    Thus   the page fault rate is higher if   alloci     =  n  than if
alloci = m. This satisfies the page fault characteristic of Figure 12.6. The page
fault rates will be identical if these m - n pages are not referenced in the next few
page references. However, in no case will the page fault rate increase when the
memory allocation for a process is increased. If a page replacement policy does
not exhibit the stack property, then {pi}mk may not contain some page(s) contained
in {pi}nk. References to these pages would result in page faults. Hence the page
fault rate can increase when the memory allocation for a process is increased.
     Example 12.7 illustrates that the FIFO page replacement policy does not
exhibit the stack property. One can prove that the LRU page replacement policy
exhibits the stack property (see Exercise 12.9).
                                                                                                                             Â·
Problems in FIFO Page Replacement                                                                          Example     12.7
Consider the following page reference and reference time strings for a process:
     Page reference string           5, 4, 3, 2, 1, 4, 3, 5, 4, 3, 2, 1, 5, . . .          (12.6)
     Reference time string           t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, . . .
                                                                                           (12.7)
Figure 12.17 shows operation of the FIFO and LRU page replacement policies
for this page reference string. Page references that cause page faults and result
in page replacement are marked with a  mark. A column of boxes is associated
with each time instant. Each box is a page frame; the number contained in
it indicates which page occupies it after execution of the memory reference
marked under the column.
      For FIFO page replacement, we have {pi}412 = {2, 1, 4, 3}, while {pi}132 =
{1, 5, 2}. Thus, FIFO page replacement does not exhibit the stack property.
This      leads  to  a  page  fault  at  t13  when  alloci  =  4,  but  not  when  alloci           =  3.
Thus, a total of 10 page faults arise in 13 time instants when alloci = 4, while
9 page faults arise when alloci = 3. For LRU, we see that {pi}3  {pi}4 at all
time instants.
                                                                                                       Â·
     Figure 12.18 illustrates the page fault characteristic of FIFO and LRU page
replacement for page reference string (12.6). For simplicity, the vertical axis
shows the total number of page faults rather than the page fault frequency.
Figure 12.18(a) illustrates an anomaly in behavior of FIFO page replacement--
the number of page faults increases when memory allocation for the process is
increased. This anomalous behavior was first reported by Belady and is therefore
known as Belady's anomaly.



444  Part 3  Memory Management
                              FIFO      alloci =  3                3   3   3   4    4      4   4    4     2       2  2
                                                            4      4   4   1   1    1      5   5    5     5       5  5
                                                     5      5      5   2   2   2    3      3   3    3     3       1  1
                                                     5      4*     3*  2*  1*  4*   3*     5*  4    3     2*   1*    5
                                        alloci =  4                    2   2   2    2      2   2    3     3       3  3
                                                                   3   3   3   3    3      3   4    4     4       4  5
                                                            4      4   4   4   4    4      5   5    5     5       1  1
                                                     5      5      5   5   1   1    1      1   1    1     2       2  2
                                                     5      4*     3*  2*  1*  4    3      5*  4*   3*    2*   1*    5*
                                     Time line       t1     t2     t3  t4  t5  t6   t7     t8  t9   t10   t11  t12   t13
                              LRU       alloci =  3                3   3   3   4    4      4   4    4     4       1  1
                                                            4      4   4   1   1    1      5   5    5     2       2  2
                                                     5      5      5   2   2   2    3      3   3    3     3       3  5
                                                     5      4*     3*  2*  1*  4*   3*     5*  4    3     2*   1*    5*
                                        alloci =  4                    2   2   2    2      5   5    5     5       1  1
                                                                   3   3   3   3    3      3   3    3     3       3  3
                                                            4      4   4   4   4    4      4   4    4     4       4  5
                                                     5      5      5   5   1   1    1      1   1    1     2       2  2
                                                     5      4*     3*  2*  1*  4    3      5*  4    3     2*   1*    5*
                                     Time line
                                                     t1     t2     t3  t4  t5  t6   t7     t8  t9   t10   t11  t12   t13
             Figure 12.17     Performance of      FIFO      and  LRU page      replacement.
                              14                                                    14
                              12                                                    12
             Number 10                                                     Number 10
                              8                                                         8
                     of page                                               of page
                              6                                                         6
                     faults                                                faults
                              4                                                         4
                              2                                                         2
                                  0  1  2         3      4      5                          0   1       2       3     4    5
                                        FIFO                alloc                                        LRU              alloc
             Figure  12.18    (a) Belady's anomaly in FIFO             page replacement;       (b)  page fault       characteristic  for
             LRU page replacement.
             The virtual memory manager cannot use FIFO page replacement because
             increasing the allocation to a process may increase the page fault frequency of the
             process. This feature would make it difficult to combat thrashing in the system.
             However, when LRU page replacement is used, the number of page faults is
             a nonincreasing function of alloc. Hence it is possible to combat thrashing by
             increasing the value of alloc for each process.
             12.4.1 Practical Page Replacement Policies
             Figure 12.19 shows a schematic diagram of a practical virtual memory manager.
             The virtual memory manager maintains a free frames list and tries to keep a
             few page frames in this list at all times. The virtual memory manager consists



                                                                           Chapter 12    Virtual Memory  445
                                              Virtual
                                              memory
                                              manager
              Page                   Free                      Page
              fault                  frames                    I/O
              handler                manager           manager
                       Page tables           Free frames list        Swap
                                                                     disk
Figure 12.19  Page replacement in practice.
of two daemon threads. The thread called free frames manager is activated by
the virtual memory manager when the number of free page frames drops below
a threshold defined by the virtual memory manager. The free frames manager
scans the pages present in memory to identify a few pages that can be freed, and
adds the page frames occupied by these pages to the free frames list. If the page
contained in a newly added page frame is dirty, it marks the page frame as dirty
in the free frames list. It also resets the valid bit of this page in the page table of
the process to which it belongs. The free frames manager puts itself to sleep when
the number of free page frames exceeds another threshold of the virtual memory
manager. The thread called page I/O manager performs page-out operations on
dirty page frames in the free frames list; it resets the dirty bit of a page frame
when its page-out operation is completed.
The page fault handler runs as an event handler of the kernel. It is activated
when a page fault occurs. It first checks whether the required page exists in any of
the page frames in the free frames list. If so, it simply reclaims the page by removing
its page frame from the free frames list, setting the valid bit of the page in the page
table of the process and copying the value of the dirty bit of the page frame into
the modified bit of the page. This operation makes the required page available
without having to perform a page-in operation. If the required page does not
exist in any page frame, it takes a clean page frame off the free frames list and
starts the page-in operation on it.
Effectively, the page replacement policy is implemented in the free frames
manager of the virtual memory manager; however, in the following we will dis-
cuss page replacement as if it were done directly by the virtual memory manager.
The LRU page replacement policy should be the automatic choice for imple-
mentation in a virtual memory manager because it exhibits the stack property.
However, LRU page replacement is not feasible because computers do not pro-
vide sufficient bits in the ref info field to store the time of last reference--most
computers provide a single reference bit for collecting information about refer-
ences to a page. Therefore page replacement policies have to be implemented



446  Part 3  Memory Management
                   using only the reference bit. This requirement has led to a class of policies called
                   not recently used (NRU) policies, where the reference bit is used to determine
                   whether a page has been recently referenced, and some page that has not been
                   recently referenced is replaced.
                   A simple NRU policy is as follows: The reference bit of a page is initialized
                   to 0 when the page is loaded, and it is set to 1 when the page is referenced. When
                   page replacement becomes necessary, if the virtual memory manager finds that
                   the reference bits of all pages have become 1, it resets the bits of all pages to 0
                   and arbitrarily selects one of the pages for replacement; otherwise, it replaces a
                   page whose reference bit is 0. Future page replacement would depend on which
                   of the pages were referenced after the reference bits were reset. Page replacement
                   algorithms called clock algorithms provide better discrimination between pages
                   by resetting reference bits of pages periodically, rather than only when all of them
                   become 1, so that it would be possible to know whether a page has been referenced
                   in the immediate past, say within the past 100 instructions, rather than since the
                   time when all reference bits were reset to 0.
                   Clock Algorithms    In clock algorithms, pages of all processes in memory are
                   entered in a circular list and pointers used by the algorithms move over the pages
                   repeatedly. The algorithms get their name from the fact that movement of the
                   pointers is analogous to movement of the hands of a clock over the clock dial.
                   The page pointed to by a pointer is examined, a suitable action is performed on
                   it, and the pointer is advanced to point to the next page. The clock algorithms
                   can also be applied at the level of a single process when the memory allocation
                   for a process is to be decreased. In this case, the virtual memory manager would
                   maintain a separate list of pages for each process and the clock algorithm would
                   scan only the list of the process whose memory allocation is to be decreased.
                   In the one-handed clock algorithm, a scan consists of two passes over all
                   pages. In the first pass, the virtual memory manager simply resets the reference
                   bit of the page pointed to by the pointer. In the second pass it finds all pages
                   whose reference bits are still off and adds them to the free list. In the two-handed
                   clock algorithm, two pointers are maintained. One pointer, which we will call the
                   resetting pointer (RP), is used for resetting the reference bits and the other pointer,
                   which we will call the examining pointer (EP), is used for checking the reference
                   bits. Both pointers are incremented simultaneously. The page frame to which the
                   checking pointer points is added to the free frames list if its reference bit is off.
                   Example 12.8 describes operation of the two-handed clock algorithm.
Â·
     Example 12.8  Two-Handed Clock Algorithm
                   Figure 12.20 illustrates operation of the two-handed clock algorithm when
                   used  by  the free  frames  manager  of  Figure  12.19.  The ref  mark  against     a
                   page implies that the reference bit of the page is set to 1; absence of this
                   mark implies that the reference bit is 0. When the free frames manager is
                   activated, it examines page 7, which is pointed to by the examining pointer
                   [see Figure 12.20(a)]. Its reference bit is 1, so both the resetting and examining



                                                                              Chapter 12  Virtual Memory  447
                               7ref                              7ref
                                     18ref                             18ref
                      EP
                                        9
                               RP                                EP
                                        6ref        RP                    6
                                     2                                 2
                           11  3                        11       3
                 (a)                           (b)
   Figure 12.20  Operation of  the two-handed  clock algorithm.
   pointers are advanced. At this time, the reference bit of page 6 is reset to 0
   because RP was pointing to it. The examining pointer moves over page 18
   (and the resetting pointer moves over page 2) because it, too, has its reference
   bit set to 1. It now rests on page 9. Page 9 has its reference bit 0, so it is
   removed from the list of pages in memory and added to the free frames list.
   The resetting and examining pointers now point to pages 6 and 11, respectively
   [see Figure 12.20(b)].
                                                                                          Â·
   The distance between the resetting and examining pointers gives different
properties to the page replacement algorithm. If the clock pointers are close
together, a page will be examined very soon after its reference bit has been reset,
hence only recently used pages will survive in memory. If the clock pointers are
far apart, only pages that have not been used in a long time would be removed.
12.5  CONTROLLING MEMORY ALLOCATION TO A PROCESS                                                          Â·
Section 12.2 described how an overcommitment of memory to processes affects
system performance because of a low degree of multiprogramming, whereas an
undercommitment of memory to processes leads to thrashing, which is charac-
terized by high page I/O, low CPU efficiency, and poor performance of processes
and the system. Keeping the memory allocation for a process within the desirable
operating zone shown in Figure 12.6 avoids both overcommitment and under-
commitment of memory to a process. However, it is not clear how the virtual
memory manager should decide the correct number of page frames to be allocated
to each process, that is, the correct value of alloc for each process.
   Two approaches have been used to control the memory allocation for a
process:
Â·  Fixed memory allocation: The memory allocation for a process is fixed. Hence
   performance of a process is independent of other processes in the system.
   When a page fault occurs in a process, one of its own pages is replaced. This
   approach is called local page replacement.



448  Part 3  Memory Management
             Â·  Variable memory allocation: The memory allocation for a process may be
                varied in two ways: When a page fault occurs, all pages of all processes that
                are present in memory may be considered for replacement. This is called global
                page replacement. Alternatively, the virtual memory manager may revise the
                memory allocation for a process periodically on the basis of its locality and
                page fault behavior, but perform local page replacement when a page fault
                occurs.
                In fixed memory allocation, memory allocation decisions are performed stat-
             ically. The memory to be allocated to a process is determined according to some
             criterion when the process is initiated. To name a simple example, the memory
             allocated to a process could be a fixed fraction of its size. Page replacement is
             always performed locally. The approach is simple to implement, and the over-
             head of page replacement is moderate, as only pages of the executing process are
             considered in a page replacement decision. However, the approach suffers from
             all problems connected with a static decision. An undercommitment or overcom-
             mitment of memory to a process would affect the process's own performance and
             performance of the system. Also, the system can encounter thrashing.
                In variable memory allocation using global page replacement, the allocation
             for the currently operating process may grow too large. For example, if an LRU
             or NRU page replacement policy is used, the virtual memory manager will be
             replacing pages of other processes most of the time because their last references
             will precede references to pages of the currently operating process. Memory allo-
             cation to a blocked process would shrink, and so the process would face high
             page fault rates when it is scheduled again.
                In variable memory allocation using local page replacement, the virtual mem-
             ory manager determines the correct value of alloc for a process from time to time.
             In the following, we discuss how this can be done in practice.
             Working Set Model  The concept of a working set provides a basis for deciding
             how many and which pages of a process should be in memory to obtain good
             performance of the process. A virtual memory manager following the working
             set model is said to be using a working set memory allocator.
             Definition 12.4 Working Set       The set of pages of a process that have been
             referenced in the previous    instructions of the process, where  is a parameter
             of the system.
                The previous    instructions are said to constitute the working set window.
             We introduce the following notation for our discussion:
             WS i (t,    )      Working set for process proci at time t for window size
             WSS i (t,      )   Size of the working set WSi(t,  ), i.e., the number of pages
                                in WSi(t,  ).
             Note that WSSi(t,  )          because a page may be referenced more than once in
             a working set window. We omit (t,  ) when t and          are either unimportant or
             obvious from the context.



                                                                              Chapter 12  Virtual Memory  449
A working set memory allocator either holds the complete working set of a
process in memory, or suspends the process. Thus, at any time instant t, a process
proci either has WSi in memory and alloci = WSSi, or it has alloci = 0. This
strategy helps in ensuring a good hit ratio in memory through the law of locality
of reference. It also prevents undercommitment of memory to a process, thereby
preventing thrashing.
The working set memory allocator must vary the degree of multiprogram-
ming in accordance with changes in working set sizes of processes. For example,
if { prock} is the set of processes in memory, the degree of multiprogramming
should be decreased if
                                       kWSSk > #frames
where #frames is the total number of page frames in memory. The working
set memory allocator removes some processes from memory until                   kWSSk 
#frames. The degree of multiprogramming should be increased if                  kWSSk <
#frames and there exists a process procg such that
                            WSSg  (#frames -              k WSS k )
procg should now be allocated WSSg page frames and its operation should be
resumed.
Variations in the degree of multiprogramming are implemented as follows:
The virtual memory manager maintains two items of information for each
process--alloci       and   WSS i .  When    the  degree  of  multiprogramming  is    to  be
reduced, the virtual memory manager selects the process to be suspended, say,
process proci. It now performs a page-out operation for each modified page of
proci and changes the status of all page frames allocated to it to free. alloci is
set to 0; however, the value of WSSi is left unchanged. When the degree of mul-
tiprogramming is to be increased and the virtual memory manager decides to
resume   proci ,  it  sets  alloci  =  WSSi  and  allocates   as  many  page  frames  as  the
value of alloci. It now loads the page of proci that contains the next instruction to
be executed. Other pages would be loaded when page faults occur. Alternatively,
the virtual memory manager loads all pages of WSi when execution of proci is
resumed. However, this approach may lead to redundant loading of pages because
some pages in WSi may not be referenced again.
Performance of a working set memory allocator is sensitive to the value of
. If     is too large, memory will contain some pages that are not likely to be
referenced again. This is overcommitment of memory to processes. Too large
a value of        also forces the virtual memory manager to reduce the degree of
multiprogramming, thereby affecting system performance. If              is too small, there
is a danger of undercommitment of memory to processes, leading to an increase
in page fault frequency and the possibility of thrashing.
Implementation of a Working Set Memory Allocator              Use of a working set mem-
ory allocator suffers from one practical difficulty. It is expensive to determine
WSi (t,   ) and alloci at every time instant t. To address this difficulty, a virtual



450  Part 3  Memory Management
                                  Process      t100          t200          t300            t400
                                           WSS  alloc    WSS  alloc    WSS      alloc  WSS  alloc
                                  P1       14        14  12        12  14        14    13        13
                                  P2       20        20  24        24  11        11    25        25
                                  P3       18        18  19        19  20        20    18        18
                                  P4       10        0   10        0   10        10    12        0
                   Figure  12.21  Operation of a working set memory allocator.
                   memory manager using a working set memory allocator can determine the work-
                   ing sets of processes periodically rather than at every time instant. Working sets
                   determined at the end of an interval are used to decide values of alloc for use
                   during the next interval. The next example illustrates this approach.
Â·
     Example 12.9  Working Set Memory Allocator
                   A virtual memory manager has 60 page frames available for allocation to
                   user processes. It recomputes the working sets of all processes at time instants
                   tj+Ã100, j = 1, 2 . . . . Following the computation of working sets, it handles each
                   process Pi as follows: It sets alloci = WSSi if it can allocate WSSi page frames
                   to it, else it sets alloci = 0 and removes all pages of Pi from memory. The value
                   of alloc assigned at t+jÃ100 is held constant until t(j+1)Ã100.
                      Figure 12.21 illustrates operation of the working set memory allocator. It
                   shows values of alloc and WSS for all processes at time instants t1+00, t2+00, t3+00,
                   and t4+00. At t1+00, WSS4 = 10, alloc4 = 0, and
                                                                       i=1, . . . ,3WSSi = 52. It implies
                   that the working set size of P4 is 10 page frames, however, its operation has been
                   suspended because only 60 - 52, i.e., 8, page frames are free. At t2+00, values of
                   WSSi, i = 1, . . . , 3 are recomputed. The value of WSS4 is carried over from
                   t1+00 since P4 has not been executed in the interval t100Â­t200. alloci, i = 1, . . . , 3
                   are now assigned new values. P4 still cannot be swapped in for lack of memory
                   since   i=1,...,3WSSi = 55, so only five page frames are free and WSS4 = 10.
                   At t300, P4 is swapped in; however, it is swapped out again at t400. Note that
                   during the interval t100 - t400 the smallest allocation for P2 is 11 page frames
                   and the largest allocation is 25 page frames. This variation is performed to
                   adjust the process's memory allocation to its recent behavior.
                      Expansion and contraction of alloc is performed as follows: At t200, the
                   virtual memory manager decides to reduce alloc1 from 14 page frames to 12
                   page frames, so it uses an NRU-like policy to remove two pages of P1. At t300,
                   it increases alloc1 to 14 page frames, so it allocates two more page frames to
                   alloc1. These page frames will be used when page faults arise during operation
                   of P1.
                   Â·
                      The virtual memory manager can use the reference bits provided by the pag-
                   ing hardware to determine the working sets. Reference bits of all pages in memory
                   can be turned off when working sets are determined. These bits will be turned on



                                                                      Chapter 12       Virtual Memory  451
again as these pages get referenced during the next interval. While performing
page replacements, the virtual memory manager can record which of the replaced
pages had their reference bits on. The working set at the end of the next interval
will consist of these pages and all pages in memory whose reference bits are on.
Implementation of working sets in this manner faces one problem. Resetting
of reference bits at the end of an interval would interfere with page replace-
ment decisions. If a page fault occurs in a process soon after working sets are
determined, the reference bits of most of the process's pages in memory will be
off. Hence the virtual memory manager cannot differentiate between these pages
for page replacement. If some processes either remain blocked or do not get
an opportunity to execute all through an interval, their allocations will shrink
unnecessarily. This aspect makes it difficult to decide on the correct size of    , the
working set window.
An alternative is to use a working set window for each process individually.
However, it would complicate the virtual memory manager and add to its over-
head. It would also not address the issue of interference with the page replacement
decisions. For these reasons, operating systems do not actually determine work-
ing sets of processes according to Definition 12.4. In Section 12.8.4 we describe
how the Windows operating systems use the notion of working set of a process.
12.6  SHARED PAGES                                                                                     Â·
Sharing of programs was discussed in Section 11.3.3. Static sharing results from
static binding performed by a linker or loader before execution of a program
begins (see Section 11.3.3.2). Figure 12.22(a) shows the logical address space of
program C. The Add (4,12) instruction in page 1 has its operand in page 4. With
static binding, if two processes A and B statically share program C, then C is
included in the code of both A and B. Let the 0th page of C become page i of
process A [see Figure 12.22(a)]. The instruction Add (4,12) in page 1 of program C
would be relocated to use the address (i +4,12). If the 0th page of C becomes page
j in process B, the Add instruction would be relocated to become Add            (j + 4, 12)
Thus, each page of program C has two copies in the address spaces of A and B.
These copies may exist in memory at the same time if processes A and B are in
operation simultaneously.
Dynamic binding (see Section 11.2) can be used to conserve memory by bind-
ing the same copy of a program or data to several processes. In this case, the
program or data to be shared would retain its identity [see Figure 12.22(c)]. It is
achieved as follows: The virtual memory manager maintains a shared pages table
to hold information about shared pages in memory. Process A makes a system
call to bind program C as a shared program starting at a specific page, say, page
i, in its logical address space. The kernel invokes the virtual memory manager,
which creates entries in the page table of A for pages of program C, and sets an
s flag in each of these entries to indicate that it pertains to a shared page. It now
checks whether the pages of program C have entries in the shared pages table. If
not, it makes such entries now, sets up the swap space for program C, and invokes



452  Part 3  Memory Management
             the dynamic linker, which dynamically binds program C to the code of process
             A. During this binding, it relocates the address-sensitive instructions of C. Thus,
             the Add instruction in page 1 of program C is modified to read Add (i + 4, 12)
             [see Figure 12.22(c)]. When a reference to an address in program C page faults,
             the virtual memory manager finds that it is a shared page, so it checks the shared
             pages table to check whether the required page is already in memory, which would
             happen if another process had used it recently. If so, it copies the page frame num-
             ber of the page from the shared pages table into the entry of that page in A's page
             table; otherwise, it loads the page in memory and updates its entry in A's page
             table and in the shared pages table. Similar actions are performed when process
             B dynamically binds program C to the start address of page i and references to
             C's pages in process B's instructions cause page faults. Figure 12.23 shows the
             resulting arrangement.
             Two conditions should be satisfied for dynamic binding of programs to work.
             The program to be shared should be coded as a reentrant program so that it
             can be invoked by many processes at the same time (see Section 11.3.3.2). The
             program should also be bound to identical logical addresses in every process that
                                                   Page  Process A      Page  Process A
                                                   #0                   #0
                                        Program    i     Add (i+4, 12)
                                             C     i+1
             Page    Program C                                                                   Page  Program C
             #0                                                                                  #0
                     Add (4, 12)                   Page  Process B      Page  Process B                Add (i+4, 12)
                                                   #0                   #0
                                        Program    j     Add (i+4, 12)
                                             C     j+1
             (a)                        (b)                             (c)
             Figure  12.22  Sharing of program C by processes A and B: (a) program         C; (b) static binding      of
             C to the codes of processes A and B; and (c) dynamic binding of C.
                                                         Page table              Swap space
                                  Page  Process A            of A            Memory        of C
                                  #
                     Pages of     i     s                i                                       Page Frame
                                        s               i+1
                     program C    i+1   s
                                                                                                 id     #
                                                                                                 C, 0
                                                                                                 C, 1
                                                         Page table
                                  Page
                                        Process B            of B            Add (i+4,12)
                                  #
                     Pages of     i     s                i                                       Shared pages
                     program C    i+1   s               i+1                                      table
                                        s
             Figure  12.23  Dynamic sharing of     program C by processes A and B.



                                                                                    Chapter 12      Virtual Memory  453
shared it. It would ensure that an instruction like Add           (i + 4, 12) in page i + 1 of
Figure 12.23 will function correctly in each of the processes. These conditions are
unnecessary when data, rather than a program, is dynamically bound to several
processes; however, processes sharing the data would have to synchronize their
accesses to the shared data to prevent race conditions.
When sharing of pages is implemented by making the page table entries of
sharing processes point at the same page frame, page reference information for
shared pages will be dispersed across many page tables. The page replacement
algorithm will have to gather this information together to get the correct picture
about references to shared pages. This is rather cumbersome. A better method
would be to maintain information concerning shared pages in the shared pages
table and collect page reference information for shared pages in entries in this
table. This arrangement also permits a different page replacement criterion to be
used for managing shared pages. In Section 12.8.4, we describe a related technique
used in Windows operating systems.
12.6.1 Copy-on-Write
The copy-on-write feature is used to conserve memory when data in shared pages
could be modified but the modified values are to be private to a process. When
processes A and B dynamically bind such data, the virtual memory manager
sets up the arrangement shown in Figure 12.24(a), which is analogous to the
arrangement illustrated in Figure 12.23 except for a copy-on-write flag in each page
table entry, which indicates whether the copy-on-write feature is to be employed
for that page. The mark c in a page table entry in Figure 12.23 indicates that
the copy-on-write flag is set for that page. If process A tries to modify page k,
the MMU raises a page fault on seeing that page k is a copy-on-write page. The
virtual memory manager now makes a private copy of page k for process A,
accordingly changes the page frame number stored in page k's entry in the page
table of A, and also turns off the copy-on-write flag in this entry [ Figure 12.24(b)].
               Page table                         Page table
                     of A    Memory                     of A             Memory
               k+k1  c                            k+k1 c
                     c
               Page table                         Page table
                     of B                               of B
               k+k1  c                            k+k1  c
                     c                                  c
               (a)                                (b)
Figure  12.24  Implementing  copy-on-write:  (a)  before and (b)  after  process A  modifies  page  k.



454  Part 3  Memory Management
             Other processes sharing page k would continue to use the original copy of page k
             in memory; each of them would get a private copy of the page if they modified it.
             In Unix systems, a child process starts off with the code and data of the parent
             process; however, it can modify the data and the modified values are private to
             it. The copy-on-write feature is used for the entire address spaces of the parent
             and child processes. It speeds up process creation. It also avoids copying of code
             pages because they are never modified; only data pages would be copied if they
             are modified.
             12.7  MEMORY-MAPPED FILES                                                               Â·
             Memory mapping of a file by a process binds that file to a part of the logical
             address space of the process. This binding is performed when the process makes
             a memory map system call; it is analogous to dynamic binding of programs and
             data discussed earlier in Section 12.6. After memory mapping a file, the process
             refers to data in the file as if it were data located in pages of its own address
             space, and the virtual memory manager coordinates with the file system to load
             page-size parts of the file into memory on demand. When the process updates
             the data contained in such pages, the modified bits of the pages are set on but the
             data is not immediately written out into the file; dirty pages of data are written
             out to the file when the page frames containing them are to be freed. When the
             process makes a memory unmap call, the virtual memory manager writes out any
             dirty pages that still contain the file's data and deletes the file from the logical
             address space of the process.
             Figure 12.25 shows the arrangement used for memory mapping of file info
             by process A. Note that the page-in and page-out operations on those pages of
             process A that do not belong to file info involve the swap space of the process
             and are performed by the virtual memory manager. Reading and writing of data
             from file info are performed by the file system in conjunction with the virtual
             memory manager. If several processes memory map the same file, we have an
             arrangement analogous to that shown in Figure 12.23; these processes would
             effectively share the memory-mapped file.
                           Page                  Page table
                            #         Process A        of A        Memory
             Memory-            i+i1             i+i1
                   mapped                                                              Swap space
                   file                                                                of process A
                   info
                                                                                       File
                                                                                       info
                                                                               File
                                                                               system
             Figure 12.25 Memory mapping         of file info  by  process A.



                                                     Chapter 12                           Virtual Memory  455
Table 12.5        Advantages of Memory-Mapped Files
Advantage               Description
File data as pages      Access to file data is looked upon as access to pages,
                        which is inherently more efficient because of virtual
                        memory hardware.
Avoids                  File data is a part of the process space. Hence the
memory-to-memory        process does not have to copy it into a variable for
copying                 processing.
Fewer read/write        File data is read in or written out one page at a time,
operations              rather than at every file operation, and so a single
                        read/write operation may suffice for several file
                        operations.
Prefetching of data     For sequential reads, data will already be in memory if
                        the page that contains the data was read in during a
                        previous file operation.
Efficient data access   File data can be accessed efficiently irrespective of file
                        organization.
Table 12.5 summarizes the advantages of memory mapping of files. Memory-
mapping makes file records accessible through the virtual memory hardware. This
is inherently more efficient. Memory-to-memory copy operations are avoided as
follows: When a process accesses some data in a non-memory-mapped input file,
the file system first copies the record into a memory area used as a file buffer or disk
cache (see Chapter 14). The process now copies the data from the buffer or the disk
cache into its own address space (i.e., into some variables) for accessing it. Thus
one disk-to-memory copy operation and one memory-to-memory copy operation
are performed. When a file is memory-mapped, the memory-to-memory copy
operation from the buffer to the process address space is not necessary since the
data is already a part of the process address space. Similarly, fewer copy operations
are performed when file data is modified. Data located in a page that was read in
or written into during a previous file operation can be accessed without disk I/O,
so memory mapping reduces the number of I/O operations performed during file
processing.
The last advantage, efficient access to data in a file irrespective of its organi-
zation, arises from the fact that data in a memory-mapped file is accessed through
the virtual memory hardware. Hence any part of the data can be accessed equally
efficiently, whereas, as discussed in Chapter 13, efficiency of access to the same
data through file operations would depend on the manner in which the data is
organized in the file.
Memory mapping of files poses some performance problems. The open and
close operations on a memory-mapped file incur more overhead than the open and
close operations on normal files. It is caused by updating of page table and TLB
entries while setting up and dismantling that part of the process address space
where the file is mapped. The virtual memory manager also has to differentiate
between memory-mapped pages and other pages in an address space--dirty data



456  Part 3  Memory Management
             pages of the address space are written out to a disk only when a memory crunch
             exists, whereas the dirty pages of a memory-mapped file have to be written to the
             disk periodically for reasons of file reliability. Thus, the virtual memory manager
             has to create a special thread that keeps writing out dirty pages of memory-
             mapped files.
             12.8  CASE STUDIES OF VIRTUAL MEMORY USING PAGING                                        Â·
             12.8.1 Unix Virtual Memory
             Unix has been ported on computer systems with diverse hardware designs. A
             variety of ingenuous schemes have been devised to exploit features of the pag-
             ing hardware of different host machines. This section describes some features
             common to all Unix virtual memory implementations and some interesting tech-
             niques used in different versions of Unix. Its purpose is to provide a view of the
             practical issues in virtual memory implementations rather than to study the vir-
             tual memory manager of any specific Unix version in detail. Wherever possible,
             we are replacing the Unix terminology with terminology we used in previous
             sections of this chapter.
             Logical Address Space and Swap Space  The page table of a process differenti-
             ates among three kinds of pages--resident, unaccessed, and swapped-out pages.
             A resident page is currently in memory. An unaccessed page is one that has not
             been accessed even once during operation of the process and therefore has never
             been loaded in memory. It will be loaded when a reference to it causes a page fault.
             As described later, the page exists either in a file or in the swap space, depending
             on whether it is a text page, i.e., it contains instructions, or it is a data page. A
             swapped-out page is a page that is currently in the swap space; at a page fault, it
             is loaded back in memory from its location in the swap space.
             An unaccessed page may be a text page or a data page. A text page is loaded
             from an executable file existing in the file system. Locating such a page in the
             file system may require reading of several disk blocks in the inode and the file
             allocation table (see Section 13.14.1). To avoid this overhead, the virtual memory
             manager maintains information about text pages in a separate table and refers
             to it when a page needs to be loaded. As described later, the 4.3BSD virtual
             memory manager maintains this information in the page table entry itself. This
             information gets overwritten by the page frame number when the page is loaded
             in memory, and so it is not available if the page gets removed from memory
             and has to be reloaded. To overcome this difficulty, the virtual memory manager
             writes out a text page into the swap space when it is removed from memory for
             the first time, and thereafter loads it from the swap space on demand. A data
             page is called a zero-fill page; it is filled with zeroes when its first use leads to a
             page fault. Thereafter, it is either a resident page or a swapped-out page.
             A text page may remain in memory even if it is marked nonresident in its page
             table entry. This situation arises if some other process is using the page (or has
             used it in the past). When a page fault occurs for a text page, the virtual memory



                                                                   Chapter 12              Virtual Memory  457
manager first checks whether the page already exists in memory. If so, it simply
puts the page frame information in its page table entry and marks it as resident.
This action avoids a page-in operation and also conserves memory.
To conserve disk space, an effort is made to allocate as little swap space as
possible. To start with, sufficient swap space is allocated to accommodate the
user stack and the data area. Thereafter swap space is allocated in large chunks
whenever needed. This approach suffers from the problem that swap space in the
system may become exhausted when the data area of a process grows; the process
then has to be suspended or aborted.
Copy-on-Write     The semantics of fork require that the child process should
obtain a copy of the parent's address space. These semantics can be implemented
by allocating distinct memory areas and a swap space for the child process. How-
ever, child processes frequently discard the copy of their parent's address space
by loading some other program for execution through the exec call. In any case,
a child process may not wish to modify much of the parent's data. Hence mem-
ory and swap space can be optimized through the copy-on-write feature (see
Section 12.6.1).
Copy-on-write is implemented as follows: When a process is forked, the ref-
erence count of all data pages in the parent's address space is incremented by 1
and all data pages are made read-only by manipulating bits in the access privi-
leges field of their page table entries. Any attempt at modifying a data page raises
a protection fault. The virtual memory manager finds that the reference count
of the page is > 1, so it realizes that this is not a protection fault but a reference
to a copy-on-write page. It now reduces the count, makes a copy of the page for
the child process and assigns the read and write privileges to this copy by setting
appropriate bits in its page table entry. If the new reference count is = 1, it also
enables the read and write privileges in the page table entry that had led to the
page fault because the entry no longer pertains to a shared page.
Efficient Use of Page Table and Paging Hardware  If a page is not present in
memory, the valid bit of its page table entry is "off." Under these circumstances,
bits in other fields of this entry, like the ref info field or the page frame # field, do
not contain any useful information. Hence these bits can be used for some other
purposes. Unix 4.3BSD uses these bits to store the address of a disk block in the
file system that contains a text page.
The VAX 11 architecture does not provide a reference bit to collect page
reference information. Its absence is compensated by using the valid bit in a
novel manner. Periodically, the valid bit of a page is turned off even if the page
is in memory. The next reference to the page causes a page fault. However, the
virtual memory manager knows that this is not a genuine page fault, and so it
sets the valid bit and resumes the process. In effect, the valid bit is used as the
reference bit.
Page Replacement  The system permits a process to fix a certain fraction of its
pages in memory to reduce its own page fault rate and improve its own perfor-
mance. These pages cannot be removed from memory until they are unfixed by



458  Part 3  Memory Management
             the process. Interestingly, there is no I/O fixing of pages in Unix since I/O opera-
             tions take place between a disk block and a block in the buffer cache rather than
             between a disk block and the address space of a process.
             Unix page replacement is analogous to the schematic of Figure 12.19, includ-
             ing the use of a clock algorithm. To facilitate fast page-in operations, Unix virtual
             memory manager maintain a list of free page frames and try to keep at least 5
             percent of total page frames on this list at all times. A daemon called the pageout
             daemon (which is labeled process 2 in the system) is created for this purpose. It
             is activated any time the total number of free page frames falls below 5 percent.
             It tries to add pages to the free list and puts itself to sleep when the free list
             contains more than 5 percent free page frames. Some versions of Unix use two
             thresholds--a high threshold and a low threshold--instead of a single threshold
             at 5 percent. The daemon goes to sleep when it finds that the number of pages in
             the free list exceeds the high threshold. It is activated when this number falls below
             the low threshold. This arrangement avoids frequent activation and deactivation
             of the daemon.
             The virtual memory manager divides pages that are not fixed in memory into
             active pages, i.e., pages that are actively in use by a process, and inactive pages,
             i.e., pages that have not been referenced in the recent past. The virtual memory
             manager maintains two lists, the active list and the inactive list. Both lists are
             treated as queues. A page is added to the active list when it becomes active, and to
             the inactive list when it is deemed to have become inactive. Thus the least recently
             activated page is at the head of the active list and the oldest inactive page is at the
             head of the inactive list. A page is moved from the inactive list to the active list
             when it is referenced. The pageout daemon tries to maintain a certain number
             of pages, computed as a fraction of total resident pages, in the inactive list. If it
             reaches the end of the inactive list while adding page frames to the free list, it
             checks whether the total number of pages in the inactive list is smaller than the
             expected number. If so, it transfers a sufficient number of pages from the active
             list to the inactive list.
             The pageout daemon is activated when the number of free page frames falls
             below the low threshold while the system is handling a page fault. It frees page
             frames in the following order: page frames containing pages of inactive processes,
             page frames containing inactive pages of active processes, and page frames con-
             taining active pages of active processes. The daemon finds inactive processes, if
             any, and activates the swapper to swap them out. It goes back to sleep if the
             number of free page frames now exceeds the high threshold.
             If the number of free page frames after swapping out inactive processes is
             still below the high threshold, the pageout daemon scans the inactive list and
             decides whether and when to add page frames occupied by inactive pages to
             the free list. A page frame containing an inactive page is added to the free list
             immediately if the page is unreferenced and not dirty. If the page is dirty and
             not already being swapped out, the pageout daemon starts a page-out operation
             on the page and proceeds to examine the next inactive page. If a page is being
             swapped out, the daemon merely skips it. The modified bit of a page is reset when
             its page-out operation is completed. The page frame containing this page would



                                                    Chapter 12                                        Virtual Memory  459
be added to the free list in a subsequent pass if it is still inactive and the daemon
finds that its page-out operation is complete. The daemon activates the swapper
if it cannot add a sufficient number of page frames to the free list. The swap-
per swaps out one or more active processes to free a sufficient number of page
frames.
To optimize page traffic, the virtual memory manager writes out dirty pages
to the swap space in clusters. When the page daemon finds a dirty page during its
scan, it examines adjacent pages to check if they are also dirty. If so, a cluster of
dirty pages is written out to the disk in a single I/O operation. Another optimiza-
tion concerns redundant page-in operations. When a page frame fi occupied by
some clean page pi is added to the free list, the valid bit of pi's page table entry
is set to 0. However, the page is not immediately overwritten by loading another
page in the page frame. This happens sometime later when the page's entry comes
to the head of the free list and it is allocated to some process. The next reference
to pi would create a page fault since the valid bit in its page table entry has been
set to 0. If pi is still in fi, i.e., if fi is still in the free list, fi can be simply taken out of
the free list and pi can be "reconnected" to the logical address space of the pro-
cess. This saves a page-in operation and consequent delays to the page-faulting
process.
Swapping  The Unix virtual memory manager does not use a working set mem-
ory allocator because of the high overhead of such an allocator. Instead it focuses
on maintaining needed pages in memory. A process is swapped out if all its
required pages cannot be maintained in memory and conditions resembling
thrashing exist in the system. An inactive process, i.e., a process that is blocked for
a long time, may also be swapped out in order to maintain a sufficient number of
free page frames. When this situation arises and a swap-out becomes necessary,
the pageout daemon activates the swapper, which is always process 0 in the system.
The swapper finds and swaps out inactive processes. If that does not free sufficient
memory, it is activated again by the pageout daemon. This time it swaps out the
process that has been resident the longest amount of time. When swapped out
processes exist in the system, the swapper periodically checks whether sufficient
free memory exists to swap some of them back in. A swap-in priority--which is
a function of when the process was swapped out, when it was last active, its size
and its nice value--is used for this purpose (see Section 7.6.1 for details of the nice
value). This function ensures that no process remains swapped out indefinitely.
In Unix 4.3BSD, a process was swapped-in only if it could be allocated as much
memory as it held when it was swapped out. In Unix 4.4BSD this requirement
was relaxed; a process is brought in if enough memory to accommodate its user
structure and kernel stack can be allocated to it.
12.8.2 Linux Virtual Memory
Linux uses a page size of 4 KB. On 64-bit architectures, it uses a three-level page
table (see Section 12.2.3.2). The three levels are the page global directory, the
page middle directory and the page table. Accordingly, a logical address consists



460  Part 3  Memory Management
             of four parts; three of these are for the three levels and the fourth one is the byte
             number within a page.
             Linux uses an interesting arrangement to eliminate page-in operations for
             pages that were loaded previously in memory, but were marked for removal.
             This is achieved by using the following states for page frames: A free page frame
             is one that has not been allocated to a process, while an active page frame is
             one that is in use by a process to which it has been allocated. An inactive dirty
             page frame was modified by the process to which it was allocated but it is not
             in use by the process any more. An inactive laundered page is one what was
             inactive dirty and is therefore being written out to the disk. An inactive laun-
             dered page becomes inactive clean when its contents are copied to the disk. If
             a process page faults for a page that is in a page frame marked inactive clean,
             the page frame is once again allocated to the process, and the page is simply
             marked as present in memory. If the page is in a page frame marked inactive
             laundered, these actions are performed when its disk operation completes. Apart
             from saving on disk operations, this arrangement also prevents access to a stale
             copy of a page. An inactive clean page can also be allocated to another process
             straightaway.
             Page replacement in Linux is based on a clock algorithm. The kernel tries to
             maintain a sufficient number of free page frames at all times so that page faults
             can be quickly serviced by using one of the free page frames. It uses two lists called
             active list and inactive list, and maintains the size of the active list to two-thirds
             the size of the inactive list. When the number of free page frames falls below a
             lower threshold, it executes a loop until a few page frames are freed. In this loop
             it examines the page frame at the end of the inactive list. If its reference bit is
             set, it resets the bit and moves the page frame to the head of the list; otherwise,
             it frees the page frame. When the balance between the active and inactive lists is
             to be maintained, it processes a few page frames from the end of the active list in
             a similar manner and either moves them to the head of the active list, or moves
             them to the head of the inactive list with their reference bits on. A page frame is
             moved from the inactive list to the active list if it is referenced by a process.
             Linux uses a buddy system allocator for allocating page frames to processes
             (see Section 11.5.2). This method facilitates performing of I/O operations through
             older DMA buses that use physical addresses, because such I/O operations require
             memory to be contiguously allocated (see Section 12.2.4).
             The logical address space of a process can consist of several virtual memory
             regions; each region can have different characteristics and is handled by using
             separate policies for loading and replacement of pages. A page in a zero-filled
             memory region is filled with zeroes at its first use. A file-backed region facilitates
             memory mapping of files. The page table entries of its pages point at the disk
             buffers used by the file system. This way, any update in a page of such a region
             is immediately reflected in the file and is visible to concurrent users of the file.
             A private memory region is handled in a different manner. When a new process
             is forked, the child process is given a copy of the parent's page table. At this
             time, pages of a private memory region are given a copy-on-write status. When a
             process modifies such a page, a private copy of the page is made for it.



                                  Chapter 12                                              Virtual Memory  461
12.8.3 Virtual Memory in Solaris
Solaris provides multiple page size support, whereby it uses both normal pages
and superpages. Superpages are used automatically for processes with large
address spaces; other processes can request use of superpages through the mem-
cntl system call. Superpages are not used for memory-mapped files because a
small change in a superpage requires the complete superpage to be written to
the file, which poses a sizable performance penalty because dirty superpages of a
memory-mapped file are written to the disk frequently to ensure reliability of the
file (see Section 12.7).
A component of the virtual memory manager, called the page scanner, tries
to keep a sufficient number of page frames on the cyclic page cache, which is
like the inactive clean list of Linux, so that the virtual memory manager can
allocate a page frame from the cyclic page cache straightaway when a page fault
occurs. It selects a page for removal from memory, using a two-handed clock
algorithm on a global basis; writes it out to the disk if it is dirty; and adds its page
frame to the cyclic page cache. The page scanner is implemented as two kernel
threads analogous to those shown in Figure 12.19. One thread identifies page
frames for addition to the cyclic page cache, while the other thread writes out
dirty pages from these page frames to the disk. If the page for which a process
page faulted exists in a page frame included in the cyclic page cache, the virtual
memory manager simply removes the page frame from the cyclic page cache and
attaches it to the page table of the process. This arrangement saves on a page-in
operation. To reduce page traffic, the page scanner does not put shared pages
on the cyclic page cache if a sufficiently large number of processes are sharing
them.
lotsfree is a parameter of the page scanner that indicates how many page
frames should be free at any time. The page scanner starts scanning pages using
the two-handed clock algorithm when the number of free page frames falls below
lotsfree. The scan rate, which is the number of pages scanned per second, is varied
according to the number of page frames that are actually free--it is smaller when
this number is close to lotsfree and it is increased as the number falls below lotsfree.
The spread between the two hands of the clock algorithm is calculated at boot
time on the basis of the amount of memory in the system. This spread and the
scan rate together determine the elapsed time between the resetting of a bit by one
hand of the two-handed clock algorithm and its examination by the other hand
of the algorithm. A smaller elapsed time implies that only most recently accessed
pages will survive in memory, and a larger elapsed time means that only pages
that have not been accessed for a long time will be removed from memory. To
safeguard system performance, the virtual memory manager limits the amount
of CPU overhead that the page scanner can cause. If the page scanner is not able
to keep pace with the demand for free pages using the clock algorithm, the virtual
memory manager swaps out inactive processes and frees all page frames occupied
by them.
Solaris virtual memory manager has evolved into its present form through
several design updates. Prior to Solaris 6, the page scanner maintained a free



462  Part 3  Memory Management
             list that contained clean page frames allocated to both user processes and files.
             The file system took pages from the free list to accommodate data read from
             files. During periods of heavy file activity, the file system effectively stole pages
             from address spaces of user processes, which affected their performance. Solaris 6
             introduced the feature called priority paging, which ensured that only those page
             frames in the free list that were allocated to file pages would be considered for
             allocation to data read from files. This way, file processing activity did not affect
             operation of processes; however, page frames were still allocated from the free
             list, which caused high scan rates and high overhead of the page scanner. Solaris
             8 introduced the cyclic page cache described earlier and made the file system steal
             pages from itself directly, so that the file processing activity does not affect scan
             rates and overhead of the page scanner.
             12.8.4 Virtual Memory in Windows
             Windows operates on several architectures, hence it supports both 32-bit and
             64-bit logical addresses. The page size is 4 KB. The address space of a process is
             either 2 GB or 3 GB. The remainder of the logical address space is reserved for
             OS use; the kernel is mapped into this part of every process's address space. On
             different architectures, Windows uses two-, three- or four-level page tables and
             various page table entry formats. The page table of a process is itself stored in the
             reserved part of the logical address space of the process.
             On an Intel 80x86 architecture, Windows uses a two-level page table orga-
             nization similar to the one shown in Figure 12.11. The higher-level page table
             is called a page directory (PD). The PD contains 1024 entries of 4 bytes each.
             Each entry in the PD points to a page table (PT). Each page table contains 1024
             page table entries of 4 bytes each. Each 32-bit logical address is split into three
             components as shown below:
                                   10 bits - 10 bits -  12 bits          -
                                  PD index  PT index    byte index
             During address translation, the PD index field is used to locate a page table.
             The PT index field is used to select a 32-bit page table entry (PTE) in the page
             table, which contains a 20-bit address of the page frame that contains the page;
             the byte index is concatenated with this address to obtain the effective physical
             address. The virtual memory manager uses the remaining 12 bits in a page table
             entry to indicate how the process may access the page--whether read-only or
             read/write--and whether the page frame allocated to it is dirty, i.e., modified, or
             accessed, i.e., read from or modified. If the page is not in memory, the 20 address
             bits would specify the offset into the paging file, i.e., the swap space. If the page
             contains code, a copy of it would exist in a code file, hence 28 bits in the page
             table entry would point to a system data structure that indicates its position in
             the code file. Such a page is directly loaded from the code file, so it is not copied
             into a paging file.



                                                                      Chapter 12        Virtual Memory  463
   A page frame can be in any one of eight states. Some of these states are:
Â·  valid: the page is in active use,
Â·  free: the page is not in active use,
Â·  zeroed: the page is cleaned out and available for immediate use,
Â·  standby: the page has been removed from the working set of the process to
   which it was allocated, but it could be "reconnected" to the process if it were
   referenced again,
Â·  modified: the page is dirty and yet to be written out,
Â·  bad: the page cannot be accessed because of a hardware problem.
   A process cannot use the virtual address space available to it straightaway--it
must first reserve it for use, and then actually commit it for accommodating spe-
cific entities like files and objects. Thus, only some portions of the logical address
space of a process may have been reserved at any time, and only a part of the
reserved logical address space may be in actual use. An access to a page that has
not been reserved and committed leads to an access violation. When a thread
in the process makes a system call to commit virtual memory to a region, the
virtual memory manager constructs a virtual address descriptor (VAD) describ-
ing the range of logical addresses committed to it. To minimize the size of the
page table of a process, the virtual memory manager builds it incrementally--
the page table entry for a committed page is created only when an access to it
leads to a page fault. To facilitate this operation, the VADs for committed por-
tions of the logical address space are stored in an AVL tree, which is a balanced
binary tree.
   A section object represents a section of memory that can be shared. It can
be connected to a file, in which case it provides memory-mapped files, or to
memory, in which case it provides shared memory. A process maps a view of a
section into its own address space by making a system call with parameters that
indicate an offset into the section object, the number of bytes to be mapped, and
the logical address in its address space where the object is to be mapped. When the
process accesses a page in the view for the first time, the virtual memory manager
allocates a page frame and loads it, unless it is already present in memory as
a result of access by another process. If the memory section has the attribute
based, the shared memory has the same virtual address in the logical address
space of each sharing process. It facilitates sharing of code among processes
(see Section 12.6).
   A copy-on-write feature is used for sharing the pages (see Section 12.6.1). It
is implemented by setting the protection field of a page to read only. A protection
exception is raised when a process tries to modify the page. The virtual memory
manager now makes a private copy of the page for use by the process.
   Loading, accessing, and removal of shared pages is performed as follows: A
prototype PTE is created for each shared page in an area of memory reserved
for prototype PTEs. Each process that uses the shared page has a PTE for the
page in its page table. When the shared page does not exist in memory, that is,
it is either not yet loaded in memory or it has been removed from memory, it is
marked invalid in the prototype PTE and in the PTEs in page tables of all sharing



464  Part 3  Memory Management
             processes. In addition, the PTEs in the page tables of processes are set to point
             to the prototype PTE. When the shared page is referenced by one of the sharing
             processes, it is loaded in memory and the page frame number where it is loaded
             is stored in both the prototype PTE and the PTE of the process. When another
             process references this page, its PTE is updated by simply copying the page frame
             number information from the prototype PTE.
             Translation look-aside buffers are employed to speed up address translation.
             In 32-bit architectures, they are managed entirely by the MMU hardware, while
             in 64-bit architectures they are managed by the virtual memory manager. When
             a memory access by a thread leads to a page fault, the thread is blocked until
             the page-in operation for the page completes. Several threads may page-fault for
             a shared page at the same time. These page faults are called collided page faults.
             The virtual memory manager ensures that all threads whose page faults collided
             become unblocked when the page-in operation is completed.
             To reduce the number of page faults through page reference locality, the
             virtual memory manager always loads a few pages preceding and following a page-
             faulted page into memory. While booting the system or starting an application,
             the logical prefetcher loads a few pages into memory and monitors page faults
             that arise so that it could load a more effective set of pages in memory the next
             time the system is booted or the application is started.
             The Windows kernel uses the notion of working sets to control the amount of
             memory allocated to a process. It defines a minimum and maximum working set
             size for each process; these sizes are determined by the memory configuration of
             the system, rather than by the size or nature of a process. For large memory con-
             figurations, the minimum and maximum working set sizes are 50 and 345 pages,
             respectively. At a page fault, the kernel considers the amount of free memory
             in the system, the current working set size of the process, and its minimum and
             maximum working set sizes. It allocates an additional page frame to the process
             if its current allocation is smaller than the maximum working set size and free
             memory exists; otherwise, it replaces one of the pages of the process in memory
             through a clock algorithm implemented by using the accessed bits in the page
             table. The working set manager is activated periodically, and when working sets
             of processes need to be adjusted. If the amount of free memory has fallen below a
             threshold due to allocation of page frames, it examines working sets whose sizes
             exceed the minimum working set size and removes from memory those pages
             that have not been used for a long time. This, too, is performed by using a clock
             algorithm.
             The virtual memory manager maintains a number of page lists--a free list,
             a list of zero-initialized pages, a modified list, and a standby list. When a page
             is to be removed from memory, or when its process has terminated, it would be
             moved to the standby list if it were a clean page; otherwise, it would be moved to
             the modified list. (Recall that a standby page could be simply "reconnected" to
             a process that wished to use it.) The page writer writes out modified pages and
             changes their status to standby. It uses two thresholds--an upper threshold on
             the number of modified pages in the system and a lower threshold on the number
             of available pages--to decide when pages need to be written out.



                                                                              Chapter 12     Virtual Memory  465
12.9    VIRTUAL MEMORY USING SEGMENTATION                                                                    Â·
A segment is a logical entity in a program, such as a function, a data structure,
or an object; or it is a module that consists of some or all of these. A program is
composed of segments. During a program's execution, the kernel treats segments
as the unit for memory allocation. This results in noncontiguous memory alloca-
tion for processes, which provides efficient use of memory by reducing memory
fragmentation. Being a logical entity, a segment is also a convenient unit for shar-
ing and protection. This feature is useful in constructing large software systems
that comprise of a set of modules or objects.
A logical address in a segmented process is viewed as a pair (si, bi) where si
and bi are the segment and byte ids, respectively. There are variations in the way si
and bi are indicated in a logical address. One method is to represent each of them
numerically. The logical address thus consists of a segment number and a byte
number. We shall discuss the second method separately later in this section. The
logical address space of a process is two-dimensional in nature. One dimension is
defined by the set of segments in the process. The number of segments can vary,
subject to a maximum number that may be specified by the computer architecture
or the virtual memory manager. The other dimension is defined by the set of bytes
in a segment. The number of bytes in a segment can vary, subject to the maximum
imposed by the number of bits available to represent bi in a logical address. The
two-dimensional nature of the address space implies that the last byte of a segment
and the first byte of another segment are not logically adjoining bytes--if we add
1 to the address of the last byte in a segment, it does not spill over into the
next segment; it is merely an invalid address. These are significant differences
from paging. There are also significant similarities, which we now discuss in the
context of address translation.
Figure 12.26 shows how address translation is performed in virtual memory
using segmentation. Some parallels with paging are the existence of a segment
table (ST) for a process, and a special hardware register called the segment table
address register (STAR) that points to the segment table of a process. For a
             Memory                  MMU
                                                                   Valid        Misc
                                          STAR                     bit    Addr         info
        Add     si   bi                            2
                         1
                                     si   bi                       1      ai
                                                3
                                 ai       bi                            Segment Table
             4
Figure  12.26   Virtual memory implementation using segmentation.



466  Part 3  Memory Management
                  logical address (si, bi), address translation is performed by using the memory
                  address found in si's entry in the segment table and the byte number bi in the
                  segment. A missing segment fault is raised if segment si does not exist in memory.
                  A difference with paging is that segments do not have a standard length. Hence
                  address translation involves adding the byte number bi to the start address of si; it
                  cannot be performed by using bit concatenation as in paging. Address translation
                  can be speeded up by using address translation buffers. An entry in the address
                  translation buffer would contain a segment id and its address in memory, which
                  is copied from its segment table entry.
                     In a logical address (si, bi), si and bi could also be specified in a symbolic
                  form, i.e., as ids. In this case, a logical address is of the form (alpha, beta)
                  where alpha is the name of a segment and beta is an id associated with a byte
                  contained in segment alpha. Address translation of such logical addresses is
                  performed as follows: While compiling a segment, the compiler builds a table
                  showing byte ids defined in the segment and the byte numbers of corresponding
                  bytes in the segment. This table is made available to the virtual memory manager
                  for use during address translation. We will call it the segment linking table (SLT),
                  and refer to the segment linking table for alpha as SLTalpha. During address
                  translation, the MMU obtains the start address of alpha from the segment table,
                  picks up the address of SLTalpha from the misc info field of alpha's entry and
                  obtains the byte number of beta from SLTalpha, and adds the two to obtain the
                  effective memory address.
Â·
   Example 12.10  Effective Address Calculation in Segmentation
                  Figure 12.27 illustrates effective address calculation for the logical address
                  (alpha, beta). Part (a) of the figure shows segment alpha. beta and gamma
                  are two ids associated with specific instructions or data in alpha. These ids are
                  associated with the bytes numbered 232 and 478 in the segment, respectively.
                  The segment linking table SLTalpha contains entries for beta and gamma,
                  showing their byte numbers as 232 and 476, respectively. The segment table
                  entry of alpha indicates that it exists in the memory area with the start address
                  23480. The byte number associated with beta is 232. Hence the effective
                  address of (alpha, beta) would be computed as 23480 + 232 = 23712.
                  Â·
                     Both numeric and symbolic ids have been used in segmented virtual memory.
                  MULTICS is a well-known system that used symbolic identifiers.
                  12.9.1 Management of Memory
                  Memory management in virtual memory using segmentation has some simi-
                  larities to memory management in paging. A segment fault indicates that a
                  required segment is not present in memory. A segment-in operation is performed
                  to load the segment. If there is insufficient free memory, some segment-out oper-
                  ations may have to precede loading of the required segment. The virtual memory



                                                                  Chapter 12             Virtual Memory  467
(a)     Segment  (b)            Valid                       Misc
        alpha         Name      bit    Addr           Size  info  Name   Offset
     beta:...
                                                                  beta   232
                      alpha     1      23480          764         gamma  478
     gamma:...
                                     Segment Table                Segment Linking Table
                                                                  (SLT alpha)
Figure  12.27  Use of symbolic segment and word ids.
manager can use a working set of segments to control memory allocation for
a process. Segments could be replaced on an NRU basis by collecting segment
reference information in each segment entry.
     One difference from virtual memory using paging is that segments do not
have a fixed size. The memory freed by removing one segment from memory may
not suffice for loading another segment. Hence many segments may have to be
removed before a new segment can be loaded. Differences in segment sizes can
lead to external fragmentation, which can be tackled either through compaction
or through memory reuse techniques such as first-fit or best-fit. Compaction is
aided by presence of the MMU--only the address field of the segment table entry
needs to be modified when a segment is moved in memory. However, the virtual
memory manager should ensure that segments being moved are not involved in
I/O operations.
     The two-dimensional nature of the logical address space permits a segment to
dynamically grow or shrink in size. Dynamic growth can be handled by allocating
a larger memory area to a segment and releasing the memory area allocated to it
earlier. A segment can be permitted to grow in its present location in memory if
an adjoining free area exists.
12.9.2 Sharing and Protection
Two important issues in sharing and protection of segments are:
Â· Static and dynamic sharing of segments
Â· Detecting use of invalid addresses
     A segment is a convenient unit for sharing because it is a logical entity in a
process. It can be shared statically or dynamically by using the schemes described
earlier in Section 12.6. If segment ids are numeric, segments must occupy iden-
tical positions in logical address spaces of sharing processes. This requirement is
analogous to that concerning shared pages in virtual memory using paging (see
Section 12.6 and Figure 12.23). It does not apply if segment ids are symbolic.
Processes sharing a segment may have different access privileges to programs and
data in it. The virtual memory manager puts the access privileges in the misc
info field of a segment table entry. While translating a logical address (si, bi), the



468            Part 3  Memory Management
                           MMU makes two kinds of protection checks. It checks whether the kind of access
                           being made to the logical address is consistent with the access privileges of the
                           process for the segment. It also checks whether (si, bi) is a valid address by check-
                           ing whether bi < size of si. It raises a memory protection violation interrupt if
                           any of these checks fails.
                           12.9.3 Segmentation with Paging
                           External fragmentation exists in a virtual memory using segmentation because
                           segment sizes are different. This problem can be addressed by superimposing
                           paging on a segment-oriented addressing scheme. A system using this approach
                           retains the fundamental advantage of segmentation--the logical address space
                           is two-dimensional, which permits dynamic changes in the size of a segment--
                           while avoiding external fragmentation. Each segment contains an integral number
                           of pages, and memory management is performed through demand paging. This
                           arrangement may achieve more effective utilization of memory since only required
                           pages of a segment need to be present in memory at any time. However, paging
                           introduces internal fragmentation in the last page of a segment.
                               A logical address in such a system has the form (si, pi, bi). Since each segment
                           consists of a number of pages, a page table is built for each segment. The segment
                           table entry of a segment points to its page table. Figure 12.28 illustrates this
                           arrangement. The name field of the segment table is needed only if symbolic
                           segment ids are used. Address translation now involves an access to the segment
                           table followed by an access to the page table of the segment. It requires two
                           memory references if the segment and page tables are held in memory. To speed
                           up address translation, address translation buffers would have to be employed
                           for both the segment and page table references. A simple extension to the scheme
                           described earlier in Section 12.2.2 can be used for this purpose. Alternatively,
                           a single address translation buffer may be employed, each entry in the buffer
                           containing a pair (si, pi) and the corresponding page frame number.
                                                                        Valid       Page table  Misc
                                                                 Name   bit         addr        info
                                              Segment table
                       Memory
                                              MMU
                                          si  pi       bi        Valid  Page        Misc
                                                                 bit    frame #     info
               Add     si  pi  bi
                                                                        ai
                                              ai       bi               Page table
Figure  12.28  Address translation in segmentation with paging.



                                                                         Chapter 12     Virtual Memory  469
Memory protection can be performed at the level of segments through the
scheme described in Section 12.9.2. Protection information for a segment can
be put in its entry in the segment table, and it can be copied into its entry in the
address translation buffer. Page level access validation is not needed.
12.10  SUMMARY                                                                                                 Â·
Virtual memory is a part of the memory hierarchy       used in practice because they require less memory
consisting of memory and a disk. During operation      than the conventional page table.
of a process, some components of its address space     The virtual memory manager has to make two
exist in memory, while others reside on a disk. This   key decisions that influence the performance of a
arrangement permits the total memory require-          process: Which page should it remove from mem-
ments of a process to exceed size of the system's      ory to make space for a new page required by a
memory. It also permits a larger number of pro-        process, and how much memory should it allocate
cesses to exist in memory simultaneously, because      to a process? It uses a page replacement algorithm to
each of them occupies less memory than its own         decide which page should be removed from mem-
size. The performance of a process depends on the      ory. The empirical principle of locality of reference
rate at which its parts have to be loaded in mem-      indicates that a recently accessed page is more
ory from the disk. In this chapter, we studied the     likely to be accessed in future than a page that
techniques used by the kernel to ensure efficient      has not been recently accessed. Accordingly, the
operation of a process and good performance of         least recently used (LRU) page replacement algo-
the system.                                            rithm removes the page that has been least recently
Two basic actions in the operation of virtual          used. It possesses the stack property, which guaran-
memory using paging are address translation and        tees that the page fault rate would not increase if
demand loading of pages. The memory management         the memory allocation to a process is increased.
unit (MMU), which is a hardware unit, and the vir-     However,  it      is  expensive  to  collect  information
tual memory manager, which is a part of the kernel,    about when a page was last referenced. Hence
jointly implement these two actions. The mem-          MMUs typically provide a single bit for collecting
ory is divided into parts called page frames, whose    information about page references, and a class of
size matches the size of pages. The virtual memory     page replacement algorithms called the not recently
manager maintains a page table for each process        used (NRU) algorithms are used in practice. Clock
to indicate which of its pages exist in which page     algorithms are a widely used subclass of NRU
frames of memory. When an operand in the current       algorithms.
instruction in a process exists in one of the pages    The working set of a process is the collection
that is present in memory, the MMU obtains the         of distinct pages referenced by it recently. Its size
page frame number where it exists from the page        provides a useful pointer to how many pages of
table and uses it to compute the effective memory      the process should be in memory to ensure good
address of the operand. If the page is not in mem-     performance of the process. The virtual memory
ory, the MMU raises an interrupt called a page         manager can use the notion of working sets to
fault, and the virtual memory manager loads the        avoid the situation called thrashing in which most
page in memory. A fast translation look-aside buffer   processes in the system have insufficient amounts
(TLB) is used to speed up address translation; it      of memory allocated to them, so they produce page
caches some entries of page tables of processes. The   faults at a high rate and little useful work gets done
inverted page table and the multilevel page table are  in the system.



470         Part 3  Memory Management
     An operating system uses special techniques           memory, while memory mapping of files enables a
that exploit the virtual memory to speed up oper-          file to be treated as a part of the address space
ation of processes. The copy-on-write technique            of a process, thereby speeding up accesses to its
avoids keeping identical copies of shared pages in         data.
TEST  YOUR CONCEPTS                                                                                                     Â·
12.1  Classify each of the following statements as true               the reference bit in the entry of page pi of
      or false:                                                       process Pk indicates:
      a. In a computer providing virtual memory, the                  i. Whether page pi is likely to be referenced
      number of bits in a logical address can exceed                        in the future
      the number of bits in a physical address.                       ii. Whether page pi will be the next page to
      b. A  page-out       operation  is  always  needed                    be referenced during operation of Pk
      in a page replacement operation, irrespec-                      iii. Whether page pi has been referenced since
      tive of whether the page being replaced is                            it was last loaded in memory
      dirty.                                                          iv. Whether        page  pi  is  the  most   recently
      c. Loss of protection can result if an entry in the                   referenced page of Pk
      translation look-aside buffer (TLB) that was                b.  During operation of a process Pk, the trans-
      made during operation of one process is used                    lation look-aside buffer contains:
      during operation of another process.                            i. Some arbitrary entries from the page table
      d. The inverted page table organization requires                      of Pk
      more       accesses   to   memory   during  address             ii. The most recently referenced entries of
      translation than the conventional organiza-                           the page table of Pk
      tion of page tables.                                            iii. The last few entries of the page table of
      e. The FIFO page replacement policy guaran-                           Pk
      tees that allocating more page frames to a                      iv. The least recently referenced entries of the
      program would reduce its page fault rate.                             page table of Pk
      f. If the virtual memory hardware provides a                c.  The stack property of a page replacement
      single reference bit and the reference bits in                  algorithm implies that if more memory would
      the page table entries of all memory-resident                   have been allocated to a process:
      pages      are  set,  the  LRU  page  replacement               i. Fewer page faults would have occurred
      algorithm degenerates to FIFO replacement.                      ii. More page faults would have occurred
      g. Page faults would not occur during operation                 iii. The number of page faults would have
      of a process if all pages included in the work-                       been smaller or the same
      ing set of a process are in memory at every                     iv. None of (i)Â­(iii)
      instant.                                                    d.  If pfri and pfrj are the page fault rates of
      h. Heavy page traffic implies that thrashing has                processes Pi and Pj when process Pi has 5
      occurred.                                                       percent of its pages in memory, process Pj
      i. If a single copy of a program C is shared by                 has 10 percent of its pages in memory, and the
      two processes A and B, pages of C should                        page replacement policy possesses the stack
      occupy identical positions in the page tables                   property, then:
      of processes A and B.                                           i.    pfri < pfrj
12.2  Select the most appropriate alternative in each                 ii.   pfri  pfrj
      of the following questions:                                     iii.  pfri > pfrj
      a. If the virtual memory hardware provides a                    iv.   Nothing can    be  said    about  the  relative
      single reference bit in an entry of a page table,                     magnitudes of pfri and pfrj



                                                                             Chapter 12      Virtual Memory           471
      e.   If pfri and pfri are the page fault rates of pro-                 iv. Nothing can be said about the relative
           cess Pi when it is operated with 5 percent                        magnitudes of pfri and pfri
           of its pages and 10 percent of its pages in                   f.  Thrashing can be overcome if
           memory, respectively, and the page replace-                       i. The  degree  of     multiprogramming           is
           ment   policy     possesses  the  stack  property,                increased
           then:                                                             ii. The I/O speed is increased
           i. pfri < pfri                                                    iii. Memory allocation for a process is con-
           ii. pfri  pfri                                                    trolled by its working set size
           iii. pfri > pfri                                                  iv. None of (i)Â­(iii)
EXERCISES                                                                                                                      Â·
12.1  Page tables are stored in a memory that has an                     page replacement policy is used with alloc = 5
      access time of 100 nanoseconds. The translation                    than when the optimal page replacement policy
      look-aside buffer (TLB) can hold 64 page table                     is used with alloc = 5.
      entries and has an access time of 10 nanosec-               12.6   A process makes r page references during its
      onds. During operation of a process, it is found                   operation. The page reference string of the pro-
      that 85 percent of the time a required page table                  cess contains d distinct page numbers in it. The
      entry exists in the TLB and only 2 percent of                      size of the process is p pages and it is allocated f
      the references lead to page faults. The average                    page frames all through its operation.
      time for page replacement is 2 ms. Compute the                     a. What is the least number of page faults that
      effective memory access time.                                          can occur during its operation?
12.2  Using    the    access    speeds  and  hit  ratios  men-           b. What is the maximum number of page faults
      tioned in Exercise 12.1, compute the effective                         that can occur during its operation?
      memory access time in two-level, three-level, and           12.7   Prove the validity of the following statement if
      four-level page table organizations.                               the page replacement policy uses a fixed memory
12.3  Three approaches to paging of the kernel in                        allocation and local page replacement: "If a pro-
      virtual memory are:                                                cess does not modify any of its pages, then it is
      a. Make       the     kernel  permanently     memory-              optimal to replace the page whose next reference
           resident.                                                     is farthest in the page reference string." Show
      b. Page the kernel in a manner analogous to the                    that this policy may not lead to the minimum
           paging of user processes.                                     number of page-in and page-out operations if
      c. Make the kernel a compulsory part of the                        the process modifies some of its pages.
           logical address space of every process in the          12.8   What is Belady's anomaly? Show that a page
           system and manage its pages as shared pages.                  replacement algorithm that possesses the stack
      Which approach would you recommend? Give                           property cannot exhibit Belady's anomaly.
      reasons.                                                    12.9   Prove that the LRU page replacement policy
12.4  Execution performance of a process in virtual                      possesses the stack property.
      memory depends on locality of reference dis-                12.10  Optimal page replacement can be implemented
      played during its operation. Develop a set of                      by replacing the page whose next reference is
      guidelines      that   a  programmer   can    follow    to         farthest in the page reference string. Does this
      obtain good performance of a process. Describe                     policy possess the stack property? Does the clock
      the  rationale        behind  each  guideline.      (Hint:         algorithm possess the stack property?
      Consider array references occurring in nested               12.11  For the page reference string (12.6),
      loops!)                                                            a. Show the working set at each time instant if
12.5  Give a sample page reference string for a process                      the size of the working set window is (i) three
      that produces more page faults when the LRU                            instructions, (ii) four instructions.



472         Part 3  Memory Management
       b. Compare the operation and performance of                        most recently executed instruction is said to be
            the working set allocator with the FIFO and                   "1 instruction in the past" of the process, the
            LRU allocators.                                               instruction before it is said to be "2 instruc-
12.12  A working set allocator is used for a page refer-                  tions  in  the   past,"   etc.  A  memory     allocator
       ence string with two values of         ,   1<   2. pfr1            refers to the page reference in the instruction
       and pfr2 are page fault rates when              1 and  2           that is i instructions in the past as the -i page
       are used, respectively. Is pfr1  pfr2 if working                   reference. It uses a parameter w, and the fol-
       sets are recomputed (a) after every instruction                    lowing rules for memory allocation and page
       and (b) after every n instructions for some n?                     replacement:
12.13  Describe the actions of a virtual memory man-                      a. Do nothing if the next page reference matches
       ager using a working set memory allocator when                     the -w page reference.
       it  decides  to  reduce  the    degree     of   multipro-          b. Else, if the next page reference matches the
       gramming.    Clearly     indicate  how      it  uses   and         -i     page reference     for some i     <    w,  do the
       manipulates its data structures for this purpose.                  following: if the -w page reference does not
12.14  Explain, with the help of examples, why the                        match with the -j page reference for some
       working set size of a process may increase or                      j < w, then reduce the memory allocation for
       decrease during its operation.                                     the process by one page frame and remove
12.15  Justify the following statement: "Thrashing can                    the least recently used page, otherwise do
       arise when a working set memory allocator is                       nothing.
       used. However, it cannot last for long."                           c. Else,   if  the  next  page     reference  causes  a
12.16  A virtual memory manager uses the following                        page fault and the -w page reference does
       page replacement policy: When a combination                        not       match     with  the   page  reference   in  the
       of a high page fault rate in the system and low                    -j     instruction for some j         <  w, then per-
       CPU efficiency is noticed, reduce the allocation                   form a page replacement using the LRU page
       for each process and load one more process.                        replacement policy.
       Comment on the effectiveness of this policy.                       d. Else, increase the memory allocation for the
12.17  Explain why the two-handed clock algorithm for                     process by one page frame and load the page
       page replacement is superior to the one-handed                     contained in the next page reference.
       clock algorithm (see Section 12.8.1).                              Show that the actions of the memory alloca-
12.18  A virtual memory manager implements a work-                        tor are equivalent to actions of the working set
       ing set memory allocator and uses dynamic shar-                    memory allocator with           = w.
       ing of pages. Describe the housekeeping actions             12.22  Compare        the  following      memory     manage-
       performed by it in the following situations.                       ment   proposals    in    virtual     memory      using
       a. When a page fault occurs.                                       segmentation-with-paging.
       b. When a shared page drops out of the working                     a. Use the LRU policy within a process.
            set of one of the sharing processes.                          b. Use the LRU policy within a segment.
12.19  The amount of memory allocated to a process in              12.23  Comment on the validity of the following state-
       a system using virtual memory is held constant                     ment: "In virtual memory using segmentation-
       and the page size is varied. (This action varies                   with-paging, the role of segmentation is limited
       the number of pages of the process in memory.)                     to sharing. It does not play any role in memory
       Draw a graph of page size versus expected page                     management."
       fault rate.                                                 12.24  An I/O operation consists of the execution of a
12.20  The degree of multiprogramming in a system                         sequence of I/O commands. A self-describing I/O
       using virtual memory is varied by changing the                     operation is an I/O operation some of whose I/O
       memory allocation for processes. Draw a graph                      commands are read in by a previous I/O com-
       of degree of multiprogramming versus CPU effi-                     mand of the same I/O operation. For example,
       ciency. Explain the nature of the graph in the                     consider the I/O operation
       region of high degree of multiprogramming.
12.21  We   refer   to  "instructions     in  the  past"     dur-         1. Read d, 6, aaa
       ing  operation   of   a  process       as  follows:    The         2. Read d, count, bbb



                                                                                   Chapter 12   Virtual Memory           473
       where  d  is  the  id  of  the  I/O  device.  The  first            the swap space when it is used for the first time.
       I/O command reads 6 bytes into the memory                           Discuss the advantages and drawbacks of the
       area with address aaa. Let this be the area where                   optimization.
       the fields containing count (2 bytes) and bbb (4          12.26     Performance of a virtual memory is determined
       bytes) of the second I/O command are stored.                        by the interplay of three factors--CPU speed,
       Thus, the first I/O command modifies the sec-                       size of memory, and peak throughput of the
       ond I/O command. Let n and ccc be the values                        paging device. Possible causes of low or high
       read into fields count and bbb, respectively, by                    efficiency of the CPU and the paging disk can
       the first I/O command. After I/O for the first                      be summarized as follows:
       I/O command is completed, the second I/O com-
       mand reads n bytes into the memory area with                                       High          Low
       address ccc. The data for this I/O operation                                       utilization   utilization
       would be
                     n, ccc,                                               CPU            Processes     Only few of the
                                                                                          are CPU-      processes are
                              n bytes of data                                             bound, or     CPU-bound,
                                                                                          CPU is slow   or thrashing is
       Can  the  methods      of  performing   I/O   in   vir-                                          present
       tual memory described in Section 12.2.4 handle                      Paging         Thrashing is  Memory is
       self-describing I/O operations correctly? Clearly                   disk           present, or   overcommited
       justify your answer. In a simplified form of self-                                 disk is slow  to each process
       describing I/O, the first I/O command reads in
       only 2 bytes and stores them in the count field.
       Can the methods described in Section 12.2.4                         Performance of virtual memory may improve
       handle such I/O operations correctly?                               if one or several of the following changes are
12.25  While initiating a process, the virtual memory                      made: the CPU is replaced by a faster CPU,
       manager copies the code of the process, which                       the paging disk is replaced by a faster disk, the
       exists in a file, into the swap space reserved for                  memory is increased, or the degree of multi-
       the process. From the swap space, code pages are                    programming is increased. In each of the fol-
       loaded into memory when needed. Explain the                         lowing situations, which of the above changes
       advantages of this arrangement. Why not load                        would you recommend for improving system
       code pages directly from the file when needed?                      performance?
       Some code pages may not be used during a run,                       a. Low CPU efficiency, low disk efficiency
       hence it is redundant to copy them into the swap                    b. Low CPU efficiency, high disk efficiency
       space. To avoid redundant copying, some vir-                        c. High CPU efficiency, low disk efficiency
       tual memory managers copy a code page into                          d. High CPU efficiency, high disk efficiency
CLASS PROJECT: SIMULATION OF VIRTUAL MEMORY MANAGER                                                                            Â·
A virtual memory manager uses the two-thread arrange-            illustrated in Figure 12.20. It performs page replacement
ment shown in Figure 12.19, where the thread called              on a global basis.
free frames manager tries to maintain a sufficient num-          The working of this virtual memory manager is to
ber of free page frames at all times and the thread called       be simulated. The simulation is controlled by commands
page I/O manager performs page-out operations on dirty           in an input file, where each command has the format
page frames. The virtual memory manager uses the two-            <action> <parameters>. Details of the actions are as
handed clock algorithm discussed in Example 12.8 and             follows:



474       Part 3  Memory Management
                      Action name                  Parameters and explanation
                      Memory_size                  Number of page frames (integer)
                      Lower_threshold              Minimum number of free page frames (integer)
                      Upper_threshold              Maximum number of free page frames (integer)
                      Distance                     Distance between clock hands, in terms of number of page
                                                   frames (integer)
                      #processes                   Number of processes (integer). Process id's are P0, P1, . . .
                      Process_size                 Process id, number of pages (both are integers)
                      Read                         Process id, Page number : The indicated process reads the
                                                   indicated page (both are integers)
                      Modify                       Process id, Page number : The indicated process modifies
                                                   the indicated page (both are integers)
                      Page_table                   No parameters. Simulator displays the page tables of
                                                   processes
                      IO_list                      No parameters. Simulator displays the list of page frames
                                                   on which page-out operations need to be performed
                      Hit_ratio                    Simulator displays hit ratios for processes
                      Reset_counters               Simulator resets counters used for calculation of hit ratios
Develop a simulator of the virtual memory manager.           numbers in this list. The page I/O manager performs
The  simulator  must  maintain  page  tables  and  swap      page-out operations on the page frames in a suitable
spaces of the processes. It must also maintain a list of     order; it informs the free frames manager when the
page frames on which page-out operations should be           page-out operation of a page frame has been completed.
performed. The free frames manager puts page frame
BIBLIOGRAPHY                                                                                                          Â·
Randell (1969) is an early paper on the motivation for vir-  (1998) compares virtual memory features in MIPS, Pen-
tual memory systems. Ghanem (1975) discusses memory          tium, and PowerPC architectures. Swanson et al. (1998)
partitioning in virtual memory systems for multipro-         and Navarro et al. (2002) describe superpages.
gramming. Denning (1970) is a survey article on virtual           Car and Hennessy (1981) discusses the clock algo-
memory. Hatfield (1971) discusses aspects of program         rithm. Bach (1986) and Vahalia (1996) describe Unix
performance in a virtual memory system.                      virtual memory, Beck et al. (2002), Gorman (2004),
     Belady (1966) discusses the anomaly that carries his    Bovet and Cesati (2005), and Love (2005) discuss Linux
name. Mattson et al. (1970) discusses stack property of      virtual  memory,   Mauro   and  McDougall       (2006)  dis-
page replacement algorithms. Denning (1968a, 1968b)          cusses virtual memory in Solaris, while Russinovich and
discusses thrashing and the fundamental working set          Solomon (2005) discusses Windows virtual memory.
model. Denning (1980) is a comprehensive discussion on            Organick      (1972)  describes  virtual  memory    in
working sets. Smith (1978) is a bibliography on paging       MULTICS.
and related topics. Wilson et al. (1995) discusses memory
allocation in virtual memory environments. Johnstone          1.  Aho, A. V., P. J. Denning, and J. D. Ullman
and Wilson (1998) discusses the memory fragmentation              (1971): "Principles of optimal page replacement,"
problem.                                                          Journal of ACM, 18 (1), 80Â­93.
     Chang and Mergen (1988) describes the inverted           2.  Bach, M. J. (1986): The Design of the Unix
page table, while Tanenbaum (2001) discusses the two-             Operating System, Prentice Hall, Englewood
level page tables used in Intel 30386. Jacob and Mudge            Cliffs, N.J.



                                                                            Chapter 12  Virtual Memory          475
3.   Beck, M., H. Bohme, M. Dziadzka, U. Kunitz,         19.  Jacob, B., and T. Mudge (1998): "Virtual memory
     R. Magnus, C. Schroter, and D. Verworner                 in contemporary microprocessors," IEEE Micro
     (2002): Linux Kernel Programming, 3rd ed.,               Magazine, 18, 60Â­75.
     Pearson Education, New York.                        20.  Johnstone, M. S., and P. R. Wilson (1998):
4.   Belady, L. A. (1966): "A study of replacement            "The memory fragmentation problem: solved?,"
     algorithms for virtual storage computers," IBM           Proceedings of the First International Symposium
     Systems Journal, 5 (2), 78Â­101.                          on Memory Management, 26Â­36.
5.   Bensoussen, A., C. T. Clingen, and R. C. Daley      21.  Love, R. (2005): Linux Kernel Development, 2nd
     (1972): "The MULTICS virtual                             ed. Novell Press.
     memory--concepts and design," Communications        22.  Mauro, J., and R. McDougall (2006): Solaris
     of the ACM, 15 (5), 308Â­318.                             Internals, 2nd ed., Prentice-Hall, Englewood
6.   Bryant, P. (1975): "Predicting working set sizes,"       Cliffs, N.J.
     IBM Journal of R and D, 19 (5), 221Â­229.            23.  Mattson, R. L., J. Gecsei, D. R. Slutz, and
7.   Bovet, D. P., and M. Cesati (2005): Understanding        I. L. Traiger (1970): "Evaluation techniques for
     the Linux Kernel, 3rd ed., O'Reilly, Sebastopol,         storage hierarchies," IBM Systems Journal, 9 (2),
     Calif.                                                   78Â­117.
8.   Carr, W. R., and J. L. Hennessy (1981):             24.  Navarro, J., S. Iyer, P. Druschel, and A. Cox
     "WSClock--a simple and effective algorithm for           (2002): "Practical, transparent operating system
     virtual memory management," Proceedings of the           support for superpages," ACM SIGOPS
     ACM Symposium on Operating Systems                       Operating Systems Review, 36, issue SI, 89Â­104.
     Principles, 87Â­95.                                  25.  Organick, E. I. (1972): The MULTICS System,
9.   Chang, A., and M. Mergen (1988): "801 storage:           MIT Press, Cambridge, Mass.
     architecture and programming," ACM                  26.  Randell, B. (1969): "A note on storage
     Transactions on Computer Systems, 6, 28Â­50.              fragmentation and program segmentation,"
10.  Daley, R. C., and J. B. Dennis (1968): "Virtual          Communications of the ACM, 12 (7), 365Â­369.
     memory, processes and sharing in MULTICS,"          27.  Rosell, J. R., and J. P. Dupuy (1973): "The design,
     Communications of the ACM, 11 (5), 305Â­322.              implementation and evaluation of a working set
11.  Denning, P. J. (1968a): "The working set model           dispatcher," Communications of the ACM, 16,
     for program behavior," Communications of the             247Â­253.
     ACM, 11 (5), 323Â­333.                               28.  Russinovich, M. E., and D. A. Solomon (2005):
12.  Denning, P. J. (1968b): "Thrashing : Its causes          Microsoft Windows Internals, 4th ed., Microsoft
     and prevention," Proceedings of AFIPS FJCC,              Press, Redmond, Wash.
     33, 915Â­922.                                        29.  Smith, A. J. (1978): "Bibliography on paging and
13.  Denning, P. J. (1970): "Virtual Memory,"                 related topics," Operating Systems Review, 12 (4),
     Computing Surveys, 2 (3), 153Â­189.                       39Â­56.
14.  Denning, P. J. (1980): "Working sets past and       30.  Swanson, M., L. Stoller, and J. Carter (1998):
     present," IEEE Transactions on Software                  "Increasing TLB reach using superpages backed
     Engineering, 6 (1), 64Â­84.                               by shadow memory," Proceedings of the 25th
15.  Ghanem, M. Z. (1975): "Study of memory                   International Symposium on Computer
     partitioning for multiprogramming systems with           Architecture, 204Â­213.
     virtual memory," IBM Journal of R and D, 19,        31.  Tanenbaum, A. S. (2001): Modern Operating
     451Â­457.                                                 Systems, 2nd ed., Prentice Hall, Englewood
16.  Gorman, M. (2004): Understanding the Linux               Cliffs, N.J.
     Virtual Memory Manager, Prentice Hall,              32.  Vahalia, U. (1996): Unix Internals--The New
     Englewood Cliffs, N.J.                                   Frontiers, Prentice Hall, Englewood Cliffs, N.J.
17.  Guertin, R.L.(1972): "Programming in a paging       33.  Wilson, P. R., M. S. Johnstone, M. Neely and
     environment," Datamation, 18 (2), 48Â­55.                 D. Boles (1995): "Dynamic storage allocation: a
18.  Hatfield, D. J., and J. Gerald (1971): "Program          survey and critical review," Proceedings of the
     restructuring for virtual memory," IBM Systems           International Workshop on Memory Management,
     Journal, 10 (3), 169Â­192.                                1Â­116.






                                                                    part                     4
    File Systems and I/O
                    Management
C omputer    users  expect   convenience    and    efficiency       while  creating     and
manipulating files, and sharing them with other users of the system. They
also expect a file system to possess protection, security and reliability fea-
tures so that their files are not subjected to illegal accesses or tampering by other
persons, or damage due to faults in the system. A system administrator expects
a file system to ensure efficient use of I/O devices and contribute towards high
performance of the system.
The file system uses a hierarchy of views and organizations to meet these
diverse requirements. The logical view is employed to provide the features desired
by users. In this view, a file is an entity that is owned by some user, shared by a
group of users, and reliably stored over a period of time. The logical organization
implements the logical view. It consists of different kinds of files and operations
on files, directory structures and arrangements used for sharing and protection of
files, and arrangements for reliable operation of the file system.
The physical view is employed to ensure speedy access to data, good perfor-
mance of file operations in a process, and good performance of I/O devices. In
this view, a file is a collection of data, which need to be accessed speedily, that is
stored on I/O devices, which need to be used efficiently. The physical organization
consists of arrangements using buffers and caches to implement the physical view.
We  discuss  the    logical  and  physical  views  of  a  file      system  in  separate
chapters. The third chapter in this part discusses protection and security measures
employed in an OS.
Chapter 13: File Systems
This chapter discusses a programmer's view of files and the file system. It describes
fundamental file organizations, directory structures, operations on files and direc-
tories, and file sharing semantics, which specify the manner in which results of
file manipulations performed by concurrent processes are visible to one another.
Issues that compromise reliability of a file system are discussed. Fault tolerance
using atomic actions and recovery using backups are described.
                                                                                                477



478  Part 4  File  Systems and I/O Management
                                                  Road Map for Part        4
                                                  File
                                                  Systems
                                                  Implementation
                                                  of
                                                  File Operations
                                                  Security
                                                  and
                                                  Protection
                   Schematic diagram showing the  order in which chapters  of  this  part  should  be  covered  in  a
                   course.
                   This chapter also discusses the role of the file control block as the interface
                   between the logical and physical organizations used in a file system. Its use in
                   implementing file operations and file sharing semantics is discussed.
                   Chapter 14: Implementation of File Operations
                   This chapter discusses the physical organization used in file systems. It starts with
                   an overview of I/O devices and their characteristics, and discusses different RAID
                   organizations that provide high reliability, fast access, and high data transfer rates.
                   The arrangements used to implement device-level I/O are then discussed, includ-
                   ing use of buffers and caches to speed up I/O operations and use of disk scheduling
                   policies to improve throughput of disk devices.
                   Chapter 15: Security and Protection
                   Security and protection measures together ensure that only authorized users can
                   access a file. This chapter discusses different kinds of security and protection
                   threats in an operating system, measures used to thwart these threats, and the
                   role played by the encryption technique in implementing these measures.
