Process Synchronization


                                                 Chapte                                   r  6
Process Synchronization
I nteracting processes are concurrent processes that share data or coordinate
     their activities with respect to one another. Data access synchronization
     ensures that shared data do not lose consistency when they are updated by
interacting processes. It is implemented by ensuring that processes access shared
data only in a mutually exclusive manner. Control synchronization ensures that
interacting processes perform their actions in a desired order. Together, these two
kinds of synchronization make up what we refer to as process synchronization.
Computer systems provide indivisible instructions (also called atomic instructions)
to support process synchronization.
     We discuss critical sections, which are sections of code that access shared
data in a mutually exclusive manner, and indivisible signaling operations, which
are used to implement control synchronization, and show how both are imple-
mented by using indivisible instructions. Following this discussion, we introduce
some classic problems of process synchronization, which are representative of
synchronization problems in various application domains. We analyze their syn-
chronization requirements and study important issues involved in fulfilling them.
     In the remainder of the chapter, we discuss semaphores and monitors, which
are the primary facilities for synchronization in programming languages and oper-
ating systems. We will see how they offer ways to fulfill the process synchronization
requirements of the classic problems.
6.1  WHAT IS PROCESS SYNCHRONIZATION?                                                  ·
In this chapter, we use the term process as a generic term for both a process and a
thread. Applications employ concurrent processes either to achieve computation
speedup (see Table 5.2), or to simplify their own design, as in multithreaded
servers (see Section 5.3). As summarized in Table 5.7, processes of an application
interact among themselves to share data, coordinate their activities, and exchange
messages or signals. We use the following notation to formally define the term
interacting processes:
     read_seti   set of data items read by process Pi and interprocess messages
                 or signals received by it
     write_seti  set of data items modified by process Pi and interprocess
                 messages or signals sent by it
                                                                                                165



166  Part 2  Process Management
             We use the term "update of a data item" for a modification of the data item's value
             that is based on its own previous value, e.g., x := x + 1 is an update, whereas
             x := 5 is not.
             Definition 6.1 Interacting Processes      Processes Pi and Pj are interacting pro-
             cesses if the write_set of one of the processes overlaps the write_set or read_set
             of the other.
                  The nature of interaction between processes when the write_set of one over-
             laps the read_set of another is obvious--the first process may set the value of a
             variable which the other process may read. The situation when the write_sets of
             two processes overlap is included in Definition 6.1 because the manner in which
             the processes perform their write operations can lead to incorrect results, so the
             processes must cooperate to avoid such situations. Processes that do not interact
             are said to be independent processes; they can execute freely in parallel.
                  Two kinds of requirements arise in interacting processes:
             ·    A process should perform a specific operation opi only when some condition
                  concerning shared data holds. The process must be delayed if these require-
                  ments are not met when it wishes to perform operation opi, and it must be
                  allowed to resume when these requirements have been met.
             ·    A process should perform an operation opi only after some other process
                  performs another specific operation opj. This requirement is met by using
                  some shared data to note whether operation opj has been performed, so that
                  the process can be delayed and resumed as described above.
                  Process synchronization is a generic term for the techniques used to delay
             and resume processes to implement process interactions. The execution speed
             of a process, or the relative execution speeds of interacting processes, cannot be
             known in advance because of factors such as time-slicing, priorities of processes,
             and I/O activities in processes. Hence a process synchronization technique must
             be designed so that it will function correctly irrespective of the relative execution
             speeds of processes.
                  Throughout this chapter, we will use the conventions shown in Figure 6.1 in
             the pseudocode for concurrent processes.
             6.2  RACE CONDITIONS                                                                   ·
             In Section 5.2.5, we mentioned that uncoordinated accesses to shared data may
             affect consistency of data. To see this problem, consider processes Pi and Pj that
             update the value of a shared data item ds through operations ai and aj, respectively.
                                   Operation ai :      ds := ds + 10;
                                   Operation aj :      ds := ds + 5;



                                                                                   Chapter 6        Process Synchronization  167
             ·   The control structure Parbegin <list of statements> Parend encloses
                 code that is to be executed in parallel. (Parbegin stands for parallel-begin,
                 and Parend for parallel-end.) If <list of statements> contains n
                 statements, execution of the Parbegin­Parend control structure
                 spawns n processes, each process consisting of the execution of one
                 statement in <list of statements>. For example, Parbegin S1, S2, S3, S4
                 Parend initiates four processes that execute S1, S2, S3 and S4, respectively.
                 The statement grouping facilities of a language such as begin­end,
                 can be used if a process is to consist of a block of code instead of a
                 single statement. For visual convenience, we depict concurrent
                 processes created in a Parbegin­Parend control structure as follows:
                 Parbegin  S11            S21               ...              Sn1
                           ...            ...                                ...
                           S1m            S2m               ...              Snm
                 Parend
                           Process P1     Process P2             Process Pn
                 where statements S11 . . . S1m form the code of process P1, etc.
             ·   Declarations of shared variables are placed before a Parbegin.
             ·   Declarations of local variables are placed at the start of a process.
             ·   Comments are enclosed within braces "{}".
             ·   Indentation is used to show nesting of control structures.
Figure 6.1   Pseudocode conventions for concurrent programs.
Let (ds)initial be the initial value of ds, and let process Pi be the first one to per-
form its operation. The value of ds after operation ai will be (ds)initial + 10.
If  process  Pj  performs  operation  aj  now,  the         resulting              value        of  ds  will  be
(ds)new = ((ds)initial + 10) + 5, i.e., (ds)initial + 15. If the processes perform their
operations in the reverse order, the new value of ds would be identical.
    If processes Pi and Pj perform their operations concurrently, we would expect
the result to be (ds)initial +15; however, it is not guaranteed to be so. This situation
is called a race condition. This term is borrowed from electronics, where it refers
to the principle that an attempt to examine a value, or make measurements on a
waveform, while it is changing can lead to wrong results.
    The race condition can be explained as follows: Operation ai is typically
implemented by using three machine instructions. The first instruction loads the
value of ds in a data register, say, register r1, the second instruction adds 10 to
the contents of r1, and the third instruction stores the contents of r1 back into the
location assigned to ds. We call this sequence of instructions the load-add-store
sequence. Operation aj is similarly implemented by a load-add-store sequence.
The result of performing operations ai and aj would be wrong if both ai and aj
operated on the old value of ds. This could happen if one process were engaged in
performing the load-add-store sequence, but the other process was performing a
load instruction before this sequence was completed. In such a case the value of
ds at the end of both the operations would be either (ds)initial + 5 or (ds)initial + 10,
depending on which of the operations completed later.



168  Part 2  Process Management
                     We define a race condition formally as follows: Let function fi(ds) represent
                  the operation ai on ds, i.e., for a given value of ds, fi(ds) indicates the value ds
                  would have after executing operation ai. Function fj(ds) analogously represents
                  the operation aj on ds. Let process Pi be the first one to perform its operation. The
                  value of ds after the operation would be fi(ds). If process Pj performs operation
                  aj now, operation aj will operate on fi(ds), so the resulting value of ds will be
                  fj( fi(ds)). If the processes perform their operations in the reverse order, the new
                  value of ds will be fi( fj (ds)).
                     Definition 6.2 Race Condition   A condition in which the value of a shared data
                     item ds resulting from execution of operations ai and aj on ds in interacting
                     processes may be different from both fi( fj (ds)) and fj ( fi(ds)).
                     The  next     example   illustrates  a  race  condition   in  an     airline  reservation
                  application and its consequences.
·
     Example 6.1  Race Condition in an Airline Reservation Application
                  The left column in the upper half of Figure 6.2 shows the code used by processes
                  in an airline reservation application. The processes use identical code, hence ai
                  and aj, the operations performed by processes Pi and Pj, are identical. Each of
                  these operations examines the value of nextseatno and updates it by 1 if a seat
                  is available. The right column of Figure 6.2 shows the machine instructions
                  corresponding to the code. Statement S3 corresponds to three instructions
                  S3.1, S3.2 and S3.3 that form a load-add-store sequence of instructions for
                  updating the value of nextseatno.
                     The lower half of Figure 6.2 is a timing diagram for the applications. It
                  shows three possible sequences in which processes Pi and Pj could execute their
                  instructions when nextseatno = 200 and capacity = 200. In case 1, process Pi
                  executes the if statement that compares values of nextseatno with capacity and
                  proceeds to execute instructions S2.1, S3.1, S3.2 and S3.3 that allocate a seat
                  and increment nextseatno. When process Pj executes the if statement, it finds
                  that no seats are available so it does not allocate a seat.
                     In case 2, process Pi executes the if statement and finds that a seat can be
                  allocated. However, it gets preempted before it can execute instruction S2.1.
                  Process Pj now executes the if statement and finds that a seat is available. It
                  allocates a seat by executing instructions S2.1, S3.1, S3.2 and S3.3 and exits.
                  nextseatno is now 201. When process Pi is resumed, it proceeds to execute
                  instruction S2.1, which allocates a seat. Thus, seats are allocated to both
                  requests. This is a race condition because when nextseatno = 200, only one
                  seat should be allocated.
                     In case 3, process Pi gets preempted after it loads 200 in regj through
                  instruction S3.1. Now, again both Pi and Pj allocate a seat each, which is a
                  race condition.
                  ·



                                                                                           Chapter 6        Process       Synchronization  169
            Code of processes                                  Corresponding machine instructions
S1          if nextseatno  capacity                     S 1.1  Load nextseatno in regk
                                                        S 1.2  If regk > capacity goto S4.1
            then
S2               allotedno:=nextseatno;                 S 2.1  Move nextseatno to allotedno
S3               nextseatno:=nextseatno+1;              S 3.1  Load nextseatno in regj
                                                        S 3.2  Add 1 to regj
                                                        S 3.3  Store regj in nextseatno
                                                        S 3.4  Go to S5.1
            else
S4               display "sorry, no seats               S 4.1  Display "sorry, . . . "
                  available"
S5          ...                                         S 5.1  ...
Some  execution cases
            Pi    S 1.1  S 1.2  S   2 .1  S 3.1  S 3.2  S 3.3  S 3.4
Case 1      Pj                                                        S 1.1  S 1.2  S 4.1
            Pi    S 1.1  S 1.2                                                      S 2.1  S  3  .1  S 3.2  S 3.3  S 3.4
Case 2      Pj                  S   1 .1  S 1.2  S 2.1  S 3.1  S 3.2  S 3.3  S 3.4
            Pi    S 1.1  S 1.2  S   2 .1  S 3.1                                                      S 3.2  S 3.3  S 3.4
Case 3      Pj                                   S 1.1  S 1.2  S 2.1  S 3.1  S 3.2  S 3.3  S  3  .4
                                                 Execution of instructions by processes                     Time   
Figure 6.2  Data sharing        by  processes of a reservation application.
A program containing a race condition may produce correct or incorrect
results depending on the order in which instructions of its processes are executed.
This feature complicates both testing and debugging of concurrent programs, so
race conditions should be prevented.
Data Access Synchronization                      Race conditions are prevented if we ensure that
operations ai and aj of Definition 6.2 do not execute concurrently--that is, only
one of the operations can access shared data ds at any time. This requirement
is called mutual exclusion. When mutual exclusion is ensured, we can be sure
that the result of executing operations ai and aj would be either fi( fj (ds)) or
fj( fi(ds)). Data access synchronization is coordination of processes to implement
mutual exclusion over shared data. A technique of data access synchronization
is used to delay a process that wishes to access ds if another process is cur-
rently accessing ds, and to resume its operation when the other process finishes
using ds.
To prevent race conditions, we first check if the logic of processes in an
application causes a race condition. We use the following notation for this
purpose:



170  Part 2  Process Management
                  update_seti        set of data items updated by process Pi, that is, the set of data
                                     items whose values are read, modified, and written back by
                                     process Pi
                  The  logic     of  a  pair  of  processes  Pi  and  Pj  causes  a  race  condition    if
             update_seti  update_setj =           i.e., if some variable is updated by both Pi and Pj.
             The logic of processes Pi and Pj in the airline reservation application of Exam-
             ple 6.1 causes a race condition because update_seti = update_setj = {nextseatno}.
             Once we know the data item whose updates cause a race condition, we use data
             access synchronization techniques to ensure that this data item is used in a mutu-
             ally exclusive manner. The next section discusses a conceptual basis for data
             access synchronization.
             6.3  CRITICAL SECTIONS                                                                     ·
             Mutual exclusion between actions of concurrent processes is implemented by
             using critical sections of code. A critical section is popularly known by its
             acronym CS.
             Definition 6.3 Critical Section      A critical section for a data item ds is a section
             of code that is designed so that it cannot be executed concurrently either with
             itself or with other critical section(s) for ds.
                  If some process Pi is executing a critical section for ds, another process wish-
             ing to execute a critical section for ds will have to wait until Pi finishes executing
             its critical section. Thus, a critical section for a data item ds is a mutual exclusion
             region with respect to accesses to ds.
                  We mark a critical section in a segment of code by a dashed rectangular box.
             Note that processes may share a single copy of the segment of code that contains
             one critical section, in which case only a single critical section for ds exists in
             the application. In all other cases, many critical sections for ds may exist in the
             application. Definition 6.3 covers both situations. A process that is executing a
             critical section is said to be "in a critical section." We also use the terms "enter a
             critical section" and "exit a critical section" for situations where a process starts
             and completes an execution of a critical section.
                  Figure 6.3(a) shows the code of a process that contains several critical
             sections. The process has a cyclic behavior due to the statement repeat forever.
             In each iteration, it enters a critical section when it needs to access a shared data
             item. At other times, it executes other parts of code in its logic, which together
             constitute "remainder of the cycle." For simplicity, whenever possible, we use the
             simple process form shown in Figure 6.3(b) to depict a process. The following
             example illustrates the use of a critical section to avoid race conditions.



                                                                                      Chapter 6  Process  Synchronization       171
repeat           forever                                                  repeat forever
                          ...
                 Critical section
                          ...
                 Critical section                Remainder of                  Critical section
                          ...                    the cycle
                 Critical section
                          ...                                                  {Remainder of the cycle}
end                                                                       end
(a)                                                                       (b)
Figure 6.3  (a)  A process with    many    critical sections;     (b)  a  simpler way of depicting this
process.
                 if nextseatno  capacity                    if nextseatno  capacity
                 then                                       then
                       allotedno:=nextseatno;                     allotedno:=nextseatno;
                       nextseatno:=nextseatno+1;                  nextseatno:=nextseatno+1;
                 else                                       else
                       display "sorry, no seats                   display "sorry, no seats
                                   available";                                 available";
                               Process Pi                                 Process Pj
Figure 6.4  Use of critical sections in an airline  reservation system.
                                                                                                                                ·
Preventing a Race Condition through a Critical Section                                                       Example       6.2
Figure 6.4 shows use of critical sections in the airline reservation system of
Figure 6.2. Each process contains a critical section in which it accesses and
updates the shared variable nextseatno. Let fi(nextseatno) and fj(nextseatno)
represent the operations performed in critical sections of Pi and Pj, respec-
tively. If Pi and Pj attempt to execute their critical sections concurrently, one
of them will be delayed. Hence, the resulting value of nextseatno will be either
fi( fj (nextseatno)) or fj ( fi(nextseatno)). From Definition 6.2, a race condition
does not arise.
                                                                                                          ·
Use of critical sections causes delays in operation of processes. Both processes
and the kernel must cooperate to reduce such delays. A process must not execute
for too long inside a critical section and must not make system calls that might put
it in the blocked state. The kernel must not preempt a process that is engaged in
executing a critical section. This condition requires the kernel to know whether
a process is inside a critical section at any moment, and it cannot be met if
processes implement critical sections on their own, i.e., without involving the
kernel. Nevertheless, in this chapter we shall assume that a process spends only a
short time inside a critical section.



172  Part 2  Process Management
             Table 6.1           Essential Properties of a CS Implementation
             Property            Description
             Mutual exclusion    At any moment, at most one process may execute a CS for
                                 a data item ds.
             Progress            When no process is executing a CS for a data item ds, one
                                 of the processes wishing to enter a CS for ds will be
                                 granted entry.
             Bounded wait        After a process Pi has indicated its desire to enter a CS for
                                 ds, the number of times other processes can gain entry to a
                                 CS for ds ahead of Pi is bounded by a finite integer.
             6.3.1 Properties of a Critical Section Implementation
             When several processes wish to use critical sections for a data item ds, a critical
             section implementation must ensure that it grants entry into a critical section in
             accordance with the notions of correctness and fairness to all processes. Table 6.1
             summarizes three essential properties a critical section implementation must pos-
             sess to satisfy these requirements. The mutual exclusion property guarantees that
             two or more processes will not be in critical sections for ds simultaneously, which
             is the crux of Definition 6.3. It ensures correctness of the implementation. The
             second and third property of Table 6.1 together guarantee that no process wish-
             ing to enter a critical section will be delayed indefinitely; i.e., starvation will not
             occur. We discuss this aspect in the following.
                  The progress property ensures that if some processes are interested in entering
             critical sections for a data item ds, one of them will be granted entry if no process
             is currently inside any critical section for ds--that is, use of a CS cannot be
             "reserved" for a process that is not interested in entering a critical section at
             present. However, this property alone cannot prevent starvation because a process
             might never gain entry to a CS if the critical section implementation always favors
             other processes for entry to the CS. The bounded wait property ensures that this
             does not happen by limiting the number of times other processes can gain entry to
             a critical section ahead of a requesting process Pi. Thus the progress and bounded
             wait properties ensure that every requesting process will gain entry to a critical
             section in finite time; however, these properties do not guarantee a specific limit
             to the delay in gaining entry to a CS.
             6.4  CONTROL SYNCHRONIZATION
                  AND INDIVISIBLE OPERATIONS                                                          ·
             Interacting processes need to coordinate their execution with respect to one
             another, so that they perform their actions in a desired order. This requirement
             is met through control synchronization.



                                                                                   Chapter  6   Process  Synchronization  173
                      {Perform operation ai only after Pj         Perform operation aj
                       performs operation aj}                     ...
                             Process Pi                                Process Pj
Figure  6.5  Processes requiring control synchronization.
        var
                 operation_aj_ performed : boolean;
                 pi_blocked : boolean;
        begin
                 operation_aj_ performed := false;
                 pi_blocked := false;
        Parbegin
                 ...                                       ...
                 if operation_aj_ performed    =  false    {perform operation aj}
                 then                                      if pi_blocked = true
                      pi_blocked := true;                  then
                      block  (Pi);                                pi_blocked := false;
                 {perform operation ai}                           activate (Pi);
                 ...                                       else
                 ...                                              operation_aj_ performed   :=  true;
                 ...                                       ...
        Parend;
        end.
                       Process Pi                                      Process Pj
Figure  6.6  A naive attempt at signaling through        boolean variables.
Figure 6.5 shows a pseudocode for processes Pi and Pj, wherein process Pi
would perform an operation ai only after process Pj has performed an operation
aj. Signaling is a general technique of control synchronization. It can be used
to meet the synchronization requirement of Figure 6.5 as follows: When process
Pi reaches the point where it wishes to perform operation ai, it checks whether
process Pj has performed operation aj. If it is so, Pi would perform operation
ai right away; otherwise, it would block itself waiting for process Pj to perform
operation aj . After performing operation aj , process Pj would check whether Pi
is waiting for it. If so, it would signal process Pi to resume its operation.
Figure 6.6 shows a naive attempt at signaling. The synchronization data
consists of two boolean variables: operation_aj_performed is a flag that indicates
whether process Pj has performed operation aj, and pi_blocked is a flag which
indicates whether process Pi has blocked itself waiting for process Pj to execute
operation aj. Both these flags are initialized to false. The code makes system calls
to block and activate processes to achieve the desired control synchronization.
Before         performing           operation     ai ,   process  Pi   consults    the  variable       oper-
ation_aj_ performed          to     check  whether         process     Pj  has     already      performed
operation aj. If so, it goes ahead to perform operation ai; otherwise, it sets



174  Part 2  Process Management
             Table  6.2          Race Condition in       Process  Synchronization
             Time                Actions of process P i           Actions of process Pj
             t1          if action_aj_performed          = false
             t2                                                   {perform action aj }
             t3                                                   if pi_blocked = true
             t4                                                   action_aj_performed :=true
             ...
             t20         pi_blocked :=true;
             t21         block (Pi);
             pi_blocked to true and makes a system call to block itself. Process Pj performs
             operation aj and checks whether process Pi has already become blocked to wait
             until it has performed operation aj. If so, it makes a system call to activate Pi;
             otherwise, it sets operation_aj_performed to true so that process Pi would know
             that it has performed operation aj.
                  However, this naive signaling arrangement does not work because process Pi
             may face indefinite blocking in some situations. Table 6.2 shows such a situation.
             Process Pi checks the value of operation_aj_performed and finds that operation
             aj has not been performed. At time t2, it is poised to set the variable pi_blocked
             to true, but at this time it is preempted. Process Pj is now scheduled. It performs
             operation aj and checks whether process Pi is blocked. However, pi_blocked is
             false, so Pj simply sets operation_aj_performed to true and continues its execution.
             Pi is scheduled at time t20. It sets pi_blocked to true and makes a system call to
             block itself. Process Pi will sleep for ever!
                  In the notation of Section 6.2, consider the if statements in processes Pi and
             Pj to represent the operations fi and fj on the state of the system. The result of
             their execution should have been one of the following: process Pi blocks itself,
             gets activated by Pj and performs operation ai; or process Pi finds that Pj has
             already performed aj and goes ahead to perform operation ai. However, in the
             execution shown in Table 6.2, process Pi blocks itself and is never activated. From
             Definition 6.2, this is a race condition.
                  The race condition has two causes--process Pi can be preempted after find-
             ing operation_aj_performed = false but before setting pi_blocked to true, and
             process Pj can be preempted after finding pi_blocked = false but before setting
             operation_aj_performed to true. The race condition can be prevented if we could
             ensure that processes Pi and Pj would not be preempted before they set the respec-
             tive flags to true. An indivisible operation (also called an atomic operation) is the
             device that ensures that processes can execute a sequence of actions without being
             preempted.
             Definition 6.4 Indivisible Operation           An operation on a set of data items that
             cannot be executed concurrently either with itself or with any other operation
             on a data item included in the set.



                                                                         Chapter 6  Process Synchronization  175
                         procedure check_aj
                         begin
                         if operation_aj_performed=false
                               then
                                pi_blocked:=true;
                                block (Pi)
                         end;
                         procedure post_aj
                         begin
                         if pi_blocked=true
                               then
                                pi_blocked:=false;
                                activate(Pj)
                         else
                                operation_aj_performed:=true;
                         end;
Figure 6.7  Indivisible  operations check_aj and post_aj for signaling.
     Since an indivisible operation cannot be performed concurrently with any
other operation involving the same data, it must be completed before any other
process accesses the data. The situation shown in Table 6.2 would not arise if
the if statements in Figure 6.6 were implemented as indivisible operations on
data items operation_aj_performed and pi_blocked, because if process Pi found
operation_aj_performed = false, it would be able to set pi_blocked = true without
being preempted, and if process Pj found pi_blocked to be false, it would be able
to set operation_aj_performed to true without being preempted. Accordingly, we
define two indivisible operations check_aj and post_aj to perform the if statements
of processes Pi and Pj, respectively, and replace the if statements by invocations of
these indivisible operations. Figure 6.7 shows details of the indivisible operations
check_aj and post_aj. When operation_aj_performed is false, indivisible operation
check_aj is deemed to be complete after process Pi is blocked; it would enable
process Pj to perform operation post_aj.
     An indivisible operation on the set of data items {ds} is like a critical section
on {ds}. However, we differentiate between them because a critical section has
to be explicitly implemented in a program, whereas the hardware or software of
a computer system may provide some indivisible operations among its primitive
operations.
6.5  SYNCHRONIZATION APPROACHES                                                                              ·
In this section we discuss how the critical sections and indivisible operations
required for process synchronization can be implemented.
6.5.1 Looping versus Blocking
A critical section for {ds} and an indivisible signaling operation on {ds} have the
same basic requirement--processes should not be able to execute some sequences



176  Part 2  Process Management
             of instructions concurrently or in parallel. Hence both could be implemented
             through mutual exclusion as follows:
                                 while (some process is in a critical section on {ds} or
                                          is executing an indivisible operation using {ds})
                                 { do nothing }
                                          Critical section or
                                          indivisible operation
                                                    using {ds}
             In the while loop, the process checks if some other process is in a critical section
             for the same data, or is executing an indivisible operation using the same data. If
             so, it keeps looping until the other process finishes. This situation is called a busy
             wait because it keeps the CPU busy in executing a process even as the process does
             nothing! The busy wait ends only when the process finds that no other process is
             in a critical section or executing an indivisible operation.
             A busy wait in a process has several adverse consequences. An implementa-
             tion of critical sections employing busy waits cannot provide the bounded wait
             property because when many processes are in a busy wait for a CS, the implemen-
             tation cannot control which process would gain entry to a CS when the process
             currently in CS exits. In a time-sharing OS, a process that gets into a busy wait
             to gain entry to a CS would use up its time slice without entering the CS, which
             would degrade the system performance.
             In an OS using priority-based scheduling, a busy wait can result in a situation
             where processes wait for each other indefinitely. Consider the following situation:
             A high-priority process Pi is blocked on an I/O operation and a low-priority
             process Pj enters a critical section for data item ds. When Pi's I/O operation
             completes, Pj is preempted and Pi is scheduled. If Pi now tries to enter a critical
             section for ds using the while loop described earlier, it would face a busy wait.
             This busy wait denies the CPU to Pj, hence it is unable to complete its execution
             of the critical section and exit. In turn, this situation prevents Pi from entering its
             critical section. Processes Pi and Pj now wait for each other indefinitely. Because
             a high-priority process waits for a process with a low priority, this situation is
             called priority inversion. The priority inversion problem is typically addressed
             through the priority inheritance protocol, wherein a low-priority process that holds
             a resource temporarily acquires the priority of the highest-priority process that
             needs the resource. In our example, process Pj would temporarily acquire the
             priority of process Pi, which would enable it to get scheduled and exit from its
             critical section. However, use of the priority inheritance protocol is impractical
             in these situations because it would require the kernel to know minute details of
             the operation of processes.
             To avoid busy waits, a process waiting for entry to a critical section should be
             put into the blocked state. Its state should be changed to ready only when it can



                                                       Chapter 6    Process Synchronization  177
be allowed to enter the CS. This approach can be realized through the following
outline:
             if (some process is in a critical section on {ds} or
             is executing an indivisible operation using {ds})
             then make a system call to block itself;
                          Critical section or
                          indivisible operation
                          using {ds}
In this approach, the kernel must activate the blocked process when no other pro-
cess is operating in a critical section on {ds} or executing an indivisible operation
using {ds}.
When a critical section or an indivisible operation is realized through any of
the above outlines, a process wishing to enter a CS has to check whether any other
process is inside a CS, and accordingly decide whether to loop (or block). This
action itself involves executing a few instructions in a mutually exclusive way to
avoid a race condition (see Section 6.4), so how is that to be done? Actually, it
can be done in two ways. In the first approach, called the algorithmic approach,
a complex arrangement of checks is used in concurrent processes to avoid race
conditions. We shall discuss the features of this approach, and its drawbacks, in
Section 6.8. The second approach uses some features in computer hardware to
simplify this check. We discuss this approach in the next section.
6.5.2 Hardware Support for Process Synchronization
Process synchronization involves executing some sequences of instructions in a
mutually exclusive manner. On a uniprocessor system, this can be achieved by dis-
abling interrupts while a process executes such a sequence of instructions, so that
it will not be preempted. However, this approach involves the overhead of system
calls to disable interrupts and enable them again, and also delays processing of
interrupts, which can lead to undesirable consequences for system performance
or user service. It is also not applicable to multiprocessor systems. For these rea-
sons, operating systems implement critical sections and indivisible operations
through indivisible instructions provided in computers, together with shared vari-
ables called lock variables. In this section, we use illustrations of the looping
approach to process synchronization; however, the techniques discussed here are
equally applicable to the blocking approach to process synchronization. Note that
indivisible instructions merely assist in implementing critical sections; the proper-
ties of CS implementation summarized in Table 6.1 have to be ensured separately
by enabling processes to enter CS in an appropriate manner (see Exercise 6.12).
Indivisible Instructions  Since the mid-1960s, computer systems have provided
special features in their hardware to prevent race conditions while accessing a
memory location containing shared data. The basic theme is that all accesses
to a memory location made by one instruction should be implemented without
permitting another CPU to access the same location. Two popular techniques



178  Part 2  Process Management
             used for this purpose are locking the memory bus during an instruction (e.g.,
             in Intel 80x86 processors) and providing special instructions that perform some
             specific operations on memory locations in a race-free manner (e.g., in IBM/370
             and M68000 processors). We will use the term indivisible instruction as a generic
             term for all such instructions.
             Use of a Lock Variable           A lock variable is a two-state variable that is used to
             bridge the semantic gap (see Definition 4.1) between critical sections or indi-
             visible operations, on the one hand, and indivisible instructions provided in a
             computer system, on the other. To implement critical sections for a data item ds,
             an application associates a lock variable with ds. The lock variable has only two
             possible values--open and closed. When a process wishes to execute a critical
             section for ds, it tests the value of the lock variable. If the lock is open, it closes
             the lock, executes the critical section, and opens the lock while exiting from the
             critical section. To avoid race conditions in setting the value of the lock variable,
             an indivisible instruction is used to test and close the lock. Lock variables assist
             in implementing indivisible operations in a similar manner.
             Figure 6.8 illustrates how a critical section or an indivisible operation is imple-
             mented by using an indivisible instruction and a lock variable. The indivisible
             instruction performs the actions indicated in the dashed box: if the lock is closed,
             it loops back to itself; otherwise, it closes the lock. In the following, we illustrate
             use of two indivisible instructions--called test-and-set and swap instructions--to
             implement critical sections and indivisible operations.
             Test-and-Set (TS) Instruction    This indivisible instruction performs two actions.
             It "tests" the value of a memory byte and sets the condition code field (i.e., the
             flags field) of the PSW to indicate whether the value was zero or nonzero. It also
             sets all bits in the byte to 1s. No other CPU can access the memory byte until both
             actions are complete. This instruction can be used to implement the statements
             enclosed in the dashed box in Figure 6.8.
             Figure 6.9 is a segment of an IBM/370 assembly language program for imple-
             menting a critical section or an indivisible operation. LOCK is a lock variable used
             with the convention that a nonzero value implies that the lock is closed, and a zero
             implies that it is open. The first line in the assembly language program declares
             LOCK and initializes it to 0. The TS instruction sets the condition code according
             to the value of LOCK and then sets the value of LOCK to closed. Thus, the condition
             code indicates if the lock was closed before the TS instruction was executed. The
             branch instruction BC   7,       ENTRY_TEST checks the condition code and loops
                                 entry_test:  if lock = closed        Performed by
                                              then goto entry_test;   an indivisible
                                              lock := closed;         instruction
                                              {Critical section or
                                              indivisible operation}
                                              lock := open;
             Figure 6.8  Implementing a critical section or indivisible operation by using a lock variable.



                                                                              Chapter 6     Process Synchronization  179
        LOCK             DC      X`00'                 Lock    is   initialized        to   open
        ENTRY_TEST       TS      LOCK                  Test-and-set       lock
                         BC      7,  ENTRY_TEST        Loop    if   lock  was   closed
                        ...                            {  Critical     section      or
                                                          indivisible         operation     }
                         MVI     LOCK,  X`00'          Open    the    lock(by   moving      0s)
Figure  6.9   Implementing a critical section or indivisible operation by using test-and-set.
TEMP                DS       1                      Reserve      one   byte   for   TEMP
LOCK                DC       X`00'                  Lock   is    initialized       to  open
                    MVI      TEMP,     X`FF'        X`FF'    is  used     to  close    the     lock
ENTRY_TEST          SWAP     LOCK,     TEMP
                    COMP     TEMP,     X`00'        Test   old   value    of   lock
                    BC       7,  ENTRY_TEST         Loop   if    lock  was    closed
                    ...                             {  Critical       section   or
                                                       indivisible        operation      }
                    MVI      LOCK,     X`00'        Open   the   lock
Figure 6.10   Implementing   a critical section or  indivisible operation by using a swap
instruction.
back to the TS instruction if the lock was closed. This way a process that finds
the lock closed would execute the loop in a busy wait until lock was opened. The
MVI instruction puts 0s in all bits of LOCK; i.e., it opens the lock. This action
would enable only one of the processes looping at ENTRY_TEST to proceed.
Swap Instruction        The swap instruction exchanges contents of two memory
locations. It is an indivisible instruction; no other CPU can access either of
the locations during swapping. Figure 6.10 shows how a critical section or an
indivisible operation can be implemented by using the swap instruction. (For
convenience, we use the same coding conventions as used for the TS instruc-
tion.) The temporary location TEMP is initialized to a nonzero value. The SWAP
instruction swaps its contents with LOCK. This action closes the lock. The old
value of LOCK is now available in TEMP. It is tested to find whether the lock was
already closed. If so, the process loops on the swap instruction until the lock is
opened. The process executing the critical section or indivisible operation opens
the lock at the end of the operation. This action enables one process to get past
the BC instruction and enter the critical section or the indivisible operation.
Many computers provide a Compare-and-swap instruction. This instruc-
tion has three operands. If the first two operands are equal, it copies the third
operand's value into the second operand's location; otherwise, it copies the
second operand's value into the first operand's location. It is easy to rewrite
the program of Figure 6.10 by using the instruction Compare-and-swap
first_opd,    LOCK,           third_opd       where       the    values   of   first_opd          and
third_opd correspond to the open and closed values of the lock. In effect,
this instruction closes the lock and puts its old value in first_opd.



180  Part 2  Process Management
             6.5.3 Algorithmic Approaches, Synchronization Primitives,
                      and Concurrent Programming Constructs
             Historically, implementation of process synchronization has gone through three
             important stages--algorithmic approaches, synchronization primitives, and con-
             current  programming     constructs.  Each  stage    in  its  history  solved  practical
             difficulties that were faced in the previous stage.
                  Algorithmic approaches were largely confined to implementing mutual exclu-
             sion. They did not use any special features in computer architecture, programming
             languages, or the kernel to achieve mutual exclusion; instead they depended on
             a complex arrangement of checks to ensure that processes accessed shared data
             in a mutually exclusive manner. Thus the algorithmic approaches were inde-
             pendent of hardware and software platforms. However, correctness of mutual
             exclusion depended on correctness of these checks, and was hard to prove because
             of logical complexity of the checks. This problem inhibited development of large
             applications. Since the algorithmic approaches worked independently of the ker-
             nel, they could not employ the blocking approach to process synchronization
             (see Section 6.5.1), so they used the looping approach and suffered from all
             its drawbacks.
                  A set of synchronization primitives were developed to overcome deficiencies of
             the algorithmic approach. Each primitive was a simple operation that contributed
             to process synchronization; it was implemented by using indivisible instructions
             in the hardware and support from the kernel for blocking and activation of pro-
             cesses. The primitives possessed useful properties for implementing both mutual
             exclusion and indivisible operations, and it was hoped that these properties could
             be used to construct proofs of correctness of a concurrent program. However,
             experience showed that these primitives could be used haphazardly, a property
             that caused its own difficulties with correctness of programs. Most modern oper-
             ating systems provide the wait and signal primitives of semaphores; however, they
             are employed only by system programmers because of the problems mentioned
             above.
                  The next important step in the history of process synchronization was
             the development of concurrent programming constructs, which provided data
             abstraction and encapsulation features specifically suited to the construction of
             concurrent programs. They had well-defined semantics that were enforced by
             the language compiler. Effectively, concurrent programming constructs incorpo-
             rated functions that were analogous to those provided by the synchronization
             primitives, but they also included features to ensure that these functions could
             not be used in a haphazard or indiscriminate manner. These properties helped
             in ensuring correctness of programs, which made construction of large appli-
             cations practical. Most modern programming languages provide a concurrent
             programming construct called a monitor.
                  We discuss algorithmic approaches to process synchronization in Section 6.8,
             and  semaphores     and  synchronization    primitives   for  mutual   exclusion     in
             Section 6.9. Section 6.10 describes monitors.



                                                                                Chapter 6     Process Synchronization  181
6.6      STRUCTURE OF CONCURRENT SYSTEMS                                                                               ·
A concurrent system consists of three key components:
·  Shared data
·  Operations on shared data
·  Interacting processes
     Shared data include two kinds of data--application data used and manipu-
lated by processes, and synchronization data, i.e., data used for synchronization
between processes. An operation is a convenient unit of code, typically a func-
tion or a procedure in a programming language, which accesses and manipulates
shared data. A synchronization operation is an operation on synchronization data.
     A snapshot of a concurrent system is a view of the system at a specific time
instant. It shows relationships between shared data, operations and processes
at that instant of time. We use the pictorial conventions shown in Figure 6.11
to depict a snapshot. A process is shown as a circle. A circle with a cross in it
indicates a blocked process. A data item, or a set of data items, is represented by
a rectangular box. The value(s) of data, if known, are shown inside the box.
     Operations on data are shown as connectors or sockets joined to the data. An
oval shape enclosing a data item indicates that the data item is shared. A dashed
line connects a process and an operation on data if the process is currently engaged
in executing the operation. Recall that a dashed rectangular box encloses code
executed as a critical section. We extend this convention to operations on data.
Hence mutually exclusive operations on data are enclosed in a dashed rectangular
box. A queue of blocked processes is associated with the dashed box to show the
processes waiting to perform one of the operations.
     The execution of a concurrent system is represented by a series of snapshots.
                Pi         : Process Pi               d
                                                      5                      : Shared data d
                Pi         : Blocked
                           process Pi
                                                      d      OP1
     ..  .                 : Queue of blocked                            Pi
                           processes                  2                      :  Process Pi performs
                                                             OP2                OP1 on shared data d
            d
            25             : Data d                          OP1
                                                      d
            d       OP1                               8                      :  OP1, OP2 are mutually
            37             : Operations                                         exclusive operations
                    OP2    on data d                         OP2
Figure   6.11   Pictorial  conventions for snapshots     of  concurrent  systems.



182  Part 2  Process Management
·
     Example 6.3  Snapshots of a Concurrent System
                  Consider the system of Figure 6.5, where process Pi performs action ai only
                  after process Pj performs action aj . We assume that operations ai and aj operate
                  on shared data items X and Y , respectively. Let the system be implemented
                  using the operations check_aj and post_aj of Figure 6.7. This system comprises
                  the following components:
                  Shared data                            Boolean variables operation_aj_performed
                                                         and pi_blocked, both initialized to false,
                                                         and data items X and Y.
                  Operations on application data         Operations ai and aj .
                  Synchronization operations             Operations check_aj and post_aj.
                  Processes                              Processes Pi and Pj .
                     Figure 6.12 shows three snapshots of this system. T and F indicate val-
                  ues true and false, respectively. Operations check_aj and post_aj both use the
                  boolean variables operation_aj_performed and pi_blocked. These operations
                  are indivisible operations, so they are mutually exclusive. Accordingly, they
                  are enclosed in a dashed box. Figure 6.12(a) shows the situation when process
                  Pj is engaged in performing operation aj and process Pi wishes to perform
                  operation ai, so it invokes operation check_aj. Operation check_aj finds that
                  operation_aj_performed is false, so it sets pi_blocked to true, blocks process
                  Pi and exits. When Pj finishes performing operation aj, it invokes operation
                  post_aj (see Figure 6.12(b)). This operation finds that pi_blocked is true, so it
                  sets pi_blocked to false, activates process Pi, and exits. Process Pi now performs
                  operation ai (see Figure 6.12(c)).
                  ·
                                       ai                        ai                        ai         Pi
                                 X                       X                        X
                                       aj        Pj              aj                        aj
                                 Y                       Y                        Y
                                       check_aj  Pi              check_aj  Pi             check_aj
                     pi_blocked     F                         T                        F
                     operation_     F                         F            ?           F
                     aj_performed      post_aj                   post_aj   Pj             post_aj     Pj
                     (a)                                 (b)                      (c)
                  Figure 6.12 Snapshots of the   system  of Example 6.3.



                                                                  Chapter 6  Process Synchronization  183
6.7     CLASSIC PROCESS SYNCHRONIZATION PROBLEMS                                                      ·
A solution to a process synchronization problem should meet three important
criteria:
·    Correctness:     Data  access  synchronization  and   control    synchronization
     should be performed in accordance with synchronization requirements of
     the problem.
·    Maximum concurrency: A process should be able to operate freely except
     when it needs to wait for other processes to perform synchronization actions.
·    No busy waits: To avoid performance degradation, synchronization should
     be    performed  through  blocking  rather      than  through  busy     waits   (see
     Section 6.5.1).
     As discussed in sections 6.3 and 6.4, critical sections and signaling are the
key elements of process synchronization, so a solution to a process synchroniza-
tion problem should incorporate a suitable combination of these elements. In
this section, we analyze some classic problems in process synchronization, which
are representative of synchronization problems in various application domains,
and discuss issues (and common mistakes) in designing their solutions. In later
Sections we implement their solutions using various synchronization features
provided in programming languages.
6.7.1 Producers--Consumers with Bounded Buffers
A producers­consumers system with bounded buffers consists of an unspecified
number of producer and consumer processes and a finite pool of buffers (see
Figure 6.13). Each buffer is capable of holding one item of information--it is said
to become full when a producer writes a new item into it, and become empty when
a consumer copies out an item contained in it; it is empty when the producers­
consumers system starts its operation. A producer process produces one item of
information at a time and writes it into an empty buffer. A consumer process
consumes information one item at a time from a full buffer.
     A producers­consumers system with bounded buffers is a useful abstraction
for many practical synchronization problems. A print service is a good example
              Producers                                    Consumers
                                    Buffer pool
Figure  6.13  A producers­consumers system with bounded buffers.



184  Part 2  Process Management
             in the OS domain. A fixed-size queue of print requests is the bounded buffer. A
             process that adds a print request to the queue is a producer process, and a print
             daemon is a consumer process. The data logging application of Example 5.1 would
             also be an instance of the producers­consumers problem if the housekeeping
             process is omitted--the copy_sample process is the producer since it writes a data
             sample into a buffer. The record_sample process is a consumer since it removes a
             data sample from the buffer and writes it into the disk file.
                 A solution to the producers­consumers problem must satisfy the following
             conditions:
             1.  A producer must not overwrite a full buffer.
             2.  A consumer must not consume an empty buffer.
             3.  Producers        and  consumers          must  access  buffers  in  a   mutually     exclusive
                 manner.
             The following condition is also sometimes imposed:
             4.  Information must be consumed in the same order in which it is put into the
                 buffers, i.e., in FIFO order.
                 Figure 6.14 shows an outline for the producers­consumers problem. Pro-
             ducer and consumer processes access a buffer inside a critical section. A producer
             enters its critical section and checks whether an empty buffer exists. If so, it pro-
             duces into that buffer; otherwise, it merely exits from its critical section. This
             sequence is repeated until it finds an empty buffer. The boolean variable produced
             is used to break out of the while loop after the producer produces into an empty
             buffer. Analogously, a consumer makes repeated checks until it finds a full buffer
             to consume from.
                 This outline suffers from two problems--poor concurrency and busy waits.
             The pool contains many buffers, and so it should be possible for producers and
             consumers to concurrently access empty and full buffers, respectively. However,
                           begin
                           Parbegin
                           var produced : boolean;              var consumed : boolean;
                           repeat                               repeat
                                  produced := false             consumed := false;
                                  while produced = false        while consumed = false
                                     if an empty buffer exists            if a full buffer exists
                                     then                                 then
                                       { Produce in a buffer }            { Consume a buffer       }
                                       produced := true;                  consumed := true;
                                  { Remainder of the cycle }    { Remainder of the cycle }
                           forever;                             forever;
                           Parend;
                           end.
                                     Producer                             Consumer
             Figure  6.14  An outline for producers­consumers using critical sections.



                                                                     Chapter 6   Process Synchronization  185
both produce and consume actions take place in critical sections for the entire
buffer pool, and so only one process, whether producer or consumer, can access
a buffer at any time.
Busy waits exist in both producers and consumers. A producer repeatedly
checks for an empty buffer and a consumer repeatedly checks for a full buffer. To
avoid busy waits, a producer process should be blocked if an empty buffer is not
available. When a consumer consumes from a buffer, it should activate a producer
that is waiting for an empty buffer. Similarly, a consumer should be blocked if
a full buffer is not available. A producer should activate such a consumer after
producing in a buffer.
When we reanalyze the producers­consumers problem in this light, we notice
that though it involves mutual exclusion between a producer and a consumer
that use the same buffer, it is really a signaling problem. After producing an
item of information in a buffer, a producer should signal a consumer that wishes
to consume the item from that buffer. Similarly, after consuming an item in a
buffer, a consumer should signal a producer that wishes to produce an item of
information in that buffer. These requirements can be met by using the signaling
arrangement discussed in Section 6.4.
An improved outline using this approach is shown in Figure 6.15 for a simple
producers­consumers system that consists of a single producer, a single consumer,
and a single buffer. The operation check_b_empty performed by the producer
blocks it if the buffer is full, while the operation post_b_ full sets buffer_ full to
true and activates the consumer if the consumer is blocked for the buffer to become
full. Analogous operations check_b_ full and post_b_empty are defined for use by
the consumer process. The boolean flags producer_blocked and consumer_blocked
are used by these operations to note whether the producer or consumer process
             var
                      buffer : . . . ;
                      buffer_ full : boolean;
                      producer_blocked, consumer_blocked : boolean;
             begin
                      buffer_ full := false;
                      producer_blocked := false;
                      consumer_blocked := false;
             Parbegin
             repeat                               repeat
                    check_b_empty;                check_b_ full;
                    {Produce in the buffer}       {Consume from the buffer}
                    post_b_ full;                 post_b_empty;
                    {Remainder of the cycle}      {Remainder of the cycle}
             forever;                             forever;
             Parend;
             end.
                        Producer                            Consumer
Figure 6.15  An improved outline for a single buffer producers­consumers system  using
signaling.



186  Part 2  Process Management
                     procedure check_b_empty                 procedure check_b_full
                     begin                                   begin
                           if buffer_full = true                   if buffer_full = false
                           then                                    then
                                 producer_blocked := true;          consumer_blocked := true;
                                 block (producer);                  block (consumer);
                     end;                                    end;
                     procedure post_b_full                   procedure post_b_empty
                     begin                                   begin
                           buffer_full := true;                    buffer_full := false;
                           if consumer_blocked = true              if producer_blocked = true
                           then                                    then
                                 consumer_blocked := false;         producer_blocked := false;
                                 activate (consumer);               activate (producer);
                     end;                                    end;
                            Operations of producer                 Operations of consumer
             Figure  6.16  Indivisible operations for the producers­consumers problem.
             is blocked at any moment. Figure 6.16 shows details of the indivisible opera-
             tions. This outline will need to be extended to handle multiple buffers or multiple
             producer/consumer processes. We discuss this aspect in Section 6.9.2.
             6.7.2 Readers and Writers
             A readers­writers system consists of shared data, an unspecified number of reader
             processes that only read the data, and an unspecified number of writer processes
             that modify or update the data. We use the terms reading and writing for accesses
             to the shared data made by reader and writer processes, respectively. A solution
             to the readers­writers problem must satisfy the following conditions:
             1.  Many readers can perform reading concurrently.
             2.  Reading is prohibited while a writer is writing.
             3.  Only one writer can perform writing at any time.
             Conditions 1­3 do not specify which process should be preferred if a reader and
             a writer process wish to access the shared data at the same time. The following
             additional condition is imposed if it is important to give a higher priority to
             readers in order to meet some business goals:
             4.  A reader has a nonpreemptive priority over writers; i.e., it gets access to
                 the shared data ahead of a waiting writer, but it does not preempt an active
                 writer.
             This system is called a readers preferred readers­writers system. A writers preferred
             readers­writers system is analogously defined.



                                                                         Chapter 6  Process Synchronization  187
                                       Bank account
                   print                                         credit
                   statement
                   stat                                          debit
                   analysis
                              Readers                   Writers
Figure  6.17  Readers and writers in a banking system.
Figure 6.17 illustrates an example of a readers­writers system. The readers
and writers share a bank account. The reader processes print statement and stat
analysis merely read the data from the bank account; hence they can execute
concurrently. credit and debit modify the balance in the account. Clearly only
one of them should be active at any moment and none of the readers should
be concurrent with it. In an airline reservation system, processes that merely
query the availability of seats on a flight are reader processes, while processes that
make reservations are writer processes since they modify parts of the reservation
database.
We determine the synchronization requirements of a readers­writers system
as follows: Conditions 1­3 permit either one writer to perform writing or many
readers to perform concurrent reading. Hence writing should be performed in
a critical section for the shared data. When a writer finishes writing, it should
either enable another writer to enter its critical section, or activate all waiting
readers using a signaling arrangement and a count of waiting readers. If readers
are reading, a waiting writer should be enabled to perform writing when the last
reader finishes reading. This action would require a count of concurrent readers
to be maintained.
Figure 6.18 is an outline for a readers­writers system. Writing is performed
in a critical section. A critical section is not used in a reader, because that would
prevent concurrency between readers. A signaling arrangement is used to handle
blocking and activation of readers and writers. For simplicity, details of main-
taining and using counts of waiting readers and readers reading concurrently are
not shown in the outline; we shall discuss these in Section 6.9.3. The outline of
Figure 6.18 does not provide bounded waits for readers and writers; however, it
provides maximum concurrency. This outline does not prefer either readers or
writers.
6.7.3 Dining Philosophers
Five philosophers sit around a table pondering philosophical issues. A plate of
spaghetti is kept in front of each philosopher, and a fork is placed between each
pair of philosophers (see Figure 6.19). To eat, a philosopher must pick up the
two forks placed between him and the neighbors on either side, one at a time.
The problem is to design processes to represent the philosophers such that each
philosopher can eat when hungry and none dies of hunger.



188  Part 2  Process Management
                     Parbegin
                           repeat                                        repeat
                                 If a writer is writing                  If reader(s) are reading, or a
                                 then                                            writer is writing
                                   { wait };                             then
                                 { read }                                        { wait };
                                 If no other readers reading             { write }
                                 then                                    If reader(s) or writer(s) waiting
                                   if writer(s) waiting                  then
                                   then                                          activate either one waiting
                                           activate one waiting writer;            writer or all waiting readers;
                           forever;                                      forever;
                     Parend;
                     end.
                                     Reader(s)                                     Writer(s)
             Figure  6.18  An outline for a readers­writers system.
                                                              P
                                                P                                  P
                                                         P               P
             Figure  6.19  Dining philosophers.
             The correctness condition in the dining philosophers system is that a hungry
             philosopher should not face indefinite waits when he decides to eat. The challenge
             is to design a solution that does not suffer from either deadlocks, where processes
             become blocked waiting for each other (see Section 1.4.2), or livelocks, where pro-
             cesses are not blocked but defer to each other indefinitely. Consider the outline
             of a philosopher process Pi shown in Figure 6.20, where details of process syn-
             chronization have been omitted. A philosopher picks up the forks one at a time,
             say, first the left fork and then the right fork. This solution is prone to deadlock,
             because if all philosophers simultaneously lift their left forks, none will be able
             to lift the right fork! It also contains race conditions because neighbors might
             fight over a shared fork. We can avoid deadlocks by modifying the philosopher
             process so that if the right fork is not available, the philosopher would defer to
             his left neighbor by putting down the left fork and repeating the attempt to take
             the forks sometime later. However, this approach suffers from livelocks because
             the same situation may recur.



                                                                                   Chapter 6      Process Synchronization  189
                             repeat
                                  if left fork is not available
                                  then
                                      block (Pi);
                                  lift left fork;
                                  if right fork is not available
                                  then
                                      block (Pi);
                                  lift right fork;
                                  { eat }
                                  put down both forks
                                  if left neighbor is waiting for his right  fork
                                  then
                                      activate (left neighbor);
                                  if right neighbor is waiting for his left  fork
                                  then
                                      activate (right neighbor);
                                  { think }
                             forever
Figure 6.20  Outline  of  a  philosopher process Pi.
                             var        successful : boolean;
                             repeat
                                  successful := false;
                                  while (not successful)
                                     if both forks are available then
                                        lift the forks one at a time;
                                        successful := true;
                                      if successful = false
                                      then
                                  { eat }  block (Pi);
                                  put down both forks;
                                  if left neighbor is waiting for his right  fork
                                  then
                                      activate (left neighbor);
                                  if right neighbor is waiting for his left  fork
                                  then
                                      activate (right neighbor);
                                  { think }
                             forever
Figure 6.21  An  improved outline of a philosopher process.
An  improved              outline     for    the    dining       philosophers      problem    is  given  in
Figure 6.21. A philosopher checks availability of forks in a CS and also picks
up the forks in the CS. Hence race conditions cannot arise. This arrangement
ensures that at least some philosopher(s) can eat at any time and deadlocks can-
not arise. A philosopher who cannot get both forks at the same time blocks



190  Part 2  Process Management
             himself. He gets activated when any of his neighbors puts down a shared fork,
             hence he has to check for availability of forks once again. This is the purpose
             of the while loop. However, the loop also causes a busy wait condition. Some
             innovative solutions to the dining philosophers problem prevent deadlocks with-
             out busy waits (see Exercise 6.14). Deadlock prevention is discussed in detail in
             Chapter 8.
             6.8    ALGORITHMIC APPROACH TO IMPLEMENTING
                    CRITICAL SECTIONS                                                            ·
             The algorithmic approach to implementing critical sections did not employ either
             the process blocking and activation services of the kernel to delay a process, or
             indivisible instructions in a computer to avoid race conditions. Consequently,
             process synchronization implemented through this approach was independent of
             both the OS and the computer. However, these features required the approach to
             use a busy wait to delay a process at a synchronization point (see Section 6.5.1),
             and use a complex arrangement of logical conditions to ensure absence of race
             conditions, which complicated proofs of correctness. The algorithmic approach
             was not widely used in practice due to these weaknesses.
                   This section describes the algorithmic approach to implementing critical
             sections which, as we saw in Section 6.5.2, can be used for both data access
             synchronization and control synchronization. This study provides an insight into
             how to ensure mutual exclusion while avoiding both deadlocks and livelocks.
             We begin by discussing critical section implementation schemes for use by two
             processes. Later we see how to extend some of these schemes for use by more than
             two processes.
             6.8.1 Two-Process Algorithms
             Algorithm 6.1 First Attempt
             var      turn : 1 .. 2;
             begin
                      turn := 1;
             Parbegin
                  repeat                               repeat
                      while turn = 2                   while turn = 1
                            do { nothing };                      do { nothing };
                      { Critical Section }             { Critical Section }
                      turn := 2;                       turn := 1;
                      { Remainder of the     cycle  }  { Remainder of the cycle }
                  forever;                             forever;
             Parend;
             end.
                            Process P1                           Process P2



                                                                     Chapter 6  Process Synchronization  191
      The variable turn is a shared variable. The notation 1 .. 2 in its declaration
indicates that it takes values in the range 1­2; i.e., its value is either 1 or 2. It is
initialized to 1 before processes P1 and P2 are created. Each process contains a
critical section for some shared data ds. The shared variable turn indicates which
process can enter its critical section next. Suppose process P1 wishes to enter
its critical section. If turn = 1, P1 can enter right away. After exiting its critical
section, P1 sets turn to 2 so that P2 can enter its critical section. If P1 finds turn
= 2 when it wishes to enter its critical section, it waits in the while turn = 2 do
{ nothing } loop until P2 exits from its critical section and executes the assignment
turn := 1. Thus the correctness condition is satisfied.
      Algorithm 6.1 violates the progress condition of critical section implemen-
tation described in Table 6.1 because of the way it uses shared variable turn. Let
process P1 be in its critical section and process P2 be in the remainder of the
cycle. When P1 exits from its critical section, it would set turn to 2. If it finishes
the remainder of its cycle and wishes to enter its critical section once again, it will
encounter a busy wait until after P2 uses its critical section and sets turn to 1.
Thus, P1 is not granted entry to its critical section even though no other process
is interested in using its critical section. Algorithm 6.2 is an attempt to eliminate
this problem.
Algorithm 6.2 Second Attempt
var      c1, c2 : 0 .. 1;
begin
         c1 := 1;
         c2 := 1;
Parbegin
     repeat                               repeat
         while c2 = 0                     while c1 = 0
               do { nothing };                      do { nothing };
         c1 := 0;                         c2 := 0;
         { Critical Section }             { Critical Section }
         c1 := 1;                         c2 := 1;
         { Remainder of the     cycle  }  { Remainder of the cycle }
     forever;                             forever;
Parend;
end.
               Process P1                           Process P2
      The algorithm uses two shared variables c1 and c2, whose values are restricted
to either a 0 or a 1. These variables can be looked upon as status flags for processes
P1 and P2, respectively. P1 sets c1 to 0 while entering its critical section, and sets
it back to 1 after exiting from its critical section. Thus c1 = 0 indicates that P1
is in its critical section and c1 = 1 indicates that it is not in its critical section.
Similarly, the value of c2 indicates whether P2 is in its critical section. Before
entering its critical section, each process checks whether the other process is in
its critical section. If not, it enters its own critical section right away; otherwise,
it loops until the other process exits its critical section, and then enters its own



192  Part 2  Process Management
             critical section. The progress violation of Algorithm 6.1 is eliminated because
             processes are not forced to take turns using their critical sections.
                  Algorithm 6.2 violates the mutual exclusion condition when both processes
             try to enter their critical sections at the same time. Both c1 and c2 will be 1
             (since none of the processes is in its critical section), and so both processes will
             enter their critical sections. To avoid this problem, the statements "while c2 = 0 do
             { nothing };" and "c1 := 0;" in process P1 could be interchanged and the statements
             "while c1 = 0 do { nothing };" and "c2 := 0;" could be interchanged in process
             P2. This way c1 will be set to 0 before P1 checks the value of c2, and hence both
             processes will not be able to be in their critical sections at the same time. However,
             if both processes try to enter their critical sections at the same time, both c1 and
             c2 will be 0, and so both processes will wait for each other indefinitely. This is a
             deadlock situation (see Section 1.4.2).
                  Both--the correctness violation and the deadlock possibility--can be elimi-
             nated if a process defers to the other process when it finds that the other process
             also wishes to enter its critical section. This can be achieved as follows: if P1
             finds that P2 is also trying to enter its critical section, it can set c1 to 0. This
             will permit P2 to enter its critical section. P1 can wait for some time and make
             another attempt to enter its critical section after setting c1 to 1. Similarly, P2 can
             set c2 to 0 if it finds that P1 is also trying to enter its critical section. However,
             this approach may lead to a situation in which both processes defer to each other
             indefinitely. This is a livelock situation we discussed earlier in the context of dining
             philosophers (see Section 6.7.3).
             Dekker's     Algorithm   Dekker's     algorithm  combines        the  useful  features    of
             Algorithms 6.1 and 6.2 to avoid a livelock situation. If both processes try to
             enter their critical sections at the same time, turn indicates which of the processes
             should be allowed to enter. It has no effect at other times.
             Algorithm 6.3 Dekker's Algorithm
             var    turn : 1 .. 2;
                    c1, c2 : 0 .. 1;
             begin
                    c1 := 1;
                    c2 := 1;
                    turn := 1;
             Parbegin
                  repeat                           repeat
                    c1 := 0;                          c2 := 0;
                    while c2 = 0 do                   while c1 = 0 do
                          if turn = 2 then                 if turn = 1 then
                          begin                            begin
                          c1 := 1;                            c2 := 1;
                          while turn = 2                      while turn = 1
                                 do { nothing  };                 do { nothing };
                          c1 := 0;                            c2 := 0;
                          end;                             end;



                                                                        Chapter 6  Process Synchronization  193
         { Critical Section }               { Critical Section }
         turn := 2;                         turn := 1;
         c1 := 1;                           c2 := 1;
         { Remainder of the cycle }         { Remainder of the cycle }
     forever;                               forever;
Parend;
end.
               Process P1                             Process P2
      Variables c1 and c2 are used as status flags of the processes as in Algorithm 6.2.
The statement while c2 = 0 do in P1 checks if it is safe for P1 to enter its critical
section. To avoid the correctness problem of Algorithm 6.2, the statement c1 :=0
in P1 precedes the while statement. If c2 = 1 when P1 wishes to enter a critical
section, P1 skips the while loop and enters its critical section right away. If both
processes try to enter their critical sections at the same time, the value of turn
will force one of them to defer to the other. For example, if P1 finds c2 = 0, it
defers to P2 only if turn = 2; otherwise, it simply waits for c2 to become 1 before
entering its critical section. Process P2, which is also trying to enter its critical
section at the same time, is forced to defer to P1 only if turn = 1. In this manner
the algorithm satisfies mutual exclusion and also avoids deadlock and livelock
conditions. The actual value of turn at any time is immaterial to correctness of the
algorithm.
Peterson's Algorithm           Peterson's algorithm is simpler than Dekker's algorithm.
It uses a boolean array flag that contains one flag for each process; these flags
are equivalent to the status variables c1, c2 of Dekker's algorithm. A process sets
its flag to true when it wishes to enter a critical section and sets it back to false
when it exits from the critical section. Processes are assumed to have the ids P0
and P1. A process id is used as a subscript to access the status flag of a process in
the array flag. The variable turn is used for avoiding livelocks; however, it is used
differently than in Dekker's algorithm.
Algorithm 6.4        Peterson's Algorithm
var      flag : array [0 .. 1] of boolean;
         turn : 0 .. 1;
begin
         flag[0] := false;
         flag[1] := false;
Parbegin
     repeat                                 repeat
         flag[0] := true;                   flag[1] := true;
         turn := 1;                         turn := 0;
         while flag[1] and turn = 1         while flag[0] and turn = 0
               do {nothing};                          do {nothing};
         { Critical Section }               { Critical Section }
         flag[0] :=false;                   flag[1] :=false;
         { Remainder of the cycle }         { Remainder of the cycle }



194  Part 2  Process Management
                  forever;               forever;
             Parend;
             end.
                            Process P0             Process P1
                   A process wishing to enter a critical section begins by deferring to another
             process by setting turn to point to the other process. However, it goes ahead and
             enters its critical section if it finds that the other process is not interested in using
             its own critical section. If both processes try to enter their critical sections at the
             same time, the value of turn decides which process may enter. As an example,
             consider process P0. It sets flag[0] to true and turn to 1 when it wishes to enter its
             critical section. If process P2 is not interested in using its critical section, flag[1]
             will be false, and so P0 will come out of the while loop to enter its critical section
             right away. If P1 is also interested in entering its critical section, flag[1] will be
             true. In that case, the value of turn decides which process may enter its critical
             section.
                   It is interesting to consider operation of Peterson's algorithm for different
             relative speeds of P0 and P1. Consider the situation when both P0 and P1 wish to
             use their critical sections and P0 is slightly ahead of P1. If both processes execute
             at the same speed, P0 will enter its critical section ahead of P1 because P1 will
             have changed turn to 0 by the time P1 reaches the while statement. P1 now waits
             in the while loop until P0 exits from its critical section. If, however, P0 is slower
             than P1, it will set turn to 1 sometime after P1 sets it to 0. Hence P0 will wait in
             the while loop and P1 will enter its critical section.
             6.8.2 n-Process Algorithms
             In an algorithmic implementation of a critical section, the algorithm has to know
             the number of processes that use a critical section for the same data item. This
             awareness is reflected in many features of its code--the size of the array of status
             flags, the checks to determine whether any other process wishes to enter a critical
             section, and the arrangement for one process to defer to another. Each of these
             features has to change if the number of processes to be handled by the critical
             section implementation changes. For example, in a two-process critical section
             implementation, any process needs to check the status of only one other process,
             and possibly defer to it, to ensure correctness and absence of deadlocks and
             livelocks. In an n-process critical section implementation, a process must check
             the status of n - 1 other processes, and do it in a manner that prevents race
             conditions. It makes an n-process algorithm more complex. We see this in the
             context of the algorithm by Eisenberg and McGuire [1972], which extends the
             two-process solution of Dekker's algorithm to n processes.
             Algorithm 6.5 An n-Process Algorithm (Eisenberg and McGuire [1972])
             const     n = . . .;
             var       flag : array [0 .. n - 1] of (idle, want_in, in_CS);



                                                                Chapter 6  Process Synchronization  195
         turn : 0 .. n - 1;
begin
         for j := 0 to n - 1 do
          flag[j] := idle;
Parbegin
process Pi :
         repeat
          repeat
                  flag[i] := want_in;
                  j := turn;
                  while j = i
                    do if flag[j] = idle
                             then j := turn { Loop here! }
                             else j := j + 1 mod n;
                  flag[i] := in_CS;
                  j := 0;
                  while (j < n) and (j = i or flag[j] = in_CS)
                    do j := j + 1;
          until (j  n) and (turn = i or flag[turn] = idle);
          turn := i;
          { Critical Section }
          j := turn +1 mod n;
          while (flag[j] = idle) do j := j + 1 mod n;
          turn := j;
          flag[i] := idle;
          { Remainder of the cycle }
         forever
process Pk : . . .
Parend;
end.
      The variable turn indicates which process may enter its critical section next.
Its initial value is immaterial to correctness of the algorithm. Each process has
a 3-way status flag that takes the values idle, want_in and in_CS. It is initialized
to the value idle. A process sets its flag to want_in whenever it wishes to enter a
critical section. It now has to decide whether it may change the flag to in_CS. To
make this decision, it checks the flags of other processes in an order that we call
the modulo n order. The modulo n order is Pturn, Pturn+1, . . ., Pn-1, P0, P1, . . . ,
Pturn-1. In the first while loop, the process checks whether any process ahead of
it in the modulo n order wishes to use its own critical section. If not, it turns its
flag to in_CS.
      Since processes make this check concurrently, more than one process may
simultaneously reach the same conclusion. Hence another check is made to ensure
correctness. The second while loop checks whether any other process has turned
its flag to in_CS. If so, the process changes its flag back to want_in and repeats all
the checks. All other processes that had changed their flags to in_CS also change
their flags back to want_in and repeat the checks. These processes will not tie for



196  Part 2  Process Management
             entry to a critical section again because they have all turned their flags to want_in,
             and so only one of them will be able to get past the first while loop. This feature
             avoids the livelock condition. The process earlier in the modulo n order from
             Pturn will get in and enter its critical section ahead of other processes. It changes
             its flag to idle when it leaves its critical section. Thus the flag has the value idle
             whenever a process is in the remainder of its cycle.
                   This solution contains a certain form of unfairness since processes do not
             enter their critical sections in the same order in which they requested entry to a
             critical section. This unfairness is eliminated in the Bakery algorithm by Lamport
             [1974].
             Bakery Algorithm         When a process wishes to enter a critical section, it chooses
             a number that is larger than any number chosen by any process earlier. choosing
             is an array of boolean flags. choosing[i] is used to indicate whether process Pi is
             currently engaged in choosing a number. number[i] contains the number chosen
             by process Pi. number[i] = 0 if Pi has not chosen a number since the last time
             it entered the critical section. The basic idea of the algorithm is that processes
             should enter their critical sections in the order of increasing numbers chosen by
             them. We discuss the operation of the algorithm in the following.
             Algorithm 6.6       Bakery Algorithm (Lamport [1974])
             const    n= ...;
             var      choosing : array [0 .. n - 1] of boolean;
                      number : array [0 .. n - 1] of integer;
             begin
                      for j := 0 to n - 1 do
                       choosing[j] := false;
                       number[j] := 0;
             Parbegin
                  process Pi :
                      repeat
                       choosing[i] := true;
                       number[i] := max (number[0], .. ,number[n - 1])+1;
                       choosing[i] := false;
                       for j := 0 to n - 1 do
                       begin
                                 while choosing[j] do { nothing };
                                 while number[j] = 0 and (number[j], j) < (number[i],i)
                                      do { nothing };
                       end;
                       { Critical Section }
                       number[i] := 0;
                       { Remainder of the cycle }
                      forever;
                  process Pj : . . .
             Parend;
             end.



                                                           Chapter 6          Process Synchronization  197
     A process wishing to enter a critical section defers to a process with a smaller
number. However, a tie-breaking rule is needed because processes that choose
their numbers concurrently may obtain the same number. The algorithm uses the
pair (number[i], i) for this purpose--a process enters a critical section if its pair
precedes every other pair, where the precedes relation < is defined as follows:
     (number[j], j) < (number[i], i) if
                number[j] < number[i], or
                number[j] = number[i] and j < i.
Thus, if many processes obtain the same number, the process with the smallest
process id enters its critical section first. In all other cases, processes enter critical
sections in the order in which they raise their requests for entry to a critical
section.
6.9  SEMAPHORES                                                                                        ·
As mentioned in Section 6.5.3, synchronization primitives were developed to over-
come the limitations of algorithmic implementations. The primitives are simple
operations that can be used to implement both mutual exclusion and control
synchronization. A semaphore is a special kind of synchronization data that can
be used only through specific synchronization primitives.
Definition 6.5 Semaphore        A shared integer variable with nonnegative values
that can be subjected only to the following operations:
1. Initialization (specified as part of its declaration)
2. The indivisible operations wait and signal
     The wait and signal operations on a semaphore were originally called the
P and V operations, respectively, by Dijkstra. Their semantics are shown in
Figure 6.22. When a process performs a wait operation on a semaphore, the
operation checks whether the value of the semaphore is > 0. If so, it decrements
the value of the semaphore and lets the process continue its execution; other-
wise, it blocks the process on the semaphore. A signal operation on a semaphore
activates a process blocked on the semaphore, if any, or increments the value of
the semaphore by 1. Due to these semantics, semaphores are also called counting
semaphores. Indivisibility of the wait and signal operations is ensured by the pro-
gramming language or the operating system that implements it. It ensures that
race conditions cannot arise over a semaphore (see Section 6.9.4).
     Processes  use  wait  and  signal   operations  to   synchronize  their  execution
with respect to one another. The initial value of a semaphore determines how
many processes can get past the wait operation. A process that does not get
past a wait operation is blocked on the semaphore. This feature avoids busy
waits. Section 6.9.1 describes uses of semaphores. Sections 6.9.2 and 6.9.3 discuss
implementation of the producers­consumers and readers­writers problems using
semaphores.



198  Part 2  Process Management
                                       procedure wait (S)
                                       begin
                                       if S > 0
                                              then S := S­1;
                                              else block the process on S;
                                       end;
                                       procedure signal (S)
                                       begin
                                       if some processes are blocked on S
                                              then activate one blocked process;
                                              else S := S+1;
                                       end;
             Figure  6.22  Semantics of the wait and signal operations on a semaphore.
             Table 6.3           Uses of Semaphores in Implementing Concurrent             Systems
             Use                 Description
             Mutual              Mutual exclusion can be implemented by using a semaphore
             exclusion           that is initialized to 1. A process performs a wait operation on
                                 the semaphore before entering a CS and a signal operation on
                                 exiting from it. A special kind of semaphore called a binary
                                 semaphore further simplifies CS implementation.
             Bounded             Bounded concurrency implies that a function may be executed,
             concurrency         or a resource may be accessed, by n processes concurrently,
                                 1  n  c, where c is a constant. A semaphore initialized to c
                                 can be used to implement bounded concurrency.
             Signaling           Signaling is used when a process Pi wishes to perform an
                                 operation ai only after process Pj has performed an operation
                                 aj . It is implemented by using a semaphore initialized to 0. Pi
                                 performs a wait on the semaphore before performing operation
                                 ai. Pj performs a signal on the semaphore after it performs
                                 operation aj .
             6.9.1 Uses of Semaphores in Concurrent Systems
             Table 6.3 summarizes three uses of semaphores in implementing concurrent
             systems. Mutual exclusion is useful in implementing critical sections. Bounded
             concurrency is important when a resource can be shared by up to c processes,
             where c is a constant  1. Signaling is useful in control synchronization. We
             discuss details of these uses in this section.
             6.9.1.1 Mutual Exclusion
             Figure 6.23 shows implementation of a critical section in processes Pi and Pj
             by using a semaphore named sem_CS. sem_CS is initialized to 1. Each pro-
             cess performs a wait operation on sem_CS before entering its critical section,
             and a signal operation after exiting from its critical section. The first process to
             perform wait(sem_CS) finds that sem_CS is > 0. Hence it decrements sem_CS



                                                                       Chapter   6  Process Synchronization       199
             var sem_CS : semaphore := 1;
             Parbegin
             repeat                              repeat
                   wait (sem_CS);                wait (sem_CS);
                   { Critical Section }          { Critical Section }
                   signal (sem_CS);              signal (sem_CS);
                   { Remainder of the cycle }    { Remainder of the    cycle  }
             forever;                            forever;
             Parend;
             end.
                       Process Pi                          Process Pj
Figure 6.23  CS implementation with semaphores.
by 1 and goes on to enter its critical section. When the second process performs
wait(sem_CS), it is blocked on sem_CS because its value is 0. It is activated
when the first process performs signal(sem_CS) after exiting from its own critical
section; the second process then enters its critical section. If no process is blocked
on sem_CS when a signal(sem_CS) operation is performed, the value of sem_CS
becomes 1. This value of sem_CS permits a process that is performing a wait
operation at some later time to immediately enter its critical section. More pro-
cesses using similar code can be added to the system without causing correctness
problems. The next example illustrates operation of this system using snapshots.
                                                                                                                  ·
Critical Sections through Semaphores                                                       Example           6.4
Figure 6.24 shows snapshots taken during operation of the system shown
in Figure 6.23. The wait and signal operations on sem_CS are enclosed in
a dashed rectangular box because they are mutually exclusive (refer to the
pictorial conventions of Figure 6.11). Let process Pi perform wait(sem_CS).
Figure 6.24(a) illustrates the situation at the start of Pi's wait operation.
Figure 6.24(b) shows the situation after Pi completes the wait operation and Pj
executes a wait operation--Pi's wait(sem_CS) operation has reduced the value
of sem_CS to 0, so Pj becomes blocked on the wait operation. Figure 6.24(c)
shows the situation after process Pi performs a signal operation. The value of
sem_CS remains 0, but process Pj has been activated. Process Pj performs a
signal operation on exiting from its critical section. Since no process is cur-
rently blocked on sem_CS, Pj's signal operation simply results in increasing
the value of sem_CS by 1 (see Figure 6.24(d)).
                                                                                        ·
It is interesting to check which properties of critical section implementa-
tions mentioned in Table 6.1 are satisfied by the implementation of Figure 6.23.
Mutual exclusion follows from the fact that sem_CS is initialized to 1. The imple-
mentation possesses the progress property because a process performing the wait
operation gets to enter its critical section if no other process is in its critical



200  Part 2  Process Management
                                          wait    sem_CS                             wait  sem_CS
                                 Pi                                  Pi
                                                  1               ·  ··                    0
                                 Pj       signal                          Pj  signal
                                 (a)                                 (b)
                                          wait    sem_CS                             wait  sem_CS
                                 Pi                                  Pi
                                                  0                                        1
                                 Pj       signal                     Pj       signal
                                 (c)                                 (d)
             Figure 6.24  Snapshots   of  the concurrent system of Figure     6.23.
             section. However, the bounded wait property does not hold because the order in
             which blocked processes are activated by signal operations is not defined in the
             semantics of semaphores. Hence a blocked process may starve if other processes
             perform wait and signal operations repeatedly.
             Correctness problems can arise because the wait and signal operations are
             primitives, and so a program can use them in a haphazard manner. For example,
             process Pi of Figure 6.23 could have been erroneously written as
                                      repeat
                                               signal (sem_CS );
                                               { Critical Section }
                                               signal (sem_CS );
                                               { Remainder of the cycle }
                                      forever
             where a signal(sem_CS) has been used instead of a wait(sem_CS) at Pi's entry to
             its critical section. Now the critical section would not be implemented correctly
             because many processes would be able to enter their critical sections at the same
             time. As another example, consider what would happen if the code of process
             Pi erroneously uses a wait(sem_CS) operation in place of the signal(sem_CS)
             operation following its critical section. When Pi executes its critical section, it will
             be blocked on the wait operation after exiting from its critical section because the
             value of sem_CS will be 0. Other processes wishing to enter the critical section
             will be blocked on the wait operation preceding their critical sections. Since no
             process performs a signal operation on sem_CS, all these processes will remain
             blocked indefinitely, which is a deadlock situation.
             Binary Semaphores        A binary semaphore is a special kind of semaphore used
             for implementing mutual exclusion. Hence it is often called a mutex. A binary
             semaphore is initialized to 1 and takes only the values 0 and 1 during execution
             of a program. The wait and signal operations on a binary semaphore are slightly
             different from those shown in Figure 6.22; the statement S := S-1 in the wait
             operation is replaced by the statement S := 0 and the statement S := S+1 in the
             signal operation is replaced by the statement S := 1.



                                                          Chapter 6           Process Synchronization  201
             var sync : semaphore := 0;
             Parbegin
                   ···                    ···
                   wait (sync);           { Performaction aj }
                   { Performaction ai }   signal (sync);
             Parend;
             end.
                        Process Pi             Process Pj
Figure 6.25  Signaling using semaphores.
6.9.1.2 Bounded Concurrency
We use the term bounded concurrency for the situation in which up to c processes
can concurrently perform an operation opi, where c is a constant  1. Bounded
concurrency is implemented by initializing a semaphore sem_c to c. Every pro-
cess wishing to perform opi performs a wait(sem_c) before performing opi and
a signal(sem_c) after performing it. From the semantics of the wait and signal
operations, it is clear that up to c processes can concurrently perform opi.
6.9.1.3 Signaling between Processes
Consider the synchronization requirements of processes Pi and Pj shown in
Figure 6.6--process Pi should perform an operation ai only after process Pj per-
forms an operation aj. A semaphore can be used to achieve this synchronization
as shown in Figure 6.25. Here process Pi performs a wait(sync) before executing
operation ai and Pj performs a signal(sync) after executing operation aj. The
semaphore sync is initialized to 0, and so Pi will be blocked on wait(sync) if Pj
has not already performed a signal(sync). It will proceed to perform operation
ai only after process Pj performs a signal. Unlike the solution of Figure 6.6, race
conditions cannot arise because the wait and signal operations are indivisible.
The signaling arrangement can be used repetitively, as the wait operation makes
the value of sync 0 once again.
6.9.2 Producers--Consumers Using Semaphores
As discussed in Section 6.7.1, the producers­consumers problem is a signaling
problem. After producing an item of information in a buffer, a producer signals
to a consumer that is waiting to consume from the same buffer. Analogously, a
consumer signals to a waiting producer. Hence we should implement producers­
consumers using the signaling arrangement shown in Figure 6.25.
For simplicity, we first discuss the solution for the single buffer case shown
in Figure 6.26. The buffer pool is represented by an array of buffers with a single
element in it. Two semaphores full and empty are declared. They are used to
indicate the number of full and empty buffers, respectively. A producer performs
a wait(empty) before starting the produce operation and a consumer performs a
wait(full) before a consume operation.



202  Part 2  Process Management
                                  type   item = . . .;
                                  var
                                           full : Semaphore := 0; { Initializations }
                                         empty : Semaphore := 1;
                                         buffer : array [0] of item;
                                  begin
                                  Parbegin
                                  repeat                              repeat
                                         wait (empty);                wait (full);
                                         buffer [0] := . . .;         x := buffer [0];
                                            { i.e., produce }                 { i.e., consume }
                                         signal (full);               signal (empty);
                                         { Remainder of the cycle }   { Remainder of the cycle }
                                  forever;                            forever;
                                  Parend;
                                  end.
                                            Producer                            Consumer
                  Figure  6.26  Producers­consumers with a single buffer.
                     Initially the semaphore full has the value 0. Hence consumer(s) will be
                  blocked on wait(full). empty has the value 1, and so one producer will get
                  past the wait(empty) operation. After completing the produce operation it per-
                  forms signal(full). This enables one consumer to enter, either immediately or
                  later. When the consumer finishes a consume operation, it performs a sig-
                  nal(empty) that enables a producer to perform a produce operation. This solution
                  avoids busy waits since semaphores are used to check for empty or full buffers,
                  and so a process will be blocked if it cannot find an empty or full buffer as
                  required. The total concurrency in this system is 1; sometimes a producer exe-
                  cutes and sometimes a consumer executes. Example 6.5 describes the operation of
                  this solution.
·
     Example 6.5  Producers-Consumers with a Single Buffer
                  through Semaphores
                  The snapshot of Figure 6.27(a) shows the initial situation in the producers­
                  consumers system of Figure 6.26. Figure 6.27(b) shows the situation when the
                  producer and consumer processes attempt to produce and consume, respec-
                  tively. The producer process has got past its wait operation on empty since
                  empty was initialized to 1. The value of semaphore empty becomes 0 and the
                  producer starts producing in the buffer. The consumer process is blocked on
                  the wait (full) operation because full is 0. When the producer performs a sig-
                  nal(full) after the produce operation, the consumer process is activated and
                  starts consuming from the buffer. Figure 6.27(c) shows this situation.
                  ·
                     Figure 6.28 shows how semaphores can be used to implement a solution of the
                  n-buffer producers­consumers problem, n  1, containing one producer and one



                                                                                           Chapter 6      Process  Synchronization  203
        buffer           Producer           buffer             Producer            buffer           Producer
              Produce                                 Produce                              Produce
              Consume    Consumer                     Consume                           Consume     Consumer
        full    wait                        full      wait                                 wait
                                                                         ···
        0                                   0                                      0
                                                               Consumer
                signal                                signal                               signal
        empty   wait                        empty     wait                         empty   wait
        1                                   0                                      0
                signal                                signal                               signal
        (a)                                 (b)                                    (c)
Figure 6.27 Snapshots of single         buffer producers­consumers            using semaphores.
                const     n = . . .;
                type      item = . . .;
                var
                          buffer : array [0..n ­ 1] of item;
                          full : Semaphore := 0; { Initializations }
                          empty : Semaphore := n;
                          prod_ ptr, cons_ ptr : integer;
                begin
                          prod_ ptr := 0;
                          cons_ ptr := 0;
                Parbegin
                repeat                                         repeat
                        wait (empty);                          wait (full);
                        buffer [prod_ ptr] := . . .;           x := buffer [cons_ ptr];
                          { i.e. produce }                               { i.e. consume }
                        prod_ ptr := prod_ ptr + 1 mod n;      cons_ ptr := cons_ ptr + 1 mod         n;
                        signal (full);                         signal (empty);
                        { Remainder of the cycle }             { Remainder of the cycle }
                forever;                                       forever;
                Parend;
                end.
                          Producer                                       Consumer
Figure  6.28    Bounded buffers using semaphores.
consumer process. This solution is a simple extension of the single-buffer solution
shown in Figure 6.26. The values of the semaphores empty and full indicate the
number of empty and full buffers, respectively, hence they are initialized to n and 0,
respectively. prod_ptr and cons_ptr are used as subscripts of the array buffer. The



204  Part 2  Process Management
             producer produces in buffer[prod_ptr] and increments prod_ptr. The consumer
             consumes from buffer[cons_ptr] and increments cons_ptr in the same manner.
             This feature ensures that buffers are consumed in FIFO order. A producer and a
             consumer can operate concurrently so long as some full and some empty buffers
             exist in the system.
             It is easy to verify that this solution implements the correctness conditions
             of the bounded buffer problem described in Section 6.7.1. However, if many
             producer and consumer processes exist in the system, we need to provide mutual
             exclusion among producers to avoid race conditions on prod_ptr. Analogously,
             mutual exclusion should be provided among consumers to avoid race conditions
             on cons_ptr.
             6.9.3 Readers--Writers Using Semaphores
             A key feature of the readers­writers problem is that readers and writers must
             wait while a writer is writing, and when the writer exits, either all waiting readers
             should be activated or one waiting writer should be activated (see the outline of
             Figure 6.18). To implement this feature, we use four counters as follows:
             runread             count of readers currently reading
             totread             count of readers waiting to read or currently reading
             runwrite            count of writers currently writing
             totwrite            count of writers waiting to write or currently writing
             With these counters, the outline of Figure 6.18 is refined as shown in
             Figure 6.29; we do not show details of how the counters are updated. A reader
             is allowed to begin reading when runwrite = 0 and a writer is allowed to begin
             writing when runread = 0 and runwrite = 0. The value of totread is used to activate
             all waiting readers when a writer finishes writing. This solution does not use an
             explicit critical section for writers. Instead writers are blocked until they can be
             allowed to start writing.
                     Parbegin
                           repeat                                 repeat
                                 if runwrite  0                   if runread  0 or
                                 then                                       runwrite  0
                                     { wait };                    then { wait };
                                 { read }                         { write }
                                 if runread = 0 and               if totread  0 or totwrite  0
                                     totwrite  0                  then
                                 then                                       activate either one waiting  writer
                                     activate one waiting writer            or all waiting readers
                           forever;                               forever;
                     Parend;
                                     Reader(s)                              Writer(s)
             Figure  6.29  Refined solution outline for readers­writers.



                                                                  Chapter 6  Process Synchronization  205
Blocking of readers and writers resembles blocking of producers and con-
sumers in the producers­consumers problem. Hence it is best handled by using
semaphores for signaling. We introduce two semaphores named reading and
writing. A reader process would perform wait(reading) before starting to read.
This operation should block the reader process if conditions permitting it to
read are not currently satisfied; otherwise, the reader should be able to get past
it and start reading. Similarly, a writer process would perform a wait(writing)
before writing and it would get blocked if appropriate conditions are not sat-
isfied. The conditions on which readers and writers are blocked may change
when any of the counter values change, i.e., when a reader finishes reading or
a writer finishes writing. Hence the reader and writer processes should them-
selves perform appropriate signal operations after completing a read or a write
operation.
This solution is implemented as follows (see Figure 6.30): To avoid race con-
ditions all counter values are examined and manipulated inside critical sections
implemented by using a binary semaphore named sem_CS. When a reader wishes
to start reading, it enters a critical section for sem_CS to check whether runwrite
= 0. If so, it increments runread, exits the critical section and starts reading. If
not, it must perform wait(reading); however, performing a wait(reading) opera-
tion inside the critical section for sem_CS may cause a deadlock, so it performs
a wait(reading) after exiting the critical section. If conditions permitting the start
of a read operation were satisfied when it examined the counter values inside its
critical section, it would have itself performed a signal(reading) inside the criti-
cal section. Such a reader will get past the wait(reading) operation. A writer will
similarly perform a signal(writing) inside its critical section for sem_CS under
the correct set of conditions and wait(writing) after exiting from the critical
section.
Readers and writers that get blocked on their respective wait operations
are activated as follows: When a reader finishes reading, it performs a signal
operation to activate a writer if no readers are active and a writer is waiting.
When a writer finishes writing, it performs signal operations to activate all wait-
ing readers, if any; otherwise, it performs a signal operation to wake a waiting
writer, if any. Hence the resulting system is a readers-preferred readers­writers
system.
The solution appears to have two redundant features (see Exercise 6.10).
First,  it  uses  two  semaphores,  reading  and  writing,  even  though     only       one
resource--the     shared  data--is  to  be  controlled.  Second,  every      reader  per-
forms a wait(reading) operation even though the operation is clearly redundant
when some other readers are already engaged in reading. However, both fea-
tures are needed to implement a writers-preferred readers­writers system (see
Exercise 6.11).
6.9.4 Implementation of Semaphores
Figure 6.31 shows a scheme for implementing semaphores. A semaphore type
is defined. It has fields for the value of a semaphore, a list that is used to store



206  Part 2  Process Management
                     var
                                 totread, runread, totwrite, runwrite : integer;
                                 reading, writing : semaphore := 0;
                                 sem_CS : semaphore := 1;
                     begin
                                 totread := 0;
                                 runread := 0;
                                 totwrite := 0;
                                 runwrite := 0;
                     Parbegin
                           repeat                              repeat
                                 wait (sem_CS);                wait (sem_CS);
                                 totread := totread + 1;       totwrite := totwrite + 1;
                                 if runwrite = 0 then          if runread = 0 and runwrite =      0  then
                                   runread := runread +    1;          runwrite := 1;
                                   signal (reading);                   signal (writing);
                                 signal (sem_CS);              signal (sem_CS);
                                 wait (reading);               wait (writing);
                                 { Read }                      { Write }
                                 wait (sem_CS);                wait (sem_CS);
                                 runread := runread­1;         runwrite := runwrite­1;
                                 totread := totread­1;         totwrite := totwrite­1;
                                 if runread = 0 and            while (runread < totread) do
                                     totwrite > runwrite       begin
                                     then                                runread := runread + 1;
                                     runwrite := 1;                      signal (reading);
                                     signal (writing);         end;
                                 signal (sem_CS);              if runread = 0 and
                           forever;                            totwrite > runwrite then
                                                                         runwrite := 1;
                                                                         signal (writing);
                                                               signal (sem_CS);
                                                               forever;
                     Parend;
                     end.
                                     Reader(s)                           Writer(s)
             Figure  6.30  A readers­preferred readers­writers system using semaphores.
             ids of processes blocked on the semaphore, and a lock variable that is used to
             ensure indivisibility of the wait and signal operations on the semaphore. The wait
             and signal operations on semaphores are implemented as procedures that take a
             variable of the semaphore type as a parameter. A concurrent program declares
             semaphores as variables of the semaphore type, and its processes invoke the wait
             and signal procedures to operate on them.
             To avoid race conditions while accessing the value of the semaphore, proce-
             dures wait and signal first invoke the function Close_lock to set the lock variable
             sem.lock. Close_lock uses an indivisible instruction and a busy wait; however, the
             busy waits are short since the wait and signal operations are themselves short.
             The procedures invoke the function Open_lock to reset the lock after completing



                                                                                Chapter 6  Process Synchronization  207
Type declaration for Semaphore
type
        semaphore =       record
                          value : integer;       { value of the semaphore }
                          list : . . .           { list of blocked processes }
                          lock : boolean;        { lock variable for operations on this semaphore }
                          end;
Procedures for implementing wait and signal operations
procedure wait (sem)
begin
        Close_lock (sem.lock);
        if sem.value > 0
              then
                    sem.value := sem.value­1;
                    Open_lock (sem.lock);
              else
                    Add id of the process to list of processes blocked on sem;
                    block_me (sem.lock);
end;
procedure signal (sem)
begin
        Close_lock (sem.lock);
        if some processes are blocked on sem
              then
                    proc_id := id of a process blocked on sem;
                    activate (proc_id);
              else
                    sem.value := sem.value + 1;
        Open_lock (sem.lock);
end;
Figure  6.31  A scheme for implementing wait and signal operations on           a  semaphore.
their execution. Recall from Section 6.5.1 that a busy wait may lead to priority
inversion in an OS using priority-based scheduling; we assume that a priority
inheritance protocol is used to avoid this problem. In a time-sharing system, a
busy wait can cause delays in synchronization, but does not cause more serious
problems.
The wait procedure checks whether the value of sem is > 0. If so, it decre-
ments the value and returns. If the value is 0, the wait procedure adds the id
of the process to the list of processes blocked on sem and makes a block me
system call with the lock variable as a parameter. This call blocks the process that
invoked the wait procedure and also opens the lock passed to it as a parameter.
Note that the wait procedure could not have performed these actions itself--
race conditions would arise if it opened the lock before making a block_me call,
and a deadlock would arise if it made made a block_me call before opening
the lock!



208  Part 2  Process Management
             The signal procedure checks whether any process is blocked on sem. If so, it
             selects one such process and activates it by making the system call activate. If no
             processes are waiting for sem, it increments the value of sem by 1. It is convenient
             to maintain the list of blocked processes as a queue and activate the first blocked
             process at a signal operation. This way, the semaphore implementation would
             also possess the bounded wait property. However, the semantics of the signal
             operation do not specify the order in which processes should be activated, so an
             implementation could choose any order it desired.
             The   wait          operation  has  a  very  low  failure  rate  in  most  systems  using
             semaphores, i.e., processes performing wait operations are seldom blocked. This
             characteristic is exploited in some methods of implementing semaphores to
             reduce the overhead. In the following, we describe three methods of implement-
             ing semaphores and examine their overhead implications. Recall that we use the
             term process as a generic term for both processes and threads.
             Kernel-Level Implementation         The kernel implements the wait and signal proce-
             dures of Figure 6.31. All processes in a system can share a kernel-level semaphore.
             However, every wait and signal operation results in a system call; it leads to
             high overhead of using semaphores. In a uniprocessor OS with a noninterrupt-
             ible kernel, it would not be necessary to use a lock variable to eliminate race
             conditions, so the overhead of the Close_lock and Open_lock operations can be
             eliminated.
             User-Level Implementation      The wait and signal operations are coded as library
             procedures, which are linked with an application program so that processes of
             the application can share user-level semaphores. The block_me and activate calls
             are actually calls on library procedures, which handle blocking and activation of
             processes themselves as far as possible and make system calls only when they need
             assistance from the kernel. This implementation method would suit user-level
             threads because the thread library would already provide for blocking, activation,
             and scheduling of threads. The thread library would make a block_me system call
             only when all threads of a process are blocked.
             Hybrid Implementation          The wait and signal operations are again coded as library
             procedures, and processes of an application can share the hybrid semaphores.
             block_me and activate are system calls provided by the kernel and the wait and
             signal operations make these calls only when processes have to be blocked and
             activated. Because of the low failure rate of the wait operation, these system calls
             would be made seldom, so a hybrid implementation of semaphores would have
             a lower overhead than a kernel-level implementation.
             6.10  MONITORS                                                                            ·
             Recall from Section 6.5.3 that a concurrent programming construct provides data
             abstraction and encapsulation features specifically suited to the construction of
             concurrent programs. A monitor type resembles a class in a language like C++ or



                                                         Chapter 6        Process Synchronization  209
Java. It contains declarations of shared data. It may also contain declarations of
special synchronization data called condition variables on which only the built-in
operations wait and signal can be performed; these operations provide convenient
means of setting up signaling arrangements for process synchronization. Proce-
dures of the monitor type encode operations that manipulate shared data and
perform process synchronization through condition variables. Thus, the monitor
type provides two of the three components that make up a concurrent system (see
Section 6.6).
A concurrent system is set up as follows: A concurrent program has a monitor
type. The program creates an object of the monitor type during its execution. We
refer to the object as a monitor variable, or simply as a monitor. The monitor con-
tains a copy of the shared and synchronization data declared in the monitor type
as its local data. The procedures defined in the monitor type become operations
of the monitor; they operate on its local data. The concurrent program creates
processes through system calls. These processes invoke operations of the monitor
to perform data sharing and control synchronization; they become blocked or
activated when the monitor operations perform wait or signal operations.
The data abstraction and encapsulation features of the monitor assist in syn-
chronization as follows: Only the operations of a monitor can access its shared
and synchronization data. To avoid race conditions, the compiler of the pro-
gramming language implements mutual exclusion over operations of a monitor
by ensuring that at most one process can be executing a monitor operation at any
time. Invocations of the operations are serviced in a FIFO manner to satisfy the
bounded wait property.
Condition Variables  A condition is some situation of interest in a monitor. A
condition variable, which is simply a variable with the attribute condition, is asso-
ciated with a condition in the monitor. Only the built-in operations wait and
signal can be performed on a condition variable. The monitor associates a queue
of processes with each condition variable. If a monitor operation invoked by a
process performs a wait operation on a condition variable, the monitor blocks the
process, enters its id in the process queue associated with the condition variable,
and schedules one of the processes, if any, waiting to begin or resume execution
of a monitor operation. If a monitor operation performs the signal operation
on a condition variable, the monitor activates the first process in the process
queue associated with the condition variable. When scheduled, this process would
resume execution of the monitor operation in which it was blocked. The signal
operation has no effect if the process queue associated with a condition variable
is empty when the condition is signaled.
Implementation of a monitor maintains several process queues--one for each
condition variable and one for processes waiting to execute monitor operations.
To ensure that processes do not get stuck halfway through execution of an oper-
ation, the monitor favors processes that were activated by signal operations over
those wishing to begin execution of monitor operations.
The following example describes use of a monitor to implement a binary
semaphore. We discuss an interesting implementation issue after the example.



210  Part 2  Process Management
·
     Example 6.6  Monitor Implementation of a Binary Semaphore
                  The upper half of Figure 6.32 shows a monitor type Sem_Mon_type that
                  implements a binary semaphore, and the lower half shows three processes
                  that use a monitor variable binary_sem. Recall from Section 6.9.1 that a binary
                  semaphore takes only values 0 and 1, and is used to implement a critical section.
                  The boolean variable busy is used to indicate whether any process is currently
                  using the critical section. Thus, its values true and false correspond to the
                  values 0 and 1 of the binary semaphore, respectively. The condition variable
                  non_busy corresponds to the condition that the critical section is not busy; it is
                  used to block processes that try to enter a critical section while busy = true. The
                  procedures sem_wait and sem_signal implement the wait and signal operations
                  on the binary semaphore. Binary_sem is a monitor variable. The initialization
                  part of the monitor type, which contains the statement busy :=false; is invoked
                  when binary_sem is created. Hence variable busy of binary_sem is initialized
                  to false.
                                type Sem_Mon_type = monitor
                                       var
                                            busy : boolean;
                                            non_busy : condition;
                                       procedure sem_wait;
                                       begin
                                             if busy = true then non_busy.wait;
                                             busy := true;
                                       end;
                                       procedure sem_signal;
                                       begin
                                             busy := false;
                                             non_busy.signal;
                                       end;
                                       begin { initialization }
                                             busy := false;
                                end;
                                var binary_sem : Sem_Mon_type;
                                begin
                                Parbegin
                             repeat                          repeat                      repeat
                                binary_sem.sem_wait;             binary_sem.sem_wait;    binary_sem.sem_wait;
                                { Critical Section }             { Critical Section }    { Critica lSection }
                                binary_sem.sem_signal;           binary_sem.sem_signal;  binary_sem.sem_signal;
                                { Remainder of                   { Remainder of          { Remainder of
                                     the cycle }                     the cycle }                 the cycle }
                             forever;                        forever;                    forever;
                                Parend;
                                end.
                                Process P1                       Process P2              Process P3
                  Figure  6.32  Monitor implementation of a binary semaphore.



                                                                      Chapter 6       Process Synchronization  211
                                 Queue 2      busy
                                                                Data
                                 ···          non_busy
                                          Procedure sem_wait
                    Queue 1                   non_busy.wait
               ···                                                Operations
                                          Procedure sem_signal
                                              non_busy.signal
                                              busy := false     Initializations
Figure  6.33   A monitor implementing a binary semaphore.
      Figure 6.33 depicts the monitor Sem_Mon_type. The monitor maintains
two queues of processes. Queue 1 contains processes waiting to execute opera-
tion sem_wait or sem_signal of the monitor, while queue 2 contains processes
waiting for a non_busy.signal statement to be executed.
      Let P1 be the first process to perform binary_sem.sem_wait. Since busy
is false, it changes busy to true and enters its critical section. If P2 performs
binary_sem.sem_wait while P1 is still inside its critical section, it will be blocked
on the statement non_busy.wait. It will wait in queue 2. Now let P1 start exe-
cuting binary_sem.sem_signal and let P3 try to perform binary_sem.sem_wait
before P1 finishes executing binary_sem.sem_signal. Due to mutual exclusion
over monitor operations, P3 will be blocked and put in the queue associated
with entry to the monitor, i.e., in queue 1. Figure 6.34 shows a snapshot of the
system at this instant. When process P1 executes the statement non_busy.signal
and exits from the monitor, P2 will be activated ahead of P3 because queues
associated with condition variables enjoy priority over the queue associated
with entry to the monitor. Process P3 will start executing binary_sem.sem_wait
only when process P2 completes execution of binary_sem.sem_wait, exits the
monitor and enters its critical section. P3 will now block itself on the condi-
tion non_busy. It will be activated when P2 executes the binary_sem.sem_signal
operation.
                                                                                        ·
     If procedure sem_signal of Example 6.6 contained some statements following
the  signal  statement,      an  interesting  synchronization     problem        would  arise
when  process  P1   invokes      binary_sem.sem_signal       and  executes       the  statement
non_busy.signal. The signal statement is expected to activate process P2, which



212  Part 2  Process Management
                                                   P1   sem_wait
                                                                                 busy
                                   Queue 1    ...                          non-busy
                                                   P3   sem_signal                              ...  Queue 2
                                                                                           P2
                  Figure 6.34 A snapshot of the         system of Example  6.6.
                  should resume its execution of binary_sem.sem_wait. At the same time, process
                  P1 should continue its execution of binary_sem.sem_signal by executing state-
                  ments that follow the non_busy.signal statement. Since monitor operations are
                  performed in a mutually exclusive manner, only one of them can execute and the
                  other one will have to wait. So which of them should be selected for execution?
                        Selecting process P2 for execution would delay the signaling process P1,
                  which   seems       unfair.      Selecting   P1  would   imply       that    P2    is  not  really  acti-
                  vated   until    P1  leaves      the  monitor.    Hoare  (1974)      proposed          the  first  alterna-
                  tive. Brinch Hansen (1973) proposed that a signal statement should be the
                  last  statement      of   a      monitor    procedure,   so    that  the     process   executing       sig-
                  nal   exits   the   monitor          procedure   immediately   and       the  process       activated     by
                  the signal statement can be scheduled. We will follow this convention in our
                  examples.
·
     Example 6.7  Producers-Consumers Using Monitors
                  Figure  6.35       shows     a   solution   to   the  producers­consumers              problem     that
                  uses monitors. It follows the same approach as the solution of Figure 6.28,
                  using semaphores. The upper half of Figure 6.35 shows a monitor type
                  Bounded_buffer_type. Variable full is an integer that indicates the number of
                  full buffers. In the procedure produce, a producer executes a buffer_empty.wait
                  if full = n. It would be activated only when at least one empty buffer exists in
                  the pool. Similarly, the consumer executes a buffer_ full.wait if full = 0. Waiting
                  consumers and producers are activated by the statements buff_ full.signal and
                  buff_empty.signal in the procedures produce and consume, respectively.
                          The lower half of Figure 6.35 shows a system containing two producer
                  processes P1, P2 and a consumer process P3. Operation of a single buffer sys-
                  tem;   i.e.,  n  =   1  in  Figure    6.35,     can  be  depicted    as  shown     in  Figure      6.36.
                  Let processes P1 and P2 try to produce and let process P3 try to consume,
                  all at the same time. Let us assume that process P1 enters the procedure
                  produce, gets past the wait statement and starts producing, while processes P2



                                                                  Chapter 6  Process Synchronization  213
    and P3 are blocked on entry to the monitor (see Part (a) of the snapshot).
    P1 executes buff_full.signal and exits. Process P2 is now activated. However,
    it becomes blocked again on buff_empty.wait because full = 1. Process P3 is
    activated when P2 becomes blocked and starts consuming [see Figure 6.36(b)].
    Process P2 will be activated when P3 exits after consuming.
                                                                                     ·
6.10.1 Monitors in Java
A Java class becomes a monitor type when the attribute synchronized is
associated with one or more methods in the class. An object of such a class
is  a  monitor.     The  Java  virtual  machine  ensures  mutual  exclusion  over    the
synchronized methods in a monitor as follows: When a thread calls a synchro-
nized method of an object, the Java virtual machine checks whether the object
is currently locked. If it is unlocked, the lock is set now and the thread is per-
mitted to execute the method; otherwise, the thread has to wait until the object
is unlocked. When a thread exits a synchronized method, the object is unlocked
and a waiting thread, if any, is activated.
       Each monitor contains a single unnamed condition variable. A thread waits
on the condition variable by executing the call wait(). The notify() call is like
the signal operation described in Section 6.10. It wakes one of the threads waiting
on the condition variable, if any. The Java virtual machine does not implement
FIFO behavior for the wait and notify calls. Thus, wait and notify do not satisfy
the bounded wait property. The notifyall() call activates all threads waiting
on the condition variable.
       Provision of a single condition variable in a monitor can lead to busy waits
in an application. Consider the readers­writers system as an example. When a
writer is active, all readers wishing to read and all writers wishing to write have
to wait on the condition variable. When the writer finishes writing, it would have
to use a notifyall() call to activate all waiting threads. If readers are pre-
ferred, all writer threads will have to perform wait() calls once again. If writers
are preferred, all reader threads and some writer threads will have to perform
wait() calls once again. Thus, a reader or writer thread may be activated many
times before it gets an opportunity to perform reading or writing. A producers­
consumers system with many producer and consumer processes would similarly
suffer from busy waits.
6.11   CASE STUDIES OF PROCESS SYNCHRONIZATION                                                        ·
6.11.1 Synchronization of POSIX Threads
As mentioned in Section 5.3.1, POSIX threads provide mutexes for mutual
exclusion and condition variables for control synchronization between processes.
A   mutex  is    a  binary  semaphore.  An   OS  may      implement  POSIX   threads



214  Part 2  Process Management
                     type Bounded_buffer_type = monitor
                           const
                            n = . . .;
                                                               { Number of  buffers }
                           type
                            item = . . .;
                           var
                            buffer : array [0..n­1] of item;
                                full, prod_ptr, cons_ptr : integer;
                            buff_full : condition;
                            buff_empty : condition;
                           procedure produce (produced_info : item);
                           begin
                            if full = n then buff_empty.wait;
                            buffer [prod_ptr] := produced_info;               { i.e., Produce }
                            prod_ptr := prod_ptr + 1 mod n;
                            full := full + 1;
                            buff_full.signal;
                           end;
                           procedure consume (for_consumption : item);
                           begin
                            if full = 0 then buff_full.wait;
                            for_consumption := buffer[cons_ptr];              { i.e., Consume }
                            cons_ptr := cons_ptr + 1 mod n;
                            full := full­1;
                            buff_empty.signal;
                           end;
                           begin { initialization }
                            full := 0;
                            prod_ptr := 0;
                            cons_ptr := 0;
                     end;
                     begin
                     var B_buf : Bounded_buffer_type;
                     Parbegin
                     var info : item;                var info : item;         var area : item;
                     repeat                          repeat                   repeat
                            info := . . .              info := . . .                   B_buf.consume (area);
                            B_buf.produce (info);      B_buf.produce (info);           { Consume area }
                            { Remainder of             { Remainder of                  { Remainder of
                                 the cycle }                  the cycle }              the cycle }
                     forever;                        forever;                 forever;
                     Parend;
                     end.
                            Producer P1                Producer P2                     Consumer P3
             Figure  6.35   Producers­consumers using monitors.



                                                                            Chapter 6  Process Synchronization  215
           P1    produce                             P1            produce             P2
                                 buff_empty                                        ?  buff_empty
                                 buff_ full                                            buff_ full
                 consume                             P3            consume
       P3  P2
(a)                                                  (b)
Figure 6.36    Snapshots of the  monitor of Example  6.7  with  a  single buffer.
as kernel-level threads or user-level threads. Accordingly, mutexes would be
implemented through either a kernel-level implementation or a hybrid imple-
mentation described in Section 6.9.4 when threads are implemented as kernel-
level  threads,  and  through    the  user-level     implementation         when       threads     are
implemented through user-level threads. Analogously, condition variables are
also implemented through a kernel-level, hybrid, or user-level implementation
scheme.
6.11.2 Process Synchronization in Unix
Unix system V provides a kernel-level implementation of semaphores. The name
of a semaphore is called a key. The key is actually associated with an array of
semaphores, and individual semaphores in the array are distinguished with the
help of subscripts. Processes share a semaphore by using the same key. A process
wishing to use a semaphore obtains access to it by making a semget system call
with a key as a parameter. If a semaphore array with matching key already exists,
the kernel makes that array accessible to the process making the semget call;
otherwise, it creates a new semaphore array, assigns the key to it and makes it
accessible to the process.
       The kernel provides a single system call semop for wait and signal opera-
tions. It takes two parameters: a key, i.e., the name of a semaphore array, and
a list of (subscript, op) specifications where subscript identifies a semaphore in
the semaphore array and op is a wait or signal operation to be performed. The
entire set of operations defined in the list is performed in an atomic manner; that
is, either all the operations are performed and the process is free to continue its
execution, or none of the operations is performed and the process is blocked.
A blocked process is activated only when all operations indicated in semop can
succeed.
       The semantics of semop can be used to prevent deadlocks. Consider the
following example: Semaphores sem1 and sem2 are associated with resources R1
and R2, respectively. A process performs a wait(semi) before using a resource Ri
and a signal(semi) after finishing with it. If each of processes P1 and P2 require
both resources simultaneously, it is possible that P1 will obtain access to R1
but will become blocked on wait(sem2) and process P2 will obtain access to R2



216  Part 2  Process Management
             but will become blocked on wait(sem1). This is a deadlock situation because both
             processes wait for each other indefinitely. Such a deadlock would not arise if
             processes performed both wait operations through a single semop, since a process
             would be either allocated both resources or it would not be allocated any of the
             resources. The situation now resembles the all resources together approach to
             deadlock prevention described later in Section 8.5.1.
             Unix SVR4 provides an interesting feature to make programs using sema-
             phores more reliable. It keeps track of all operations performed by a process
             on each semaphore used by it, and performs an undo on these operations when
             the process terminates. This action helps to prevent disruptions in a concurrent
             application due to misbehavior of some process. For example, if a process Pi
             performed more wait operations than signal operations on semaphore semi and
             terminated, it could cause indefinite waits for other processes in the application.
             Performing an undo operation on all wait and signal operations performed by Pi
             might prevent such disasters. To perform undo operations efficiently, the kernel
             maintains a cumulative count of changes in the value of a semaphore caused by
             the operations in a process, and subtracts it from the value of the semaphore when
             the process terminates. If a process Pi performed more wait operations than signal
             operations on semaphore semi, its cumulative count for semi would be negative.
             Subtracting this count would nullify the effect of Pi on semi. Pi's cumulative count
             would be 0 if it had performed an equal number of wait and signal operations
             on semi. Thus the undo operation does not interfere with normal operation of
             processes using semaphores.
             Unix 4.4BSD places a semaphore in memory areas shared by a set of pro-
             cesses, and provides a hybrid implementation of semaphores along the lines
             discussed in Section 6.9.4. This way, it avoids making system calls in cases
             where a wait operation does not lead to blocking of a process and a sig-
             nal operation does not lead to activation of a process, which provides fast
             synchronization.
             6.11.3 Process Synchronization in Linux
             Linux provides a Unix-like semaphore (see Section 6.11.2) for use by user
             processes. It also provides two kinds of semaphores for use by the kernel--
             a conventional semaphore and a reader­writer semaphore. The conventional
             semaphore is implemented by a kernel-level scheme that is more efficient than
             the kernel-level scheme discussed in Section 6.9.4. It uses a data structure that
             contains the value of a semaphore, a flag to indicate whether any processes
             are blocked on it, and the actual list of such processes. Unlike the scheme of
             Section 6.9.4, a lock is not used to avoid race conditions on the value of the
             semaphore; instead, the wait and signal operations use indivisible instructions
             to decrement or increment the value of the semaphore. These operations lock
             the list of blocked processes only if they find that processes are to be added to
             it or removed from it--the wait operation locks the list only if the process that
             performed the wait operation is to be blocked, whereas the signal operation locks
             it only if the semaphore's flag indicates that the list is nonempty.



                                                           Chapter 6  Process Synchronization  217
The reader­writer semaphore provides capabilities that can be used to imple-
ment the readers­writers problem of Section 6.9.3 within a kernel so that many
processes can read a kernel data structure concurrently but only one process can
update it at a time. Its implementation does not favor either readers or writers--it
permits processes to enter their critical sections in FIFO order, except that con-
secutive readers can read concurrently. It is achieved by simply maintaining a list
of processes waiting to perform a read or write operation, which is organized in
the chronological order.
Kernels older than the Linux 2.6 kernel implemented mutual exclusion in the
kernel space through system calls. However, as mentioned in Section 6.9.4, a wait
operation has a low failure rate; i.e., a process is rarely blocked on a wait call, so
many of the system calls are actually unnecessary. The Linux 2.6 kernel provides
a fast user space mutex called futex. A futex is an integer in shared memory on
which only certain operations can be performed. The wait operation on a futex
makes a system call only when a process needs to be blocked on the futex, and
the signal operation on a futex makes a system call only when a process is to be
activated. The wait operation also provides a parameter through which a process
can indicate how long it is prepared to be blocked on the wait. When this time
elapses, the wait operation fails and returns an error code to the process that made
the call.
6.11.4 Process Synchronization in Solaris
Process synchronization in the Sun Solaris operating system contains three inter-
esting features--reader­writer semaphores and adaptive mutexes, a data structure
called a turnstile, and use of the priority inversion protocol. The reader­writer
semaphore is analogous to the reader­writer semaphore in Linux. An adaptive
mutex is useful in a multiprocessor OS, hence it is discussed in Chapter 10; only
an overview is included here.
Recall from Section 5.4.3 that the Solaris kernel provides parallelism through
kernel threads. When a thread Ti performs a wait operation on a semaphore that
is currently used by another thread Tj, the kernel can either block Ti or let it spin.
The blocking approach involves the overhead of blocking thread Ti, scheduling
another thread, and activating thread Ti when Tj releases the semaphore. Spin-
ning, on the other hand, incurs the overhead of a busy wait until Tj releases
the semaphore. If Tj is currently operating on another CPU, it may release the
semaphore before either Ti or Tj is preempted, so it is better to let Ti spin. If Tj
is not operating currently, Ti may spin for long, so it is better to conserve CPU
time by blocking it. The adaptive mutex uses this method.
The Solaris kernel uses a data structure called a turnstile to hold informa-
tion concerning threads that are blocked on a mutex or reader­writer semaphore.
This information is used for both synchronization and priority inheritance. To
minimize the number of turnstiles needed at any time, the kernel of Solaris 7
attaches a turnstile with every new thread it creates. It performs the follow-
ing actions when a kernel thread is to be blocked on a mutex: If no threads



218  Part 2  Process Management
             are already blocked on the mutex, it detaches the turnstile from the thread,
             associates it with the mutex, and enters the thread's id in the turnstile. If a turn-
             stile is already associated with the mutex, i.e., if some other threads are already
             blocked on it, the kernel detaches the turnstile of the thread and returns it to
             the pool of free turnstiles, and enters the thread's id into the turnstile that is
             already associated with the mutex. When a thread releases a mutex or a reader­
             writer semaphore, the kernel obtains information about threads blocked on the
             mutex or reader­writer semaphore, and decides which thread(s) to activate. It
             now attaches a turnstile from the pool of free turnstiles with the activated thread.
             A turnstile is returned to the pool of free turnstiles when the last thread in it
             wakes up.
                 The Solaris kernel uses a priority inheritance protocol to reduce synchro-
             nization delays. Consider a thread Ti that is blocked on a semaphore because
             thread Tj is in a critical section implemented through the semaphore. Thread
             Ti might suffer a long synchronization delay if Tj is not scheduled for a long
             time, which would happen if Tj has a lower priority than Ti. To reduce the
             synchronization       delay  for  Ti ,  the  kernel   raises  the  priority  of  Tj  to  that  of
             Ti  until  Tj  exits  the    critical   section.  If  many    processes  become      blocked   on
             the semaphore being used by Tj, Tj's priority should be raised to that of the
             highest-priority process blocked on the semaphore. It is implemented by obtain-
             ing priorities of the blocked processes from the turnstile associated with the
             semaphore.
             6.11.5 Process Synchronization in Windows
             Windows is an object-oriented system, hence processes, files and events are
             represented by objects. The kernel provides a uniform interface for thread syn-
             chronization over different kinds of objects as follows: A dispatcher object is a
             special kind of object that is either in the signaled state or in the nonsignaled state.
             A dispatcher object is embedded in every object over which synchronization may
             be desired, e.g., an object representing a process, file, event, mutex, or semaphore.
             Any thread that wishes to synchronize with an object would be put in the waiting
             state if the dispatcher object embedded in the object is in the nonsignaled state.
             Table 6.4 describes the semantics of various kinds of objects, which determine
             when the state of an object would change, and which of the threads waiting on it
             would be activated when it is signaled.
                 A thread object enters the signaled state when the thread terminates, whereas
             a process object enters the signaled state when all threads in the process terminate.
             In both cases, all threads waiting on the object are activated. The file object enters
             the signaled state when an I/O operation on the file completes. If any threads are
             waiting on it, all of them are activated and its synchronization state is changed
             back to nonsignaled. If no threads are waiting on it, a thread that waits on it
             sometime in future will get past the wait operation and the synchronization state
             of the file object would be changed to nonsignaled. The console input object has
             an analogous behavior except that only one waiting thread is activated when it



                                                              Chapter 6        Process Synchronization  219
Table 6.4      Windows Objects Used for Synchronization
Object           Nonsignaled state    Signaled state          Signal time action
Process          Not terminated       Last thread             Activate all threads
                                      terminates
Thread           Not terminated       The thread              Activate all threads
                                      terminates
File             I/O request          I/O completed           Activate all threads
                 pending
Console input    Input not            Input provided          Activate one thread
                 provided
File change      No changes           Change noticed          Activate one thread
Notify event     Not yet set          Set event executed      Activate all threads
Synchronization  Reset                Set event executed      Activate one thread
event                                                         and reset event
Semaphore        Successful wait      Released                Activate one thread
Mutex            Successful wait      Released                Activate one thread
Condition        Initially and after  wake or wakeall         Activate one thread or
variable         a wake or            function is             all threads
                 wakeall              performed
                 function call
Timer            Reinitialization     Set time arrives or     Same as notify
                                      interval elapses        and synchroniza-
                                                              tion events
is signaled. The file change object is signaled when the system detects changes in
the file. It behaves like the file object in other respects.
Threads use the event, semaphore, mutex, and condition variable objects for
mutual synchronization. They signal these objects by executing library functions
that lead to appropriate system calls. An event object is signaled at a set event
system call. If it is a notification event, all threads waiting on it are activated. If
it is a synchronization event, only one thread is activated and the event is reset.
The timer object is also designed for use in the notification and synchronization
modes. The kernel changes the state of the object to signaled when the specified
time arrives or the specified interval elapses. Its signal time actions are similar to
those of the notify and synchronization events.
The semaphore object implements a counting semaphore, which can be used
to control a set of resources. The number of resources is specified as the initial
value of the semaphore. A count in the semaphore object indicates how many of
these resources are currently available for use by threads. The semaphore object
is in the nonsignaled state when the count is 0, so any process performing a wait
on it would be put in the waiting state. When a thread releases a resource, the
kernel increments the number of available resources, which puts the semaphore
in the signaled state. Consequently, some thread waiting on it would be activated.



220        Part 2   Process Management
                    A thread can specify a time interval in a wait call to indicate how long it is
                    prepared to wait for an object. It would be activated before this interval elapses if
                    the object is signaled; otherwise, its wait request would be withdrawn at the end
                    of the interval and it would get activated. A mutex is implemented as a binary
                    semaphore. The mutex object is signaled when a process executes the release
                    function; the kernel releases one of the threads waiting on it.
                            The Windows kernel provides a variety of synchronization locks--a spin-
                    lock, a special lock called queued spinlock for multiprocessor configurations
                    (see Section 10.6.3), and fast mutexes and push locks, which, like the futex of
                    Linux, avoid system calls unless a thread has to wait on a synchronization object.
                    Windows Vista provides a reader­writer lock.
6.12     SUMMARY                                                                                                   ·
Process    synchronization  is  a  generic    term  for     control synchronization--processes may not wait
data  access  synchronization,     which  is  used      to  for each other's actions as expected. Hence avoid-
update shared data in a mutually exclusive man-             ance of race conditions is a primary issue in process
ner,  and  control  synchronization,  which   is    used    synchronization.
to ensure that processes perform their actions in           The computer provides indivisible instructions,
a desired order. Classic process synchronization            which  access  memory  locations  in  a  mutually
problems such as producers­consumers, readers­              exclusive manner. A process may use an indivis-
writers, and dining philosophers represent impor-           ible instruction on a lock variable to implement
tant classes of process synchronization problems.           a critical section. However, this approach suffers
In this chapter we discussed the fundamental issues         from busy waits because a process that cannot
in process synchronization, and the support for             enter the critical section keeps looping until it
process synchronization provided by the computer,           may do so, hence the kernel provides a facility
the kernel, and programming languages. We also              to block such a process until it may be permit-
analyzed classic process synchronization problems           ted to enter a critical section. Compilers of pro-
and demonstrated use of various synchronization             gramming languages implement process synchro-
facilities of programming languages and operating           nization primitives and constructs by using this
systems in implementing them.                               facility. A semaphore is a primitive that facilitates
      A race condition is a situation in which actions      blocking and activation of processes without race
of concurrent processes may have unexpected con-            conditions. A monitor is a construct that provides
sequences, such as incorrect values of shared data          two facilities--it implements operations on shared
or faulty interaction among processes. A race con-          data as critical sections over the data and it pro-
dition exists when concurrent processes update              vides statements for control synchronization.
shared data in an uncoordinated manner. It is               Operating systems provide features for effi-
avoided through mutual exclusion, which ensures             cient implementation of process synchronization;
that only one process updates shared data at any            e.g., Linux provides readers­writers semaphores,
time. A critical section on a shared data d is a            Solaris provides priority inheritance to avoid some
section of code that accesses d in a mutually exclu-        of the problems related to busy waits, and Windows
sive manner. A race condition may also exist in             provides dispatcher objects.



                                                                   Chapter 6    Process Synchronization                  221
TEST  YOUR CONCEPTS                                                                                                      ·
6.1   Classify each of the following statements as true                Figure 6.21 is modified to remove the action
      or false:                                                        "lift the forks one at a time" from the while
      a. An application can contain a race condition                   loop and put it following the while loop.
          only if the computer system servicing the           6.2  A semaphore is initialized to 1. Twelve wait
          application contains more than one CPU.                  operations and seven signal operations are per-
      b. Control synchronization is needed when pro-               formed on it. What is the number of processes
          cesses generate and analyze of Figure 1.6(b)             waiting on this semaphore?
          share the variable sample.                               a. 12,       b. 7,      c. 4,    d. 5
      c. A    process   may    be  starved  of  entry  to  a  6.3  A binary semaphore is initialized to 1. 5 wait
          critical section if the critical section imple-          operations are performed on it in a row, fol-
          mentation does not satisfy the bounded wait              lowed    by  8     signal  operations.   Now      5   more
          condition.                                               wait    operations  are    performed     on    it.   What
      d. A process may be starved of entry to a critical           is  the  number     of     processes  waiting     on  this
          section if the critical section implementation           semaphore?
          does not satisfy the progress condition.                 a. 1,       b. 2,   c. 4,      d. 5
      e. A busy wait is unavoidable unless a system           6.4  Ten    processes    share  a   critical  section     imple-
          call is made to block a process.                         mented by using a counting semaphore named
      f. Indefinite busy waits are possible in an OS               x. Nine of these processes use the code wait(x);
          using priority-based scheduling, but not pos-            {critical section} signal(x). However, one pro-
          sible in an OS using round-robin scheduling.             cess erroneously uses the code signal(x); {critical
      g. Algorithm 6.1 can be used to implement a                  section} signal(x). What is the maximum num-
          single-buffer producers­consumers system if              ber of processes that can be in the critical section
          process P1 is a producer and P2 is a consumer.           at the same time?
      h. When a lock variable is used, an indivisi-                a. 1,       b. 2,   c. 10,       d. 3
          ble instruction is not needed to implement
          a critical section.                                 6.5  In a readers­writers system, a read operation
      i. In a producers­consumers system consisting                consumes 3 time units and a write operation con-
          of many producer processes, many consumer                sumes 5 time units. No readers or writers exist
          processes, and many buffers in the buffer-               in the system at time ti - 1. One reader arrives at
          pool, it is possible for many producer pro-              time ti, and 5 readers and 1 writer arrive at time
          cesses to be producing and many consumer                 ti + 1. If no more readers or writers arrive, when
          processes to be consuming at the same time.              will the writer finish writing?
      j.  In  a  writers-preferred  readers­writers    sys-        a. ti + 8,
          tem, some reader processes wishing to read               b. ti + 20,
          the shared data may become blocked even                  c. ti + 9,
          while some other reader processes are reading            d. none of a­c
          the shared data.                                    6.6  A producer process produces a new item of infor-
      k. A    deadlock  cannot      occur   in  the  dining        mation in 10 seconds and a consumer process
          philosophers problem if one of the philoso-              consumes an item in 20 seconds. In a producers­
          phers can eat with only one fork.                        consumers system consisting of a single pro-
      l. A    critical  section     implemented        using       ducer process, a single consumer process, and
          semaphores would satisfy the bounded wait                a single buffer, both the producer and the con-
          property only if the signal operation activates          sumer processes start their operation at time 0.
          processes in FIFO order.                                 At what time will the consumer process finish
      m. A race condition can occur over forks if the              consuming 3 items?
          outline of the dining philosophers problem in            a. 20, b. 60, c. 70, d. 90, e. none of a­d



222      Part 2  Process Management
EXERCISES                                                                                                                        ·
6.1  A concurrent program contains a few updates of                 and activate for process synchronization. It has a
     a shared variable x, which occur inside critical               race condition. Describe how this race condition
     sections. Variable x is also used in the following             arises.
     section of code which is not enclosed in a critical      6.10  The readers­writers solution of Figure 6.30 uses
     section:                                                       two semaphores even though a single entity--
                         if x < c                                   the shared data--is to be controlled. Modify this
                         then y :=x;                                solution to use a single semaphore rw_permi-
                         else y :=x + 10;                           ssion    instead       of    semaphores          reading     and
                         print x, y;                                writing. (Hint: perform a wait(rw_permission)
                                                                    in the reader only if reading is not already in
     Does this program have a race condition?                       progress.)
6.2  Two concurrent processes share a data item sum,          6.11  Modify        the      readers­writers            solution   of
     which is initialized to 0. However, they do not use            Figure   6.30      to  implement          a  writer-preferred
     mutual exclusion while accessing its value. Each               readers­writers system.
     process contains a loop that executes 50 times           6.12  Implement      a       critical  section     using   the    Test-
     and contains the single statement sum :=sum+1.                 and-set or Swap instructions of Section 6.5.2.
     If no other operations are performed on sum,                   Use ideas from Section 6.8.2 to ensure that the
     indicate the lower bound and upper bound on                    bounded wait condition is satisfied.
     the value of sum when both processes terminate.          6.13  A   resource       is  to  be    allocated       to  requesting
6.3  Analyze Algorithms 6.1 and 6.2 and comment                     processes in a FIFO manner. Each process is
     on  the   critical  section   properties  violated   by        coded as
     them. Give examples illustrating the violations.
6.4  Answer the following in context of Dekker's                        repeat
     algorithm:                                                              ...
     a. Does   the   algorithm        satisfy  the  progress                 request-resource(process_id, resource_id);
         condition?                                                          { Use resource }
     b. Can a deadlock condition arise?                                      release-resource(process_id, resource_id);
     c. Can a livelock condition arise?                                      { Remainder of the cycle }
6.5  Is  the   bounded   wait      condition   satisfied  by            forever
     Peterson's algorithm?
6.6  The following changes are made in Peterson's                   Develop       the  procedures        request-resource        and
     algorithm (see Algorithm 6.4): The statements                  release-resource using semaphores.
     flag[0] :=true and flag[0] :=false in process P0         6.14  Can one or more of the following features elim-
     are interchanged, and analogous changes are                    inate deficiencies of the outline of the dining
     made in process P1. Discuss which properties                   philosophers problem shown in Figure 6.20?
     of the implementation of critical sections are                 a. If n philosophers exist in the system, have
     violated by the resulting system.                                  seats for at least n + 1 philosophers at the
6.7  The statement while flag[1] and turn = 1 in Peter-                 dining table.
     son's algorithm is changed to while flag[1] or turn            b. Make       sure     that      at  least   one     left-handed
     = 1, and analogous changes are made in process                     philosopher and at least one right-handed
     P1. Which properties of critical section imple-                    philosopher sit at the table at any time.
     mentation are violated by the resulting system?          6.15  In  Figure     6.35,       producers         and     consumers
6.8  Comment on the effect of deleting the statement                always   execute       the   statements          buf_full .signal
     while choosing[j] do { nothing }; on working of                and buf_empty.signal. Suggest and implement
     Lamport's Bakery algorithm.                                    a   method    of       reducing      the    number   of   signal
6.9  The solution of the producers­consumers prob-                  statements    executed           during     the   operation  of
     lem shown in Figure 6.37 uses kernel calls block               the system.



                                                                              Chapter 6        Process  Synchronization    223
      type
              item = . . .;
      var
              buffer : item;
              buffer_full : boolean;
              producer_blocked : boolean;
              consumer_blocked : boolean;
      begin
              buffer_full := false;
              producer_blocked := false;
              consumer_blocked := false;
      Parbegin
             repeat                                       repeat
             if buffer_full = false then                  if buffer_full = true then
                   { Produce in buffer }                           { Consume from buffer }
                   buffer_full := true;                            buffer_full := false;
                   if consumer_blocked = true then                 if producer_blocked = true then
                     activate(consumer);                            activate(producer);
                   { Remainder of the cycle }                      { Remainder of the cycle }
             else                                         else
                   producer_blocked := true;                       consumer_blocked := true;
                   block(producer);                                block(consumer);
                   consumer_blocked := false;                      producer_blocked := false;
            forever                                       forever
      Parend
                     Producer                                       Consumer
Figure 6.37  The producer­consumer problem          with a synchronization error due to a race
condition.
6.16  Implement      the     dining       philosophers    problem             of them and starts serving him; otherwise, he
      using monitors. Minimize the number of execu-                           goes to sleep in the barber's chair. A customer
      tions of signal statements in your solution and                         enters the waiting room only if there is at least
      observe its effect on the logical complexity of                         one vacant seat and either waits for the barber
      your solution.                                                          to call him if the barber is busy, or wakes the
6.17  A customer gives the following instructions to a                        barber if he is asleep. Identify the synchroniza-
      bank manager: Do not credit any funds to my                             tion requirements between the barber and cus-
      account if the balance in my account exceeds                            tomer processes. Code the barber and customer
      n, and hold any debits until the balance in the                         processes such that deadlocks do not arise.
      account is large enough to permit the debit.                  6.19      A monitor is to be written to simulate a clock
      Design a monitor to implement the customer's                            manager used for real-time control of concur-
      bank account.                                                           rent processes. The clock manager uses a variable
6.18  The    synchronization             problem  called  sleeping            named clock to maintain the current time. The
      barber is described as follows: A barber shop has                       OS supports a signal called elapsed_time that is
      a single barber, a single barber's chair in a small                     generated every 2 ms. The clock manager pro-
      room, and a large waiting room with n seats. The                        vides a signal handling action for elapsed_time
      barber and the barber's chair are visible from the                      (see Section 5.4.1) that updates clock at every
      waiting room. After servicing one customer, the                         occurrence of the signal. This action is coded as
      barber checks whether any customers are wait-                           a procedure of the monitor. A typical request
      ing in the waiting room. If so, he admits one                           made to the clock manager is "wake me up at



224            Part 2       Process Management
         9.00 a.m." The clock manager blocks the pro-                                  this system, using any synchronization primitive
         cesses making such requests and arranges to                                   or control structure of your choice. To prevent
         activate them at the designated times. Implement                              starvation of queries, it is proposed to handle a
         this monitor.                                                                 maximum of 10 queries on a part of the data
6.20     Nesting of monitor calls implies that a proce-                                at any time. Modify the monitor to incorporate
         dure in monitor A calls a procedure of another                                this feature.
         monitor, say monitor B. During execution of the                         6.23  A bridge on a busy highway is damaged by a
         nested call, the procedure of monitor A con-                                  flood. One-way traffic is to be instituted on the
         tinues to hold its mutual exclusion. Show that                                bridge by permitting vehicles traveling in oppo-
         nested monitor calls can lead to deadlocks.                                   site directions to use the bridge alternately. The
6.21     Write a short note on the implementation of                                   following rules are formulated for use of the
         monitors. Your note must discuss:                                             bridge:
         a. How to achieve mutual exclusion between the                                a. At any time, the bridge is used by vehicle(s)
             monitor procedures.                                                       traveling in one direction only.
         b. Whether         monitor      procedures          need      to              b. If vehicles are waiting to cross the bridge at
             be    coded     in   a   reentrant       manner           (see            both ends, only one vehicle from one end is
             Section 11.3.3.2).                                                        allowed to cross the bridge before a vehicle
6.22     A   large    data   collection    D  is    used    merely     to              from the other end starts crossing the bridge.
         answer queries, i.e., no updates are carried out                              c. If no vehicles are waiting at one end, then
         on D, so queries can be processed concurrently.                               any number of vehicles from the other end
         Because of the large size of D, it is split into sev-                         are permitted to cross the bridge.
         eral parts D1, D2, . . . , Dn, and at any time only                           Develop a concurrent system to implement these
         one of these parts, say D1, is loaded in mem-                                 rules.
         ory to handle queries related to it. If no queries                      6.24  When vehicles are waiting at both ends, the rules
         are active on D1, and queries exist on some other                             of Exercise 23(a) lead to poor use of the bridge.
         part of data, say D2, D2 is loaded in memory and                              Hence up to 10 vehicles should be allowed to
         queries on it are processed concurrently. When                                cross the bridge in one direction even if vehi-
         D is split into two parts D1 and D2, this system                              cles are waiting at the other end. Implement the
         is called a readers­readers system. Implement                                 modified rules.
CLASS PROJECT 1: INTERPROCESS COMMUNICATION                                                                                                ·
An interprocess message communication system uses                            20  message       buffers.  The  system  is  to  operate      as
the asymmetric naming convention described later in                          follows:
Section    9.1.1,   which       uses  the    following      rules:     To
send  a    message,     a   sender    provides      the     id   of    the   1.  Each process has a cyclic behavior. Its operation is
destination    process      to  which    it  is   to  be    delivered,           governed by commands in a command file that is
and   the   text    of  the     message.     To     receive     a    mes-        used exclusively by it. In each iteration, it reads a
sage, a process simply provides the name of a vari-                              command from the file and invokes an appropri-
able in which the message should be deposited; the                               ate operation of the monitor. Three commands are
system   provides       it  with  a   message         sent   to    it  by        supported:
some process.                                                                    a. send <process_id>, <message_text>: The pro-
      The system consists of a monitor named Com-                                      cess should send a message.
munication_Manager          and   four     processes.     The      moni-         b. receive <variable_name>: The process should
tor  provides    the    operations    send    and     receive,     which               receive a message.
implement    message        passing   using      a  global      pool   of        c. quit: The process should complete its operation.



                                                               Chapter 6          Process Synchronization    225
2.  When a process invokes a send operation, the mon-          process performing the receive operation is blocked
    itor copies the text of the message in a free message      if no message exists for it. It would be activated
    buffer from the global pool of message buffers. If         when a message is sent to it.
    the destination process of the message is currently    4.  After performing a send or receive operation, the
    blocked on a receive operation, the message is deliv-      monitor writes details of the actions performed by
    ered to it as described in Item 3 and the process          it in a log file.
    is activated. In either case, control is returned to   5.  The monitor detects a deadlock situation, in which
    the process executing the send operation. If none of       some of the processes are blocked indefinitely. It
    the message buffers in the global pool of 20 mes-          writes details of the deadlock situation in the log
    sage buffers is free, the process performing the send      file and terminates itself.
    operation is blocked until a message buffer becomes    6.  The interprocess message communication system
    free.                                                      terminates itself when all processes have completed
3.  When a process invokes a receive operation, it is          their operation.
    given a message sent to it in FIFO order. The mon-
    itor finds the message buffer that contains the first
    undelivered message that was sent to the process,          Write the monitor Communication_Manager and
    copies the text of the message into the variable       test its operation with several sets of sample command
    mentioned by the process, and frees the message        files for the processes that create various interesting sit-
    buffer. If a process executing the send operation was  uations in message passing, including some deadlock
    blocked as mentioned in Item 2, it is activated. The   situations.
CLASS PROJECT 2: DISK SCHEDULER                                                                                           ·
A disk scheduler is that part of an OS which decides the   exclusively by it. Each command is for performing a
order in which I/O operations should be performed on a     read or write operation on a disk block. In each itera-
disk to achieve high disk throughput (see Section 14.7).   tion, a process reads a command from its command file
Processes that wish to perform I/O operations on the disk  and invokes the monitor operation IO_request to pass
use a monitor named Disk_scheduler and the following       details of the I/O operation to the monitor. IO_request
pseudocode:                                                blocks the process until its I/O operation is scheduled.
    var Disk_scheduler : Disk_Mon_type;                    When the process is activated, it returns from IO_request
    Parbegin                                               and performs its I/O operation. After completing the I/O
    begin { User process Pi }                              operation, it invokes the monitor operation IO_complete
             var disk_block_address : integer;             so that the monitor can schedule the next I/O opera-
             repeat                                        tion. The monitor writes details of its actions in a log file
                                                           every time the IO_request or IO_complete operation is
              {read a command from file Fi }               invoked.
              Disk_scheduler . IO_request                      Code the monitor type Disk_Mon_type. For sim-
                      (Pi, IO_operation,                   plicity, you may assume that I/O operations are sched-
                      disk_block_address);                 uled in FIFO order, and that the number of processes
              { Perform I/O Operation }                    does not exceed 10. (Hint: Note the process id of a pro-
              Disk_scheduler . IO_complete (Pi);           cess along with details of its I/O operation in a list in
              { Remainder of the cycle }                   the monitor. Decide how many condition variables you
             forever                                       would need to block and activate the processes.)
    end;                                                       Modify Disk_Mon_type such that I/O operations
    ...               { other user processes      }        would be performed by the monitor itself rather than by
    Parend;                                                user processes. (Hint: Operation I/O_complete would no
    Each process has cyclic behavior. Its operation is     longer be needed.)
governed by commands in a command file that is used



226          Part 2  Process Management
BIBLIOGRAPHY                                                                                                           ·
Dijkstra (1965) discusses the mutual exclusion problem,        2.   Ben Ari, M. (1982): Principles of Concurrent
describes Dekker's algorithm, and presents a mutual                 Programming, Prentice Hall, Englewood Cliffs,
exclusion algorithm for n processes. Lamport (1974,                 N.J.
1979) describes and proves the Bakery algorithm. Ben           3.   Ben Ari, M. (2006): Principles of Concurrent and
Ari (1982) describes the evolution of mutual exclusion              Distributed Programming, 2nd ed., Prentice Hall,
algorithms and provides a proof of Dekker's algorithm.              Englewood Cliffs, N.J.
Ben Ari (2006) discusses concurrent and distributed pro-       4.   Bovet, D. P., and M. Cesati (2005): Understanding
gramming. Peterson (1981), Lamport (1986, 1991), and                the Linux Kernel, 3rd ed., O'Reilly, Sebastopol.
Raynal (1986) are other sources on mutual exclusion            5.   Brinch Hansen, P. (1972): "Structured
algorithms.                                                         multiprogramming," Communications of the
     Dijkstra  (1965)  proposed      semaphores.  Hoare             ACM, 15 (7), 574­578.
(1972) and Brinch Hansen (1972) discuss the critical           6.   Brinch Hansen, P. (1973): Operating System
and conditional critical regions, which are synchroniza-            Principles, Prentice Hall, Englewood Cliffs, N.J.
tion constructs that preceded monitors. Brinch Hansen          7.   Brinch Hansen, P. (1975): "The programming
(1973) and Hoare (1974) describe the monitor concept.               language concurrent Pascal," IEEE Transactions
Buhr et al. (1995) describes different monitor implemen-            on Software Engineering, 1 (2), 199­207.
tations. Richter (1999) describes thread synchronization       8.   Brinch Hansen, P. (1977): The Architecture of
in C/C++ programs under Windows. Christopher and                    Concurrent Programs, Prentice Hall, Englewood
Thiruvathukal (2001) describes the concept of monitors              Cliffs, N.J.
in Java, compares it with the monitors of Brinch Hansen        9.   Buhr, M., M. Fortier, and M. H. Coffin (1995):
and Hoare, and concludes that Java synchronization is               "Monitor classification," Computing Surveys, 27
not as well developed as the Brinch Hansen and Hoare                (1), 63­108.
monitors.                                                      10.  Chandy, K. M., and J. Misra (1988): Parallel
     A     synchronization  primitive  or  construct       is       Program Design: A Foundation, Addison-Wesley,
complete if it can be used to implement all process syn-            Reading, Mass.
chronization problems. The completeness of semaphores          11.  Christopher, T. W., and G. K. Thiruvathukal
is discussed in Patil (1971), Lipton (1974), and Kosaraju           (2001): Multithreaded and Networked
(1975).                                                             Programming, Sun Microsystems.
     Brinch Hansen (1973, 1977) and Ben Ari (1982,             12.  Courtois, P. J., F. Heymans, and D. L. Parnas
2006) discuss the methodology for building concurrent               (1971): "Concurrent control with readers and
programs. Owicki and Gries (1976) and Francez and                   writers," Communications of the ACM, 14 (10),
Pneuli (1978) deal with the methodology of proving the              667­668.
correctness of concurrent programs.                            13.  Dijkstra, E. W. (1965): "Cooperating sequential
     Vahalia (1996) and Stevens and Rago (2005) dis-                processes," Technical Report EWD-123,
cuss process synchronization in Unix, Beck et al. (2002),           Technological University, Eindhoven.
Bovet and Cesati (2005), and Love (2005), discuss syn-         14.  Eisenberg, M. A., and M. R. McGuire (1972):
chronization in Linux, Mauro and McDougall (2006)                   "Further comments on Dijkstra's concurrent
discusses  synchronization  in  Solaris,   while  Richter           programming control problem," Communications
(1999) and Russinovich and Solomon (2005) discuss                   of the ACM, 15(11), 999.
synchronization features in Windows.                           15.  Francez, N., and A. Pneuli (1978): "A proof
                                                                    method for cyclic programs," Acta Informatica, 9,
                                                                    133­157.
1.   Beck, M., H. Bohme, M. Dziadzka, U. Kunitz,               16.  Hoare, C. A. R. (1972): "Towards a theory of
     R. Magnus, C. Schroter, and D. Verworner                       parallel programming," in Operating Systems
     (2002): Linux Kernel Programming, Pearson                      Techniques, C.A.R. Hoare and R.H. Perrot (eds.),
     Education, New York.                                           Academic Press, London, 1972.



                                                             Chapter 6        Process Synchronization            227
17.  Hoare, C. A. R (1974): "Monitors: an operating     26.  Owicki, S., and D. Gries (1976): "Verifying
     system structuring concept," Communications of          properties of parallel programs: An axiomatic
     the ACM, 17(10), 549­557.                               approach," Communications of the ACM, 19,
18.  Kosaraju, S. (1973): "Limitations of Dijkstra's         279­285.
     semaphore primitives and petri nets," Operating    27.  Patil, S. (1971): "Limitations and capabilities of
     Systems Review, 7, 4, 122­126.                          Dijkstra's semaphore primitives for co-ordination
19.  Lamport, L. (1974): "A new solution of Dijkstra's       among processes," Technical Report, MIT.
     concurrent programming problem," Communica-        28.  Peterson, G. L. (1981): "Myths about the mutual
     tions of the ACM, 17, 453­455.                          exclusion problem," Information Processing
20.  Lamport, L. (1979): "A new approach to proving          Letters, 12, 3.
     the correctness of multiprocess programs," ACM     29.  Raynal, M. (1986): Algorithms for Mutual
     Transactions on Programming Languages and               Exclusion, MIT Press, Cambridge, Mass.
     Systems, 1, 84­97.                                 30.  Richter, J. (1999): Programming Applications for
21.  Lamport, L. (1986): "The mutual exclusion               Microsoft Windows, 4th ed., Microsoft Press,
     problem," Communications of the ACM, 33 (2),            Redmond, Wash.
     313­348.                                           31.  Russinovich, M. E., and D. A. Solomon (2005):
22.  Lamport, L. (1991): "The mutual exclusion               Microsoft Windows Internals, 4th ed., Microsoft
     problem has been solved," ACM Transactions on           Press, Redmond, Wash.
     Programming Languages and Systems, 1,              32.  Stevens, W. R., and S. A. Rago (2005): Advanced
     84­97.                                                  Programming in the Unix Environment, 2nd ed.,
23.  Lipton, R. (1974): "On synchronization primitive        Addison Wesley, Reading, Mass,.
     systems," Ph.D. Thesis, Carnegie-Mellon            33.  Vahalia, U. (1996): Unix Internals--The New
     University.                                             Frontiers, Prentice Hall, Englewood
24.  Love, R. (2005): Linux Kernel Development, 2nd          Cliffs, N.J.
     ed., Novell Press.
25.  Mauro, J., and R. McDougall (2006): Solaris
     Internals, 2nd ed., Prentice Hall, Englewood
     Cliffs, N.J.
