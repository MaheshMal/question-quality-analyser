Structure of Operating Systems


    4  Chapter
       Structure of
       Operating Systems
       D uring the lifetime of an operating system, we can expect several changes
                 to take place in computer systems and computing environments. To
                 adapt an operating system to these changes, it should be easy to imple-
       ment the OS on a new computer system, and to add new functionalities to it.
       These requirements are called portability and extensibility of an operating system,
       respectively.
            Early operating systems were tightly integrated with the architecture of a
       specific computer system. This feature affected their portability. Modern oper-
       ating systems implement the core of an operating system in the form of a kernel
       or a microkernel, and build the rest of the operating system by using the services
       offered by the core. This structure restricts architecture dependencies to the core
       of the operating system, hence portability of an operating system is determined
       by the properties of its kernel or microkernel. Extensibility of an OS is determined
       by the nature of services offered by the core.
            The structure of an operating system concerns the nature of the OS core
       and other parts of the operating system, and their interactions with one another.
       We describe different philosophies concerning the structure of an operating sys-
       tem and discuss their influence on portability and extensibility of operating
       systems.
       4.1  OPERATION OF AN OS                                                               ·
       When a computer is switched on, the boot procedure analyzes its configuration--
       CPU type, memory size, I/O devices, and details of other hardware connected
       to the computer (see Section 1.3). It then loads a part of the OS in memory,
       initializes its data structures with this information, and hands over control of the
       computer system to it.
            Figure 4.1 is a schematic diagram of OS operation (see Section 2.3). An
       event like I/O completion or end of a time slice causes an interrupt. When a
       process makes a system call, e.g., to request resources or start an I/O oper-
       ation, it too leads to an interrupt called a software interrupt. The interrupt
80



                                                                         Chapter  4  Structure  of  Operating  Systems  81
                         A condition in             A request by a
                      hardware causes a           process causes a
                      hardware interrupt        software interrupt
                                         Context
                                          save
                 I/O        ...          Memory         ...         ...              Event
            handler                      handler                                     handlers
                                         Scheduler
                                 CPU is switched to
                                 the scheduled process
Figure 4.1  Overview of OS operation.
Table 4.1   Functions       of   an OS
Function                         Description
Process management               Initiation and termination of processes, scheduling
Memory management                Allocation and deallocation of memory, swapping,
                                 virtual memory management
I/O management                   I/O interrupt servicing, initiation of I/O operations,
                                 optimization of I/O device performance
File management                  Creation, storage and access of files
Security and protection          Preventing interference with processes and resources
Network management               Sending and receiving of data over the network
action switches the CPU to an interrupt servicing routine. The interrupt servicing
routine performs a context save action to save information about the inter-
rupted program and activates an event handler, which takes appropriate actions
to handle the event. The scheduler then selects a process and switches the CPU
to it. CPU switching occurs twice during the processing of an event--first to
the kernel to perform event handling and then to the process selected by the
scheduler.
The functions of an OS are thus implemented by event handlers when they are
activated by interrupt servicing routines. Table 4.1 summarizes these functions,
which primarily concern management of processes and resources, and prevention
of interference with them.



82  Part 1  Overview
                 4.2   STRUCTURE OF AN OPERATING SYSTEM                                              ·
                 4.2.1 Policies and Mechanisms
                 In determining how an operating system is to perform one of its functions, the
                 OS designer needs to think at two distinct levels:
                    ·  Policy: A policy is the guiding principle under which the operating system
                       will perform the function.
                    ·  Mechanism: A mechanism is a specific action needed to implement a policy.
                       A policy decides what should be done, while a mechanism determines how
                 something should be done and actually does it. A policy is implemented as a
                 decision-making module that decides which mechanism modules to call under
                 what conditions. A mechanism is implemented as a module that performs a
                 specific action. The following example identifies policies and mechanisms in
                 round-robin scheduling.
·
    Example 4.1  Policies and Mechanisms in Round-Robin Scheduling
                 In scheduling, we would consider the round-robin technique (Section 3.6) to
                 be a policy. The following mechanisms would be needed to implement the
                 round-robin scheduling policy:
                    Maintain a queue of ready processes
                    Switch the CPU to execution of the selected process (this action is called
                    dispatching).
                 ·
                       The priority-based scheduling policy, which is used in multiprogramming
                 systems (see Section 3.5.1), would also require a mechanism for maintaining infor-
                 mation about ready processes; however, it would be different from the mechanism
                 used in round-robin scheduling because it would organize information according
                 to process priority. The dispatching mechanism, however, would be common to
                 all scheduling policies.
                       Apart  from  mechanisms     for  implementing  specific  process  or  resource
                 management policies, the OS also has mechanisms for performing housekeep-
                 ing actions. The context save action mentioned in Section 4.1 is implemented as
                 a mechanism.
                 4.2.2 Portability and Extensibility of Operating Systems
                 The design and implementation of operating systems involves huge financial
                 investments. To protect these investments, an operating system design should have
                 a lifetime of more than a decade. Since several changes will take place in com-
                 puter architecture, I/O device technology, and application environments during



                                                           Chapter 4  Structure of Operating  Systems  83
this time, it should be possible to adapt an OS to these changes. Two features are
important in this context--portability and extensibility.
   Porting is the act of adapting software for use in a new computer system.
Portability refers to the ease with which a software program can be ported--it is
inversely proportional to the porting effort. Extensibility refers to the ease with
which new functionalities can be added to a software system.
   Porting of an OS implies changing parts of its code that are architecture-
dependent so that the OS can work with new hardware. Some examples of
architecture-dependent data and instructions in an OS are:
·  An interrupt vector contains information that should be loaded in various
   fields of the PSW to switch the CPU to an interrupt servicing routine (see
   Section 2.2.5). This information is architecture-specific.
·  Information concerning memory protection and information to be pro-
   vided to the memory management unit (MMU) is architecture-specific (see
   Sections 2.2.2 and 2.2.3).
·  I/O instructions used to perform an I/O operation are architecture-specific.
The architecture-dependent part of an operating system's code is typically asso-
ciated with mechanisms rather than with policies. An OS would have high porta-
bility if its architecture-dependent code is small in size, and its complete code is
structured such that the porting effort is determined by the size of the architecture-
dependent code, rather than by the size of its complete code. Hence the issue
of OS portability is addressed by separating the architecture-dependent and
architecture-independent parts of an OS and providing well-defined interfaces
between the two parts.
   Extensibility of an OS is needed for two purposes: for incorporating new
hardware in a computer system--typically new I/O devices or network adapters--
and for providing new functionalities in response to new user expectations. Early
operating systems did not provide either kind of extensibility. Hence even addi-
tion of a new I/O device required modifications to the OS. Later operating systems
solved this problem by adding a functionality to the boot procedure. It would
check for hardware that was not present when the OS was last booted, and either
prompt the user to select appropriate software to handle the new hardware, typ-
ically a set of routines called a device driver that handled the new device, or itself
select such software. The new software was then loaded and integrated with the
kernel so that it would be invoked and used appropriately. Modern operating
systems go a step further by providing a plug-and-play capability, whereby new
hardware can be added even while an OS is in operation. The OS handles the
interrupt caused by addition of new hardware, selects the appropriate software,
and integrates it with the kernel.
   Lack of extensibility leads to difficulties in adapting an OS to new user
expectations. Several examples of such difficulties can be found in the history
of operating systems. In 1980s and 1990s, PC users desired a new feature for
setting up several sessions with an operating system at the same time. Several well-
known operating systems of that time, e.g., MS-DOS, had difficulties providing



84  Part 1  Overview
            it because they lacked sufficient extensibility. A similar difficulty was experienced
            by the Unix operating system while supporting multiprocessor computer systems.
            We discuss provisions for extensibility in Section 4.7.
            4.3       OPERATING SYSTEMS WITH MONOLITHIC STRUCTURE                                     ·
            An OS is a complex software that has a large number of functionalities and may
            contain millions of instructions. It is designed to consist of a set of software
            modules, where each module has a well-defined interface that must be used to
            access any of its functions or data. Such a design has the property that a mod-
            ule cannot "see" inner details of functioning of other modules. This property
            simplifies design, coding and testing of an OS.
                      Early operating systems had a monolithic structure, whereby the OS formed
            a single software layer between the user and the bare machine, i.e., the computer
            system's hardware (see Figure 4.2). The user interface was provided by a com-
            mand interpreter. The command interpreter organized creation of user processes.
            Both the command interpreter and user processes invoked OS functionalities and
            services through system calls.
                      Two kinds of problems with the monolithic structure were realized over a
            period of time. The sole OS layer had an interface with the bare machine. Hence
            architecture-dependent code was spread throughout the OS, and so there was
            poor portability. It also made testing and debugging difficult, leading to high
            costs of maintenance and enhancement. These problems led to the search for
            alternative ways to structure an OS. In the following sections we discuss three
            methods of structuring an OS that have been implemented as solutions to these
            problems.
            ·         Layered structure: The layered structure attacks the complexity and cost of
                      developing and maintaining an OS by structuring it into a number of layers
                      (see Section 4.4). The THE multiprogramming system of the 1960s is a well-
                      known example of a layered OS.
            ·         Kernel-based  structure:  The   kernel-based  structure  confines  architecture
                      dependence to a small section of the OS code that constitutes the kernel (see
                      Section 4.6), so that portability is increased. The Unix OS has a kernel-based
                      structure.
                                                User                User
                                        interface                   process
                                                      OS layer
                                                      Bare machine
            Figure 4.2  Monolithic OS.



                                                                  Chapter 4  Structure of Operating  Systems  85
·  Microkernel-based OS structure: The microkernel provides a minimal set of
   facilities and services for implementing an OS. Its use provides portability.
   It also provides extensibility because changes can be made to the OS without
   requiring changes in the microkernel (see Section 4.7).
4.4     LAYERED DESIGN OF OPERATING SYSTEMS                                                                   ·
The monolithic OS structure suffered from the problem that all OS components
had to be able to work with the bare machine. This feature increased the cost
and effort in developing an OS because of the large semantic gap between the
operating system and the bare machine.
Definition 4.1 Semantic Gap                The mismatch between the nature of opera-
tions needed in the application and the nature of operations provided in the
machine.
     The semantic gap can be illustrated as follows: A machine instruction imple-
ments a machine-level primitive operation like arithmetic or logical manipulation
of operands. An OS module may contain an algorithm, say, that uses OS-level
primitive operations like saving the context of a process and initiating an I/O
operation. These operations are more complex than the machine-level primi-
tive operations. This difference leads to a large semantic gap, which has to be
bridged through programming. Each operation desired by the OS now becomes
a sequence of instructions, possibly a routine (see Figure 4.3). It leads to high
programming costs.
     The semantic gap between an OS and the machine on which it operates can
be reduced by either using a more capable machine--a machine that provides
instructions to perform some (or all) operations that operating systems have to
perform--or by simulating a more capable machine in the software. The former
approach is expensive. In the latter approach, however, the simulator, which is a
        Operating                          Program                I/O
             system          management                           management
             Semantic
             gap
             Bare            Arithmetic             Logical       I/O
             machine         instructions           instructions  instructions
Figure  4.3  Semantic  gap.



86  Part 1  Overview
            program, executes on the bare machine and mimics a more powerful machine that
            has many features desired by the OS. This new "machine" is called an extended
            machine, and its simulator is called the extended machine software. Now the
            OS interfaces with the extended machine rather than with the bare machine;
            the extended machine software forms a layer between the OS and the bare
            machine.
                      The basic discipline in designing a layered OS is that the routines of one
            layer must use only the facilities of the layer directly below it--that is, no layer
            in the structure can be bypassed. Further, access to routines of a lower layer
            must take place strictly through the interface between layers. Thus, a routine
            situated in one layer does not "know" addresses of data structures or instruc-
            tions in the lower layer--it only knows how to invoke a routine of the lower
            layer. This property, which we will call information hiding, prevents misuse or
            corruption of one layer's data by routines situated in other layers of the OS.
            During debugging, localization of errors becomes easy since the cause of an
            error in a layer, e.g., an incorrect value in its data element, must lie within that
            layer itself. Information hiding also implies that an OS layer may be modified
            without affecting other layers. These features simplify testing and debugging
            of an OS.
                      Figure 4.4 illustrates a two-layered OS. The extended machine provides
            operations like context save, dispatching, swapping, and I/O initiation. The
            operating system layer is located on top of the extended machine layer. This
            arrangement considerably simplifies the coding and testing of OS modules by
            separating the algorithm of a function from the implementation of its prim-
            itive operations. It is now easier to test, debug, and modify an OS module
            than in a monolithic OS. We say that the lower layer provides an abstraction
            that is the extended machine. We call the operating system layer the top layer
            of the OS.
                      The layered structures of operating systems have been evolved in various
            ways--using different abstractions and a different number of layers. Example 4.2
            describes the THE multiprogramming OS, which uses a multilayered structure
            and provides a process as an abstraction in the lowest layer.
                      Operating                   Process                  I/O
                           system                 management             management
                           Semantic
                           gap
                      Extended           Context              Dispatch     Perform
                           machine       save                 a process    I/O
                                                           Bare machine
            Figure    4.4  Layered   OS  design.



                                                        Chapter 4    Structure of Operating Systems        87
                                                                                                           ·
Structure of the THE Multiprogramming System                                                 Example  4.2
The THE multiprogramming system was developed at Technische Hogeschool
Eindhoven in the Netherlands by Dijkstra and others using a layered design.
Table 4.2 shows the hierarchy of layers in the THE system.
Layer 0 of the system handles processor allocation to implement multi-
programming. This function involves keeping track of process states and
switching between processes, using priority-based scheduling. Layers above
layer 0 need not concern themselves with these issues. In fact, they can be
oblivious to the presence of multiple processes in the system.
Layer 1 performs memory management. It implements a memory hierar-
chy consisting of the memory and a drum, which is a secondary storage device
(see Section 2.2.3). Details of transfer between the memory and the drum need
not concern the rest of the OS.
Layer 2 implements communication between a process and the opera-
tor's console by allocating a virtual console to each process. Layer 3 performs
I/O management. Intricacies of I/O programming (see Section 14.4) are thus
hidden from layer 4, which is occupied by user processes.
                                                                                        ·
The  layered  approach  to       OS  design    suffers  from  three  problems.          The
operation of a system may be slowed down by the layered structure. Recall that
each layer can interact only with adjoining layers. It implies that a request for OS
service made by a user process must move down from the highest numbered layer
to the lowest numbered layer before the required action is performed by the bare
machine. This feature leads to high overhead.
The second problem concerns difficulties in developing a layered design.
Since a layer can access only the immediately lower layer, all features and facilities
needed by it must be available in lower layers. This requirement poses a problem
in the ordering of layers that require each other's services. This problem is often
solved by splitting a layer into two and putting other layers between the two halves.
For example, a designer may wish to put process handling functions in one layer
and memory management in the next higher layer. However, memory allocation
is required as a part of process creation. To overcome this difficulty, process han-
dling can be split into two layers. One layer would perform process management
functions like context save, switching, scheduling, and process synchronization.
Table 4.2     Layers in the THE Multiprogramming System
     Layer    Description
Layer 0       Processor allocation and multiprogramming
Layer 1       Memory and drum management
Layer 2       Operator­process communication
Layer 3       I/O management
Layer 4       User processes



88  Part 1  Overview
            This layer would continue to be lower than the memory management layer. The
            other layer would perform process creation. It would be located above the memory
            management layer.
                      The third problem concerns stratification of OS functionalities. Stratification
            occurs because each functionality has to be divided into parts that fit into different
            layers of a layered OS. These parts must use interfaces between the various layers
            to communicate with one another. For example, consider a certain functionality
            F of the OS that consists of two modules, Fl1 and Fl2 , belonging to layers l1 and l2
            respectively. If layer l2 can be entered only through an interrupt, Fl1 must cause
            an interrupt to communicate with Fl2 . This fact can lead to a complex design
            and a loss of execution efficiency. Stratification also leads to poor extensibility
            because addition of a new functionality requires new code to be added in many
            layers of the OS, which, in turn, may require changes in the layer interfaces.
                      It may be noted that the design of a multilayered OS does not focus on
            separating architecture-dependent parts of OS code; for example, four out of the
            five layers of the THE multiprogramming system described in Table 4.2 contain
            architecture-dependent parts. Thus, a layered structure does not guarantee high
            portability.
            4.5       VIRTUAL MACHINE OPERATING SYSTEMS                                                ·
            Different classes of users need different kinds of user service. Hence running a sin-
            gle OS on a computer system can disappoint many users. Operating the computer
            under different OSs during different periods is not a satisfactory solution because
            it would make accessible services offered under only one of the operating systems
            at any time. This problem is solved by using a virtual machine operating sys-
            tem (VM OS) to control the computer system. The VM OS creates several virtual
            machines. Each virtual machine is allocated to one user, who can use any OS of his
            own choice on the virtual machine and run his programs under this OS. This way
            users of the computer system can use different operating systems at the same time.
            We call each of these operating systems a guest OS and call the virtual machine
            OS the host OS. The computer used by the VM OS is called the host machine.
                      A virtual machine is a virtual resource (see Section 1.3.2). Let us consider a
            virtual machine that has the same architecture as the host machine; i.e., it has a vir-
            tual CPU capable of executing the same instructions, and similar memory and I/O
            devices. It may, however, differ from the host machine in terms of some elements
            of its configuration like memory size and I/O devices. Because of the identical
            architectures of the virtual and host machines, no semantic gap exists between
            them, so operation of a virtual machine does not introduce any performance loss
            (contrast this with the use of the extended machine layer described in Section 4.4);
            software intervention is also not needed to run a guest OS on a virtual machine.
                      The VM OS achieves concurrent operation of guest operating systems
            through an action that resembles process scheduling--it selects a virtual machine
            and arranges to let the guest OS running on it execute its instructions on the CPU.
            The guest OS in operation enjoys complete control over the host machine's



                                                      Chapter 4      Structure of Operating  Systems       89
environment, including interrupt servicing. The absence of a software layer
between the host machine and guest OS ensures efficient use of the host machine.
A guest OS remains in control of the host machine until the VM OS decides
to switch to another virtual machine, which typically happens in response to
an interrupt. The VM OS can employ the timer to implement time-slicing and
round-robin scheduling of guest OSs.
A somewhat complex arrangement is needed to handle interrupts that arise
when a guest OS is in operation. Some of the interrupts would arise in its own
domain, e.g., an I/O interrupt from a device included in its own virtual machine,
while others would arise in the domains of other guest OSs. The VM OS can
arrange to get control when an interrupt occurs, find the guest OS whose domain
the interrupt belongs to, and "schedule" that guest OS to handle it. However, this
arrangement incurs high overhead because of two context switch operations--the
first context switch passes control to the VM OS, and the second passes control
to the correct guest OS. Hence the VM OS may use an arrangement in which the
guest OS in operation would be invoked directly by interrupts arising in its own
domain. It is implemented as follows: While passing control to a guest operating
system, the VM OS replaces its own interrupt vectors (see Section 2.2.5) by those
defined in the guest OS. This action ensures that an interrupt would switch the
CPU to an interrupt servicing routine of the guest OS. If the guest OS finds that
the interrupt did not occur in its own domain, it passes control to the VM OS
by making a special system call "invoke VM OS." The VM OS now arranges to
pass the interrupt to the appropriate guest OS. When a large number of virtual
machines exists, interrupt processing can cause excessive shuffling between virtual
machines, hence the VM OS may not immediately activate the guest OS in whose
domain an interrupt occurred--it may simply note occurrence of interrupts that
occurred in the domain of a guest OS and provide this information to the guest
OS the next time it is "scheduled."
Example 4.3 describes how IBM VM/370--a well-known VM OS of the
1970s--operates.
                                                                                                           ·
Structure of VM/370                                                                          Example  4.3
Figure  4.5  shows  three  of        the  guest  OSs  supported  by  VM/370.         The
Conversational Monitor System (CMS) is a single-user operating system, while
the OS/370 and DOS/370 are multiprogramming operating systems. A user
process is unaware of the presence of the VM/370--it sees only the guest OS
that it uses. To prevent interference between the guest OSs, the CPU is put in
the user mode while executing a guest OS. Initiation of I/O operations, which
involves use of privileged instructions, is handled as follows: When the kernel
of a guest OS executes an I/O instruction, it appears as an attempt to execute a
privileged instruction while the CPU is in the user mode, so it causes a program
interrupt. The interrupt is directed to the VM/370 rather than to the guest OS.
The VM/370 now initiates the I/O operation by executing the I/O instruction
that had caused the interrupt.
                                                                                     ·



90  Part 1  Overview
                                        CMS        OS/370         DOS/370
                                                   VM/370
            Figure 4.5  Virtual machine operating system VM/370.
                      Distinction between kernel and user modes of the CPU causes some diffi-
            culties in the use of a VM OS. The VM OS must protect itself from guest OSs,
            so it must run guest OSs with the CPU in the user mode. However, this way
            both a guest OS and user processes under it run in the user mode, which makes
            the guest OS vulnerable to corruption by a user process. The Intel 80x86 family
            of computers has a feature that provides a way out of this difficulty. The 80x86
            computers support four execution modes of the CPU. Hence the host OS can
            run with the CPU in the kernel mode, a guest OS can execute processes running
            under it with the CPU in the user mode but can itself run with the CPU in one
            of the intermediate modes.
                      Virtualization is the process of mapping the interfaces and resources of a
            virtual machine into the interfaces and resources of the host machine. Full virtu-
            alization would imply that the host machine and a virtual machine have identical
            capabilities, hence an OS can operate identically while running on a bare machine
            and on a virtual machine supported by a VM OS. However, full virtualization
            may weaken security. In Example 4.3, we saw how VM/370 lets a guest OS execute
            a privileged instruction, but its execution causes an interrupt and VM/370 itself
            executes the instruction on behalf of the guest OS. This arrangement is insecure
            because VM/370 cannot determine whether use of the privileged instruction is
            legitimate--it would be legitimate if a guest OS used it, but illegitimate if a user
            process used it.
                      Modern virtual machine environments employ the technique of paravirtual-
            ization to overcome the problems faced in full virtualization. Paravirtualization
            replaces a nonvirtualizable instruction, i.e., an instruction that cannot be made
            available in a VM, by easily virtualized instructions. For example, the security
            issue in VM/370 could be resolved through paravirtualization as follows: The
            privileged instructions would not be included in a virtual machine. Instead, the
            virtual machine would provide a special instruction for use by a guest OS that
            wished to execute a privileged instruction. The special instruction would cause
            a software interrupt and pass information about the privileged instruction the
            guest OS wished to execute to the VM OS, and the VM OS would execute the
            privileged instruction on behalf of the guest OS. The host OS, guest OS, and user
            processes would use different execution modes of the CPU so that the host OS
            would know whether the special instruction in the virtual machine was used by
            a guest OS or by a user process--the latter usage would be considered illegal.
            Paravirtualization has also been used  to enhance performance of a host OS.



                                                       Chapter 4  Structure of Operating  Systems  91
The kernel of an OS typically puts the CPU into an idle loop when none of
the user processes in the OS wishes to use the CPU. However, CPU time of
the host machine would be wasted when a guest OS enters into an idle loop.
Hence paravirtualization could be employed to provide a special instruction
in the virtual machine to notify this condition to the host OS, so that the
host OS could take away the CPU from the guest OS for a specified period
of time.
   Use of paravirtualization implies that a virtual machine would differ from the
host machine, so the code of a guest OS would have to be modified to avoid use
of nonvirtualizable instructions. It can be done by porting a guest OS to operate
under the VM OS. Alternatively, it can be achieved by employing the technique
of dynamic binary translation for the kernel of a guest OS, which replaces a por-
tion of kernel code that contains nonvirtualizable instructions by code that does
not contain such instructions. To reduce the overhead of this arrangement, the
modified kernel code is cached so that binary translation does not have to be
repeated often.
   Virtual machines are employed for diverse purposes:
·  To use an existing server for a new application that requires use of a different
   operating system. This is called workload consolidation; it reduces the hard-
   ware and operational cost of computing by reducing the number of servers
   needed in an organization.
·  To provide security and reliability for applications that use the same host
   and the same OS. This benefit arises from the fact that virtual machines of
   different applications cannot access each other's resources.
·  To test a modified OS (or a new version of application code) on a server
   concurrently with production runs of that OS.
·  To     provide  disaster  management  capabilities  by  transferring  a   virtual
   machine from a server that has to shut down because of an emergency to
   another server available on the network.
   A VM OS is large, complex and expensive. To make the benefits of virtual
machines available widely at a lower cost, virtual machines are also used without
a VM OS. Two such arrangements are described in the following.
Virtual Machine Monitors (VMMs)    A VMM, also called a hypervisor, is a soft-
ware layer that operates on top of a host OS. It virtualizes the resources of the
host computer and supports concurrent operation of many virtual machines.
When a guest OS is run in each virtual machine provided by a VMM, the host
OS and the VMM together provide a capability that is equivalent of a VM OS.
VMware and XEN are two VMMs that aim at implementing hundreds of guest
OSs on a host computer while ensuring that a guest OS suffers only a marginal
performance  degradation     when  compared  to   its  implementation    on  a  bare
machine.
Programming Language Virtual Machines        Programming languages have used
virtual machines to obtain some of the benefits discussed earlier. In the 1970s, the



92  Part 1  Overview
            Pascal programming language employed a virtual machine to provide portability.
            The virtual machine had instructions called P-code instructions that were well-
            suited to execution of Pascal programs. It was implemented in the software
            in the form of an interpreter for P-code instructions. A compiler converted
            a Pascal program into a sequence of P-code instructions, and these could be
            executed on any computer that had a P-code interpreter. The virtual machine
            had a small number of instructions, so the interpreter was compact and eas-
            ily portable. This feature facilitated widespread use of Pascal in the 1970s.
            However, use of the VM incurred a substantial performance penalty due to
            the       semantic  gap  between  P-code          instructions  and  instructions  in  the  host
            computer.
                      The Java programming language employs a virtual machine to provide secu-
            rity and reliability. A Java program consists of objects, whose structure and
            behavior is specified in classes. Each class is compiled into a bytecode form,
            where the bytecode is a sequence of instructions for the Java virtual machine
            (JVM). During execution of an application coded in Java, the class loader is
            activated whenever an object of a new class is encountered. The loader fetches
            the bytecode form of the class, either from a library or from the Internet, and
            verifies that the class conforms to the security and reliability standards--that
            it has a valid digital signature (see Section 21.3.2), and does not use features
            such as pointer arithmetic. The application would be aborted if a class file
            fails any of these checks. If several Java applications run on the same host,
            each of them would execute in its own virtual machine, hence their opera-
            tion cannot cause mutual interference. The performance penalty implicit in
            use of the virtual machine can be offset by implementing the JVM in the
            hardware.
            4.6       KERNEL-BASED OPERATING SYSTEMS                                                    ·
            Figure 4.6 is an abstract view of a kernel-based OS. The kernel is the core of the OS;
            it provides a set of functions and services to support various OS functionalities.
            The rest of the OS is organized as a set of nonkernel routines, which implement
            operations on processes and resources that are of interest to users, and a user
                                              User interface
                                              Nonkernel routines
                                                              Kernel
                                              Bare machine
            Figure 4.6  Structure    of  a  kernel-based OS.



                         Chapter 4                                 Structure of Operating  Systems  93
interface. Recall from Section 4.1 and Figure 4.1 that the operation of the kernel
is interrupt-driven. The kernel gets control when an interrupt such as a timer
interrupt or an I/O completion interrupt notifies occurrence of an event to it, or
when the software-interrupt instruction is executed to make a system call. When
the interrupt occurs, an interrupt servicing routine performs the context save
function and invokes an appropriate event handler, which is a nonkernel routine
of the OS.
A system call may be made by the user interface to implement a user
command, by a process to invoke a service in the kernel, or by a nonkernel routine
to invoke a function of the kernel. For example, when a user issues a command to
execute the program stored in some file, say file alpha, the user interface makes
a system call, and the interrupt servicing routine invokes a nonkernel routine
to set up execution of the program. The nonkernel routine would make system
calls to allocate memory for the program's execution, open file alpha, and load
its contents into the allocated memory area, followed by another system call to
initiate operation of the process that represents execution of the program. If a
process wishes to create a child process to execute the program in file alpha, it,
too, would make a system call and identical actions would follow.
The historical motivations for the kernel-based OS structure were portabil-
ity of the OS and convenience in the design and coding of nonkernel routines.
Portability of the OS is achieved by putting architecture-dependent parts of
OS code--which typically consist of mechanisms--in the kernel and keeping
architecture-independent parts of code outside it, so that the porting effort is
limited only to porting of the kernel. The kernel is typically monolithic to ensure
efficiency; the nonkernel part of an OS may be monolithic, or it may be further
structured into layers.
Table 4.3 contains a sample list of functions and services offered by the kernel
to support various OS functionalities. These functions and services provide a set
of abstractions to the nonkernel routines; their use simplifies design and coding of
nonkernel routines by reducing the semantic gap faced by them (see Section 4.4).
For example, the I/O functions of Table 4.3 collectively implement the abstraction
of virtual devices (see Section 1.3.2). A process is another abstraction provided
by the kernel.
A kernel-based design may suffer from stratification analogous to the layered
OS design (see Section 4.4) because the code to implement an OS command
may contain an architecture-dependent part, which is typically a mechanism that
would be included in the kernel, and an architecture-independent part, which
is typically the implementation of a policy that would be kept outside the ker-
nel. These parts would have to communicate with one another through system
calls, which would add to OS overhead because of interrupt servicing actions.
Consider the command to initiate execution of the program in a file named
alpha. As discussed earlier, the nonkernel routine that implements the command
would make four system calls to allocate memory, open file alpha, load the pro-
gram contained in it into memory, and initiate its execution, which would incur
considerable overhead. Some operating system designs reduce OS overhead by
including the architecture-independent part of a function's code also in the kernel.



94  Part 1  Overview
            Table 4.3          Typical Functions and Services Offered by the Kernel
               OS functionality                Examples of kernel functions and services
               Process management              Save context of the interrupted program, dispatch a
                                               process, manipulate scheduling lists
               Process communication           Send and receive interprocess messages
               Memory management               Set memory protection information, swap-in/
                                               swap-out, handle page fault (that is, "missing from
                                               memory" interrupt of Section 1.4)
               I/O management                  Initiate I/O, process I/O completion interrupt,
                                               recover from I/O errors
               File management                 Open a file, read/write data
               Security and protection         Add authentication information for a new user,
                                               maintain information for file protection
               Network management              Send/receive data through a message
            Thus, the nonkernel routine that initiated execution of a program would become
            a  part   of  the  kernel.  Other  such  examples  are  process  scheduling         policies,
            I/O scheduling policies of device drivers, and memory management policies.
            These inclusions reduce OS overhead; however, they also reduce portability of
            the OS.
                      Kernel-based operating systems have poor extensibility because addition of
            a new functionality to the OS may require changes in the functions and services
            offered by the kernel.
            4.6.1 Evolution of Kernel-Based Structure
                      of Operating Systems
            The structure of kernel-based operating systems evolved to offset some of its
            drawbacks. Two steps in this evolution were dynamically loadable kernel modules
            and user-level device drivers.
                      To provide dynamically loadable kernel modules, the kernel is designed as a
            set of modules that interact among themselves through well-specified interfaces.
            A base kernel consisting of a core set of modules is loaded when the system is
            booted. Other modules, which conform to interfaces of the base kernel, are loaded
            when their functionalities are needed, and are removed from memory when they
            are no longer needed. Use of loadable modules conserves memory during OS
            operation because only required modules of the kernel are in memory at any
            time. It also provides extensibility, as kernel modules can be modified separately
            and new modules can be added to the kernel easily. Use of loadable kernel modules
            has a few drawbacks too. Loading and removal of modules fragments memory,
            so the kernel has to perform memory management actions to reduce its memory
            requirement. A buggy module can also crash a system. Loadable kernel modules
            are used to implement device drivers for new I/O devices, network adapters, or



                                                     Chapter 4     Structure of Operating  Systems  95
new file systems, which are simply device drivers in many operating systems;
and to add new system calls to the kernel. The Linux and Solaris systems have
incorporated support for dynamically loadable kernel modules (see Sections 4.8.2
and 4.8.3).
     A device driver handles a specific class of I/O devices. Device drivers consti-
tute the most dynamically changing part of an OS as a result of rapid changes
in the I/O device interfaces, hence the ease with which they could be tested
and added to an OS would determine the reliability and extensibility of the
OS. Dynamic loading of device drivers enhances both these aspects; however,
it is not adequate because a device driver would operate with the privileges
of the kernel, so a buggy device driver could disrupt operation of the OS and
cause frequent boot-ups. Enabling a device driver to operate in the user mode
would overcome this difficulty. Such a device driver is called a user-level device
driver.
     User-level  device  drivers  provide  ease  of  development,  debugging,         and
deployment and robustness, since both the code of the kernel and its oper-
ation are unaffected by presence of the user-level driver. However, they pose
performance problems. Early user-level drivers were found to cause a drop in
the I/O throughput or an increase in the CPU time consumed by I/O opera-
tions. Both of these resulted from the large number of system calls needed to
implement an I/O operation, e.g., the device driver had to make system calls
to set up and dismantle the DMA for the I/O operation, to wake up the user
process waiting for the I/O operation to complete, and to return control to
the kernel at the end of its operation. Later hardware and software develop-
ments have overcome the performance problems through a variety of means.
The setting up and dismantling actions have been simplified by presence of the
IOMMU unit, and system calls have been speeded up through fast system call
support.
4.7  MICROKERNEL-BASED OPERATING SYSTEMS                                                            ·
Putting all architecture-dependent code of the OS into the kernel provides
good portability. However, in practice, kernels also include some architecture-
independent code. This feature leads to several problems. It leads to a large
kernel size, which detracts from the goal of portability. It may also necessitate
kernel modification to incorporate new features, which causes low extensibil-
ity. A large kernel supports a large number of system calls. Some of these
calls may be used rarely, and so their implementations across different ver-
sions of the kernel may not be tested thoroughly. This compromises reliability of
the OS.
     The microkernel was developed in the early 1990s to overcome the problems
concerning portability, extensibility, and reliability of kernels. A microkernel is
an essential core of OS code, thus it contains only a subset of the mechanisms
typically included in a kernel and supports only a small number of system calls,
which are heavily tested and used. This feature enhances portability and reliability



96  Part 1  Overview
                                                     Servers            User processes
                                          Round-
                                          robin      ...      Memory    ...
                                          process             handler
                                          scheduler
                                                          Microkernel
                                                          Bare machine
            Figure    4.7  Structure  of  microkernel-based operating   systems.
            of the microkernel. Less essential parts of OS code are outside the microkernel
            and use its services, hence these parts could be modified without affecting the
            kernel; in principle, these modifications could be made without having to reboot
            the OS! The services provided in a microkernel are not biased toward any specific
            features or policies in an OS, so new functionalities and features could be added
            to the OS to suit specific operating environments.
                      Figure 4.7 illustrates the structure of a microkernel-based OS. The micro-
            kernel includes mechanisms for process scheduling and memory management,
            etc., but does not include a scheduler or memory handler. These functions are
            implemented as servers, which are simply processes that never terminate. The
            servers and user processes operate on top of the microkernel, which merely per-
            forms interrupt handling and provides communication between the servers and
            user processes.
                      The small size and extensibility of microkernels are valuable properties for
            the embedded systems environment, because operating systems need to be both
            small and fine-tuned to the requirements of an embedded application. Exten-
            sibility of microkernels also conjures the vision of using the same microkernel
            for a wide spectrum of computer systems, from palm-held systems to large
            parallel and distributed systems. This vision has been realized to some extent.
            The Mach microkernel has been used to implement several different versions
            of Unix. The distributed operating system Amoeba uses an identical micro-
            kernel on all computers in a distributed system ranging from workstations to
            large multiprocessors.
                      Just what is the "essential core of OS code" has been a matter of some debate,
            and as a result considerable variation exists in the services included in a micro-
            kernel. For example, IBM's implementation of the Mach microkernel leaves the
            process scheduling policy and device drivers outside the kernel--these functions
            run as servers. The QNX microkernel includes interrupt servicing routines, pro-
            cess scheduling, interprocess communication, and core network services. The L4
            microkernel includes memory management and supports only seven system calls.
            Both QNX and L4 are only 32 KB in size, where 1 KB is 1024 bytes. Despite such
            variation, it can be argued that certain services must be provided by a microker-
            nel. These include memory management support, interprocess communication
            and interrupt servicing. Memory management and interprocess communication



                                              Chapter 4        Structure of Operating  Systems  97
would be invoked by higher-level modules in the OS code that exist outside the
microkernel. The interrupt servicing routine would accept interrupts and pass
them to higher-level modules for processing.
     Operating systems using first-generation microkernels suffered up to 50
percent degradation in throughput compared to operating systems that did not
use microkernels. This problem has its origin in the fact that some functionalities
of a conventional kernel are split between a microkernel and an OS implemented
by using the microkernel--the familiar stratification problem again. For exam-
ple, a kernel includes the complete process management function, which performs
creation, scheduling, and dispatching of processes, whereas a microkernel might
include only process creation and dispatching, and process scheduling might run
as a server under the microkernel. Communication between the two parts would
require use of the interprocess communication (IPC) facility. Researchers found
that up to 73 percent of the performance penalty was due to IPC. The L4 micro-
kernel, which is a second-generation microkernel, made IPC more efficient by
eliminating validity and rights checking by default, and by tuning the microker-
nel to the hardware being used. These actions made IPC 20 times faster than
in the first-generation microkernels. Paging activities related to virtual memory
management were also moved out of the microkernel and into the operating sys-
tem built by using the microkernel. After these improvements, microkernel-based
operating systems were found to suffer only 5 percent degradation in throughput
compared to operating systems that did not use a microkernel.
     The exokernel uses a radically different philosophy of structuring an OS to
reduce performance degradation: Resource management need not be centralized;
it can be performed by applications themselves in a distributed manner. Accord-
ingly, an exokernel merely provides efficient multiplexing of hardware resources,
but does not provide any abstractions. Thus an application process sees a resource
in the computer system in its raw form. This approach results in extremely fast
primitive operations, 10­100 times faster than when a monolithic Unix kernel is
used. For example, data that is read off an I/O device passes directly to the process
that requested it; it does not go through the exokernel, whereas it would have gone
through the Unix kernel. Since traditional OS functionalities are implemented at
the application level, an application can select and use an OS from a library of
operating systems. The OS executes as a process in the nonkernel mode and uses
features of the Exokernel.
4.8  CASE STUDIES                                                                               ·
Previous sections discussed the structure of an operating system, that is, arrange-
ment of its parts, and properties of these arrangements. In this section, we discuss
both structure and architecture of some modern operating systems, where archi-
tecture concerns the structure of the operating system as well as functionalities
of its components and relationships between them. Design and implementation
features of specific OS components are described in relevant chapters of Parts 2 ­ 4
of this text.



98  Part 1  Overview
            4.8.1 Architecture of Unix
            Unix is a kernel-based operating system. Figure 4.8 is a schematic diagram of the
            Unix kernel. It consists of two main components--process management and file
            management. The process management component consists of a module for inter-
            process communication, which implements communication and synchronization
            between processes, and the memory management and scheduling modules. The
            file management component performs I/O through device drivers. Each device
            driver handles a specific class of I/O devices and uses techniques like disk schedul-
            ing to ensure good throughput of an I/O device. The buffer cache is used to reduce
            both the time required to implement a data transfer between a process and an
            I/O device, and the number of I/O operations performed on devices like disks (see
            Section 1.4.4).
                      The process management and file management components of the kernel are
            activated through interrupts raised in the hardware, and system calls made by
            processes and nonkernel routines of the OS. The user interface of the OS is a
            command interpreter, called a shell, that runs as a user process. The Unix kernel
            cannot be interrupted at any arbitrary moment of time; it can be interrupted only
            when a process executing kernel code exits, or when its execution reaches a point
            at which it can be safely interrupted. This feature ensures that the kernel data
            structures are not in an inconsistent state when an interrupt occurs and another
            process starts executing the kernel code, which considerably simplifies coding of
            the kernel (see Section 2.3.2).
                      The Unix kernel has a long history of over four decades. The original kernel
            was small and simple. It provided a small set of abstractions, simple but power-
            ful features like the pipe mechanism, which enabled users to execute several
            programs concurrently, and a small file system that supported only one file
            organization called the byte stream organization. All devices were represented
            as files, which unified the management of I/O devices and files. The kernel was
                                               Nonkernel routines
                                                                  System calls
                           Interrupts                                                         Kernel
                           File management                        Interprocess
                                               Scheduler          communi-            Memory
                             Buffer cache                          cation             management
                           Device drivers                         Process management
                                               Hardware
            Figure    4.8  Kernel of the Unix  operating system.



                                             Chapter 4            Structure of Operating  Systems  99
written in the C language and had a size of less than 100 KB. Hence it was easily
portable.
     However, the Unix kernel was monolithic and not very extensible. So it
had  to  be  modified  as  new  computing    environments,  like  the     client­server
environment, evolved. Interprocess communication and threads were added to
support client­server computing. Networking support similarly required kernel
modification.
     A major strength of Unix was its use of open standards. It enabled a large
number of organizations ranging from the academia to the industry to partic-
ipate in its development, which led to widespread use of Unix, but also led
to the development of a large number of variants because of concurrent and
uncoordinated development. The kernel became bulky, growing to a few million
bytes in size, which affected its portability. Around this time, a feature was added
to dynamically load kernel modules in memory. It enabled kernel modules to be
loaded only when needed. This feature reduced the memory requirement of the
kernel, but not its code size. Hence it did not enhance its portability.
     Several efforts have been made to redesign the Unix kernel to make it modular
and extensible. The Mach kernel, which has a specific emphasis on multiprocessor
systems, is an example of this trend. Later Mach developed into a microkernel-
based operating system.
4.8.2 The Kernel of Linux
The Linux operating system provides the functionalities of Unix System V
and Unix BSD; it is also compliant with the POSIX standard. It was initially
implemented on the Intel 80386 and has since been implemented on later Intel
processors and several other architectures.
     Linux has a monolithic kernel. The kernel is designed to consist of a set of
individually loadable modules. Each module has a well-specified interface that
indicates how its functionalities can be invoked and its data can be accessed by
other modules. Conversely, the interface also indicates the functions and data
of other modules that are used by this module. Each module can be individu-
ally loaded into memory, or removed from it, depending on whether it is likely
to be used in near future. In principle, any component of the kernel can be
structured as a loadable module, but typically device drivers become separate
modules.
     A few kernel modules are loaded when the system is booted. A new kernel
module is loaded dynamically when needed; however, it has to be integrated
with the kernel modules that already existed in memory so that the modules
can collectively function as a monolithic kernel. This integration is performed as
follows: The kernel maintains a table in which it records the addresses of functions
and data that are defined in the modules existing in memory. While loading a new
module, the kernel analyzes its interface and finds which functions and data of
other modules it uses, obtains their addresses from the table, and inserts them in
appropriate instructions of the new module. At the end of this step, the kernel



100  Part 1  Overview
             updates its table by adding the addresses of functions and data defined in the new
             module.
                       Use of kernel modules with well-specified interfaces provides several advan-
             tages. Existence of the module interface simplifies testing and maintenance of
             the kernel. An individual module can be modified to provide new functionalities
             or enhance existing ones. This feature overcomes the poor extensibility typically
             associated with monolithic kernels. Use of loadable modules also limits the mem-
             ory requirement of the kernel, because some modules may not be loaded during
             an operation of the system. To enhance this advantage, the kernel has a feature
             to automatically remove unwanted modules from memory--it produces an inter-
             rupt periodically and checks which of its modules in memory have not been used
             since the last such interrupt. These modules are delinked from the kernel and
             removed from memory. Alternatively, modules can be individually loaded and
             removed from memory through system calls.
                       The Linux 2.6 kernel, which was released in 2003, removed many of the
             limitations of the Linux 2.5 kernel and also enhanced its capabilities in several
             ways. Two of the most prominent improvements were in making the system more
             responsive and capable of supporting embedded systems. Kernels up to Linux 2.5
             were non-preemptible, so if the kernel was engaged in performing a low-priority
             task, higher-priority tasks of the kernel were delayed. The Linux 2.6 kernel is
             preemptible, which makes it more responsive to users and application programs.
             However, the kernel should not be preempted when it is difficult to save its state,
             or when it is performing sensitive operations, so the kernel disables and enables
             its own preemptibility through special functions. The Linux 2.6 kernel can also
             support architectures that do not possess a memory management unit (MMU),
             which makes it suitable for embedded systems. Thus, the same kernel can now be
             used in embedded systems, desktops and servers. The other notable feature in the
             Linux 2.6 kernel is better scalability through an improved model of threads, an
             improved scheduler, and fast synchronization between processes; these features
             are described in later chapters.
             4.8.3 The Kernel of Solaris
             Early operating systems for Sun computer systems were based on BSD Unix;
             however, later development was based on Unix SVR4. The pre-SVR4 versions
             of the OS are called SunOS, while the SVR4-based and later versions are called
             Solaris. Since the 1980s, Sun has focused on networking and distributed com-
             puting; several networking and distributed computing features of its operating
             systems have become industry standards, e.g., remote procedure calls (RPC), and
             a file system for distributed environments (NFS). Later, Sun also focused on mul-
             tiprocessor systems, which resulted in an emphasis on multithreading the kernel,
             making it preemptible (see Section 2.3.2), and employing fast synchronization
             techniques in the kernel.
                       The Solaris kernel has an abstract machine layer that supports a wide range
             of processor architectures of the SPARC and Intel 80x86 family, including multi-
             processor architectures. The kernel is fully preemptible and provides real-time



                                                      Chapter 4   Structure of Operating  Systems  101
capabilities. Solaris 7 employs the kernel-design methodology of dynamically
loadable kernel modules (see Section 4.6.1). The kernel has a core module that
is always loaded; it contains interrupt servicing routines, system calls, process
and memory management, and a virtual file system framework that can sup-
port different file systems concurrently. Other kernel modules are loaded and
unloaded dynamically. Each module contains information about other modules
on which it depends and about other modules that depend on it. The ker-
nel maintains a symbol table containing information about symbols defined
in currently loaded kernel modules. This information is used while loading
and linking a new module. New information is added to the symbol table
after a module is loaded and some information is deleted after a module is
deleted.
   The Solaris kernel supports seven types of loadable modules:
·  Scheduler classes
·  File systems
·  Loadable system calls
·  Loaders for different formats of executable files
·  Streams modules
·  Bus controllers and device drivers
·  Miscellaneous modules
   Use of loadable kernel modules provides easy extensibility. Thus, new file
systems, new formats of executable files, new system calls, and new kinds of buses
and devices can be added easily. An interesting feature in the kernel is that when a
new module is to be loaded, the kernel creates a new thread for loading, linking,
and initializing working of the new module. This arrangement permits module
loading to be performed concurrently with normal operation of the kernel. It also
permits loading of several modules to be performed concurrently.
4.8.4 Architecture of Windows
Figure 4.9 shows architecture of the Windows OS. The hardware abstraction layer
(HAL) interfaces with the bare machine and provides abstractions of the I/O
interfaces, interrupt controllers, and interprocessor communication mechanisms
in a multiprocessor system. The kernel uses the abstractions provided by the
HAL to provide basic services such as interrupt processing and multiprocessor
synchronization. This way, the kernel is shielded from peculiarities of a specific
architecture, which enhances its portability. The HAL and the kernel are together
equivalent to a conventional kernel (see Figure 4.6). A device driver also uses the
abstractions provided by the HAL to manage I/O operations on a class of devices.
   The kernel performs the process synchronization and scheduling functions.
The executive comprises nonkernel routines of the OS; its code uses facilities in
the kernel to provide services such as process creation and termination, virtual
memory management, an interprocess message passing facility for client­server
communication called the local procedure call (LPC), I/O management and a file
cache to provide efficient file I/O, and a security reference monitor that performs



102         Part 1     Overview
                                                                                User
                                                       Environment              application
                                                       subsystem                Subsystem
                                                                                DLL
                                                                    Executive
                                       I/O
                                       Manager                   Kernel         Device drivers
                                                                 Hardware abstraction layer (HAL)
                                                                  Bare machine
                       Figure    4.9  Architecture of  Windows.
                       file access validation. The I/O manager uses device drivers, which are loaded
                       dynamically when needed. Many functions of the executive operate in the kernel
                       mode, thus avoiding frequent context switches when the executive interacts with
                       the kernel; it has obvious performance benefits.
                                 The environment subsystems provide support for execution of programs
                       developed for other operating systems like MS-DOS, Win32, and OS/2. Effec-
                       tively, an environment subsystem is analogous to a guest operating system within
                       a virtual machine OS (see Section 4.5). It operates as a process that keeps track of
                       the state of user applications that use its services. To implement the interface of
                       a guest OS, each environment subsystem provides a dynamic link library (DLL)
                       and expects a user application to invoke the DLL when it needs a specific system
                       service. The DLL either implements the required service itself, passes the request
                       for service to the executive, or sends a message to the environment subsystem
                       process to provide the service.
4.9  SUMMARY                                                                                                 ·
Portability of an operating system refers to the ease   Portability and extensibility have become crucial
with which the OS can be implemented on a com-          requirements because of long life-spans of mod-
puter having a different architecture. Extensibility    ern operating systems. In this chapter we discussed
of an operating system refers to the ease with which    different ways of structuring operating systems to
its functionalities can be modified or enhanced         meet these requirements.
to   adapt  it  to  a  new  computing  environment.



                                                                             Chapter 4   Structure of Operating Systems               103
     An OS functionality typically contains a pol-                              The virtual machine operating system (VM OS)
icy, which specifies the principle that is to be used                     supported operation of several operating systems
to perform the functionality, and a few mechanisms                        on a computer simultaneously, by creating a virtual
that perform actions to implement the functional-                         machine for each user and permitting the user to
ity. Mechanisms such as dispatching and context                           run an OS of his choice in the virtual machine.
save interact closely with the computer, so their                         The VM OS interleaved operation of the users'
code      is  inherently     architecture-dependent;             poli-    virtual machines on the host computer through
cies are architecture-independent. Hence porta-                           a  procedure   analogous   to      scheduling.  When        a
bility    and        extensibility     of   an  OS       depends    on    virtual  machine      was  scheduled,      its  OS     would
how the code of its policies and mechanisms is                            organize execution of user applications running
structured.                                                               under it.
     Early     operating         systems        had   a   monolithic            In a kernel-based design of operating systems,
structure. These operating systems had poor porta-                        the kernel is the core of the operating system, which
bility    because       architecture-dependent             code   was     invokes the nonkernel routines to implement opera-
spread        throughout      the      OS.  They      also     suffered   tions on processes and resources. The architecture-
from high design complexity. The layered design                           dependent code in an OS typically resides in the
of operating systems used the principle of abstrac-                       kernel;  this  feature     enhances       portability  of   the
tion to control complexity of designing the OS.                           operating system.
It   viewed    the      OS   as     a  hierarchy      of   layers,  in          A microkernel is the essential core of OS code.
which     each       layer  provided        a   set  of   services  to    It is small in size, contains a few mechanisms,
the  layer     above    it,   and      itself   used     the   services   and does not contain any policies. Policy mod-
in the layer below it. Architecture dependencies                          ules  are     implemented  as      server  processes;      they
were often restricted to lower layers in the hier-                        can be changed or replaced without affecting the
archy; however, the design methodology did not                            microkernel, thus providing high extensibility of
guarantee it.                                                             the OS.
TEST      YOUR CONCEPTS                                                                                                               ·
     4.1  Classify each of the following statements as true                        (refer to relevant sections of Chapters 1 and 3):
          or false:                                                                a. Preempting a program
              a. Mechanisms      of    the      OS    are      typically           b. Priority-based scheduling used in multipro-
              architecture-independent.                                                 gramming systems
              b. A layered OS organization reduces the sem-                        c. Loading     a  swapped-out     program          into
              antic gap between the top layer of the OS and                             memory
              the bare machine.                                                    d. Checking whether a user program can be
              c. In a virtual machine OS, each user can run                             permitted to access a file
              an OS of his choice.                                           4.3   Which of the following operating systems has
              d. A   kernel-based      OS       structure      provides            the highest portability?
              extensibility.                                                       a. An OS with a monolithic structure.
              e. In  a  microkernel-based       OS,       the  process             b. An OS with a layered structure.
              scheduler may run as a user process.                                 c. A virtual machine OS.
     4.2  Classify each of the following functions per-                            d. A kernel-based OS.
          formed by an OS as a policy or a mechanism



104       Part 1      Overview
EXERCISES                                                                                                           ·
4.1  The    scheduling    mechanism     "manipulate  sch-        requires less-than-full virtualization of its re-
     eduling lists" (see Table 4.3) is invoked to modify         sources; however, it may degrade efficiency of
     scheduling lists in response to events in the sys-          operation of a guest OS."
     tem and actions of the scheduler. Describe the         4.3  What are the consequences of merging nonker-
     functions this mechanism should perform for (a)             nel routines with (a) the user interface, (b) the
     round-robin scheduling and (b) priority-based               kernel? (Hint: Refer to Section 1.1.)
     scheduling (as used in a multiprogramming OS).         4.4  List the differences between a kernel employ-
4.2  Justify the following statement: "Secure oper-              ing dynamically loadable modules and (a) a
     ation  of     a  virtual  machine  operating  system        monolithic kernel and (b) a microkernel.
BIBLIOGRAPHY                                                                                                        ·
Dijkstra (1968) describes the structure of the THE multi-   3.   Beck, M., H. Bohme, M. Dziadzka, U. Kunitz,
programming system. The virtual machine operating                R. Magnus, C. Schroter, and D. Verworner
system VM/370 is based on CP/67, and is described in             (2002): Linux Kernel Programming, 3rd ed.,
Creasy (1981). The XEN and VMware virtual machine                Pearson Education, New York.
products are described in Barham et al. (2003) and          4.   Bovet, D. P., and M. Cesati (2005): Understanding
Sugarman et al. (2001), respectively. The May 2005 issue         the Linux Kernel, 3rd ed., O'Reilly, Sebastopol.
of IEEE Computer is a special issue on virtualization       5.   Creasy, R. J. (1981): "The origin of the VM/370
technologies. Rosenblum and Garfinkel (2005) discusses           time-sharing system," IBM Journal of Research
trends in the design of virtual machine monitors.                and Development, 25 (5), 483­490.
     Warhol (1994) discusses the strides made by micro-     6.   Dijkstra, E. W. (1968): "The structure of THE
kernels in the early 1990s while Liedtke (1996) describes        multiprogramming system," Communications of
the principles of microkernel design. Hartig et al. (1997)       the ACM, 11, 341­346.
describes porting and performance of the Linux OS           7.   Engler D. R., M. F. Kasshoek, and J. O'Toole
on the L4 microkernel. Engler et al. (1995) discusses            (1995): "Exokernel: An operating system
design of an Exokernel. Bach (1986), Vahalia (1996), and         architecture for application-level resource
McKusick et al. (1996) describe the Unix kernel. Beck            management," Symposium on OS Principles,
et al. (2002), Bovet and Cesati (2005), and Love (2005)          251­266.
describe the Linux kernel, while Mauro and McDougall        8.   Hartig, H., M. Hohmuth, J. Liedtke,
(2006) describes the kernel of Solaris. Tanenbaum (2001)         S. Schonberg, and J. Wolter (1997): "The
describes microkernels of the Amoeba and Mach operat-            performance of microkernel-based systems,"
ing systems. Russinovich and Solomon (2005) describes            16th ACM Symposium on Operating System
architecture of Windows.                                         Principles.
1.   Bach, M. J. (1986): The Design of the Unix             9.   Liedtke J. (1996): "Towards real microkernels,"
     Operating System, Prentice Hall, Englewood                  Communications of the ACM, 39 (9), 70­77.
     Cliffs, N.J.                                           10.  Love, R. (2005): Linux Kernel Development,
2.   Barham, P., B. Dragovic, K. Fraser, S. Hand,                2nd ed., Novell Press.
     T. Harris, A. Ho, R. Neugebauer, I. Pratt, and         11.  Mauro, J., and R. McDougall (2006): Solaris
     A. Warfield (2003): "XEN and the art of                     Internals, 2nd ed., Prentice Hall, Englewood
     virtualization," ACM Symposium on Operating                 Cliffs, N.J.
     System Principles, 164­177.



                                                        Chapter 4  Structure of Operating Systems               105
12.  McKusick, M. K., K. Bostic, M. J. Karels,          16.  Sugarman, J., G. Venkitachalam, and
     and J. S. Quarterman (1996): The Design and             B. H. Lim (2001): "Virtualizing I/O devices on
     Implementation of the 4.4 BSD Operating System,         VMware workstation's hosted virtual machine
     Addison-Wesley, Reading, Mass.                          monitor," 2001 USENIX Annual Technical
13.  Meyer, J., and L. H. Seawright (1970): "A virtual       Conference.
     machine time-sharing system," IBM Systems          17.  Tanenbaum, A. S. (2001): Modern Operating
     Journal, 9 (3), 199­218.                                Systems, 2nd ed., Prentice Hall, Englewood
14.  Rosenblum, M., and T. Garfinkel (2005): "Virtual        Cliffs, N.J.
     machine monitors: current technology and future    18.  Vahalia, U. (1996): UNIX Internals--the New
     trends," IEEE Computer, 38 (5), 39­47.                  Frontiers, Prentice-Hall, Englewood Cliffs, N.J.
15.  Russinovich, M. E., and D. A. Solomon (2005):      19.  Warhol, P. D. (1994): "Small kernels hit it big,"
     Microsoft Windows Internals, 4th ed., Microsoft         Byte, January 1994, 119­128.
     Press, Redmond, Wash.






                                                                                                       107
                                                                                part                2
         Process Management
A process is an execution of a program. An application may be designed
         to have many processes that operate concurrently and interact among
         themselves to jointly achieve a goal. This way, the application may be
able to provide a quicker response to the user.
An OS contains a large number of processes at any time. Process management
involves creating processes, fulfilling their resource requirements, scheduling them
for use of a CPU, implementing process synchronization to control their interac-
tions, avoiding deadlocks so that they do not wait for each other indefinitely,
and terminating them when they complete their operation. The manner in which
an OS schedules processes for use of a CPU determines the response times of
processes, resource efficiency, and system performance.
A thread uses the resources of a process but resembles a process in all other
respects. An OS incurs less overhead in managing threads than in managing
processes. We use the term process as generic to both processes and threads.
                                 Road Map for Part 2
                                                 Processes
                                                 and Threads
                                        Process                Scheduling
                                        Synchronization
                                        Message                Synchronization
                    Deadlocks           Passing                and Scheduling in
                                                               Multiprocessor OSs
Schematic  diagram  showing the  order  in which chapters  of  this part should be  covered  in  a
course.



108  Part 2  Process Management
             Chapter 5: Processes and Threads
             This chapter begins by discussing how an application creates processes through
             system calls and how the presence of many processes achieves concurrency and
             parallelism within the application. It then describes how the operating system
             manages a process--how it uses the notion of process state to keep track of
             what a process is doing and how it reflects the effect of an event on states of
             affected processes. The chapter also introduces the notion of threads, describes
             their benefits, and illustrates their features.
             Chapter 6: Process Synchronization
             Processes of an application work toward a common goal by sharing data and
             coordinating with one another. The key concepts in process synchronization are
             the use of mutual exclusion to safeguard consistency of shared data and the use of
             indivisible operations in coordinating activities of processes. This chapter discusses
             the synchronization requirements of some classic problems in process synchro-
             nization and discusses how they can be met by using synchronization features such
             as semaphores and monitors provided in programming languages and operating
             systems.
             Chapter 7: Scheduling
             Scheduling is the act of selecting the next process to be serviced by a CPU. This
             chapter discusses how a scheduler uses the fundamental techniques of priority-
             based scheduling, reordering of requests, and variation of time slice to achieve
             a suitable combination of user service, efficient use of resources, and system
             performance. It describes different scheduling policies and their properties.
             Chapter 8: Deadlocks
             A deadlock is a situation in which processes wait for one another indefinitely due
             to resource sharing or synchronization. This chapter discusses how deadlocks can
             arise and how an OS performs deadlock handling to ensure an absence of dead-
             locks, either through detection and resolution of deadlocks, or through resource
             allocation policies that perform deadlock prevention or deadlock avoidance.
             Chapter 9: Message Passing
             Processes exchange information by sending interprocess messages. This chapter
             discusses the semantics of message passing, and OS responsibilities in buffering
             and delivery of interprocess messages. It also discusses how message passing is
             employed in higher-level protocols for providing electronic mail facility and in
             providing intertask communication in parallel or distributed programs.



                                            Part 2  Process Management             109
Chapter 10: Synchronization and Scheduling
in Multiprocessor OSs
Presence of many CPUs in a multiprocessor computer system holds the promise
of high throughput and fast response to applications. This chapter discusses dif-
ferent kinds of multiprocessor systems, and describes how the OS achieves high
throughput and fast response by using special techniques of structuring its ker-
nel, so that many CPUs can execute kernel code in parallel, and of synchronizing
and scheduling processes.



