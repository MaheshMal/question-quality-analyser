Performance Analysis of Scheduling Policies
             Performance analysis of a scheduling policy is a study of its performance, using
             measures such as response time of a process, efficiency of use of the CPU, and
             throughput of the system. Performance analysis can be used to compare perfor-
             mance of alternative scheduling policies, and to determine "good" values of key
             system parameters like the time slice, number of active users, and the size of the
             list of ready processes.
                  Performance of a scheduling policy is sensitive to the nature of requests
             directed at it, and so performance analysis should be conducted in the environ-
             ment in which the policy is to be put into effect. The set of requests directed at
             a scheduling policy is called its workload. The first step in performance analysis
             of a policy is to accurately characterize its typical workload. In the following, we
             discuss some issues involved in this step.
                  As mentioned in Section 7.2 in the context of the SRN policy, user estimates
             of service times are not reliable either because users lack the experience to pro-
             vide good estimates of service time or because knowledgeable users may provide
             misleading estimates to obtain a favored treatment from the system. Some users
             may even resort to changes in their requests to obtain better service; for instance,
             a user who knows that the SRN policy is being used may split a long-running pro-
             gram into several programs with short service times. All these factors distort the
             workload. Hence the characterization of a typical workload should be developed
             without involving the users.
                  Three approaches could be used for performance analysis of scheduling
             policies:
             ·  Implementation of a scheduling policy in an OS
             ·  Simulation
             ·  Mathematical modeling
                  Both simulation and mathematical modeling avoid the need for implementing
             a scheduling policy in an OS, thereby avoiding the cost, complexity, and delays
             involved in implementing the policy. However, to produce the same results as
             an implementation, these approaches require a very detailed characterization
             of requests in the workload, which is generally not feasible in practice. Hence,
             performance aspects like the scheduling overhead or service to individual requests
             are best studied through implementation, whereas simulation and mathematical
             modeling are well suited for studying performance of a scheduling policy and for
             determining "good" values of system parameters like the time slice, number of
             users, or the size of the list of ready processes.
             7.7.1 Performance Analysis through Implementation
             The scheduling policy to be evaluated is implemented in a real operating sys-
             tem that is used in the target operating environment. The OS receives real user



                                                                                      Chapter 7  Scheduling  267
                                              Data              Simulated
                                              collection         Clock
                                  Scheduling  module
              Requests to         lists
                                                      Scheduler            Completed
              be serviced                                                  requests
                                                      PCB
                                                           I/O
                                                      Simulator
Figure  7.15  Simulation   of  a  scheduling policy.
requests; services them using the scheduling policy; and collects data for statistical
analysis of the policy's performance. This approach to performance analysis is
disruptive, because a real OS has to be decommissioned, modified, and recommis-
sioned for every scheduling policy that is to be analyzed. This disruption could
be avoided by using virtual machine software, which permits a guest kernel to
be modified without affecting operation of the host kernel; however, the over-
head introduced by use of the virtual machine would cause inaccuracies in the
performance measurement.
7.7.2 Simulation
Simulation is achieved by coding the scheduling policy and relevant OS functions
as a program--the simulator program--and using a typical workload as its input.
The workload is a recording of some real workload directed at the OS during a
sample period. Analysis may be repeated with many workloads to eliminate the
effect of variations across workloads.
Figure 7.15 shows a schematic of a simulator. The simulator operates as
follows: It maintains the data structures that are used by the simulated scheduling
policy, in which it puts information concerning user requests as they arrive in the
system, get admitted, and receive service. It also maintains a clock to keep track
of the simulated time. From time to time, it mimics the scheduling action and
selects a request for processing. It estimates the length of time for which the
request would use the CPU before an event like the initiation of an I/O operation
or completion of a request, occurs. It now advances the simulated clock by the
amount of time for which the request would have used the CPU before the event
occurred, and moves the request out of the scheduling queue. It then performs
scheduling once again, and so on. It may contain other modules like an I/O
simulator module which would predict when the I/O operation initiated by a
request would complete. When the simulated clock shows this time, it adds the
request to a scheduling queue. The data collection module collects useful data
for performance analysis. The level of detail handled in a simulator governs the
cost of simulation and the quality of its results.



268  Part 2  Process Management
             7.7.3 Mathematical Modeling
             A mathematical model consists of two components--a model of the server
             and a model of the workload being processed. The model provides a set of
             mathematical expressions for important performance characteristics like service
             times of requests and overhead. These expressions provide insights into the influ-
             ence of various parameters on system performance. The workload model differs
             from workloads used in simulations in that it is not a recording of actual work-
             load in any specific time period. It is a statistical distribution that represents the
             workload; that is, it is a function that generates fictitious requests that have the
             same statistical properties as the actual workload during any period.
             Queuing Theory      Widespread use of mathematical models to analyze perfor-
             mance of various systems led to development of a separate branch of mathematics
             known as queuing theory. Performance analysis using queuing theory is called
             queuing analysis. The earliest well-known application of queuing analysis was by
             Erlang (1909) in evaluating the performance of a telephone exchange with the
             number of trunk lines as the controlling parameter.
             The fundamental queuing theory model of a system is identical with the
             simple scheduler model discussed at the start of this Chapter (see Figure 7.1).
             This is known as the single-server model. Queuing analysis is used to develop
             mathematical expressions for server efficiency, mean queue length, and mean
             wait time.
             A request arriving at time ai with service time xi is completed at time ci. The
             elapsed time (ci - ai) depends on two factors--arrival times and service times
             of requests that are either in execution or in the scheduling queue at some time
             during the interval (ci - ai), and the scheduling policy used by the server. It is
             reasonable to assume that arrival times and service times of requests entering
             the system are not known in advance; i.e., these characteristics of requests are
             nondeterministic in nature.
             Although characteristics of individual requests are unknown, they are cus-
             tomarily assumed to conform to certain statistical distributions. A computing
             environment is thus characterized by two parameters--a statistical distribution
             governing arrival times of requests, and a statistical distribution governing their
             service times. We give a brief introduction to statistical distributions and their
             use in mathematical modeling, using the following notation:
                         Mean arrival rate (requests per second)
                         Mean execution rate (requests per second)
                         /
              is called the utilization factor of the server. When  > 1, the work being
             directed at the system exceeds its capacity. In this case, the number of requests
             in the system increases indefinitely. Performance evaluation of such a system is
             of little practical relevance since turnaround times can be arbitrarily large. When
              < 1, the system capacity exceeds the total work directed at it. However, this is
             true only as a long-term average; it may not hold in an arbitrary interval of time.



                                                                                    Chapter 7    Scheduling  269
Hence the server may be idle once in a while, and a few requests may exist in the
queue at certain times.
Most practical systems satisfy  < 1. Even when we consider a slow server, 
does not exceed 1 because most practical systems are self-regulatory in nature--
the number of users is finite and the arrival rate of requests slackens when the
queue length is large because most users' requests are locked up in the queue!
A system reaches a steady state when all transients in the system induced due
to its abrupt initiation at time t = 0 die down. In the steady state, values of mean
queue lengths, mean wait times, mean turnaround times, etc., reflect performance
of the scheduling policy. For obtaining these values, we start by assuming certain
distributions for arrival and servicing of requests in the system.
Arrival Times  The time between arrival of two consecutive requests is called
interarrival time. Since  is the arrival rate, the mean interarrival time is 1/. A
statistical distribution that has this mean interarrival time and that fits empiri-
cal data reasonably well can be used for workload characterization. Arrival of
requests in the system can be regarded as random events totally independent of
each other. Two assumptions leading to a Poisson distribution of arrivals are now
made. First, the number of arrivals in an interval t to t + dt is assumed to depend
only on the value of dt and not on past history of the system during the interval
(0, t). Second, for small values of dt, probability of more than one arrival in the
interval t to (t + dt) is assumed to be negligible. The first assumption is known as
the memoryless property of the arrival times distribution. An exponential distri-
bution function giving the probability of an arrival in the interval 0 to t for any t
has the form:
                                     F (t) = 1 - e-. t
This distribution has the mean interarrival time 1/ since           t.  dF  (t)  =  1/.  It  is
                                                           0
found that the exponential distribution fits the interarrival times in empirical data
reasonably well. (However, a hyperexponential distribution with the same mean
of 1/ is found to be a better approximation for the experimental data (Coffman
and Wood [1966]).
Service Times  The function S(t) gives the probability that the service time of a
request is less than or equal to t.
                                     S(t) = 1 - e-. t
As in the case of arrival times, we make two assumptions that lead to a Poisson
distribution of service times. Hence the probability that a request that has already
consumed t units of service time will terminate in the next dt seconds depends
only on the value of dt and not on t. In preemptive scheduling, it applies every
time a request is scheduled to run after an interruption.
The memoryless property of service times implies that a scheduling algo-
rithm cannot make any predictions based on past history of a request in the
system. Thus, any preemptive scheduling policy that requires knowledge of future
behavior of requests must depend on estimates of service times supplied by a pro-
grammer. The scheduling performance will then critically depend on user inputs



270  Part 2  Process Management
             and may be manipulated by users. In a practical situation, a system must strive
             to achieve the opposite effect--that is, system performance should be immune
             to user specification (or misspecification) of the service time of a request. This
             requirement points toward round-robin scheduling with time-slicing as a practical
             scheduling policy.
             Performance Analysis                   The relation between L, the mean queue length and W ,
             the mean wait time for a request before its servicing begins is given by Little's
             formula,
                                                                         L =  ×W                                                             (7.6)
             This relation follows from the fact that while a request waits in the queue,  × W
             new requests join the queue.
             When a new request arrives, it is added to the request queue. In nonpreemptive
             scheduling, the new request would be considered only after the server completes
             the request it is servicing. Let W0 be the expected time to complete the current
                                                                                                                         =W01=-2e.-0. t.t2WdF,
             request.    Natually,       W0    is independent of a                 scheduling policy.                                           (t),
             and has     the value             for an exponential                  distribution F (t)                                           the
                                         2
             mean wait time for a request when a specific scheduling policy is used, is computed
             from W0 and features of the scheduling policy. We outline how the mean wait
             times for FCFS and SRN policies are derived. Derivations for HRN and round-
             robin policies are more complex and can be found in Brinch Hansen (1973).
             Table 7.7 summarizes the mean wait time for a request whose service time is t
             when different scheduling policies are used.
             W , the waiting time for some request r , is the amount of time r                                                    spends in
             the queue before its service begins. Hence in FCFS scheduling
                                                                   W = W0 +                    i xi
             Table 7.7            Summary of Performance Analysis
             Scheduling policy                 Mean wait time for a request with service time = t
             FCFS                              W0
                                               1-
             SRN                               W0       ,  where      t  =      t     ·  y  ·  dS(y)
                                               1-t                           o
             HRN                               For small t:           W0  +        2     ×     t
                                                                                1-             2
                                               For      large  t:           W0
                                                                   (1-   )(1-      +   2.W0    )
                                                                                         t
             Round-robin                            n          -   1  ,  where     P0    =             1
                                                                   
                                                (1-P0 )                                           n    n!      ×()j
                                                                                                  j=0  (n-j)!
                                               (P0 is the probability that no terminal awaits                                  a  response)
             Note: W0 =     .     t2 dF  (t).  For  an  exponential      distribution F (t)    =     1 - e-. t,  it  is     .
                         2     0                                                                                         2



                                                                                                             Chapter 7  Scheduling        271
where request i is ahead of request r               in the scheduling queue. Since the system
is in the steady state, we can replace the              i  term       by   n  ×     1  ,  where  n  is  the  number
                                  1                                                 
of requests ahead of r     and       is the mean service time. Since n is the mean queue
length, n =  × W from Little's formula. Hence
                                W    =    W0        +      ×   W×          1
                                                                           
                                     =    W0        +      ×   W.
Therefore, W  =    W0   .  Thus,  the  mean         wait   time   in    FCFS        scheduling      rises    sharply
                   1-
for high values of .
In SRN scheduling, requests whose service times < xr , where xr is the service
time of r , are serviced before request r . Hence the waiting time for request r                                  is
                   W = W0 +               i xi, where xi < xr
                              W0                                     r
                           =  1 - r    ,  where r          =             · y · dS(y).
                                                                  0
Capacity Planning      Performance analysis can be used for capacity planning. For
example, the formulae shown in Table 7.7 can be used to determine values of
important parameters like the size of the list of ready processes used by the
kernel.
As an example, consider an OS in which the mean arrival rate of requests
is 5 requests per second, and the mean response time for requests is 3 seconds.
The mean queue length is computed by Little's formula [Eq. (7.6)] as 5 × 3 = 15.
Note that queues will exceed this length from time to time. The following example
provides a basis for deciding the capacity of the ready queue.
                                                                                                                                          ·
Capacity Planning Using Queuing Analysis                                                                                Example     7.15
A kernel permits up to n entries in the queue of ready requests. If the queue is
full when a new request arrives, the request is rejected and leaves the OS. pi,
the probability that the ready queue contains i processes at any time, can be
shown to be:
                                          pi  =     i × (1 - )                                               (7.7)
                                                        1 - n+1
For      =  0.5 and n      =  3, p0  =        8  ,  p1  =  4   ,  p2    =     2  ,  and   p3  =     1   .  Hence  6.7
                                          15               15              15                       15
percent of requests are lost. A higher value of n should be used to reduce the
number of lost requests.
                                                                                                                  ·
