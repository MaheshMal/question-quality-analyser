Issues in Multiprocessor Operating Systems
To realize the benefits of high throughput and computation speedup offered by
a multiprocessor system, the CPUs must be used effectively and processes of an
application should be able to interact harmoniously. These two considerations
will, of course, influence process scheduling and process synchronization. They
also affect the operating system's own methods of functioning in response to
interrupts and system calls. Table 10.3 highlights the three fundamental issues
raised by these considerations.
Early multiprocessor operating systems functioned in the master­slave mode.
In this mode, one CPU is designated as the master, and all other CPUs operate as
its slaves. Only the master CPU executes the kernel code. It handles interrupts and
system calls, and performs scheduling. It communicates its scheduling decisions
to other CPUs through interprocessor interrupts ( IPIs). The primary advantage
of the master­slave kernel structure is its simplicity. When a process makes a
system call, the CPU on which it operated is idle until either the process resumes
its operation or the master CPU assigns new work to the CPU. None of these can
Table 10.3        Issues in  Synchronization and Scheduling
in a Multiprocessor OS
Issue                        Description
Kernel structure             Many CPUs should be able to execute kernel code in
                             parallel, so that execution of kernel functions does not
                             become a bottleneck.
Process synchronization      Presence of multiple CPUs should be exploited to
                             reduce the overhead of switching between processes,
                             and synchronization delays.
Process scheduling           The scheduling policy should exploit presence of
                             multiple CPUs to provide computation speedup for
                             applications.



344  Part 2  Process Management
             happen until the master CPU handles the system call and performs scheduling.
             Hence execution of kernel functions by the master is a bottleneck that affects
             system performance. This problem can be solved by structuring the kernel so
             that many CPUs can execute its code in parallel.
             Presence of multiple CPUs can be exploited to reduce synchronization delays.
             In a uniprocessor system, letting a process loop until a synchronization condi-
             tion is met denies the CPU to other processes and may lead to priority inversion
             (see Section 6.5.1). Hence synchronization is performed through blocking of a
             process until its synchronization condition is met. However, in a multiproces-
             sor system, synchronization through looping does not lead to priority inversion
             because the process holding the lock can execute on another CPU in parallel with
             the looping process. It would be preferable to let a process loop, rather than block
             it, if the amount of time for which it would loop is less than the total CPU overhead
             of blocking it and scheduling another process, and activating and rescheduling it
             sometime in future. This condition would be met if a process looping for entry to
             a critical section and the holder of the critical section are scheduled in parallel.
             Multiprocessor operating systems provide special synchronization techniques for
             exploiting this feature.
             Scheduling of processes is influenced by two factors--cache performance
             during operation of a process, and synchronization requirements of processes
             of an application. Scheduling a process on the same CPU every time may lead
             to a high cache hit ratio, which would improve performance of the process and
             also contribute to better system performance. If the processes of an application
             interact frequently, scheduling them at the same time on different CPUs would
             provide them an opportunity to interact in real time, which would lead to a
             speedup of the application. For example, a producer and a consumer in a single-
             buffer producers­consumers system may be able to perform several cycles of
             producing and consuming of records in a time slice if they are scheduled to run
             in parallel.
             Thus, kernel structure and the algorithms it uses for scheduling and syn-
             chronization together determine whether a multiprocessor OS will achieve high
             throughput. However, computer systems grow in size with advances in technology
             or requirements of their users, so another aspect of performance, called scalabil-
             ity, is equally important. Scalability of a system indicates how well the system will
             perform when its size grows. The size of a multiprocessor OS may grow through
             addition of more CPUs, memory units and other resources to the system, or
             through creation of more processes in applications. Two kinds of performance
             expectations arise when a system grows in size--the throughput of the system
             should increase linearly with the number of CPUs and delays faced by individual
             processes, due to either synchronization or scheduling, should not increase as the
             number of processes in the system increases.
             Scalability is important in the design of both hardware and software. Inter-
             connection technologies that work well when the system contains a small number
             of CPUs and memory units may not work as well when their number grows. To be
             scalable, the effective bandwidth of an interconnection network should increase
             linearly as the number of CPUs is increased. As we discussed in Section 10.1,
