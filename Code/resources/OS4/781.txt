Design Issues in Distributed File Systems
     A distributed file system (DFS) stores user files in several nodes of a distributed
     system, so a process and a file being accessed by it often exist in different nodes
     of the distributed system. This situation has three likely consequences:
     · A user may have to know the topology of the distributed system to open and
     access files located in various nodes of the system.
     · A file processing activity in a process might be disrupted if a fault occurs in
     the node containing the process, the node containing the file being accessed,
     or a path connecting the two.
760



                                                            Chapter 20       Distributed  File  Systems  761
· Performance of the file system may be poor because of the network traffic
   involved in accessing a file.
   The need to avoid these consequences motivates the three design issues
summarized in Table 20.1 and discussed in the following.
Transparency     A file system finds the location of a file during path name resolu-
tion (see Section 13.9.1). Two relevant issues in a distributed file system are: How
much information about the location of a file should be reflected in its path name,
and can a DFS change the location of a file to optimize file access performance?
The notion of transparency has two facets that address these issues.
·  Location transparency: The name of a file should not reveal its location.
·  Location independence: The file system should be able to change the location
   of a file without having to change its name.
Location transparency provides user convenience, as a user or a computation
need not know the location of a file. Location independence enables a file system
to optimize its own performance. For example, if accesses to files stored at a
node cause network congestion and result in poor performance, the DFS may
move some of those files to other nodes. This operation is called file migration.
Location independence can also be used to improve utilization of storage media
in the system. We discuss these two facets of transparency in Section 20.2.
Fault Tolerance  A fault disrupts an ongoing file processing activity, thereby
threatening consistency of file data and metadata, i.e., control data, of the file
system. A DFS may employ a journaling technique as in a conventional file
Table 20.1       Design Issues in Distributed File Systems
Design issue     Description
Transparency     High transparency of a file system implies that a user need
                 not know much about location of files in a system.
                 Transparency has two aspects. Location transparency implies
                 that the name of a file should not reveal its location in the file
                 system. Location independence implies that it should be
                 possible to change the location of a file without having to
                 change its name.
Fault tolerance  A fault in a computer system or a communication link may
                 disrupt ongoing file processing activities. It affects availability
                 of the file system and also impairs consistency of file data and
                 metadata, i.e., control data, of the file system. A DFS should
                 employ special techniques to avoid these consequences of
                 faults.
Performance      Network latency is a dominant factor of file access times in a
                 DFS; it affects both efficiency and scalability of a DFS.
                 Hence a DFS should use techniques that reduce network
                 traffic generated by file accesses.



762  Part 5  Distributed Operating Systems
             system to protect consistency of metadata, or it may use a stateless file server
             design, which makes it unnecessary to protect consistency of metadata when a
             fault occurs. To protect file data, it may provide transaction semantics, which
             are useful in implementing atomic transactions (see Section 19.4.1), so that an
             application may itself achieve fault tolerance if it so desires. We discuss fault
             tolerance issues in Section 20.4.
             Performance  Performance of a DFS has two facets--efficiency and scalabil-
             ity. In a distributed system, network latency is the dominant factor influencing
             efficiency of a file processing activity. Network latency typically exceeds the pro-
             cessing time for a file record so, unlike I/O device latency, it cannot be masked by
             blocking and buffering of records (see Sections 14.8 and 14.9). A DFS employs the
             technique of file caching, which keeps a copy of a remote file in the node of a pro-
             cess that accesses the file. This way accesses to the file do not cause network traffic,
             though staleness of data in a file cache has to be prevented through cache coherence
             techniques. Scalability of DFS performance requires that response times should
             not degrade when system size increases because of addition of nodes or users. A
             distributed system is composed of clusters, which are groups of computer systems
             having high-speed LANs (see Section 16.2), so caching a single copy of a file in a
             cluster ensures that file access performance for accesses from a computer system
             within a cluster would be independent of system size. It also reduces network traf-
             fic. Both these effects help in enhancing scalability of DFS performance. When
             several processes access the same file in parallel, distributed locking techniques
             are employed to ensure that synchronization of the file processing activities scales
             well with an increase in system size. We discuss DFS performance enhancement
             techniques in Section 20.5.
             20.1.1 Overview of DFS Operation
             Figure 20.1 shows a simplified schematic of file processing in a DFS. A pro-
             cess in node N1 opens a file with path name . . . alpha. We call this process
             a client process of this file, or simply a client of this file, and call node N1 the
             client node. Through path name resolution, the DFS finds that this file exists in
             node N2, so it sets up the arrangement shown in Figure 20.1. The file system
             component in node N2 is called a file server, and node N2 is called the server
             node. Other nodes that were involved in path name resolution or that would be
             involved in transferring file data between nodes N1 and N2 are called intermediate
             nodes.
             We refer to this model as the remote file processing model. An arrangement
             analogous to RPC is used to implement file accesses through stub processes called
             file server agent and client agent (see Section 16.5.2). When the client opens the
             file, the request is handed over to the client agent. The client agent communicates
             the request to the file server agent in node N2, which hands over the request to the
             file server. The file server opens alpha and builds fcbalpha. When file caching
             is not employed, a read or write operation on alpha is implemented through a
