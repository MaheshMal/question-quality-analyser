Integrating Operation of Nodes of a Distributed System
             To realize the benefits of resource sharing, reliability, and computation speedup
             summarized in Table 16.1, processes of an application should be scattered across
             various nodes in the system (1) whenever possible, to achieve computation
             speedup and efficiency of resources, and (2) whenever necessary to provide reli-
             ability. It is achieved by integrating the operation of various nodes in the system
             through interactions of their kernels. In this section, we sample features of a few
             systems to illustrate different ways in which operation of nodes is integrated. In
             Section 16.8, we discuss design issues in distributed operating systems.
             Network Operating Systems         A network operating system is the earliest form
             of operating system for distributed architectures. Its goal is to provide resource
             sharing among two or more computer systems that operate under their own OSs.
             As shown in the schematic of Figure 16.2, the network OS exists as a layer between
             the kernel of the local OS and user processes. If a process requests access to a
             local resource, the network OS layer simply passes the request to the kernel of the
             local OS. However, if the request is for access to a nonlocal resource, the network
             OS layer contacts the network OS layer of the node that contains the resource
             and implements access to the resource with its help. Many network operating
             systems have been developed on top of the Unix operating system. The Newcastle
             connection, also called Unix United, is a well-known network OS developed at the
             University of Newcastle upon Tyne. It provided access to remote files by using
             system calls that are identical with those used for local files.
             A network OS is easier to implement than a full-fledged distributed OS. How-
             ever, local operating systems retain their identities and operate independently, so
             their functioning is not integrated and their identities are visible to users. In some
             network OSs, a user had to log into a remote operating system before he could
             utilize its resources. This arrangement implies that a user must know where a
             resource is located in order to use it. A network OS cannot balance or optimize
             utilization of resources. Thus, some resources in a node may be heavily loaded
             while identical resources in other nodes may be lightly loaded or free. The net-
             work OS also cannot provide fault tolerance--a computation explicitly uses a
             resource id while accessing a resource, so it has to be aborted if the resource fails.
                               User processes           User processes
                               Network OS               Network OS
                     Computer  layer                    layer                  Computer
                     system 1  Kernel of                Kernel of              system 2
                               local OS                 local OS
             Figure  16.2  A network operating system.



                                  Chapter 16  Distributed Operating                   Systems  657
Windows and Sun Cluster Software  Cluster software is not a distributed oper-
ating system; however, it contains several features found in distributed operating
systems--it provides availability through redundancy of resources such as CPUs
and I/O devices and computation speedup by exploiting presence of several CPUs
within the cluster.
The Windows cluster server provides fault tolerance support in clusters con-
taining two or more server nodes. An application has to use a special application
program interface (API) to access cluster services. Basic fault tolerance is pro-
vided through RAIDs of level 0, 1, or 5 (see Section 14.3.5) that are shared by all
server nodes. In addition, when a fault or a shutdown occurs in one server, the
cluster server moves its functions to another server without causing a disruption
in its services.
A cluster is managed by distributed control algorithms, which are implemented
through actions performed in all nodes (see Chapter 18). These algorithms require
that all nodes must have a consistent view of the cluster, i.e., they must possess
identical lists of nodes within the cluster. The following arrangement is used to
satisfy this requirement: Each node has a node manager, which maintains the
list of nodes in a cluster. The node manager periodically sends messages called
heartbeats to other node managers to detect node faults. The node manager that
detects a fault broadcasts a message containing details of the fault on the private
LAN. On receiving this message, each node corrects its list of nodes. This event
is called a regroup event.
A resource in the cluster server can be a physical resource, a logical resource,
or a service. A resource is implemented as a dynamic link library (DLL), so it is
specified by providing a DLL interface. A resource belongs to a group. A group
is owned by one node in the cluster at any time; however, it can be moved to
another node in the event of a fault. The resource manager in a node is respon-
sible for starting and stopping a group. If a resource fails, the resource manager
informs the failover manager and hands over the group containing the resource
so that it can be restarted at another node. When a node fault is detected, all
groups located in that node are "pulled" to other nodes so that resources in
them can be accessed. Use of a shared disk facilitates this arrangement. When a
node is restored after a failure, the failover manager decides which groups can
be handed over to it. This action is called a failback; it safeguards resource effi-
ciency in the system. The handover and failback actions can also be performed
manually.
The network load balancing feature distributes the incoming network traf-
fic among the server nodes in a cluster. It is achieved as follows: A single IP
address is assigned to the cluster; however, incoming messages go to all server
nodes in the cluster. On the basis of the current load distribution arrangement,
exactly one of the servers accepts the message and responds to it. When a node
fails, its load is distributed among other nodes, and when a new node joins, the
load distribution is reconfigured to direct some of the incoming traffic to the
new node.
The Sun cluster framework integrates a cluster of two or more Sun systems
operating under the Solaris OS to provide availability and scalability of services.



658  Part 5  Distributed Operating Systems
             Availability is provided through failover, whereby the services that were running
             at a failed node are relocated to another node. Scalability is provided by sharing
             the load across servers. Three key components of the Sun Cluster are global
             process management, distributed file system, and networking. Global process
             management provides globally unique process ids. This feature is useful in process
             migration, wherein a process is transferred from one node to another to balance
             the computational loads in different nodes, or to achieve computation speedup.
             A migrated process should be able to continue using the same path names to
             access files from a new node. Use of a distributed file system provides this feature.
             Amoeba  Amoeba is a distributed operating system developed at the Vrije Uni-
             versiteit in the Netherlands during the 1980s. The primary goal of the Amoeba
             project is to build a transparent distributed operating system that would have the
             look and feel of a standard time-sharing OS like Unix. Another goal is to provide
             a testbed for distributed and parallel programming.
             The Amoeba system architecture has three main components--X terminals,
             a processor pool, and servers such as file and print servers. The X terminal is a user
             station consisting of a keyboard, a mouse and a bit-mapped terminal connected
             to a computer. The processor pool has the features described in Section 16.2.
             The Amoeba microkernel runs on all servers, pool processors and terminals, and
             performs the following four functions:
             1. Managing processes and threads
             2. Providing low-level memory management support
             3. Supporting communication
             4. Handling low-level I/O
             Amoeba provides kernel-level threads and two communication protocols. One
             protocol supports the clientÂ­server communication model through remote proce-
             dure calls (RPCs), while the other protocol provides group communication. For
             actual message transmission, both these protocols use an underlying Internet
             protocol called the fast local Internet protocol (FLIP), which is a network layer
             protocol in the ISO protocol stack (see Section 16.6.6).
             Many functions performed by traditional kernels are implemented through
             servers that run on top of a microkernel. Thus actions like booting, process
             creation, and process scheduling are performed by servers. The file system is also
             implemented as a file server. This approach reduces the size of the microkernel
             and makes it suitable for a wide range of computer systems from servers to pool
             processors. The concept of objects is central to Amoeba. Objects are managed
             by servers and they are protected by using capabilities (see Section 15.7).
             When a user logs in, a shell is initiated in some host in the system. As the user
             issues commands, processes are created in some other hosts to execute the com-
             mands. Thus a user's computation is spread across the hosts in the system; there is
             no notion of a home machine for a user. This disregard for machine boundaries
             shows how tightly all resources in the system are integrated. Amoeba uses the
             processor pool model of nodes in the system. When a user issues a command,
             the OS allocates a few pool processors to the execution of the command. Where
             necessary, pool processors are shared across users.
