the most successful men in the end are those whose success is the result of steady accretion. alexander graham bell the greedy strategy considered in the preceding chapter constructs a solution to an optimization problem piece by piece always adding a locally optimal piece to a partially constructed solution. in this chapter we discuss a different approach to designing algorithms for optimization problems. it starts with some feasible solution a solution that satisfies all the constraints of the problem and proceeds to improve it by repeated applications of some simple step. this step typically involves a small localized change yielding a feasible solution with an improved value of the objective function. when no such change improves the value of the objective function the algorithm returns the last feasible solution as optimal and stops. there can be several obstacles to the successful implementation of this idea. first we need an initial feasible solution. for some problems we can always start with a trivial solution or use an approximate solution obtained by some other e.g. greedy algorithm. but for others finding an initial solution may require as much effort as solving the problem after a feasible solution has been identified. second it is not always clear what changes should be allowed in a feasible solution so that we can check efficiently whether the current solution is locally optimal and if not replace it with a better one. third and this is the most fundamental difficulty is an issue of local versus global extremum maximum or minimum . think about the problem of finding the highest point in a hilly area with no map on a foggy day. a logical thing to do would be to start walking up the hill from the point you are at until it becomes impossible to do so because no direction would lead up. you will have reached a local highest point but because of a limited feasibility there will be no simple way to tell whether the point is the highest global maximum you are after in the entire area. fortunately there are important problems that can be solved by iterativeimprovement algorithms. the most important of them is linear programming. we have already encountered this topic in section . . here in section . we introduce the simplex method the classic algorithm for linear programming. discovered by the u.s. mathematician george b. dantzig in this algorithm has proved to be one of the most consequential achievements in the history of algorithmics. in section . we consider the important problem of maximizing the amount of flow that can be sent through a network with links of limited capacities. this problem is a special case of linear programming. however its special structure makes it possible to solve the problem by algorithms that are more efficient than the simplex method. we outline the classic iterative improvement algorithm for this problem discovered by the american mathematicians l. r. ford jr. and d. r. fulkerson in the s. the last two sections of the chapter deal with bipartite matching. this is the problem of finding an optimal pairing of elements taken from two disjoint sets. examples include matching workers and jobs high school graduates and colleges and men and women for marriage. section . deals with the problem of maximizing the number of matched pairs section . is concerned with the matching stability. we also discuss several iterative improvement algorithms in section . where we consider approximation algorithms for the traveling salesman and knapsack problems. other examples of iterative improvement algorithms can be found in the algorithms textbook by moret and shapiro mor books on continuous and discrete optimization e.g. nem and the literature on heuristic search e.g. mic . 