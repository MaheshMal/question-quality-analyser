before we introduce a general method for solving linear programming problems let us consider a small example which will help us to see the fundamental properties of such problems. example consider the following linear programming problem in two variables maximize x y subject to x y . x y x y . by definition a feasible solution to this problem is any point x y that satisfies all the constraints of the problem the problem's feasible region is the set of all its feasible points. it is instructive to sketch the feasible region in the cartesian plane. recall that any equation ax by c where coefficients a and b are not both equal to zero defines a straight line. such a line divides the plane into two half planes for all the points in one of them ax by c while for all the points in the other ax by c. it is easy to determine which of the two half planes is which take any point x y not on the line ax by c and check which of the two inequalities hold ax by c or ax by c. in particular the set of points defined by inequality x y comprises the points on and below the line x y and the set of points defined by inequality x y comprises the points on and below the line x y . since the points of the feasible region must satisfy all the constraints of the problem the feasible region is obtained by the intersection of these two half planes and the first quadrant of the cartesian plane defined by the nonnegativity constraints x y see figure . . thus the feasible region for problem . is the convex polygon with the vertices and . the last point which is the point of intersection of the lines x y and x y is obtained by solving the system of these two linear equations. our task is to find an optimal solution a point in the feasible region with the largest value of the objective function z x y. are there feasible solutions for which the value of the objective function equals say ? the points x y for which the objective function z x y is equal to form the line x y . since this line does not have common points . george b. dantzig has received many honors including the national medal of science presented by the president of the united states in . the citation states that the national medal was awarded for inventing linear programming and discovering methods that led to wide scale scientific and technical applications to important problems in logistics scheduling and network optimization and to the use of computers in making efficient use of the mathematical theory. y x y x x y figure . feasible region of problem . . with the feasible region see figure . the answer to the posed question is no. on the other hand there are infinitely many feasible points for which the objective function is equal to say they are the intersection points of the line x y with the feasible region. note that the lines x y and x y have the same slope as would any line defined by equation x y z where z is some constant. such lines are called level lines of the objective function. thus our problem can be restated as finding the largest value of the parameter z for which the level line x y z has a common point with the feasible region. we can find this line either by shifting say the line x y south west without changing its slope! toward the feasible region until it hits the region for the first time or by shifting say the line x y north east until it hits the feasible region for the last time. either way it will happen at the point with the corresponding z value . . . this means that the optimal solution to the linear programming problem in question is x y with the maximal value of the objective function equal to . note that if we had to maximize z x y as the objective function in problem . the level line x y z for the largest value of z would coincide with the boundary line segment that has the same slope as the level lines draw this line in figure . . consequently all the points of the line segment between vertices and including the vertices themselves would be optimal solutions yielding of course the same maximal value of the objective function. y x x y x y x y figure . solving a two dimensional linear programming problem geometrically. does every linear programming problem have an optimal solution that can be found at a vertex of its feasible region? without appropriate qualifications the answer to this question is no. to begin with the feasible region of a linear programming problem can be empty. for example if the constraints include two contradictory requirements such as x y and x y there can be no points in the problem's feasible region. linear programming problems with the empty feasible region are called infeasible. obviously infeasible problems do not have optimal solutions. another complication may arise if the problem's feasible region is unbounded as the following example demonstrates. example if we reverse the inequalities in problem . to x y and x y the feasible region of the new problem will become unbounded see figure . . if the feasible region of a linear programming problem is unbounded its objective function may or may not attain a finite optimal value on it. for example the problem of maximizing z x y subject to the constraints x y x y x y has no optimal solution because there are points in the feasible region making x y as large as we wish. such problems are called unbounded. on the other hand the problem of minimizing z x y subject to the same constraints has an optimal solution which? . y x x y x y x y figure . unbounded feasible region of a linear programming problem with constraints x y x y x y and three level lines of the function x y. fortunately the most important features of the examples we considered above hold for problems with more than two variables. in particular a feasible region of a typical linear programming problem is in many ways similar to convex polygons in the two dimensional cartesian plane. specifically it always has a finite number of vertices which mathematicians prefer to call extreme points see section . . furthermore an optimal solution to a linear programming problem can be found at one of the extreme points of its feasible region. we reiterate these properties in the following theorem. theorem extreme point theorem any linear programming problem with a nonempty bounded feasible region has an optimal solution moreover an optimal solution can always be found at an extreme point of the problem's feasible region. this theorem implies that to solve a linear programming problem at least in the case of a bounded feasible region we can ignore all but a finite number of . except for some degenerate instances such as maximizing z x y subject to x y if a linear programming problem with an unbounded feasible region has an optimal solution it can also be found at an extreme point of the feasible region. points in its feasible region. in principle we can solve such a problem by computing the value of the objective function at each extreme point and selecting the one with the best value. there are two major obstacles to implementing this plan however. the first lies in the need for a mechanism for generating the extreme points of the feasible region. as we are going to see below a rather straightforward algebraic procedure for this task has been discovered. the second obstacle lies in the number of extreme points a typical feasible region has. here the news is bad the number of extreme points is known to grow exponentially with the size of the problem. this makes the exhaustive inspection of extreme points unrealistic for most linear programming problems of nontrivial sizes. fortunately it turns out that there exists an algorithm that typically inspects only a small fraction of the extreme points of the feasible region before reaching an optimal one. this famous algorithm is called the simplex method. the idea of this algorithm can be described in geometric terms as follows. start by identifying an extreme point of the feasible region. then check whether one can get an improved value of the objective function by going to an adjacent extreme point. if it is not the case the current point is optimal stop if it is the case proceed to an adjacent extreme point with an improved value of the objective function. after a finite number of steps the algorithm will either reach an extreme point where an optimal solution occurs or determine that no optimal solution exists. 