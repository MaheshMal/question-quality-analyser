once an algorithm has been specified you have to prove its correctness. that is you have to prove that the algorithm yields a required result for every legitimate input in a finite amount of time. for example the correctness of euclid's algorithm for computing the greatest common divisor stems from the correctness of the equality gcd m n gcd n m mod n which in turn needs a proof see problem in exercises . the simple observation that the second integer gets smaller on every iteration of the algorithm and the fact that the algorithm stops when the second integer becomes . for some algorithms a proof of correctness is quite easy for others it can be quite complex. a common technique for proving correctness is to use mathematical induction because an algorithm's iterations provide a natural sequence of steps needed for such proofs. it might be worth mentioning that although tracing the algorithm's performance for a few specific inputs can be a very worthwhile activity it cannot prove the algorithm's correctness conclusively. but in order to show that an algorithm is incorrect you need just one instance of its input for which the algorithm fails. the notion of correctness for approximation algorithms is less straightforward than it is for exact algorithms. for an approximation algorithm we usually would like to be able to show that the error produced by the algorithm does not exceed a predefined limit. you can find examples of such investigations in chapter . 