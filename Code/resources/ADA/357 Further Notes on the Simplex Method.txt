formal proofs of validity of the simplex method steps can be found in books devoted to a detailed discussion of linear programming e.g. dan . a few important remarks about the method still need to be made however. generally speaking an iteration of the simplex method leads to an extreme point of the problem's feasible region with a greater value of the objective function. in degenerate cases which arise when one or more basic variables are equal to zero the simplex method can only guarantee that the value of the objective function at the new extreme point is greater than or equal to its value at the previous point. in turn this opens the door to the possibility not only that the objective function's values stall for several iterations in a row but that the algorithm might cycle back to a previously considered point and hence never terminate. the latter phenomenon is called cycling. although it rarely if ever happens in practice specific examples of problems where cycling does occur have been constructed. a simple modification of steps and of the simplex method called bland's rule eliminates even the theoretical possibility of cycling. assuming that the variables are denoted by a subscripted letter e.g. x x . . . xn this rule can be stated as follows step modified among the columns with a negative entry in the objective row select the column with the smallest subscript. step modified resolve a tie among the smallest ratios by selecting the row labeled by the basic variable with the smallest subscript. another caveat deals with the assumptions made in step . they are automatically satisfied if a problem is given in the form where all the constraints imposed on nonnegative variables are inequalities ai x . . . ainxn bi with bi for i . . . m. indeed by adding a nonnegative slack variable xn i into the ith constraint we obtain the equality ai x . . . ainxn xn i bi and all the requirements imposed on an initial tableau of the simplex method are satisfied for the obvious basic feasible solution x . . . xn xn . . . xn m . but if a problem is not given in such a form finding an initial basic feasible solution may present a nontrivial obstacle. moreover for problems with an empty feasible region no initial basic feasible solution exists and we need an algorithmic way to identify such problems. one of the ways to address these issues is to use an extension to the classic simplex method called the two phase simplex method see e.g. kol . in a nutshell this method adds a set of artificial variables to the equality constraints of a given problem so that the new problem has an obvious basic feasible solution. it then solves the linear programming problem of minimizing the sum of the artificial variables by the simplex method. the optimal solution to this problem either yields an initial tableau for the original problem or indicates that the feasible region of the original problem is empty. how efficient is the simplex method? since the algorithm progresses through a sequence of adjacent points of a feasible region one should probably expect bad news because the number of extreme points is known to grow exponentially with the problem size. indeed the worst case efficiency of the simplex method has been shown to be exponential as well. fortunately more than half a century of practical experience with the algorithm has shown that the number of iterations in a typical application ranges between m and m with the number of operations per iteration proportional to mn where m and n are the numbers of equality constraints and variables respectively. since its discovery in the simplex method has been a subject of intensive study by many researchers. some of them have worked on improvements to the original algorithm and details of its efficient implementation. as a result of these efforts programs implementing the simplex method have been polished to the point that very large problems with hundreds of thousands of constraints and variables can be solved in a routine manner. in fact such programs have evolved into sophisticated software packages. these packages enable the user to enter a problem's constraints and obtain a solution in a user friendly form. they also provide tools for investigating important properties of the solution such as its sensitivity to changes in the input data. such investigations are very important for many applications including those in economics. at the other end of the spectrum linear programming problems of a moderate size can nowadays be solved on a desktop using a standard spreadsheet facility or by taking advantage of specialized software available on the internet. researchers have also tried to find algorithms for solving linear programming problems with polynomial time efficiency in the worst case. an important milestone in the history of such algorithms was the proof by l. g. khachian kha showing that the ellipsoid method can solve any linear programming problem in polynomial time. although the ellipsoid method was much slower than the simplex method in practice its better worst case efficiency encouraged a search for alternatives to the simplex method. in narendra karmarkar published an algorithm that not only had a polynomial worst case efficiency but also was competitive with the simplex method in empirical tests as well. although we are not going to discuss karmarkar's algorithm kar here it is worth pointing out that it is also based on the iterative improvement idea. however karmarkar's algorithm generates a sequence of feasible solutions that lie within the feasible region rather than going through a sequence of adjacent extreme points as the simplex method does. such algorithms are called interior point methods see e.g. arb . exercises . . consider the following version of the post office location problem problem in exercises . given n integers x x . . . xn representing coordinates of n villages located along a straight road find a location for a post office that minimizes the average distance between the villages. the post office may be but is not required to be located at one of the villages. devise an iterativeimprovement algorithm for this problem. is this an efficient way to solve this problem? . solve the following linear programming problems geometrically. a. maximize x y subject to x y x y x y b. maximize x y subject to x y y x x y . consider the linear programming problem minimize c x c y subject to x y x y x y where c and c are some real numbers not both equal to zero. a. give an example of the coefficient values c and c for which the problem has a unique optimal solution. b. give an example of the coefficient values c and c for which the problem has infinitely many optimal solutions. c. give an example of the coefficient values c and c for which the problem does not have an optimal solution. . would the solution to problem . be different if its inequality constraints were strict i.e. x y and x y respectively? . trace the simplex method on a. the problem of exercise a. b. the problem of exercise b. . trace the simplex method on the problem of example in section . a. by hand. b. by using one of the implementations available on the internet. . determine how many iterations the simplex method needs to solve the problem n maximize xj j subject to xj bj where bj for j . . . n. . can we apply the simplex method to solve the knapsack problem see example in section . ? if you answer yes indicate whether it is a good algorithm for the problem in question if you answer no explain why not. . prove that no linear programming problem can have exactly k optimal solutions unless k . . if a linear programming problem n maximize cj xj j n subject to aij xj bi for i . . . m j x x . . . xn is considered as primal then its dual is defined as the linear programming problem m minimize bi yi i m subject to aij yi cj for j . . . n i y y . . . ym . a. express the primal and dual problems in matrix notations. b. find the dual of the linear programming problem maximize x x x subject to x x x x x x x x x . c. solve the primal and dual problems and compare the optimal values of their objective functions. 