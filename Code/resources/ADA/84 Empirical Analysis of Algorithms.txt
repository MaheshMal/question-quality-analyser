in sections . and . we saw how algorithms both nonrecursive and recursive can be analyzed mathematically. though these techniques can be applied successfully to many simple algorithms the power of mathematics even when enhanced with more advanced techniques see sed pur gra and gre is far from limitless. in fact even some seemingly simple algorithms have proved to be very difficult to analyze with mathematical precision and certainty. as we pointed out in section . this is especially true for the average case analysis. the principal alternative to the mathematical analysis of an algorithm's efficiency is its empirical analysis. this approach implies steps spelled out in the following plan. general plan for the empirical analysis of algorithm time efficiency . understand the experiment's purpose. . decide on the efficiency metric m to be measured and the measurement unit an operation count vs. a time unit . . decide on characteristics of the input sample its range size and so on . . prepare a program implementing the algorithm or algorithms for the experimentation. . generate a sample of inputs. . run the algorithm or algorithms on the sample's inputs and record the data observed. . analyze the data obtained. let us discuss these steps one at a time. there are several different goals one can pursue in analyzing algorithms empirically. they include checking the accuracy of a theoretical assertion about the algorithm's efficiency comparing the efficiency of several algorithms for solving the same problem or different implementations of the same algorithm developing a hypothesis about the algorithm's efficiency class and ascertaining the efficiency of the program implementing the algorithm on a particular machine. obviously an experiment's design should depend on the question the experimenter seeks to answer. in particular the goal of the experiment should influence if not dictate how the algorithm's efficiency is to be measured. the first alternative is to insert a counter or counters into a program implementing the algorithm to count the number of times the algorithm's basic operation is executed. this is usually a straightforward operation you should only be mindful of the possibility that the basic operation is executed in several places in the program and that all its executions need to be accounted for. as straightforward as this task usually is you should always test the modified program to ensure that it works correctly in terms of both the problem it solves and the counts it yields. the second alternative is to time the program implementing the algorithm in question. the easiest way to do this is to use a system's command such as the time command in unix. alternatively one can measure the running time of a code fragment by asking for the system time right before the fragment's start tstart and just after its completion tfinish and then computing the difference between the two tfinish tstart . in c and c you can use the function clock for this purpose in java the method currenttimemillis in the system class is available. it is important to keep several facts in mind however. first a system's time is typically not very accurate and you might get somewhat different results on repeated runs of the same program on the same inputs. an obvious remedy is to make several such measurements and then take their average or the median as the sample's observation point. second given the high speed of modern computers the running time may fail to register at all and be reported as zero. the standard trick to overcome this obstacle is to run the program in an extra loop many times measure the total running time and then divide it by the number of the loop's repetitions. third on a computer running under a time sharing system such as unix the reported time may include the time spent by the cpu on other programs which obviously defeats the purpose of the experiment. therefore you should take care to ask the system for the time devoted specifically to execution of . if the system time is given in units called ticks the difference should be divided by a constant indicating the number of ticks per time unit. your program. in unix this time is called the user time and it is automatically provided by the time command. thus measuring the physical running time has several disadvantages both principal dependence on a particular machine being the most important of them and technical not shared by counting the executions of a basic operation. on the other hand the physical running time provides very specific information about an algorithm's performance in a particular computing environment which can be of more importance to the experimenter than say the algorithm's asymptotic efficiency class. in addition measuring time spent on different segments of a program can pinpoint a bottleneck in the program's performance that can be missed by an abstract deliberation about the algorithm's basic operation. getting such data called profiling is an important resource in the empirical analysis of an algorithm's running time the data in question can usually be obtained from the system tools available in most computing environments. whether you decide to measure the efficiency by basic operation counting or by time clocking you will need to decide on a sample of inputs for the experiment. often the goal is to use a sample representing a typical input so the challenge is to understand what a typical input is. for some classes of algorithms e.g. for algorithms for the traveling salesman problem that we are going to discuss later in the book researchers have developed a set of instances they use for benchmarking. but much more often than not an input sample has to be developed by the experimenter. typically you will have to make decisions about the sample size it is sensible to start with a relatively small sample and increase it later if necessary the range of instance sizes typically neither trivially small nor excessively large and a procedure for generating instances in the range chosen. the instance sizes can either adhere to some pattern e.g. . . . or . . . or be generated randomly within the range chosen. the principal advantage of size changing according to a pattern is that its impact is easier to analyze. for example if a sample's sizes are generated by doubling you can compute the ratios m n m n of the observed metric m the count or the time to see whether the ratios exhibit a behavior typical of algorithms in one of the basic efficiency classes discussed in section . . the major disadvantage of nonrandom sizes is the possibility that the algorithm under investigation exhibits atypical behavior on the sample chosen. for example if all the sizes in a sample are even and your algorithm runs much more slowly on oddsize inputs the empirical results will be quite misleading. another important issue concerning sizes in an experiment's sample is whether several instances of the same size should be included. if you expect the observed metric to vary considerably on instances of the same size it would be probably wise to include several instances for every size in the sample. there are well developed methods in statistics to help the experimenter make such decisions you will find no shortage of books on this subject. of course if several instances of the same size are included in the sample the averages or medians of the observed values for each size should be computed and investigated instead of or in addition to individual sample points. much more often than not an empirical analysis requires generating random numbers. even if you decide to use a pattern for input sizes you will typically want instances themselves generated randomly. generating random numbers on a digital computer is known to present a difficult problem because in principle the problem can be solved only approximately. this is the reason computer scientists prefer to call such numbers pseudorandom. as a practical matter the easiest and most natural way of getting such numbers is to take advantage of a random number generator available in computer language libraries. typically its output will be a value of a pseudo random variable uniformly distributed in the interval between and . if a different pseudo random variable is desired an appropriate transformation needs to be made. for example if x is a continuous random variable uniformly distributed on the interval x the variable y l x r l will be uniformly distributed among the integer values between integers l and r l r . alternatively you can implement one of several known algorithms for generating pseudo random numbers. the most widely used and thoroughly studied of such algorithms is the linear congruential method. algorithm random n m seed a b generates a sequence of n pseudorandom numbers according to the linear congruential method input a positive integer n and positive integer parameters m seed a b output a sequence r . . . rn of n pseudorandom integers uniformly distributed among integer values between and m note pseudorandom numbers between and can be obtained by treating the integers generated as digits after the decimal point r seed for i to n do ri a ri b mod m the simplicity of this pseudocode is misleading because the devil lies in the details of choosing the algorithm's parameters. here is a partial list of recommendations based on the results of a sophisticated mathematical analysis see knuii pp. for details seed may be chosen arbitrarily and is often set to the current date and time m should be large and may be conveniently taken as w where w is the computer's word size a should be selected as an integer between . m and . m with no particular pattern in its digits but such that a mod and the value of b can be chosen as . the empirical data obtained as the result of an experiment need to be recorded and then presented for an analysis. data can be presented numerically in a table or graphically in a scatterplot i.e. by points in a cartesian coordinate system. it is a good idea to use both these options whenever it is feasible because both methods have their unique strengths and weaknesses. the principal advantage of tabulated data lies in the opportunity to manipulate it easily. for example one can compute the ratios m n g n where g n is a candidate to represent the efficiency class of the algorithm in question. if the algorithm is indeed in g n most likely these ratios will converge to some positive constant as n gets large. note that careless novices sometimes assume that this constant must be which is of course incorrect according to the definition of g n . or one can compute the ratios m n m n and see how the running time reacts to doubling of its input size. as we discussed in section . such ratios should change only slightly for logarithmic algorithms and most likely converge to and for linear quadratic and cubic algorithms respectively to name the most obvious and convenient cases. on the other hand the form of a scatterplot may also help in ascertaining the algorithm's probable efficiency class. for a logarithmic algorithm the scatterplot will have a concave shape figure . a this fact distinguishes it from all the other basic efficiency classes. for a linear algorithm the points will tend to aggregate around a straight line or more generally to be contained between two straight lines figure . b . scatterplots of functions in n lg n and n will have a convex shape figure . c making them difficult to differentiate. a scatterplot of a cubic algorithm will also have a convex shape but it will show a much more rapid increase in the metric's values. an exponential algorithm will most probably require a logarithmic scale for the vertical axis in which the values of loga m n rather than those of m n are plotted. the commonly used logarithm base is or . in such a coordinate system a scatterplot of a truly exponential algorithm should resemble a linear function because m n can implies logb m n logb c n logb a and vice versa. one of the possible applications of the empirical analysis is to predict the algorithm's performance on an instance not included in the experiment sample. for example if you observe that the ratios m n g n are close to some constant c for the sample instances it could be sensible to approximate m n by the product cg n for other instances too. this approach should be used with caution especially for values of n outside the sample range. mathematicians call such predictions extrapolation as opposed to interpolation which deals with values within the sample range. of course you can try unleashing the standard techniques of statistical data analysis and prediction. note however that the majority of such techniques are based on specific probabilistic assumptions that may or may not be valid for the experimental data in question. it seems appropriate to end this section by pointing out the basic differences between mathematical and empirical analyses of algorithms. the principal strength of the mathematical analysis is its independence of specific inputs its principal weakness is its limited applicability especially for investigating the average case efficiency. the principal strength of the empirical analysis lies in its applicability to any algorithm but its results can depend on the particular sample of instances and the computer used in the experiment. count or time count or time n n a b count or time n c figure . typical scatter plots. a logarithmic. b linear. c one of the convex functions. exercises . . consider the following well known sorting algorithm which is studied later in the book with a counter inserted to count the number of key comparisons. algorithm sortanalysis a ..n input an array a ..n of n orderable elements output the total number of key comparisons made count for i to n do v a i j i while j and a j v do count count a j a j j j a j v return count is the comparison counter inserted in the right place? if you believe it is prove it if you believe it is not make an appropriate correction. . a. run the program of problem with a properly inserted counter or counters for the number of key comparisons on random arrays of sizes . . . . b. analyze the data obtained to form a hypothesis about the algorithm's average case efficiency. c. estimate the number of key comparisons we should expect for a randomly generated array of size sorted by the same algorithm. . repeat problem by measuring the program's running time in milliseconds. . hypothesize a likely efficiency class of an algorithm based on the following empirical observations of its basic operation's count size count . what scale transformation will make a logarithmic scatterplot look like a linear one? . how can one distinguish a scatterplot for an algorithm in lg lg n from a scatterplot for an algorithm in lg n ? . a. find empirically the largest number of divisions made by euclid's algorithm for computing gcd m n for n m . b. for each positive integer k find empirically the smallest pair of integers n m for which euclid's algorithm needs to make k divisions in order to find gcd m n . . the average case efficiency of euclid's algorithm on inputs of size n can be measured by the average number of divisions davg n made by the algorithm in computing gcd n gcd n . . . gcd n n . for example davg . . produce a scatterplot of davg n and indicate the algorithm's likely averagecase efficiency class. . run an experiment to ascertain the efficiency class of the sieve of eratosthenes see section . . . run a timing experiment for the three algorithms for computing gcd m n presented in section . . 