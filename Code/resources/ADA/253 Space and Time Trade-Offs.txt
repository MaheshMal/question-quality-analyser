things which matter most must never be at the mercy of things which matter less. johann wolfgang von go ethe space and time trade offs in algorithm design are a well known issue for both theoreticians and practitioners of computing. consider as an example the problem of computing values of a function at many points in its domain. if it is time that is at a premium we can precompute the function's values and store them in a table. this is exactly what human computers had to do before the advent of electronic computers in the process burdening libraries with thick volumes of mathematical tables. though such tables have lost much of their appeal with the widespread use of electronic computers the underlying idea has proven to be quite useful in the development of several important algorithms for other problems. in somewhat more general terms the idea is to preprocess the problem's input in whole or in part and store the additional information obtained to accelerate solving the problem afterward. we call this approach input enhancement and discuss the following algorithms based on it counting methods for sorting section . boyer moore algorithm for string matching and its simplified version suggested by horspool section . the other type of technique that exploits space for time trade offs simply uses extra space to facilitate faster and or more flexible access to the data. we call this approach prestructuring. this name highlights two facets of this variation of the space for time trade off some processing is done before a problem in question . the standard terms used synonymously for this technique are preprocessing and preconditioning. confusingly these terms can also be applied to methods that use the idea of preprocessing but do not use extra space see chapter . thus in order to avoid confusion we use input enhancement as a special name for the space for time trade off technique being discussed here. is actually solved but unlike the input enhancement variety it deals with access structuring. we illustrate this approach by hashing section . indexing with b trees section . there is one more algorithm design technique related to the space for time trade off idea dynamic programming. this strategy is based on recording solutions to overlapping subproblems of a given problem in a table from which a solution to the problem in question is then obtained. we discuss this well developed technique separately in the next chapter of the book. two final comments about the interplay between time and space in algorithm design need to be made. first the two resources time and space do not have to compete with each other in all design situations. in fact they can align to bring an algorithmic solution that minimizes both the running time and the space consumed. such a situation arises in particular when an algorithm uses a spaceefficient data structure to represent a problem's input which leads in turn to a faster algorithm. consider as an example the problem of traversing graphs. recall that the time efficiency of the two principal traversal algorithms depth first search and breadth first search depends on the data structure used for representing graphs it is n for the adjacency matrix representation and n m for the adjacency list representation where n and m are the numbers of vertices and edges respectively. if input graphs are sparse i.e. have few edges relative to the number of vertices say m o n the adjacency list representation may well be more efficient from both the space and the running time points of view. the same situation arises in the manipulation of sparse matrices and sparse polynomials if the percentage of zeros in such objects is sufficiently high we can save both space and time by ignoring zeros in the objects' representation and processing. second one cannot discuss space time trade offs without mentioning the hugely important area of data compression. note however that in data compression size reduction is the goal rather than a technique for solving another problem. we discuss just one data compression algorithm in the next chapter. the reader interested in this topic will find a wealth of algorithms in such books as say . 