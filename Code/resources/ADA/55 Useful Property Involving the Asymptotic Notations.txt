using the formal definitions of the asymptotic notations we can prove their general properties see problem in this section's exercises for a few simple examples . the following property in particular is useful in analyzing algorithms that comprise two consecutively executed parts. theorem if t n o g n and t n o g n then t n t n o max g n g n . the analogous assertions are true for the and notations as well. proof the proof extends to orders of growth the following simple fact about four arbitrary real numbers a b a b if a b and a b then a a max b b . since t n o g n there exist some positive constant c and some nonnegative integer n such that t n c g n for all n n . similarly since t n o g n t n c g n for all n n . let us denote c max c c and consider n max n n so that we can use both inequalities. adding them yields the following t n t n c g n c g n c g n c g n c g n g n c max g n g n . hence t n t n o max g n g n with the constants c and n required by the o definition being c max c c and max n n respectively. so what does this property imply for an algorithm that comprises two consecutively executed parts? it implies that the algorithm's overall efficiency is determined by the part with a higher order of growth i.e. its least efficient part t n o g n t n t n o max g n g n . t n o g n for example we can check whether an array has equal elements by the following two part algorithm first sort the array by applying some known sorting algorithm second scan the sorted array to check its consecutive elements for equality. if for example a sorting algorithm used in the first part makes no more than n n comparisons and hence is in o n while the second part makes no more than n comparisons and hence is in o n the efficiency of the entire algorithm will be in o max n n o n . 