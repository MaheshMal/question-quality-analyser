Multiplication of Large Integers and Strassen's Matrix Multiplication
In this section, we examine two surprising algorithms for seemingly straightfor-
          ward tasks: multiplying two integers and multiplying two square matrices. Both
     achieve a better asymptotic efficiency by ingenious application of the divide-and-
     conquer technique.
     Multiplication of Large Integers
     Some applications, notably modern cryptography, require manipulation of inte-
     gers that are over 100 decimal digits long. Since such integers are too long to fit in
     a single word of a modern computer, they require special treatment. This practi-
     cal need supports investigations of algorithms for efficient manipulation of large
     integers. In this section, we outline an interesting algorithm for multiplying such
     numbers. Obviously, if we use the conventional pen-and-pencil algorithm for mul-
     tiplying two n-digit integers, each of the n digits of the first number is multiplied by
     each of the n digits of the second number for the total of n2 digit multiplications.
     (If one of the numbers has fewer digits than the other, we can pad the shorter
     number with leading zeros to equalize their lengths.) Though it might appear that
     it would be impossible to design an algorithm with fewer than n2 digit multiplica-
     tions, this turns out not to be the case. The miracle of divide-and-conquer comes
     to the rescue to accomplish this feat.
     To demonstrate the basic idea of the algorithm, let us start with a case of
     two-digit integers, say, 23 and 14. These numbers can be represented as follows:
            23 = 2 . 101 + 3 . 100           and  14 = 1 . 101 + 4 . 100.
     Now let us multiply them:
            23  14 = (2 . 101 + 3 . 100)  (1 . 101 + 4 . 100)
                         = (2  1)102 + (2  4 + 3  1)101 + (3  4)100.
     The last formula yields the correct answer of 322, of course, but it uses the same
     four digit multiplications as the pen-and-pencil algorithm. Fortunately, we can
     compute the middle term with just one digit multiplication by taking advantage
     of the products 2  1 and 3  4 that need to be computed anyway:
            2  4 + 3  1 = (2 + 3)  (1 + 4) - 2  1 - 3  4.
     Of course, there is nothing special about the numbers we just multiplied.
     For any pair of two-digit numbers a = a1a0 and b = b1b0, their product c can be
     computed by the formula
                                c = a  b = c2102 + c1101 + c0,
     where
     c2 = a1  b1 is the product of their first digits,
     c0 = a0  b0 is the product of their second digits,
     c1 = (a1 + a0)  (b1 + b0) - (c2 + c0) is the product of the sum of the
            a's digits and the sum of the b's digits minus the sum of c2 and c0.
     Now we apply this trick to multiplying two n-digit integers a and b where n is
     a positive even number. Let us divide both numbers in the middle--after all, we
     promised to take advantage of the divide-and-conquer technique. We denote the
     first half of the a's digits by a1 and the second half by a0; for b, the notations are b1
     and b0, respectively. In these notations, a = a1a0 implies that a = a110n/2 + a0 and
     b = b1b0 implies that b = b110n/2 + b0. Therefore, taking advantage of the same
     trick we used for two-digit numbers, we get
            c = a  b = (a110n/2 + a0)  (b110n/2 + b0)
                          = (a1  b1)10n + (a1  b0 + a0  b1)10n/2 + (a0  b0)
                          = c210n + c110n/2 + c0,
     where
     c2 = a1  b1 is the product of their first halves,
     c0 = a0  b0 is the product of their second halves,
     c1 = (a1 + a0)  (b1 + b0) - (c2 + c0) is the product of the sum of the
            a's halves and the sum of the b's halves minus the sum of c2 and c0.
     If n/2 is even, we can apply the same method for computing the products c2, c0,
     and c1. Thus, if n is a power of 2, we have a recursive algorithm for computing the
     product of two n-digit integers. In its pure form, the recursion is stopped when n
     becomes 1. It can also be stopped when we deem n small enough to multiply the
     numbers of that size directly.
     How many digit multiplications does this algorithm make? Since multiplica-
     tion of n-digit numbers requires three multiplications of n/2-digit numbers, the
     recurrence for the number of multiplications M(n) is
                          M(n) = 3M(n/2)           for n > 1,  M(1) = 1.
     Solving it by backward substitutions for n = 2k yields
                          M(2k) = 3M(2k-1) = 3[3M(2k-2)] = 32M(2k-2)
                          = . . . = 3iM(2k-i) = . . . = 3kM(2k-k) = 3k.
     Since k = log2 n,
                          M(n) = 3log2 n = nlog2 3  n1.585.
     (On the last step, we took advantage of the following property of logarithms:
     alogb c = clogb a.)
     But what about additions and subtractions? Have we not decreased the num-
     ber of multiplications by requiring more of those operations? Let A(n) be the
     number of digit additions and subtractions executed by the above algorithm in
     multiplying two n-digit decimal integers. Besides 3A(n/2) of these operations
     needed to compute the three products of n/2-digit numbers, the above formulas
     require five additions and one subtraction. Hence, we have the recurrence
                           A(n) = 3A(n/2) + cn  for n > 1,  A(1) = 1.
     Applying the Master Theorem, which was stated in the beginning of the chapter,
     we obtain A(n)        (nlog2 3), which means that the total number of additions and
     subtractions have the same asymptotic order of growth as the number of multipli-
     cations.
     The asymptotic advantage of this algorithm notwithstanding, how practical is
     it? The answer depends, of course, on the computer system and program quality
     implementing the algorithm, which might explain the rather wide disparity of
     reported results. On some machines, the divide-and-conquer algorithm has been
     reported to outperform the conventional method on numbers only 8 decimal digits
     long and to run more than twice faster with numbers over 300 decimal digits
     long--the area of particular importance for modern cryptography. Whatever this
     outperformance "crossover point" happens to be on a particular machine, it is
     worth switching to the conventional algorithm after the multiplicands become
     smaller than the crossover point. Finally, if you program in an object-oriented
     language such as Java, C++, or Smalltalk, you should also be aware that these
     languages have special classes for dealing with large integers.
     Discovered by 23-year-old Russian mathematician Anatoly Karatsuba in
     1960, the divide-and-conquer algorithm proved wrong the then-prevailing opinion
     that the time efficiency of any integer multiplication algorithm must be in     (n2).
     The discovery encouraged researchers to look for even (asymptotically) faster
     algorithms for this and other algebraic problems. We will see such an algorithm
     in the next section.
     Strassen's Matrix Multiplication
     Now that we have seen that the divide-and-conquer approach can reduce the
     number of one-digit multiplications in multiplying two integers, we should not be
     surprised that a similar feat can be accomplished for multiplying matrices. Such
     an algorithm was published by V. Strassen in 1969 [Str69].             The principal insight
     of the algorithm lies in the discovery that we can find the product C of two 2 × 2
     matrices A and B with just seven multiplications as opposed to the eight required
     by the brute-force algorithm (see Example 3 in Section 2.3). This is accomplished
     by using the following formulas:
               c00    c01  =  a00      a01      b00  b01
               c10    c11     a10      a11      b10  b11
                           =  m1 + m4 - m5 + m7             m3 + m5               ,
                                       m2 + m4       m1 + m3 - m2 + m6
     where
                              m1 = (a00 + a11)  (b00 + b11),
                              m2 = (a10 + a11)  b00,
                              m3 = a00  (b01 - b11),
                              m4 = a11  (b10 - b00),
                              m5 = (a00 + a01)  b11,
                              m6 = (a10 - a00)  (b00 + b01),
                              m7 = (a01 - a11)  (b10 + b11).
     Thus, to multiply two 2 × 2 matrices, Strassen's algorithm makes seven multipli-
     cations and 18 additions/subtractions, whereas the brute-force algorithm requires
     eight multiplications and four additions. These numbers should not lead us to
     multiplying 2 × 2 matrices by Strassen's algorithm. Its importance stems from its
     asymptotic superiority as matrix order n goes to infinity.
     Let A and B be two n × n matrices where n is a power of 2. (If n is not a power
     of 2, matrices can be padded with rows and columns of zeros.) We can divide A,
     B, and their product C into four n/2 × n/2 submatrices each as follows:
                         C00  C01  =  A00  A01          B00      B01
                                                                      .
                         C10  C11     A10  A11          B10      B11
     It is not difficult to verify that one can treat these submatrices as numbers to
     get the correct product. For example, C00 can be computed either as A00  B00 +
     A01  B10 or as M1 + M4 - M5 + M7 where M1, M4, M5, and M7 are found by
     Strassen's formulas, with the numbers replaced by the corresponding submatrices.
     If the seven products of n/2 × n/2 matrices are computed recursively by the same
     method, we have Strassen's algorithm for matrix multiplication.
     Let us evaluate the asymptotic efficiency of this algorithm. If M(n) is the
     number of multiplications made by Strassen's algorithm in multiplying two n × n
     matrices (where n is a power of 2), we get the following recurrence relation for it:
                         M(n) = 7M(n/2)    for n > 1,   M(1) = 1.
     Since n = 2k,
                    M(2k) = 7M(2k-1) = 7[7M(2k-2)] = 72M(2k-2) = . . .
                         = 7iM(2k-i) . . . = 7kM(2k-k) = 7k.
     Since k = log2 n,
                              M(n) = 7log2 n = nlog2 7  n2.807,
     which is smaller than n3 required by the brute-force algorithm.
     Since this savings in the number of multiplications was achieved at the expense
     of making extra additions, we must check the number of additions A(n) made by
     Strassen's algorithm. To multiply two matrices of order n > 1, the algorithm needs
     to multiply seven matrices of order n/2 and make 18 additions/subtractions of
     matrices of size n/2; when n = 1, no additions are made since two numbers are
     simply multiplied. These observations yield the following recurrence relation:
                       A(n) = 7A(n/2) + 18(n/2)2  for n > 1,  A(1) = 0.
     Though one can obtain a closed-form solution to this recurrence (see Problem 8
     in this section's exercises), here we simply establish the solution's order of growth.
     According to the Master Theorem, A(n)        (nlog2 7). In other words, the number
     of additions has the same order of growth as the number of multiplications. This
     puts Strassen's algorithm in    (nlog2 7), which is a better efficiency class than      (n3)
     of the brute-force method.
         Since the time of Strassen's discovery, several other algorithms for multiplying
     two n × n matrices of real numbers in O(n) time with progressively smaller
     constants  have been invented. The fastest algorithm so far is that of Coopersmith
     and Winograd [Coo87] with its efficiency in O(n2.376). The decreasing values of
     the exponents have been obtained at the expense of the increasing complexity
     of these algorithms. Because of large multiplicative constants, none of them is of
     practical value. However, they are interesting from a theoretical point of view. On
     one hand, they get closer and closer to the best theoretical lower bound known
     for matrix multiplication, which is n2 multiplications, though the gap between this
     bound and the best available algorithm remains unresolved. On the other hand,
     matrix multiplication is known to be computationally equivalent to some other
     important problems, such as solving systems of linear equations (discussed in the
     next chapter).
     Exercises 5.4
     1.  What are the smallest and largest numbers of digits the product of two decimal
         n-digit integers can have?
     2.  Compute 2101  1130 by applying the divide-and-conquer algorithm outlined
         in the text.
     3.  a.  Prove the equality alogb c = clogb a, which was used in Section 5.4.
         b.  Why is nlog2 3 better than 3log2 n as a closed-form formula for M(n)?
     4.  a.  Why did we not include multiplications by 10n in the multiplication count
             M(n) of the large-integer multiplication algorithm?
         b.  In addition to assuming that n is a power of 2, we made, for the sake of
             simplicity, another, more subtle, assumption in setting up the recurrences
             for M(n) and A(n), which is not always true (it does not change the final
             answers, however). What is this assumption?
     5.  How many one-digit additions are made by the pen-and-pencil algorithm in
         multiplying two n-digit integers? You may disregard potential carries.
     6.  Verify the formulas underlying Strassen's algorithm for multiplying 2 × 2
         matrices.
          7.   Apply Strassen's algorithm to compute
                                      1  0     2  1          0  1  0  1  
                                      4  1     1  0          2  1  0  4  
                                      0  1     3  0          2  0  1  1
                                      5  0     2  1          1  3  5  0
               exiting the recursion when n = 2, i.e., computing the products of 2 × 2 matrices
               by the brute-force algorithm.
          8.   Solve the recurrence for the number of additions required by Strassen's algo-
               rithm. Assume that n is a power of 2.
          9.   V. Pan [Pan78] has discovered a divide-and-conquer matrix multiplication
               algorithm that is based on multiplying two 70 × 70 matrices using 143,640
               multiplications. Find the asymptotic efficiency of Pan's algorithm (you may
               ignore additions) and compare it with that of Strassen's algorithm.
          10.  Practical implementations of Strassen's algorithm usually switch to the brute-
               force method after matrix sizes become smaller than some crossover point.
               Run an experiment to determine such a crossover point on your computer
               system.
     