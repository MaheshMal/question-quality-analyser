an idea like a ghost . . . must be spoken to a little before it will explain itself. charles dickens dynamic programming is an algorithm design technique with a rather interesting history. it was invented by a prominent u.s. mathematician richard bellman in the s as a general method for optimizing multistage decision processes. thus the word programming in the name of this technique stands for planning and does not refer to computer programming. after proving its worth as an important tool of applied mathematics dynamic programming has eventually come to be considered at least in computer science circles as a general algorithm design technique that does not have to be limited to special types of optimization problems. it is from this point of view that we will consider this technique here. dynamic programming is a technique for solving problems with overlapping subproblems. typically these subproblems arise from a recurrence relating a given problem's solution to solutions of its smaller subproblems. rather than solving overlapping subproblems again and again dynamic programming suggests solving each of the smaller subproblems only once and recording the results in a table from which a solution to the original problem can then be obtained. this technique can be illustrated by revisiting the fibonacci numbers discussed in section . . if you have not read that section you will be able to follow the discussion anyway. but it is a beautiful topic so if you feel a temptation to read it do succumb to it. the fibonacci numbers are the elements of the sequence . . . which can be defined by the simple recurrence f n f n f n for n . and two initial conditions f f . . if we try to use recurrence . directly to compute the nth fibonacci number f n we would have to recompute the same values of this function many times see figure . for an example . note that the problem of computing f n is expressed in terms of its smaller and overlapping subproblems of computing f n and f n . so we can simply fill elements of a one dimensional array with the n consecutive values of f n by starting in view of initial conditions . with and and using equation . as the rule for producing all the other elements. obviously the last element of this array will contain f n . single loop pseudocode of this very simple algorithm can be found in section . . note that we can in fact avoid using an extra array to accomplish this task by recording the values of just the last two elements of the fibonacci sequence problem in exercises . . this phenomenon is not unusual and we shall encounter it in a few more examples in this chapter. thus although a straightforward application of dynamic programming can be interpreted as a special variety of space for time trade off a dynamic programming algorithm can sometimes be refined to avoid using extra space. certain algorithms compute the nth fibonacci number without computing all the preceding elements of this sequence see section . . it is typical of an algorithm based on the classic bottom up dynamic programming approach however to solve all smaller subproblems of a given problem. one variation of the dynamic programming approach seeks to avoid solving unnecessary subproblems. this technique illustrated in section . exploits so called memory functions and can be considered a top down variation of dynamic programming. whether one uses the classical bottom up version of dynamic programming or its top down variation the crucial step in designing such an algorithm remains the same deriving a recurrence relating a solution to the problem to solutions to its smaller subproblems. the immediate availability of equation . for computing the nth fibonacci number is one of the few exceptions to this rule. since a majority of dynamic programming applications deal with optimization problems we also need to mention a general principle that underlines such applications. richard bellman called it the principle of optimality. in terms somewhat different from its original formulation it says that an optimal solution to any instance of an optimization problem is composed of optimal solutions to its subinstances. the principle of optimality holds much more often than not. to give a rather rare example it fails for finding the longest simple path in a graph. although its applicability to a particular problem needs to be checked of course such a check is usually not a principal difficulty in developing a dynamic programming algorithm. in the sections and exercises of this chapter are a few standard examples of dynamic programming algorithms. the algorithms in section . were in fact invented independently of the discovery of dynamic programming and only later came to be viewed as examples of this technique's applications. numerous other applications range from the optimal way of breaking text into lines e.g. baa to image resizing avi to a variety of applications to sophisticated engineering problems e.g. ber . 