why this emphasis on the count's order of growth for large input sizes? a difference in running times on small inputs is not what really distinguishes efficient algorithms from inefficient ones. when we have to compute for example the greatest common divisor of two small numbers it is not immediately clear how much more efficient euclid's algorithm is compared to the other two algorithms discussed in section . or even why we should care which of them is faster and by how much. it is only when we have to find the greatest common divisor of two large numbers that the difference in algorithm efficiencies becomes both clear and important. for large values of n it is the function's order of growth that counts just look at table . which contains values of a few functions particularly important for analysis of algorithms. the magnitude of the numbers in table . has a profound significance for the analysis of algorithms. the function growing the slowest among these is the logarithmic function. it grows so slowly in fact that we should expect a program table . values some approximate of several functions important for analysis of algorithms n log n n n log n n n n n! . . . . . . . . . . . . . . . . . . . . implementing an algorithm with a logarithmic basic operation count to run practically instantaneously on inputs of all realistic sizes. also note that although specific values of such a count depend of course on the logarithm's base the formula loga n loga b logb n makes it possible to switch from one base to another leaving the count logarithmic but with a new multiplicative constant. this is why we omit a logarithm's base and write simply log n in situations where we are interested just in a function's order of growth to within a multiplicative constant. on the other end of the spectrum are the exponential function n and the factorial function n! both these functions grow so fast that their values become astronomically large even for rather small values of n. this is the reason why we did not include their values for n in table . . for example it would take about . years for a computer making a trillion operations per second to execute operations. though this is incomparably faster than it would have taken to execute ! operations it is still longer than . billion . . years the estimated age of the planet earth. there is a tremendous difference between the orders of growth of the functions n and n! yet both are often referred to as exponential growth functions or simply exponential despite the fact that strictly speaking only the former should be referred to as such. the bottom line which is important to remember is this algorithms that require an exponential number of operations are practical for solving only problems of very small sizes. another way to appreciate the qualitative difference among the orders of growth of the functions in table . is to consider how they react to say a twofold increase in the value of their argument n. the function log n increases in value by just because log n log log n log n the linear function increases twofold the linearithmic function n log n increases slightly more than twofold the quadratic function n and cubic function n increase fourfold and eightfold respectively because n n and n n the value of n gets squared because n n and n! increases much more than that yes even mathematics refuses to cooperate to give a neat answer for n! . 