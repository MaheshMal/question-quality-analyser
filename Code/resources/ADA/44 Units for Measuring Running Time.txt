the next issue concerns units for measuring an algorithm's running time. of course we can simply use some standard unit of time measurement a second or millisecond and so on to measure the running time of a program implementing the algorithm. there are obvious drawbacks to such an approach however dependence on the speed of a particular computer dependence on the quality of a program implementing the algorithm and of the compiler used in generating the machine code and the difficulty of clocking the actual running time of the program. since we are after a measure of an algorithm's efficiency we would like to have a metric that does not depend on these extraneous factors. one possible approach is to count the number of times each of the algorithm's operations is executed. this approach is both excessively difficult and as we shall see usually unnecessary. the thing to do is to identify the most important operation of the algorithm called the basic operation the operation contributing the most to the total running time and compute the number of times the basic operation is executed. as a rule it is not difficult to identify the basic operation of an algorithm it is usually the most time consuming operation in the algorithm's innermost loop. for example most sorting algorithms work by comparing elements keys of a list being sorted with each other for such algorithms the basic operation is a key comparison. as another example algorithms for mathematical problems typically involve some or all of the four arithmetical operations addition subtraction multiplication and division. of the four the most time consuming operation is division followed by multiplication and then addition and subtraction with the last two usually considered together. thus the established framework for the analysis of an algorithm's time efficiency suggests measuring it by counting the number of times the algorithm's basic operation is executed on inputs of size n. we will find out how to compute such a count for nonrecursive and recursive algorithms in sections . and . respectively. here is an important application. let cop be the execution time of an algorithm's basic operation on a particular computer and let c n be the number of times this operation needs to be executed for this algorithm. then we can estimate . on some computers multiplication does not take longer than addition subtraction see for example the timing data provided by kernighan and pike in ker pp. . the running time t n of a program implementing this algorithm on that computer by the formula t n copc n . of course this formula should be used with caution. the count c n does not contain any information about operations that are not basic and in fact the count itself is often computed only approximately. further the constant cop is also an approximation whose reliability is not always easy to assess. still unless n is extremely large or very small the formula can give a reasonable estimate of the algorithm's running time. it also makes it possible to answer such questions as how much faster would this algorithm run on a machine that is times faster than the one we have? the answer is obviously times. or assuming that c n n n how much longer will the algorithm run if we double its input size? the answer is about four times longer. indeed for all but very small values of n c n n n n n n and therefore t n copc n n . t n copc n n note that we were able to answer the last question without actually knowing the value of cop it was neatly cancelled out in the ratio. also note that the multiplicative constant in the formula for the count c n was also cancelled out. it is for these reasons that the efficiency analysis framework ignores multiplicative constants and concentrates on the count's order of growth to within a constant multiple for large size inputs. 