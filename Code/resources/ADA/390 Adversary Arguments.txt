let us revisit the same game of guessing a number used to introduce the idea of an information theoretic argument. we can prove that any algorithm that solves this problem must ask at least log n questions in its worst case by playing the role of a hostile adversary who wants to make an algorithm ask as many questions as possible. the adversary starts by considering each of the numbers between and n as being potentially selected. this is cheating of course as far as the game is concerned but not as a way to prove our assertion. after each question the adversary gives an answer that leaves him with the largest set of numbers consistent with this and all the previously given answers. this strategy leaves him with at least one half of the numbers he had before his last answer. if an algorithm stops before the size of the set is reduced to the adversary can exhibit a number that could be a legitimate input the algorithm failed to identify. it is a simple technical matter now to show that one needs log n iterations to shrink an n element set to a one element set by halving and rounding up the size of the remaining set. hence at least log n questions need to be asked by any algorithm in the worst case. this example illustrates the adversary method for establishing lower bounds. it is based on following the logic of a malevolent but honest adversary the malevolence makes him push the algorithm down the most time consuming path and his honesty forces him to stay consistent with the choices already made. a lower bound is then obtained by measuring the amount of work needed to shrink a set of potential inputs to a single input along the most time consuming path. as another example consider the problem of merging two sorted lists of size n a a . . . an and b b . . . bn into a single sorted list of size n. for simplicity we assume that all the a's and b's are distinct which gives the problem a unique solution. we encountered this problem when discussing mergesort in section . . recall that we did merging by repeatedly comparing the first elements in the remaining lists and outputting the smaller among them. the number of key comparisons in the worst case for this algorithm for merging is n . is there an algorithm that can do merging faster? the answer turns out to be no. knuth knuiii p. quotes the following adversary method for proving that n is a lower bound on the number of key comparisons made by any comparison based algorithm for this problem. the adversary will employ the following rule reply true to the comparison ai bj if and only if i j. this will force any correct merging algorithm to produce the only combined list consistent with this rule b a b a . . . bn an. to produce this combined list any correct algorithm will have to explicitly compare n adjacent pairs of its elements i.e. b to a a to b and so on. if one of these comparisons has not been made e.g. a has not been compared to b we can transpose these keys to get b b a a . . . bn an which is consistent with all the comparisons made but cannot be distinguished from the correct configuration given above. hence n is indeed a lower bound for the number of key comparisons needed for any merging algorithm. 