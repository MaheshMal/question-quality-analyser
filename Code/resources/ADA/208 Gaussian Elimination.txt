you are certainly familiar with systems of two linear equations in two unknowns a x a y b a x a y b . recall that unless the coefficients of one equation are proportional to the coefficients of the other the system has a unique solution. the standard method for finding this solution is to use either equation to express one of the variables as a function of the other and then substitute the result into the other equation yielding a linear equation whose solution is then used to find the value of the second variable. in many applications we need to solve a system of n equations in n unknowns a x a x . . . a nxn b a x a x . . . a nxn b ... an x an x . . . annxn bn where n is a large number. theoretically we can solve such a system by generalizing the substitution method for solving systems of two linear equations what general design technique would such a method be based upon? however the resulting algorithm would be extremely cumbersome. fortunately there is a much more elegant algorithm for solving systems of linear equations called gaussian elimination. the idea of gaussian elimination is to transform a system of n linear equations in n unknowns to an equivalent system i.e. a system with the same solution as the original one with an uppertriangular coefficient matrix a matrix with all zeros below its main diagonal . the method is named after carl friedrich gauss who like other giants in the history of mathematics such as isaac newton and leonhard euler made numerous fundamental contributions to both theoretical and computational mathematics. the method was known to the chinese years before the europeans rediscovered it. a x a x . . . a nxn b a x a x . . . a nxn b a... x a x . . . a nxn b a x . . . a nxn b... an x an x . . . annxn bn annxn bn. in matrix notations we can write this as ax b ax b where a a ... a n b a a ... a n b a a... a ... a n b b... a a ... a n b b... . ... an an ... ann bn ... ann bn we added primes to the matrix elements and right hand sides of the new system to stress the point that their values differ from their counterparts in the original system. why is the system with the upper triangular coefficient matrix better than a system with an arbitrary coefficient matrix? because we can easily solve the system with an upper triangular coefficient matrix by back substitutions as follows. first we can immediately find the value of xn from the last equation then we can substitute this value into the next to last equation to get xn and so on until we substitute the known values of the last n variables into the first equation from which we find the value of x . so how can we get from a system with an arbitrary coefficient matrix a to an equivalent system with an upper triangular coefficient matrix a ? we can do that through a series of the so called elementary operations exchanging two equations of the system replacing an equation with its nonzero multiple replacing an equation with a sum or difference of this equation and some multiple of another equation since no elementary operation can change a solution to a system any system that is obtained through a series of such operations will have the same solution as the original one. let us see how we can get to a system with an upper triangular matrix. first we use a as a pivot to make all x coefficients zeros in the equations below the first one. specifically we replace the second equation with the difference between it and the first equation multiplied by a a to get an equation with a zero coefficient for x . doing the same for the third fourth and finally nth equation with the multiples a a a a . . . an a of the first equation respectively makes all the coefficients of x below the first equation zero. then we get rid of all the coefficients of x by subtracting an appropriate multiple of the second equation from each of the equations below the second one. repeating this elimination for each of the first n variables ultimately yields a system with an upper triangular coefficient matrix. before we look at an example of gaussian elimination let us note that we can operate with just a system's coefficient matrix augmented as its n st column with the equations' right hand side values. in other words we need to write explicitly neither the variable names nor the plus and equality signs. example solve the system by gaussian elimination. x x x x x x x x x . row row row row row row now we can obtain the solution by back substitutions x x x and x x x . here is pseudocode of the first stage called forward elimination of the algorithm. algorithm forwardelimination a ..n ..n b ..n applies gaussian elimination to matrix a of a system's coefficients augmented with vector b of the system's right hand side values input matrix a ..n ..n and column vector b ..n output an equivalent upper triangular matrix in place of a with the corresponding right hand side values in the n st column for i to n do a i n b i augments the matrix for i to n do for j i to n do for k i to n do a j k a j k a i k a j i a i i there are two important observations to make about this pseudocode. first it is not always correct if a i i we cannot divide by it and hence cannot use the ith row as a pivot for the ith iteration of the algorithm. in such a case we should take advantage of the first elementary operation and exchange the ith row with some row below it that has a nonzero coefficient in the ith column. if the system has a unique solution which is the normal case for systems under consideration such a row must exist. since we have to be prepared for the possibility of row exchanges anyway we can take care of another potential difficulty the possibility that a i i is so small and consequently the scaling factor a j i a i i so large that the new value of a j k might become distorted by a round off error caused by a subtraction of two numbers of greatly different magnitudes. to avoid this problem we can always look for a row with the largest absolute value of the coefficient in the ith column exchange it with the ith row and then use the new a i i as the ith iteration's pivot. this modification called partial pivoting guarantees that the magnitude of the scaling factor will never exceed . the second observation is the fact that the innermost loop is written with a glaring inefficiency. can you find it before checking the following pseudocode which both incorporates partial pivoting and eliminates this inefficiency? algorithm betterforwardelimination a ..n ..n b ..n implements gaussian elimination with partial pivoting input matrix a ..n ..n and column vector b ..n output an equivalent upper triangular matrix in place of a and the corresponding right hand side values in place of the n st column for i to n do a i n b i appends b to a as the last column for i to n do pivotrow i for j i to n do if a j i a pivotrow i pivotrow j for k i to n do swap a i k a pivotrow k for j i to n do temp a j i a i i for k i to n do a j k a j k a i k temp let us find the time efficiency of this algorithm. its innermost loop consists of a single line a j k a j k a i k temp . we discuss round off errors in more detail in section . . which contains one multiplication and one subtraction. on most computers multiplication is unquestionably more expensive than addition subtraction and hence it is multiplication that is usually quoted as the algorithm's basic operation. the standard summation formulas and rules reviewed in section . see also appendix a are very helpful in the following derivation n n n n n n n c n n i n i i j i k i i j i i j i n n n i n i n i n i i i n n n n . . . . n n n n n n n n j j j j j j j n n n n n . since the second back substitution stage of gaussian elimination is in n as you are asked to show in the exercises the running time is dominated by the cubic elimination stage making the entire algorithm cubic as well. theoretically gaussian elimination always either yields an exact solution to a system of linear equations when the system has a unique solution or discovers that no such solution exists. in the latter case the system will have either no solutions or infinitely many of them. in practice solving systems of significant size on a computer by this method is not nearly so straightforward as the method would lead us to believe. the principal difficulty lies in preventing an accumulation of round off errors see section . . consult textbooks on numerical analysis that analyze this and other implementation issues in great detail. 