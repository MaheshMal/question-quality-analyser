numerical analysis is usually described as the branch of computer science concerned with algorithms for solving mathematical problems. this description needs an important clarification the problems in question are problems of continuous mathematics solving equations and systems of equations evaluating such functions as sin x and ln x computing integrals and so on as opposed to problems of discrete mathematics dealing with such structures as graphs trees permutations and combinations. our interest in efficient algorithms for mathematical problems stems from the fact that these problems arise as models of many real life phenomena both in the natural world and in the social sciences. in fact numerical analysis used to be the main area of research study and application of computer science. with the rapid proliferation of computers in business and everyday life applications which deal primarily with storage and retrieval of information the relative importance of numerical analysis has shrunk in the last years. however its applications enhanced by the power of modern computers continue to expand in all areas of fundamental research and technology. thus wherever one's interests lie in the wide world of modern computing it is important to have at least some understanding of the special challenges posed by continuous mathematical problems. we are not going to discuss the variety of difficulties posed by modeling the task of describing a real life phenomenon in mathematical terms. assuming that this has already been done what principal obstacles to solving a mathematical problem do we face? the first major obstacle is the fact that most numerical analysis problems cannot be solved exactly. they have to be solved approximately and this is usually done by replacing an infinite object by a finite approximation. for example the value of ex at a given point x can be computed by approximating its infinite taylor's series about x by a finite sum of its first terms called the nth degree taylor polynomial ex x x . . . xn . . ! n! to give another example the definite integral of a function can be approximated by a finite weighted sum of its values as in the composite trapezoidal rule that you might remember from your calculus class b h f n f x d x a f xi f b . a i where h b a n xi a ih for i . . . n figure . . the errors of such approximations are called truncation errors. one of the major tasks in numerical analysis is to estimate the magnitudes of truncation . solving a system of linear equations and polynomial evaluation discussed in sections . and . respectively are rare exceptions to this rule. h h h h x a x xi xi xi xn b figure . composite trapezoidal rule. errors. this is typically done by using calculus tools from elementary to quite advanced. for example for approximation . we have ex x x . . . xn m x n . ! n! n ! where m max e on the segment with the endpoints at and x. this formula makes it possible to determine the degree of taylor's polynomial needed to guarantee a predefined accuracy level of approximation . . for example if we want to compute e . by formula . and guarantee the truncation error to be smaller than we can proceed as follows. first we estimate m of formula . m max e e . . . using this bound and the desired accuracy level of we obtain from . m . n . n . n ! n ! to solve the last inequality we can compute the first few values of . n n n ! n ! to see that the smallest value of n for which this inequality holds is . similarly for approximation . the standard bound of the truncation error is given by the inequality b h n b a h f x dx f a f xi f b m . a i where m max f x on the interval a x b. you are asked to use this inequality in the exercises for this section problems and . the other type of errors called round off errors are caused by the limited accuracy with which we can represent real numbers in a digital computer. these errors arise not only for all irrational numbers which by definition require an infinite number of digits for their exact representation but for many rational numbers as well. in the overwhelming majority of situations real numbers are represented as floating point numbers .d d . . . dp . be . where b is the number base usually or or for unsophisticated calculators d d . . . dp are digits di b for i . . . p and d unless the number is representing together the fractional part of the number and called its mantissa and e is an integer exponent with the range of values approximately symmetric about . the accuracy of the floating point representation depends on the number of significant digits p in representation . . most computers permit two or even three levels of precision single precision typically equivalent to between and significant decimal digits double precision to significant decimal digits and extended precision to significant decimal digits . using higherprecision arithmetic slows computations but may help to overcome some of the problems caused by round off errors. higher precision may need to be used only for a particular step of the algorithm in question. as with an approximation of any kind it is important to distinguish between the absolute error and the relative error of representing a number by its approximation absolute error . relative error . . the relative error is undefined if . very large and very small numbers cannot be represented in floating point arithmetic because of the phenomena called overflow and underflow respectively. an overflow happens when an arithmetic operation yields a result outside the range of the computer's floating point numbers. typical examples of overflow arise from the multiplication of large numbers or division by a very small number. sometimes we can eliminate this problem by making a simple change in the order in which an expression is evaluated e.g. . . by replacing an expression with an equal one e.g. computing not as ! ! ! but as . or by computing a logarithm of an expression instead of the expression itself. underflow occurs when the result of an operation is a nonzero fraction of such a small magnitude that it cannot be represented as a nonzero floating point number. usually underflow numbers are replaced by zero but a special signal is generated by hardware to indicate such an event has occurred. it is important to remember that in addition to inaccurate representation of numbers the arithmetic operations performed in a computer are not always exact either. in particular subtracting two nearly equal floating point numbers may cause a large increase in relative error. this phenomenon is called subtractive cancellation. example consider two irrational numbers . . . . and . . . . . represented by floating point numbers . . and . . respectively. the relative errors of these approximations are small . . . . and . . . . . respectively. the relative error of representing the difference by the difference of the floating point representations is . . which is very large for a relative error despite quite accurate approximations for both and . note that we may get a significant magnification of round off error if a lowaccuracy difference is used as a divisor. we already encountered this problem in discussing gaussian elimination in section . . our solution there was to use partial pivoting. many numerical algorithms involve thousands or even millions of arithmetic operations for typical inputs. for such algorithms the propagation of round off errors becomes a major concern from both the practical and theoretical standpoints. for some algorithms round off errors can propagate through the algorithm's operations with increasing effect. this highly undesirable property of a numerical algorithm is called instability. some problems exhibit such a high level of sensitivity to changes in their input that it is all but impossible to design a stable algorithm to solve them. such problems are called ill conditioned. example consider the following system of two linear equations in two unknowns . x . y . x . y . its only solution is x y . to see how sensitive this system is to small changes to its right hand side consider the system with the same coefficient matrix but slightly different right hand side values . x . y . . x . y . . the only solution to this system is x y which is quite far from the solution to the previous system. note that the coefficient matrix of this system is close to being singular why? . hence a minor change in its coefficients may yield a system with either no solutions or infinitely many solutions depending on its right handside values. you can find a more formal and detailed discussion of how we can measure the degree of ill condition of the coefficient matrix in numerical analysis textbooks e.g. ger . we conclude with a well known problem of finding real roots of the quadratic equation ax bx c . for any real coefficients a b and c a . according to secondary school algebra equation . has real roots if and only if its discriminant d b ac is nonnegative and these roots can be found by the following formula x b b ac . . a although formula . provides a complete solution to the posed problem as far as a mathematician is concerned it is far from being a complete solution for an algorithm designer. the first major obstacle is evaluating the square root. even for most positive integers d d is an irrational number that can be computed only approximately. there is a method of computing square roots that is much better than the one commonly taught in secondary school. it follows from newton's method a very important algorithm for solving equations which we discuss in section . . this method generates the sequence xn of approximations to d where d is a given nonnegative number according to the formula xn d for n . . . . xn xn where the initial approximation x can be chosen among other possibilities as x d . it is not difficultto prove that sequence . is decreasing if d and always converges to d. we can stop generating its elements either when the difference between its two consecutive elements is less than a predefined error tolerance xn xn or when xn is sufficiently close to d. approximation sequence . converges very fast to d for most values of d. in particular one can prove that if . d then no more than four iterations are needed to guarantee that xn d . and we can always scale a given value of d to one in the interval . by the formula d d p where p is an even integer. example let us apply newton's algorithm to compute . for simplicity we ignore scaling. we will round off the numbers to six decimal places and use the standard numerical analysis notation . to indicate the round offs. x . x . . x x . . x x x . . x x x . . . x x x at this point we have to stop because x x . . and hence all other approximations will be the same. the exact value of is . . . .. with the issue of computing square roots squared away i do not know whether or not the pun was intended are we home free to write a program based on formula . ? the answer is no because of the possible impact of round off errors. among other obstacles we are faced here with the menace of subtractive cancellation. if b is much larger than ac b ac will be very close to b and a root computed by formula . might have a large relative error. example let us follow a paper by george forsythe for and consider the equation x x . its true roots to significant digits are x . . . george e. forsythe a noted numerical analyst played a leading role in establishing computer science as a separate academic discipline in the united states. it is his words that are used as the epigraph to this book's preface. and x . . . if we use formula . and perform all the computations in decimal floatingpoint arithmetic with say seven significant digits we obtain b . . ac . . d . . . d . . . b d x . a . . . x . b d . . a and although the relative error of approximating x by x is very small for the second root it is very large x x i.e. x to avoid the possibility of subtractive cancellation in formula . we can use instead another formula obtained as follows x b b ac a b b ac b b ac a b b ac c b b ac with no danger of subtractive cancellation in the denominator if b . as to x it can be computed by the standard formula x b b ac a with no danger of cancellation either for a positive value of b. the case of b is symmetric we can use the formulas x b b ac a and x c . b b ac the case of b can be considered with either of the other two cases. there are several other obstacles to applying formula . which are related to limitations of floating point arithmetic if a is very small division by a can cause an overflow there seems to be no way to fight the danger of subtractive cancellation in computing b ac other than calculating it with double precision and so on. these problems have been overcome by william kahan of the university of toronto see for and his algorithm is considered to be a significant achievement in the history of numerical analysis. hopefully this brief overview has piqued your interest enough for you to seek more information in the many books devoted exclusively to numerical algorithms. in this book we discuss one more topic in the next chapter three classic methods for solving equations in one unknown. exercises . . some textbooks define the number of significant digits in the approximation of number by number as the largest nonnegative integer k for which . k. according to this definition how many significant digits are there in the approximation of by a. . ? b. . ? . if . is known to approximate some number with the absolute error not exceeding find a. the range of possible values of . b. the range of the relative errors of these approximations. . find the approximate value of e . . . . obtained by the fifth degree taylor's polynomial about and compute the truncation error of this approximation. does the result agree with the theoretical prediction made in the section? . derive formula . of the composite trapezoidal rule. . use the composite trapezoidal rule with n to approximate the following definite integrals. find the truncation error of each approximation and compare it with the one given by formula . . a. x d x b. x d x . if esin xdx is to be computed by the composite trapezoidal rule how large should the number of subintervals be to guarantee a truncation error smaller than ? smaller than ? . solve the two systems of linear equations and indicate whether they are illconditioned. a. x y b. x y x . y . x . y . . write a computer program to solve the equation ax bx c . . a. prove that for any nonnegative number d the sequence of newton's method for computing d is strictly decreasing and converges to d for any value of the initial approximation x d. b. prove that if . d and x d no more than four iterations of newton's method are needed to guarantee that xn d . . . apply four iterations of newton's method to compute and estimate the absolute and relative errors of this approximation. summary given a class of algorithms for solving a particular problem a lower bound indicates the best possible efficiency any algorithm from this class can have. a trivial lower bound is based on counting the number of items in the problem's input that must be processed and the number of output items that need to be produced. an information theoretic lower bound is usually obtained through a mechanism of decision trees. this technique is particularly useful for comparisonbased algorithms for sorting and searching. specifically any general comparison based sorting algorithm must perform at least log n! n log n key comparisons in the worst case. any general comparison based algorithm for searching a sorted array must perform at least log n key comparisons in the worst case. the adversary method for establishing lower bounds is based on following the logic of a malevolent adversary who forces the algorithm into the most time consuming path. a lower bound can also be established by reduction i.e. by reducing a problem with a known lower bound to the problem in question. complexity theory seeks to classify problems according to their computational complexity. the principal split is between tractable and intractable problems problems that can and cannot be solved in polynomial time respectively. for purely technical reasons complexity theory concentrates on decision problems which are problems with yes no answers. the halting problem is an example of an undecidable decision problem i.e. it cannot be solved by any algorithm. p is the class of all decision problems that can be solved in polynomial time. np is the class of all decision problems whose randomly guessed solutions can be verified in polynomial time. many important problems in np such as the hamiltonian circuit problem are known to be np complete all other problems in np are reducible to such a problem in polynomial time. the first proof of a problem's np completeness was published by s. cook for the cnf satisfiability problem. it is not known whether p np or p is just a proper subset of np. this question is the most important unresolved issue in theoretical computer science. a discovery of a polynomial time algorithm for any of the thousands of known np complete problems would imply that p np. numerical analysis is a branch of computer science dealing with solving continuous mathematical problems. two types of errors occur in solving a majority of such problems truncation error and round off error. truncation errors stem from replacing infinite objects by their finite approximations. round off errors are due to inaccuracies of representing numbers in a digital computer. subtractive cancellation happens as a result of subtracting two near equal floating point numbers. it may lead to a sharp increase in the relative roundoff error and therefore should be avoided by either changing the expression's form or by using a higher precision in computing such a difference . writing a general computer program for solving quadratic equations ax bx c is a difficult task. the problem of computing square roots can be solved by utilizing newton's method the problem of subtractive cancellation can be dealt with by using different formulas depending on whether coefficient b is positive or negative and by computing the discriminant b ac with double precision. 