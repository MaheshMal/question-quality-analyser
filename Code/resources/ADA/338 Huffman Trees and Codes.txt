suppose we have to encode a text that comprises symbols from some n symbol alphabet by assigning to each of the text's symbols some sequence of bits called the codeword. for example we can use a fixed length encoding that assigns to each symbol a bit string of the same length m m log n . this is exactly what the standard ascii code does. one way of getting a coding scheme that yields a shorter bit string on the average is based on the old idea of assigning shorter codewords to more frequent symbols and longer codewords to less frequent symbols. this idea was used in particular in the telegraph code invented in the mid th century by samuel morse. in that code frequent letters such as e . and a . are assigned short sequences of dots and dashes while infrequent letters such as q . and z .. have longer ones. variable length encoding which assigns codewords of different lengths to different symbols introduces a problem that fixed length encoding does not have. namely how can we tell how many bits of an encoded text represent the first or more generally the ith symbol? to avoid this complication we can limit ourselves to the so called prefix free or simply prefix codes. in a prefix code no codeword is a prefix of a codeword of another symbol. hence with such an encoding we can simply scan a bit string until we get the first group of bits that is a codeword for some symbol replace these bits by this symbol and repeat this operation until the bit string's end is reached. if we want to create a binary prefix code for some alphabet it is natural to associate the alphabet's symbols with leaves of a binary tree in which all the left edges are labeled by and all the right edges are labeled by . the codeword of a symbol can then be obtained by recording the labels on the simple path from the root to the symbol's leaf. since there is no simple path to a leaf that continues to another leaf no codeword can be a prefix of another codeword hence any such tree yields a prefix code. among the many trees that can be constructed in this manner for a given alphabet with known frequencies of the symbol occurrences how can we construct a tree that would assign shorter bit strings to high frequency symbols and longer ones to low frequency symbols? it can be done by the following greedy algorithm invented by david huffman while he was a graduate student at mit huf . huffman's algorithm step initialize n one node trees and label them with the symbols of the alphabet given. record the frequency of each symbol in its tree's root to indicate the tree's weight. more generally the weight of a tree will be equal to the sum of the frequencies in the tree's leaves. step repeat the following operation until a single tree is obtained. find two trees with the smallest weight ties can be broken arbitrarily but see problem in this section's exercises . make them the left and right subtree of a new tree and record the sum of their weights in the root of the new tree as its weight. a tree constructed by the above algorithm is called a huffman tree. it defines in the manner described above a huffman code. example consider the five symbol alphabet a b c d with the following occurrence frequencies in a text made up of these symbols symbol a b c d frequency . . . . . the huffman tree construction for this input is shown in figure . . . . . . . b c d a . . . . c d a . . b . . . a . . . . b c d . . . . . . c d a . . b . . . . . . . c d a . . b figure . example of constructing a huffman coding tree. the resulting codewords are as follows symbol a b c d frequency . . . . . codeword hence dad is encoded as and is decoded as bad ad. with the occurrence frequencies given and the codeword lengths obtained the average number of bits per symbol in this code is . . . . . . . . . . . . had we used a fixed length encoding for the same alphabet we would have to use at least bits per each symbol. thus for this toy example huffman's code achieves the compression ratio a standard measure of a compression algorithm's effectiveness of . . . in other words huffman's encoding of the text will use less memory than its fixed length encoding. extensive experiments with huffman codes have shown that the compression ratio for this scheme typically falls between and depending on the characteristics of the text being compressed. huffman's encoding is one of the most important file compression methods. in addition to its simplicity and versatility it yields an optimal i.e. minimal length encoding provided the frequencies of symbol occurrences are independent and known in advance . the simplest version of huffman compression calls in fact for a preliminary scanning of a given text to count the frequencies of symbol occurrences in it. then these frequencies are used to construct a huffman coding tree and encode the text as described above. this scheme makes it necessary however to include the coding table into the encoded text to make its decoding possible. this drawback can be overcome by using dynamic huffman encoding in which the coding tree is updated each time a new symbol is read from the source text. further modern alternatives such as lempel ziv algorithms e.g. say assign codewords not to individual symbols but to strings of symbols allowing them to achieve better and more robust compressions in many applications. it is important to note that applications of huffman's algorithm are not limited to data compression. suppose we have n positive numbers w w . . . wn that have to be assigned to n leaves of a binary tree one per node. if we define the weighted path length as the sum n li wi where li is the length of the simple i path from the root to the ith leaf how can we construct a binary tree with minimum weighted path length? it is this more general problem that huffman's algorithm actually solves. for the coding application li and wi are the length of the codeword and the frequency of the ith symbol respectively. this problem arises in many situations involving decision making. consider for example the game of guessing a chosen object from n possibilities say an integer between and n by asking questions answerable by yes or no. different strategies for playing this game can be modeled by decision trees such as those depicted in figure . for n . the length of the simple path from the root to a leaf in such a tree is equal to the number of questions needed to get to the chosen number represented by the leaf. if number i is chosen with probability pi the sum . decision trees are discussed in more detail in section . . no n no n yes yes n n n n no yes no yes no yes n n n n n n no yes n n figure . two decision trees for guessing an integer between and . n li pi where li is the length of the path from the root to the ith leaf indicates i the average number of questions needed to guess the chosen number with a game strategy represented by its decision tree. if each of the numbers is chosen with the same probability of n the best strategy is to successively eliminate half or almost half the candidates as binary search does. this may not be the case for arbitrary pi's however. for example if n and p . p . p . and p . the minimum weighted path tree is the rightmost one in figure . . thus we need huffman's algorithm to solve this problem in its general case. note that this is the second time we are encountering the problem of constructing an optimal binary tree. in section . we discussed the problem of constructing an optimal binary search tree with positive numbers the search probabilities assigned to every node of the tree. in this section given numbers are assigned just to leaves. the latter problem turns out to be easier it can be solved by the greedy algorithm whereas the former is solved by the more complicated dynamic programming algorithm. exercises . . a. construct a huffman code for the following data symbol a b c d frequency . . . . . b. encode abacabad using the code of question a . c. decode using the code of question a . . for data transmission purposes it is often desirable to have a code with a minimum variance of the codeword lengths among codes of the same average length . compute the average and variance of the codeword length in two huffman codes that result from a different tie breaking during a huffman code construction for the following data symbol a b c d e probability . . . . . . indicate whether each of the following properties is true for every huffman code. a. the codewords of the two least frequent symbols have the same length. b. the codeword's length of a more frequent symbol is always smaller than or equal to the codeword's length of a less frequent one. . what is the maximal length of a codeword possible in a huffman encoding of an alphabet of n symbols? . a. write pseudocode of the huffman tree construction algorithm. b. what is the time efficiency class of the algorithm for constructing a huffman tree as a function of the alphabet size? . show that a huffman tree can be constructed in linear time if the alphabet symbols are given in a sorted order of their frequencies. . given a huffman coding tree which algorithm would you use to get the codewords for all the symbols? what is its time efficiency class as a function of the alphabet size? . explain how one can generate a huffman code without an explicit generation of a huffman coding tree. . a. write a program that constructs a huffman code for a given english text and encode it. b. write a program for decoding of an english text which has been encoded with a huffman code. c. experiment with your encoding program to find a range of typical compression ratios for huffman's encoding of english texts of say words. d. experiment with your encoding program to find out how sensitive the compression ratios are to using standard estimates of frequencies instead of actual frequencies of symbol occurrences in english texts. . card guessing design a strategy that minimizes the expected number of questions asked in the following game gar . you have a deck of cards that consists of one ace of spades two deuces of spades three threes and on up to nine nines making cards in all. someone draws a card from the shuffled deck which you have to identify by asking questions answerable with yes or no. summary the greedy technique suggests constructing a solution to an optimization problem through a sequence of steps each expanding a partially constructed solution obtained so far until a complete solution to the problem is reached. on each step the choice made must be feasible locally optimal and irrevocable. prim's algorithm is a greedy algorithm for constructing a minimum spanning tree of a weighted connected graph. it works by attaching to a previously constructed subtree a vertex closest to the vertices already in the tree. kruskal's algorithm is another greedy algorithm for the minimum spanning tree problem. it constructs a minimum spanning tree by selecting edges in nondecreasing order of their weights provided that the inclusion does not create a cycle. checking the latter condition efficiently requires an application of one of the so called union find algorithms. dijkstra's algorithm solves the single source shortest path problem of finding shortest paths from a given vertex the source to all the other vertices of a weighted graph or digraph. it works as prim's algorithm but compares path lengths rather than edge lengths. dijkstra's algorithm always yields a correct solution for a graph with nonnegative weights. a huffman tree is a binary tree that minimizes the weighted path length from the root to the leaves of predefined weights. the most important application of huffman trees is huffman codes. a huffman code is an optimal prefix free variable length encoding scheme that assigns bit strings to symbols based on their frequencies in a given text. this is accomplished by a greedy construction of a binary tree whose leaves represent the alphabet symbols and whose edges are labeled with 's and 's. 