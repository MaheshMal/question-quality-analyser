 s reads from shared memory the message shared memory message that was written by the producer process. finally both processes remove the view of the mapped file with a call to unmapviewoffileo. we provide a programming exercise at the end of this chapter using shared memory with memory mapping in the win api. . . memory mapped i o in the case of i o as mentioned in section . . each i o controller includes registers to hold commands and the data being transferred. usually special i o instructions allow data transfers between these registers and system memory. to allow more convenient access to i o devices many computer architectures provide memory mapped i o. in this case ranges of memory addresses are set aside and are mapped to the device registers. reads and writes to these memory addresses cause the data to be transferred to and from the device registers. this method is appropriate for devices that have fast response times such as video controllers. in the ibm pc each location on the screen is mapped to a memory location. displaying text on the screen is almost as easy as writing the text into the appropriate memory mapped locations. memory mapped i o is also convenient for other devices such as the serial and parallel ports used to connect modems and printers to a computer. the cpu transfers data through these kinds of devices by reading and wrriting a few device registers called an i o port. to send out a long string of bytes through a memory mapped serial port the cpu writes one data byte to the data register and sets a bit in the control register to signal that the byte is available. the device takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte. then the cpu can transfer the next byte. if the cpu uses polling to watch the control bit constantly looping to see whether the device is ready this method of operation is called programmed i o pio . if the cpu does not poll the control bit but instead receives an interrupt when the device is ready for the next byte the data transfer is said to be interrupt driven. . allocating kernel memory when a process running in user mode requests additional memory pages are allocated from the list of free page frames maintained by the kernel. this list is typically populated using a page replacement algorithm such as those discussed in section . and most likely contains free pages scattered throughout physical memory as explained earlier. remember too that if a user process requests a single byte of memory internal fragmentation will result as the process will be granted an entire page frame. kernel memory however is often allocated from a free memory pool different from the list used to satisfy ordinary user mode processes. there are two primary reasons for this . the kernel requests memory for data structures of varying sizes some of which are less than a page in size. as a result the kernel must use memory conservatively and attempt to minimize waste due to fragmentation. this chapter virtual memory is especially important because many operating systems do not subject kernel code or data to the paging system. . pages allocated to user mode processes do not necessarily have to be in contiguous physical memory. however certain hardware devices interact directly with physical memory without the benefit of a virtual memory interface and consequently may require memory residing in physically contiguous pages. in the following sections we examine two strategies for managing free memory that is assigned to kernel processes. . . buddy system the buddy system allocates memory from a fixed size segment consisting of physically contiguous pages. memory is allocated from this segment using a power of allocator which satisfies requests in units sized as a power of kb kb kb and so forth . a request in units not appropriately sized is rounded up to the next highest power of . for example if a request for kb is made it is satisfied with a kb segment. next we explain the operation of the buddy system with a simple example. let's assume the size of a memory segment is initially kb and the kernel requests kb of memory. the segment is initially divided into two buddies which we will call ai and ar each kb in size. one of these buddies is further divided into two kb buddies b and b . however the next highest power of from kb is kb so either b or br is again divided into two kb buddies c . and cr. one of these buddies is used to satisfy the kb request. this scheme is illustrated in figure . where c is the segment allocated to the kb request. physically contiguous pages kb ' ah i o ' ' ... .. .. ... .... ... jvj jq fi 'l j' 'i i' ! 'i kb . ikb l i . b . rkb j k figure . buddy system allocation. . allocating kernel memory an advantage of the buddy system is how quickly adjacent buddies dan be combined to form larger segments using a technique known as coalescing. in figure . for example when the kernel releases the q. unit it was allocated the system can coalesce c l and cr into a kb segment. this segment bl can in turn be coalesced with its buddy br to form a kb segment. ultimately we can end up with the original kb segment. the obvious drawback to the buddy system is that rounding up to the next highest power of is very likely to cause fragmentation within allocated segments. for example a kb request can only be satisfied with a kb segment. in fact we cannot guarantee that less than percent of the allocated unit will be wasted due to internal fragmentation. in the following section we explore a memory allocation scheme where no space is lost due to fragmentation. . . slab allocation a second strategy for allocating kernel memory is known as slab allocation. a slab is made up of one or more physically contiguous pages. a cache consists of one or more slabs. there is a single cache for each unique kernel data structure for example a separate cache for the data structure representing process descriptors a separate cache for file objects a separate cache for semaphores and so forth. each cache is populated with objects that are instantiations of the kernel data structure the cache represents. for example the cache representing semaphores stores instances of semaphores objects the cache representing process descriptors stores instances of process descriptor objects etc. the relationship between slabs caches and objects is shown in figure . . the figure shows two kernel objects kb in size and three objects kb in size. these objects are stored in their respective caches. kernel objects caches slabs kb objects physically contiguous pages kb objects figure . slab allocation. chapter virtual memory the slab allocation algorithm uses caches to store kernel objects. when a cache is created a number of objects which are initially marked as free are allocated to the cache. the number of objects in the cache depends on the size of the associated slab. for example a kb slab comprised of three continguous kb pages could store six kb objects. initially all objects in the cache are marked as free. when a new object for a kernel data structure is needed the allocator can assign any free object from the cache to satisfy the request. the object assigned from the cache is marked as used. let's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor. in linux systems a process descriptor is of the type struct task struct which requires approximately . kb of memory. when the linux kernel creates a new task it requests the necessary memory for the s t r u c t t a s k . s t r u c t object from its cache. the cache will fulfill the request using a struct t a s k s t r u c t object that has already been allocated in a slab and is marked as free. in linux a slab may be in one of three possible states . full. all objects in the slab are marked as used. . empty. all objects in the slab are marked as free. . partial. the slab consists of both used and free objects. the slab allocator first attempts to satisfy the request with a free object in a partial slab. if none exist a free object is assigned from an empty slab. if no empty slabs are available a new slab is allocated from contiguous physical pages and assigned to a cache memory for the object is allocated from this slab. the slab allocator provides two main benefits . no memory is wasted due to fragmentation. fragmentation is not an issue because each unique kernel data structure has an associated cache and each cache is comprised of one or more slabs that are divided into chunks the size of the objects being represented. thus when the kernel requests memory for an object the slab allocator returns the exact amount of memory required to represent the object. . memory requests can be satisfied quickly. the slab allocation scheme is thus particularly effective for managing memory where objects are frequently allocated and deallocated as is often the case with requests from the kernel. the act of allocating and releasing memory can be a time consuming process. however objects are created in advance and thus can be quickly allocated from the cache. furthermore when the kernel has finished with an object and releases it it is marked as free and returned to its cache thus making it immediately available for subsequent requests from the kernel. the slab allocator first appeared in the solaris . kernel. because of its general purpose nature this allocator is now also used for certain user mode memory requests in solaris. linux originally used the buddy system however beginning with version . the linux kernel adopted the slab allocator