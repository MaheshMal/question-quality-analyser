 network file systems are commonplace. they are typically integrated with the overall directory structure and interface of the client system. nfs is a good example of a widely used well implemented client server network file system. here we use it as an example to explore the implementation details of network file systems. nfs is both an implementation and a specification of a software system for accessing remote files across lans or even wans . nfs is part of onjc which most unix vendors and some pc operating systems support. the implementation described here is part of the solaris operating system which is a modified version of unix svr running on sun workstations and other hardware. it uses either the tcp or udp ip protocol depending on the interconnecting network . the specification and the implementation are intertwined in our description of nfs. whenever detail is needed we refer to the sun implementation whenever the description is general it applies to the specification also. . . overview n fs v iews a set of interconnected worksta tions as a set of independent machines with independent file systems. the goal is to allow some degree of sharing among these file systems on explicit request in a transparent manner. sharing s usr shared diri figure . three independent file systems. is based on a client server relationship. a machine may be and often is both a client and a server. sharing is allowed between any pair of machines. to ensure machine independence sharing of a remote file system affects only the client machine and no other machine. so that a remote directory will be accessible in a transparent manner from a particular machine say from ml a client of that machine must first carry out a mount operation. the semantics of the operation involve .mounting a remote directory over a directory of a local file system. once the mount operation is completed the mounted directory looks like an integral subtree of the local file system replacing the subtree descending from the local directory. the local directory becomes the name of the root of the newly mounted directory. specification of the remote directory as an argument for the mount operation is not done transparently the location or host name of the remote directory has to be provided. however from then on users on machine ml can access files in the remote directory in a totally transparent manner. to illustrate file mounting consider the file system depicted in figure . where the triangles represent subtrees of directories that are of interest. the figure shows three independent file systems of machines named u si and s . at this point at each machine only the local files can be accessed. in figure . a the effects of mounting si u s r s h a r e d over u u s r l o c a l are shown. this figure depicts the view users on u have of their file system. notice that after the mount is complete they can access any file within the dirl directory using the prefix usr local dirl. the original directory u s r l o c a l on that machine is no longer visible. subject to access rights accreditation any file system or any directory within a file system can be mounted remotely on top of any local directory. diskless workstations can even mount their own roots from servers. cascading mounts are also permitted in some nfs implementations. that is a file system can be mounted over another file system that is remotely mounted not local. a machine is affected by only those mounts that it has itself invoked. mounting a remote file system does not give the client access to other file systems that were by chance mounted over the former file system. thus the mount mechanism does not exhibit a transitivity property. chapter file system implementation u u usr ' c usr local o local dir l n din a b figure . mounting in nfs. a mounts b cascading mounts. in figure . b we illustrate cascading mounts by continuing our previous example. the figure shows the result of mounting s u s r d i r over u u s r l o c a l d i r l which is already remotely mounted from si. users can access files within dir on u using the prefix u s r l o c a l d i r l . if a shared file system is mounted over a user's home directories on all machines in a network the user can log into any workstation and get his home environment. this property permits user mobility. one of the design goals of nfs was to operate in a heterogeneous environment of different machines operating systems and network architectures. the nfs specification is independent of these media and thus encourages other implementations. this independence is achieved through the use of rpc primitives built on top of an external data representation xdk protocol used between two implementation independent interfaces. hence if the system consists of heterogeneous machines and file systems that are properly interfaced to nfs file systems of different types can be mounted both locally and remotely. the nfs specification distinguishes between the services provided by a mount mechanism and the actual remote file access services. accordingly two separate protocols are specified for these services a mount protocol and a protocol for remote file accesses the nfs protocol. the protocols are specified as sets of rpcs. these rfcs are the building blocks used to implement transparent remote file access. . . the mount protocol the mount protocol establishes the initial logical connection between a server and a client. in sun's implementation each machine has a server process outside the kernel performing the protocol functions. a mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it. the mount request is mapped to the corresponding rpc and is forwarded to the mount server running on the specific server machine. the server maintains an export list . nfs that specifies local file systems that it exports for mounting along with names of machines that are permitted to mount them. in solaris this list is the e t c d f s df stab which can be edited only by a superuser. the specification can also include access rights such as read only. to simplify the maintenance of export lists and mount tables a distributed naming scheme can be used to hold this information and make it available to appropriate clients. recall that any directory within an exported file system can be mounted remotely by an accredited machine. a component unit is such a directory. when the server receives a mount request that conforms to its export list it returns to the client a file handle that serves as the key for further accesses to files within the mounted file system. the file handle contains all the information that the server needs to distinguish an individual file it stores. in unix terms the file handle consists of a file system identifier and an inode number to identify the exact mounted directory within the exported file system. the server also maintains a list of the client machines and the corresponding currently mounted directories. this list is used mainly for administrative purposes for instance for notifying all clients that the server is going down. only through addition and deletion of entries in this list can the server state be affected by the mount protocol. usually a system has a static mounting preconfiguration that is established at boot time etc vf s t a b in solaris howrever this layout can be .modified m addition to the actual mount procedure the mount protocol includes several other procedures such as unmount and return export list. . . the n fs protocol the nfs protocol provides a set of rpcs for remote file operations. the procedures support the following operations searching for a file within a directory reading a set of directory entries manipulating links and directories accessing file attributes reading and writing files these procedures can be invoked only after a file handle for the remotely mounted directory has been established. the omission of openo and close operations is intentional. a prominent feature of nfs servers is that they are stateless. servers do not maintain information about their clients from one access to another. no parallels to unix's open files table or file structures exist on the server side. consequently each request has to provide a full set of arguments including a unique file identifier and an absolute offset inside the file for the appropriate operations. the resulting design is robust no special measures need be taken to recover a server after a crash. file operations must be idempotent for this purpose. every nfs request has a sequence number allowing the server to determine if a request is duplicated or if any are missing. chapter file system implementation maintaining the list of clients that we mentioned seems to violate the statelessness of the server. however this list is not essential for the correct operation of the client or the server and hence it does not need to be restored after a server crash. consequently it might include inconsistent data and is treated as only a hint. a further implication of the stateless server philosophy and a result of the synchrony of an rpc is that modified data including indirection and status blocks must be committed to the server's disk before results are returned to the client. that is a client can cache write blocks but wiien it flushes them to the server it assumes that they have reached the server's disks. the server must write all nfs data synchronously. thus a server crash and recovery will be invisible to a client all blocks that the server is managing for the client will be intact. the consequent performance penalty can be large because the advantages of caching are lost. performance can be increased by using storage with its own nonvolatile cache usually battery backed up memory . the disk controller acknowledges the disk write when the write is stored in the nonvolatile cache. in essence the host sees a very fast synchronous write. these blocks remain intact even after system crash and are written from this stable storage to disk periodically. a single nfs write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same file. the nfs protocol however does not provide concurrency control mechanisms. a write system call maybe broken down into several rfc writes because each nfs write or read call can contain up to kb of data and udp packets are limited to bytes. as a result two users writing to the same remote file may get their data intermixed. the claim is that because lock management is inherently stateful a service outside the nfs should provide locking and solaris does . users are advised to coordinate access to shared files using mechanisms outside the scope of nfs. nfs is integrated into the operating system via a vfs. as an illustration of the architecture let's trace how an operation on an already open remote file is handled follow the example in figure . . the client initiates the operation with a regular system call. the operating system layer maps this call to a vfs operation on the appropriate vnode. the vfs layer identifies the file as a remote one and invokes the appropriate nfs procedure. an rpc call is made to the nfs service layer at the remote server. this call is reinjected to the vfs layer on the remote system which finds that it is local and invokes the appropriate file system operation. this path is retraced to return the result. an advantage of this architecture is that the client and the server are identical thus a machine may be a client or a server or both. the actual service on each server is performed by kernel threads. . . path name translation path name translation in nfs involves the parsing of a path name such as u s r l o c a l d i r i f i l e . t x t into separate directory entries or components usr l o c a l and d i r l . path name translation is done by breaking the path into component names and performing a separate nfs lookup c a l l for every pair of component name and directory vnode. once a mount point is crossed every component lookup causes a separate rfc to the server. this expensive path name traversal scheme is needed since the layout of each . nfs figure . schematic view of the nfs architecture. client's logical name space is unique dictated by the mounts the client has performed. it would be much more efficient to hand a server a path name and receive a target vnode once a mount point is encountered. at any point however there can be another mount point for the particular client of which the stateless server is unaware. so that lookup is fast a directory name lookup cache on the client side holds the vnodes for remote directory names. this cache speeds up references to files with the same initial path name. the directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode. recall that mounting a remote file system on top of another already mounted remote file system a cascading mount is allowed in some implementations of nfs. however a server cannot act as an intermediary between a client and another server. instead a client must establish a direct client server connection with the second server by directly mounting the desired directory. when a client has a cascading mount more than one server can be involved in a path name traversal. however each component lookup is performed between the original client and some server. therefore when a client does a lookup on a directory on which the server has mounted a file system the client sees the underlying directory instead of the mounted directory. . . remote operations with the exception of opening and closing files there is almost a one to one correspondence between the regular unix system calls for file operations and the nfs protocol rpcs. thus a remote file operation can be translated directly to the corresponding rfc. conceptually nfs adheres to the remote service chapter file system implementation paradigm but in practice buffering and caching techniques are employed for the sake of performance. i fo direct correspondence exists between a remote operation and an rfc. instead file blocks and file attributes are fetched by the rpcs and are cached locally. future remote operations use the cached data subject to consistency constraints. there are two caches the file attribute inode information cache and the file blocks cache. when a file is opened the kernel checks with the remote server to determine whether to fetch or re validate the cached attributes. the cached file blocks are used only if the corresponding cached attributes are up to date. the attribute cache is updated whenever new attributes arrive from the server. cached attributes are by default discarded after seconds. both read ahead and delayed write techniques are used between the server and the client. clients do not free delayed write blocks until the server confirms that the data have been written to disk. in contrast to the system used in sprite distributed file system delayed write is retained even when a file is opened concurrently in conflicting modes. hence unix semantics section . . . are not preserved. tuning the system for performance makes it difficult to characterize the consistency semantics of nfs. new files created on a machine may not be visible elsewhere for seconds. furthermore writes to a file at one site may or may not be visible at other sites that have this file open for reading. new opens of a file observe only the changes that have already been flushed to the server. thus nfs provides neither strict emulation of unix semantics nor the session semantics of andrew section . . . . in spite of these drawbacks the utility and good performance of the mechanism make it the most widely used multi vendor distributed system in operation. . example the wafl file system disk i o has a huge impact on system performance. as a result file system design and implementation command quite a lot of attention from system designers. some file systems are general purpose in that they can provide reasonable performance and functionality for a wide variety of file sizes file types and i o loads. others are optimized for specific tasks in an attempt to provide better performance in those areas than general purpose file systems. the wafl file system from network appliance is an example of this sort of optimization. wafl the ivrite nin wherc file layout is a powerful elegant file system optimized for random writes. wafl is used exclusively on network file servers produced by network appliance and so is meant for use as a distributed file system. it can provide files to clients via the nfs cifs ftp and http protocols although it was designed just for nfs and cifs. when many clients use these protocols to talk to a file server the server may see a very large demand for random reads and an even larger demand for random writes. the nfs and cifs protocols cache data from read operations so writes are of the greatest concern to file server creators. wafl is used on file servers that include an nvram cache for writes. the wafl designers took advantage of running on a specific architecture to optimize the file system for random i o with a stable storage cache in front. . example the wafl file system root inode figure . the wafl file layout. ease of use is one of the guiding principles of wafl because it is designed to be used in an appliance. its creators also designed it to include a new snapshot functionality that creates multiple read only copies of the file system at different points in time as we shall see. the file system is similar to the berkeley fast file system with many modifications. it is block based and uses inodes to describe files. each inode contains pointers to blocks or indirect blocks belonging to the file described by the inode. each file system has a root inode. all of the metadata lives in files all inodes are in one file the free block map in another and the free inode map in a third as shown in figure . . because these are standard files the data blocks are not limited in location and can be placed anywhere. if a file system is expanded by addition of disks the lengths of these metadata files are automatically expanded by the file system. thus a wafl file system is a tree of blocks rooted by the root inode. to take a snapshot wafl creates a duplicate root inode. any file or metadata updates after that go to new blocks rather than overwriting their existing blocks. the new root inode points to metadata and data changed as a result of these writes. meanwhile the old root inode still points to the old blocks which have not been updated. it therefore provides access to the file system just as it was at the instant the snapshot was made and takes very little disk space to do so! in essence the extra disk space occupied by a snapshot consists of just the blocks that have been modified since the snapshot was taken. an important change from more standard file systems is that the free block map has more than one bit per block. it is a bitmap with a bit set for each snapshot that is using the block. when all snapshots that have been using the block are deleted the bit map for that block is all zeros and the block is free to be reused. used blocks are never overwritten so writes are very fast because a write can occur at the free block nearest the current head location. there are many other performance optimizations in wafl as well. many snapshots can exist simultaneously so one can be taken each hour of the day and. each day of the month. a user with access to these snapshots can access files as they were at any of the times the snapshots were taken. the snapshot facility is also useful for backups testing versioning and so on. wafl's snapshot facility is very efficient in that it does not even require that copy on write copies of each data block be taken before the block is modified. other file systems provide snapshots but frequently with less efficiency. wafl snapshots are depicted in figure . . chapter file system implementation root snoda block a b ic id e a before a snapshot. ni! rsot natte b i l rfnspswotc block a d e b after a snapshot before any blocks change. irnbtlriibfiie . . . . . . . . . . so a. block a b c d i ' c after block d has changed to d'. figure . snapshots in wafl