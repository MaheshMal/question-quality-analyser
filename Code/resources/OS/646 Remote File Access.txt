 consider a user who requests access to a remote file. the server storing the file has been located by the naming scheme and now the actual data transfer must take place. one way to achieve this transfer is through a remote service mechanism whereby requests for accesses are delivered to the server the server machine performs the accesses and their results are forwarded back to the user. one of the most common ways of implementing remote service is the remote procedure call rpc paradigm which we discussed in chapter . a direct analogy exists between disk access methods in conventional file systems and the remote service method in a dfs using the remote service method is analogous to performing a disk access for each access request. to ensure reasonable performance of a remote service mechanism we can use a form of caching. in conventional file systems the rationale for caching is to reduce disk i o thereby increasing performance whereas in dfss the goal is to reduce both network traffic and disk i o. in the following discussion we describe the implementation of caching in a dfs and contrast it with the basic remote service paradigm. . remote file access . . basic caching scheme f the concept of caching is simple. if the data needed to satisfy the access reqviest are not already cached then a copy of those data is brought from the server to the client system. accesses are performed on the cached copy. the idea is to retain recently accessed disk blocks in the cache so that repeated accesses to the same information can be handled locally without additional network traffic. a replacement policy for example least recently used keeps the cache size bounded. no direct correspondence exists between accesses and traffic to the server. files are still identified with one master copy residing at the server machine but copies or parts of the file are scattered in different caches. when a cached copy is modified the changes need to be reflected on the master copy to preserve the relevant consistency semantics. the problem of keeping the cached copies consistent with the master file is the cache consistency problem which we discuss in section . . . dfs caching could just as easily be called network virtual memory it acts similarly to demand paged virtual memory except that the backing store usually is not a local disk but rather a remote server. nfs allows the swap space to be mounted remotely so it actually can implement virtual memory over a network notwithstanding the resulting performance penalty. the granularity of the cached data in a dfs can vary from blocks of a file to an entire file. usually more data are cached than are needed to satisfy a single access so that many accesses can be served by the cached data. this procedure is much like disk read ahead section . . . afs caches files in large chunks kb . the other systems discussed in this chapter support caching of individual blocks driven by client demand. increasing the caching unit increases the hit ratio but it also increases the miss penalty because each miss requires more data to be transferred. it increases the potential for consistency problems as well. selecting the unit of caching involves considering parameters such as the network transfer unit and the rpc protocol service unit if an rpc protocol is used . the network transfer unit for ethernet a packet is about . kb so larger units of cached data need to be disassembled for delivery and reassembled on reception. block size and total cache size are obviously of importance for blockcaching schemes. in unix like systems common block sizes are kb and kb. for large caches over mb large block sizes over kb are beneficial. for smaller caches large block sizes are less beneficial because they result in fewer blocks in the cache and a lower hit ratio. . . cache location where should the cached data be stored on disk or in main memory? disk caches have one clear advantage over main memory caches they are reliable. modifications to cached data are lost in a crash if the cache is kept in volatile memory. moreover if the cached data are kept on disk they are still there during recovery and there is no need to fetch them again. main memory caches have several advantages of their own however main memory caches permit workstations to be diskless. data can be accessed more quickly from a cache in main memory than from one on a disk. chapter distributed file systems technology is moving toward larger and less expensive memory. the achieved performance speedup is predicted to outweigh the advantages of disk caches. the server caches used to speed up disk i o will be in main memory regardless of where user caches are located if we use main memory caches on the user machine too we can build a single caching mechanism for use by both servers and users. many remote access implementations can be thought of as hybrids of caching and remote service. in nfs for instance the implementation is based on remote service but is augmented with client and server side memory caching for performance. similarly sprite's implementation is based on caching but under certain circumstances a remote service method is adopted. thus to evaluate the two methods we must evaluate to what degree either method is emphasized. the nfs protocol and most implementations do not provide disk caching. recent solaris implementations of nfs solaris . and beyond include a clientside disk caching option the cachefs file system. once the nfs client reads blocks of a file from the server it caches them in memory as well as on disk. if the memory copy is flushed or even if the system reboots the disk cache is referenced. if a needed block is neither in memory nor in the cachefs disk cache an rpc is sent to the server to retrieve the block and the block is written into the disk cache as well as stored in the memory cache for client use. . . cache update policy the policy used to write modified data blocks back to the server's master copy has a critical effect on the system's performance and reliability. the simplest policy is to write data through to disk as soon as they are placed in any cache. the advantage of a write through policy is reliability little information is lost when a client system crashes. however this policy requires each write access to wait until the information is sent to the server so it causes poor write performance. caching with write through is equivalent to using remote service for write accesses and exploiting caching only for read accesses. an alternative is the delayed write policy also known as write back caching where we delay updates to the master copy. modifications are written to the cache and then are written through to the server at a later time. this policy has two advantages over write through. first because writes are made to the cache write accesses complete much more quickly. second data may be overwritten before they are written back in which case only the last update needs to be written at all. unfortunately delayed write schemes introduce reliability problems since unwritten data are lost whenever a user machine crashes. variations of the delayed write policy differ in when modified data blocks are flushed to the server. one alternative is to flush a block when it is about to be ejected from the client's cache. this option can result in good performance but some blocks can reside in the client's cache a long time before they are written back to the server. a compromise between this alternative and the write through policy is to scan the cache at regular intervals and to flush blocks that have been modified since the most recent scan just as unix scans . remote file access memory cache write through network hiserver disk storage memory cache disk object write back disk cache write through figure . cachets and its use of caching. its local cache. sprite uses this policy with a second interval. nfs uses the policy for file data but once a write is issued to the server during a cache flush the write must reach the server's disk before it is considered complete. nfs treats metadata directory data and file attribute data differently. any metadata changes are issued synchronously to the server. thus file structure loss and directory structure corruption are avoided when a client or the server crashes. for nfs with cachefs writes are also written to the local disk cache area when they are written to the server to keep all copies consistent. thus nfs with cachefs improves performance over standard nfs on a read request with a cachefs cache hit but decreases performance for read or write requests with a cache miss. as with all caches it is vital to have a high cache hit rate to gain performance. cachefs and its use of write through and write back caching is shown in figure . . yet another variation on delayed write is to write data back to the server when the file is closed. this write on close policy is used in afs. in the case of files that are open for short periods or are modified rarely this policy does not significantly reduce network traffic. in addition the write on close policy requires the closing process to delay while the file is written through which reduces the performance advantages of delayed writes. for files that are open for long periods and are modified frequently however the performance advantages of this policy over delayed write with more frequent flushing are apparent. . . consistency a client machine is faced with the problem of deciding whether or not a locally cached copy of the data is consistent with the master copy and hence can be chapter distributed file systems used . if the client machine determines that its cached data are out of date accesses can no longer be served by those cached data. an up to date copy of the data needs to be cached. there are two approaches to verifying the validity of cached data . client initiated approach. the client initiates a validity check in which it contacts the server and checks whether the local data are consistent with the master copy. the frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics. it can range from a check before every access to a check only on first access to a file on file open basically . every access coupled with a validity check is delayed compared with an access served immediately by the cache. alternatively checks can be initiated at fixed time intervals. depending on its frequency the validity check can load both the network and the server. . server initiated approach. the server records for each client the files or parts of files that it caches. when the server detects a potential inconsistency it must react. a potential for inconsistency occurs when two different clients in conflicting modes cache a file. if unix semantics section . . is implemented we can resolve the potential inconsistency by having the server play an active role. the server must be notified whenever a file is opened and the intended mode read or write must be indicated for every open. the server can then act when it detects that file has been opened simultaneously in conflicting modes by disabling caching for that particular file. actually disabling caching results in switching to a remote service mode of operation. . . a comparison of caching and remote service essentially the choice between caching and remote service trades off potentially increased performance with decreased simplicity. we evaluate this tradeoff by listing the advantages and disadvantages of the two methods when caching is used the local cache can handle a substantial number of the remote accesses efficiently. capitalizing on locality in file access patterns makes caching even more attractive. thus most of the remote accesses will be served as fast as will local ones. moreover servers are contacted only occasionally rather than for each access. consequently server load and network traffic are reduced and the potential for scalability is enhanced. by contrast when the remote service method is used every remote access is handled across the network. the penalty in network traffic server load and performance is obvious. total network overhead is lower for transmitting big chunks of data as is done in caching than for transmitting series of responses to specific requests as in the remote service method . furthermore disk access routines on the server may be better optimized if it is known that requests will always be for large contiguous segments of data rather than for random disk blocks