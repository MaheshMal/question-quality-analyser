 replication of files on different machines in a distributed file system is a useful redundancy for improving availability. multimachine replication can benefit performance too selecting a nearby replica to serve an access request results in shorter service time. . file replication nfs v our coverage of nfs thus far has only considered version or v nfs. the most recent nfs standard is version v and it differs fundamentally from previous versions. the most significant change is that the protocol is now statefid meaning that the server maintains the state of the client session from the time the remote file is opened until it is closed. thus the nfs protocol now provides openo and close operations previous versions of nfs which are stateless provide no such operations. furthermore previous versions specify separate protocols for mounting remote file systems and for locking remote files. v provides all of these features under a single protocol. in particular the mount protocol was eliminated allowing nfs to work with network firewalls. the mount protocol was a notorious security hole in nfs implementations. additionally v has enhanced the ability of clients to cache file data locally. this feature improves the performance of the distributed file system as clients are able to resolve more file accesses from the local cache rather than having to go through the server. v allows clients to request file locks from servers as well. if the server grants the request the client maintains the lock until it is released or its lease expires. clients are also permitted to renew existing leases. traditionally unix based systems provide advisory file locking whereas windows operating systems use mandatory locking. to allow nfs to work well with non unix systems v now provides mandatory locking as well. the new locking and caching mechanisms are based on the concept of delegation whereby the server delegates responsibilities for a file's lock and contents to the client that requested the lock. that delegated client maintains in cache the current version of the file and other clients can ask that delegated client for lock access and file contents until the delegated client relinquishes the lock and delegation. finally whereas previous versions of nfs are based on the udp network protocol v is based on tcp which allows it to better adjust to varying traffic loads on the network. delegating these responsibilities to clients reduces the load on the server and improves cache coherency. the basic requirement of a replication scheme is that different replicas of the same file reside on failure independent machines. that is the availability of one replica is not affected by the availability of the rest of the replicas. this obvious requirement implies that replication management is inherently a location opaque activity. provisions for placing a replica on a particular machine must be available. it is desirable to hide the details of replication from users. mapping a replicated file name to a particular replica is the task of the naming scheme. the existence of replicas should be invisible to higher levels. at lower levels however the replicas must be distinguished from one another by different lower level names. another transparency requirement is providing replication control at higher levels. replication control includes determination of the degree of replication and of the placement of replicas. under certain circumstances we may want to expose these details to users. locus for chapter distributed file systems instance provides users and system administrators with mechanisms to control the replication scheme. the main problem associated with replicas is updating. from a user's point of view replicas of a file denote the same logical entity and thus an update to any replica must be reflected on all other replicas. more precisely the relevant consistency semantics must be preserved when accesses to replicas are viewed as virtual accesses to the replicas' logical files. if consistency is not of primary importance it can be sacrificed for availability and performance. in this fundamental tradeoff in the area of fault tolerance the choice is between preserving consistency at all costs thereby creating a potential for indefinite blocking and sacrificing consistency under some we hope rare circumstances of catastrophic failures for the sake of guaranteed progress. locus for example employs replication extensively and sacrifices consistency in the case of network partition for the sake of availability of files for read and write accesses. ibis uses a variation of the primary copy approach. the domain of the name mapping is a pair primary replica identifier local replica identifier . ifno local replica exists a special value is used. thus the mapping is relative to a machine. if the local replica is the primary one the pair contains two identical identifiers. ibis supports demand replication an automatic replication control policy similar to whole file caching. under demand replication reading of a nonlocal replica causes it to be cached locally thereby generating a new nonprimary replica. updates are performed only on the primary copy and cause all other replicas to be invalidated through the sending of appropriate messages. atomic and serialized invalidation of all nonprimary replicas is not guaranteed. hence a stale replica may be considered valid. to satisfy remote write accesses we migrate the primary copy to the requesting machine. . an example afs andrew is a distributed computing environment designed and implemented at carnegie mellon university. the andrew file system afs constitutes the underlying information sharing mechanism among clients of the environment. the transarc corporation took over development of afs then was purchased by ibm. ibm has since produced several commercial implementations of afs. afs was subsequently chosen as the dfs for an industry coalition the result was transarc dfs part of the distributed computing environment dce from the osf organization. in ibm's transarc lab announced that afs would be an open source product termed openafs available under the ibm public license and transarc dfs was canceled as a commercial product. openafs is available under most commercial versions of unix as well as linux and microsoft windows systems. many unix vendors as well as microsoft support the dce system and its dfs which is based on afs and work is ongoing to make dce a cross platform universally accepted dfs. as afs and transarc dfs are very similar we describe afs throughout this section unless transarc dfs is named specifically. afs seeks to solve many of the problems of the simpler dfss such as nfs and is arguably the most feature rich nonexperimental dfs. it features a uniform name space location independent file sharing client side caching . an example afs with cache consistency and secure authentication via kerberos. it also includes server side caching in the form of replicas with high avail ability through automatic switchover to a replica if the source server is unavailable. one of the most formidable attributes of afs is scalability the andrew system is targeted to span over workstations. between afs and transarc dfs there are hundreds of implementations worldwide. . . overview afs distinguishes between client machines sometimes referred to as workstations and dedicated server machines. servers and clients originally ran only . bsd unix but afs has been ported to many operating systems. the clients and servers are interconnected by a network of lans or wans. clients are presented with a partitioned space of file names a local name space and a shared name space. dedicated servers collectively called vice after the name of the software they run present the shared name space to the clients as a homogeneous identical and location transparent file hierarchy. the local name space is the root file system of a workstation from which the shared name space descends. workstations run the virtue protocol to communicate with vice and each is required to have a local disk where it stores its local name space. servers collectively are responsible for the storage and management of the shared name space. the local name space is small is distinct for each workstation and contains system programs essential for autonomous operation and better performance. also local are temporary files and files that the workstation owner for privacy reasons explicitly wants to store locally. viewed at a finer granularity clients and servers are structured in clusters interconnected by a wan. each cluster consists of a collection of workstations on a lan and a representative of vice called a cluster server and each cluster is connected to the wan by a router. the decomposition into clusters is done primarily to address the problem of scale. for optimal performance workstations should use the server on their own cluster most of the time thereby making cross cluster file references relatively infrequent. the file system architecture is also based on considerations of scale. the basic heuristic is to offload work from the servers to the clients in light of experience indicating that server cpu speed is the system's bottleneck. following this heuristic the key mechanism selected for remote file operations is to cache files in large chunks kb . this feature reduces file open latency and allows reads and writes to be directed to the cached copy without frequently involving the servers. briefly here are a few additional issues in the design of afs client mobility. clients are able to access any file in the shared name space from any workstation. a client may notice some initial performance degradation due to the caching of files when accessing files from a workstation other than the usual one. security. the vice interface is considered the boundary of trustworthiness because no client programs are executed on vice machines. authentication and secure transmission functions are provided as part of a connectionbased communication package based on the rpc paradigm. after mutual chapter distributed file systems authentication a vice server and a client communicate via encrypted messages. encryption is performed by hardware devices or more slowly in software. information about clients and groups is stored in a protection database replicated at each server. protection. afs provides access lists for protecting directories and the regular unix bits for file protection. the access list may contain information about those users allowed to access a directory as well as information about those users not allowed to access it. thus it is simple to specify that everyone except say jim can access a directory. afs supports the access types read write lookup insert administer lock and delete. heterogeneity. defining a clear interface to vice is a key for integration of diverse workstation hardware and operating systems. so that heterogeneity is facilitated some files in the local bin directory are symbolic links pointing to machine specific executable files residing in vice. . . the shared name space afs's shared name space is made up of component units called volumes. the volumes are unusually small component units. typically they are associated with the files of a single client. few volumes reside within a single disk partition and they may grow up to a quota and shrink in size. conceptually volumes are glued together by a mechanism similar to the unix mount mechanism. however the granularity difference is significant since in unix only an entire disk partition containing a file system canbe mounted. volumes are a key administrative unit and play a vital role in identifying and locating an individual file. a vice file or directory is identified by a low level identifier called a fid. each afs directory entry maps a path name component to a fid. a fid is bits long and has three equal length components a volume number a vnode number and a iiniquifier. the vnode number is used as an index into an array containing the modes of files in a single volume. the uniquifier allows reuse of vnode numbers thereby keeping certain data structures compact. fids are location transparent therefore file movements from server to server do not invalidate cached directory contents. location information is kept on a volume basis in a volume location database replicated on each server. a client can identify the location of every volume in the system by querying this database. the aggregation of files into volumes makes it possible to keep the location database at a manageable size. to balance the available disk space and utilization of servers volumes need to be migrated among disk partitions and servers. when a volume is shipped to its new location its original server is left with temporary forwarding information so that the location database does not need to be updated synchronously. while the volume is being transferred the original server can still handle updates which are shipped later to the new server. at some point the volume is briefly disabled so that the recent modifications can be processed then the new volume becomes available again at the new site. the volume movement operation is atomic if either server crashes the operation is aborted. . an example afs read only replication at the granularity of an entire volume is supported for system executable files and for seldom updated files in the upper levels of the vice name space. the volume location database specifies the server containing the only read write copy of a volume and a list of read only replication sites. . . file operations and consistency semantics the fundamental architectural principle in afs is the caching of entire files from servers. accordingly a client workstation interacts with vice servers only during opening and closing of files and even this interaction is not always necessary. reading and writing files do not cause remote interaction in contrast to the remote service method . this key distinction has far reaching ramifications for performance as well as for semantics of file operations. the operating system on each workstation intercepts file system calls and forwards them to a client level process on that workstation. this process called venus caches files from vice when they are opened and stores modified copies of files back on the servers from which they came when they are closed. venus may contact vice only when a file is opened or closed reading and writing of individual bytes of a file are performed directly on the cached copy and bypass venus. as a result writes at some sites are not visible immediately at other sites. caching is further exploited for future opens of the cached file. venus assumes that cached entries files or directories are valid unless notified otherwise. therefore venus does not need to contact vice on a file open to validate the cached copy. the mechanism to support this policy called callback dramatically reduces the number of cache validation requests received by servers. it works as follows. when a client caches a file or a directory the server updates its state information to record this caching. we say that the client has a callback on that file. the server notifies the client before allowing another client to modify the file. in such a case we say that the server removes the callback on the file for the former client. a client can use a cached file for open purposes only when the file has a callback. if a client closes a file after modifying it all other clients caching this file lose their callbacks. therefore when these clients open the file later they have to get the new version from the server. reading and writing bytes of a file are done directly by the kernel without venus's intervention on the cached copy. venus regains control when the file is closed. if the file has been modified locally it updates the file on the appropriate server. thus the only occasions on which venus contacts vice servers are on opens of files that either are not in the cache or have had their callback revoked and on closes of locally modified files. basically afs implements session semantics. the only exceptions are file operations other than the primitive read and write such as protection changes at the directory level which are visible everywhere on the network immediately after the operation completes. in spite of the callback mechanism a small amount of cached validation traffic is still present usually to replace callbacks lost because of machine or network failures. when a workstation is rebooted venus considers all cached chapter distributed file systems files and directories suspect and it generates a cache validation request for the first use of each such entry. the callback mechanism forces each server to maintain callback information and each client to maintain validity information. if the amount of callback information maintained by a server is excessive the server can break callbacks and reclaim some storage by unilaterally notifying clients and revoking the validity of their cached files. if the callback state maintained by venus gets out of sync with the corresponding state maintained by the servers some inconsistency may result. venus also caches contents of directories and symbolic links for pathname translation. each component in the path name is fetched and a callback is established for it if it is not already cached or if the client does not have a callback on it. venus does lookups on the fetched directories locally using fids. no requests are forwarded from one server to another. at the end of a path name traversal all the intermediate directories and the target file are in the cache with callbacks on them. future open calls to this file will involve no network communication at all unless a callback is broken on a component of the path name. the only exception to the caching policy is a modification to a directory that is made directly on the server responsible for that directory for reasons of integrity. the vice interface has well defined operations for such purposes. venus reflects the changes in its cached copy to avoid re fetching the directory. . . implementation client processes are interfaced to a unix kernel with the usual set of system calls. the kernel is modified slightly to detect references to vice files in the relevant operations and to forward the requests to the client level venus process at the workstation. venus carries out path name translation component by component as described above. it has a mapping cache that associates volumes to server locations in order to avoid server interrogation for an already known volume location. if a volume is not present in this cache venus contacts any server to which it already has a connection requests the location information and enters that information into the mapping cache. unless venus already has a connection to the server it establishes a new connection. it then uses this connection to fetch the file or directory. connection establishment is needed for authentication and security purposes. when a target file is found and cached a copy is created on the local disk. venus then returns to the kernel which opens the cached copy and returns its handle to the client process. the unix file system is used as a low level storage system for both afs servers and clients. the client cache is a local directory on the workstation's disk. within this directory are files whose names are placeholders for cache entries. both venus and server processes access unix files directly by the latter's modes to avoid the expensive path name to inode translation routine namei . because the internal inode interface is not visible to client level processes both venus and server processes are client level processes an appropriate set of additional system calls was added. dfs uses its own journaling file system to improve performance and reliability over ufs