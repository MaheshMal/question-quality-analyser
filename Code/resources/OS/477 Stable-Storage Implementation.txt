 . . problems with raid unfortunately raid does not always assure that data are available for the operating system and its users. a pointer to a file could be wrong for example or pointers within the file structure could be wrong. incomplete writes if not properly recovered could result in corrupt data. some other process could accidentally write over a file system's structures too. raid protects against physical rnedia errors but not other hardware and software errors. as large as the landscape of software and hardware bugs is that is how numerous are the potential perils for data on a system. the solaris zfs file system takes an innovative approach to solving these problems. it maintains internal checksums of all blocks including data and metadata. added functionality comes in the placement of the checksums. they are not kept with the block that is being checksummed. rather they are stored with the pointer to that block. consider an inode with pointers to its data. within the inode is the checksum of each block of data. if there is a problem with the data the checksum will be incorrect and the file system will knowabout it. if the data are mirrored and there is a block with a correct checksum and one with an incorrect checksum zfs will automatically update the bad block with the good one. likewise the directory entry that points to the inode has a checksum for the inode. any problem in the mode is detected when the directory is accessed. this checksumming takes places throughout all zfs structures providing a much higher level of consistency error detection and error correction than is found in raid disk sets or standard file systems. the extra overhead that is created by the checksum calculation and extra block read modify write cycles is not noticeable because the overall performance of zfs is very fast. . stable storage implementation in chapter we introduced the write ahead log which requires the availability of stable storage. by definition information residing in stable storage is never lost. to implement such storage we need to replicate the needed information on multiple storage devices usually disks with independent failure modes. we need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that when we are recovering from a failure we can force all copies to a consistent and correct value even if another failure occurs during the recovery. in this section we discuss how to meet these needs. a disk write results in one of three outcomes . successful completion. the data were written correctly on disk. . partial failure. a failure occurred in the midst of transfer so only some of the sectors were written with the new data and the sector being written during the failure may have been corrupted. . total failure. the failure occurred before the disk write started so the previous data values on the disk remain intact. whenever a failure occurs during writing of a block the system needs to detect it and invoke a recovery procedure to restore the block to a consistent chapter mass storage structure state. to do that the system must maintain two physical blocks for each logical block. an output operation is executed as follows . write the information onto the first physical block. . when the first write completes successfully write the same information onto the second physical block . declare the operation complete only after the second write completes successfully. during recovery from a failure each pair of physical blocks is examined. if both are the same and no detectable error exists then no further action is necessary. if one block contains a detectable error then we replace its contents with the value of the other block. if neither block contains a detectable error but the blocks differ in content then wre replace the content of the first block with that of the second. this recovery procedure ensures that a write to stable storage either succeeds completely or results in no change. we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage. although having a large number of copies further reduces the probability of a failure it is usually reasonable to simulate stable storage with only two copies. the data in stable storage are guaranteed to be safe unless a failure destroys all the copies. because waiting for disk writes to complete synchronous i o is time consuming many storage arrays add nvram as a cache. since the memory is nonvolatile usually it has battery power as a backup to the unit's power it can be trusted to store the data en route to the disks. it is thus considered part of the stable storage. writes to it are much faster than to disk so performance is greatly improved