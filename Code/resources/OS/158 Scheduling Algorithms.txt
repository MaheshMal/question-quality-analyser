 cpu scheduling deals with the problem of deciding which of the processes in the ready queue is to be allocated the cpu. there are many different cpu scheduling algorithms. in this section we describe several of them. . . first come first served scheduling by far the simplest cpu scheduling algorithm is the first come first served fcfs scheduling algorithm. with this scheme the process that requests the cpu first is allocated the cpu first. the implementation of the fcfs policy is easily managed with a fifo queue. when a process enters the ready queue its pcb is linked onto the tail of the queue. when the cpu is free it is allocated to the process at the head of the queue. the running process is then removed from the queue. the code for fcfs scheduling is simple to write and understand. the average waiting time under the fcfs policy however is often quite long. consider the following set of processes that arrive at time with the length of the cpu burst given in milliseconds process burst time p pi p . scheduling algorithms if the processes arrive in the order pi po p and are served in fcfs rder we get the result shown in the following gantt chart p the waiting time is milliseconds for process pi milliseconds for process pn and milliseconds for process pj. thus the average waiting time is milliseconds. if the processes arrive in the order pi p pi however the results will be as showrn in the following gantt chart the average waiting time is now milliseconds. this reduction is substantial. thus the average waiting time under an fcfs policy is generally not minimal and may vary substantially if the process's cpu burst times vary greatly. in addition consider the performance of fcfs scheduling in a dynamic situation. assume we have one cpu bound process and many i o bound processes. as the processes flow around the system the following scenario may result. the cpu bound process will get and hold the cpu. during this time all the other processes will finish their i o and will move into the ready queue waiting for the cpu. while the processes wait in the ready queue the i o devices are idle. eventually the cpu bound process finishes its cpu burst and moves to an i o device. all the i o bound processes which have short cpu bursts execute quickly and move back to the i o queues. at this point the cpu sits idle. the cpu bound process will then move back to the ready queue and be allocated the cpu. again all the i o processes end up waiting in the ready queue until the cpu bound process is done. there is a convoy effect as all the other processes wait for the one big process to get off the cpu. this effect results in lower cpu and device utilization than might be possible if the shorter processes were allowed to go first. the fcfs scheduling algorithm is nonpreemptive. once the cpu has been allocated to a process that process keeps the cpu until it releases the cpu either by terminating or by requesting i o. the fcfs algorithm is thus particularly troublesome for time sharing systems where it is important that each user get a share of the cpu at regular intervals. it would be disastrous to allow one process to keep the cpu for an extended period. . . shortest job first scheduling a different approach to cpu scheduling is the shortest job first sjf scheduling algorithm. this algorithm associates with each process the length of the process's next cpu burst. when the cpu is available it is assigned to the process that has the smallest next cpu burst. if the next cpu bursts of two processes are chapter cpu scheduling the same fcfs scheduling is used to break the tie. note that a more appropriate term for this scheduling method would be the shortest next cpu burst algorithm because scheduling depends on the length of the next cpu burst of a process rather than its total length. we use the term sjf because most people and textbooks use this term to refer to this type of scheduling. as an example of sjf scheduling consider the following set of processes with the length of the cpu burst given in milliseconds process burst time pi pi p pa using sjf scheduling we would schedule these processes according to the following gantt chart pa pi p p the waiting time is milliseconds for process p milliseconds for process pi milliseconds for process p and milliseconds for process p . thus the average waiting time is milliseconds. by comparison if we were using the fcfs scheduling scheme the average waiting time would be . milliseconds. the sjf scheduling algorithm is provably optimal in that it gives the minimum average waiting time for a given set of processes. moving a short process before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process. consequently the average waiting time decreases. the real difficulty with the sjf algorithm is knowing the length of the next cpu request. for long term job schedtiling in a batch system we can use as the length the process time limit that a user specifies when he submits the job. thus users are motivated to estimate the process time limit accurately since a lower value may mean faster response. too low a value will cause a time limit exceeded error and require resubmission. sjf scheduling is used frequently in long term scheduling. although the sjf algorithm is optimal it cannot be implemented at the level of short term cpu scheduling. there is no way to know the length of the next cpu burst. one approach is to try to approximate sjf scheduling. we may not know the length of the next cpu burst but we may be able to predict its value. we expect that the next cpu burst will be similar in length to the previous ones. thus by computing an approximation of the length of the next cpu burst we can pick the process with the shortest predicted cpu burst. the next cpu burst is generally predicted as an exponential average of the measured lengths of previous cpu bursts. let tn be the length of the th cpu . scheduling algorithms burst and let t i be our predicted value for the next cpu burst. then for a a define t atn l a in. this formula defines an exponential average. the value of tn contains our most recent information in stores the past history. the parameter a controls the relative weight of recent and past history in our prediction. if a then t i t and recent history has no effect current conditions are assumed to be transient if a then t ! i tn and only the most recent cpu burst matters history is assumed to be old and irrelevant . more commonly a so recent history and past history are equally weighted. the initial t can be defined as a constant or as an overall system average. figure . shows an exponential average with a and to . to understand the behavior of the exponential average we can expand the formula for t i by substituting for th to find at a atn i h since both a and a are less than or equal to each successive term has less weight than its predecessor. the sjf algorithm can be either preemptive or nonpreemptive. the choice arises when a new process arrives at the ready queue while a previous process is still executing. the next cpu burst of the newly arrived process may be shorter than what is left of the currently executing process. a preemptive sjf algorithm x ti i i i i i i time cpu burst f guess i figure . prediction of the length of the next cpu burst. chapter cpu scheduling will preempt the currently executing process whereas a nonpreemptite sjf algorithm will allow the currently running process to finish its cpu burst. preemptive sjf scheduling is sometimes called shortest remaining time first scheduling. as an example consider the following four processes with the length of the cpu burst given in milliseconds ocess arrival time burst time pi pi p p if the processes arrive at the ready queue at the times shown and need the indicated burst times then the resulting preemptive sjf schedule is as depicted in the following gantt chart pi p p pi p process pi is started at time since it is the only process in the queue. process p arrives at time . the remaining time for process pi milliseconds is larger than the time required by process p milliseconds so process pi is preempted and process p is scheduled. the average waiting time for this example is . milliseconds. nonpreemptive sjf scheduling would result in an average waiting time of . milliseconds. . . priority scheduling the sjf algorithm is a special case of the general priority scheduling algorithm. a priority is associated with each process and the cpu is allocated to the process with the highest priority. equal priority processes are scheduled in fcfs order. an sjf algorithm is simply a priority algorithm where the priority p is the inverse of the predicted next cpu burst. the larger the cpu burst the lower the priority and vice versa. note that we discuss scheduling in terms of high priority and low priority. priorities are generally indicated by some fixed range of numbers such as to or to . however there is no general agreement on whether is the highest or lowest priority. some systems use low numbers to represent low priority others use low numbers for high priority. this difference can lead to confusion. in this text we assume that low numbers represent high priority. as an example consider the following set of processes assumed to have arrived at time in the order pi p p with the length of the cpu burst given in milliseconds . scheduling algorithms process burst time priority pi pi p pa ps using priority scheduling we would schedule these processes according to the following gantt chart p p pi p p the average waiting time is . milliseconds. priorities can be defined either internally or externally. internally defined priorities use some measurable quantity or quantities to compute the priority of a process. for example time limits memory requirements the number of open files and the ratio of average i o burst to average cpu burst have been used in computing priorities. external priorities are set by criteria outside the operating system such as the importance of the process the type and amount of funds being paid for computer use the department sponsoring the work and other often political factors. priority scheduling can be either preemptive or nonpreemptive. when a process arrives at the ready queue its priority is compared with the priority of the currently running process. a preemptive priority scheduling algorithm will preempt the cpu if the priority of the newly arrived process is higher than the priority of the currently running process. a nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue. a major problem with priority scheduling algorithms is indefinite blocking or starvation. a process that is ready to run but waiting for the cpu can be considered blocked. a priority scheduling algorithm can leave some lowpriority processes waiting indefinitely. in a heavily loaded computer system a steady stream of higher priority processes can prevent a low priority process from ever getting the cpu. generally one of two things will happen. either the process will eventually be run at a.m. sunday when the system is finally lightly loaded or the computer system will eventually crash and lose all unfinished low priority processes. rumor has it that when they shut down the ibm at mit in they found a low priority process that had been submitted in and had not yet been run. a solution to the problem of indefinite blockage of low priority processes is aging. aging is a technique of gradually increasing the priority of processes that wait in the system for a long time. for example if priorities range from low to high we could increase the priority of a waiting process by every minutes. eventually even a process with an initial priority of would have the highest priority in the system and would be executed. in fact chapter cpu scheduling it would take no more than hours for a priority process to age to a priority process. . . round robin scheduling the round robin rr scheduling algorithm is designed especially for timesharing systems. it is similar to fcfs scheduling but preemption is added to switch between processes. a small unit of time called a time quantum or time slice is defined. a time quantum is generally from to milliseconds. the ready queue is treated as a circular queue. the cpu scheduler goes around the ready queue allocating the cpu to each process for a time interval of up to time quantum. to implement rr scheduling we keep the ready queue as a fifo queue of processes. new processes are added to the tail of the ready queue. the cpu scheduler picks the first process from the ready queue sets a timer to interrupt after time quantum and dispatches the process. one of two things will then happen. the process may have a cpu burst of less than time quantum. in this case the process itself will release the cpu voluntarily. the scheduler will then proceed to the next process in the ready queue. otherwise if the cpu burst of the currently running process is longer than time quantum the timer will go off and will cause an interrupt to the operating system. a context switch will be executed and the process will be put at the tail of the ready queue. the cpu scheduler will then select the next process in the ready queue. the average waiting time under the rr policy is often long. consider the following set of processes that arrive at time with the length of the cpu burst given in milliseconds process burst time pi pi if we use a time quantum of milliseconds then process pi gets the first milliseconds. since it requires another milliseconds it is preempted after the first time quantum and the cpu is given to the next process in the queue process p . since process pi does not need milliseconds it quits before its time quantum expires. the cpu is then given to the next process process p . once each process has received time quantum the cpu is returned to process pi for an additional time quantum. the resulting rr schedule is pi p p pi pi pi pi pi the average waiting time is . milliseconds. in the rr scheduling algorithm no process is allocated the cpu for more than time quantum in a row unless it is the only runnable process . if a . scheduling algorithms process's cpu burst exceeds time quantum that process is preempted and is put back in the ready queue. the rr scheduling algorithm is thus preemptive. if there are n processes in the ready queue and the time quantum is q then each process gets n of the cpu time in chunks of at most q time units. each process must wait no longer than n x q time units until its next time quantum. for example with five processes and a time quantum of milliseconds each process will get up to milliseconds every milliseconds. the performance of the rr algorithm depends heavily on the size of the time quantum. at one extreme if the time quantum is extremely large the rr policy is the same as the fcfs policy if the time quantum is extremely small say millisecond the rr approach is called processor sharing and in theory creates the appearance that each of n processes has its own processor running at n the speed of the real processor. this approach was used in control data corporation cdc hardware to implement ten peripheral processors with only one set of hardware and ten sets of registers. the hardware executes one instruction for one set of registers then goes on to the next. this cycle continues resulting in ten slow processors rather than one fast one. actually since the processor was much faster than memory and each instruction referenced memory the processors were not much slower than ten real processors would have been. in software we need also to consider the effect of context switching on the performance of rr scheduling. let us assume that we have only one process of time units. if the quantum is time units the process finishes in less than time quantum with no overhead. if the quantum is time units however the process requires quanta resulting in a context switch. if the time quantum is time unit then nine context switches will occur slowing the execution of the process accordingly figure . . thus we want the time quantum to be large with respect to the contextswitch time. if the context switch time is approximately percent of the time quantum then about percent of the cpu time will be spent in context switching. in practice most modern systems have time quanta ranging from to milliseconds. the time required for a context switch is typically less than microseconds thus the context switch time is a small fraction of the time quantum. process time quantum context switches o figure . the way in which a smaller time quantum increases context switches. chapter cpu scheduling j .p. . time quantum figure . the way in which turnaround time varies with the time quantum. turnaround time also depends on the size of the time quantum. as we can see from figure . the average turnaround time of a set of processes does not necessarily improve as the time quantum size increases. in general the average turnaround time can be improved if most processes finish their next cpu burst in a single time quantum. for example given three processes of time units each and a quantum of time unit the average turnaround time is . if the time quantum is however the average turnaround time drops to . if context switch time is added in the average turnaround time increases for a smaller time quantum since more context switches are required. although the time quantum should be large compared with the contextswitch time it should not be too large. if the time quantum is too large rr scheduling degenerates to fcfs policy. a rule of thumb is that percent of the cpu bursts should be shorter than the time quantum. . . multilevel queue scheduling another class of scheduling algorithms has been created for situations in which processes are easily classified into different groups. for example a common division is made between foreground interactive processes and background batch processes. these two types of processes have different response time requirements and so may have different scheduling needs. in addition foreground processes may have priority externally defined over background processes. a multilevel queue scheduling algorithm partitions the ready queue into several separate queues figure . . the processes are permanently assigned to one queue generally based on some property of the process such as memory size process priority or process type. each queue has its own scheduling . scheduling algorithms highest priority system processes . ' rio'i . j i . r ? student processes lowest priority figure . multilevel queue scheduling. algorithm. for example separate queues might be used for foreground and background processes. the foreground quetie might be scheduled by an rr algorithm while the background queue is scheduled by an fcfs algorithm. in addition there must be scheduling among the queues which is commonly implemented as fixed priority preemptive scheduling. for example the foreground queue may have absolute priority over the background queue. let's look at an example of a multilevel queue scheduling algorithm with five queues listed below in order of priority . system processes . interactive processes . interactive editing processes . batch processes . student processes each queue has absolute priority over lower priority queues. no process in the batch queue for example could run unless the queues for system processes interactive processes and interactive editing processes were all empty. if an interactive editing process entered the ready queue while a batch process was running the batch process would be preempted. another possibility is to time slice among the queues. here each queue gets a certain portion of the cpu time which it can then schedule among its various processes. for instance in the foreground background queue example the foreground queue can be given percent of the cpu time for rr scheduling among its processes whereas the background queue receives percent of the cpu to give to its processes on an fcfs basis. chapter cpu scheduling . . multilevel feedback queue scheduling normally when the multilevel queue scheduling algorithm is used processes are permanently assigned to a queue when they enter the system. if there are separate queues for foreground and background processes for example processes do not move from one queue to the other since processes do not change their foreground or background nature. this setup has the advantage of low scheduling overhead but it is inflexible. the multilevel feedback queue scheduling algorithm in contrast allows a process to move between queues. the idea is to separate processes according to the characteristics of their cpu bursts. if a process uses too much cpu time it will be moved to a lower priority queue. this scheme leaves i o bound and interactive processes in the higher priority queues. in addition a process that waits too long in a lower priority queue may be moved to a higher priority queue. this form of aging prevents starvation. for example consider a multilevel feedback queue scheduler with three queues numbered from to figure . . the scheduler first executes all processes in queue . only when queue is empty will it execute processes in queue . similarly processes in queue will only be executed if queues and are empty. a process that arrives for queue will preempt a process in queue . a process in queue will in turn be preempted by a process arriving for queue . a process entering the ready queue is put in queue . a process in queue is given a time quantum of milliseconds. if it does not finish within this time it is moved to the tail of queue . if queue is empty the process at the head of queue is given a quantum of milliseconds. if it does not complete it is preempted and is put into queue . processes in queue are run on an fcfs basis but are run only when queues and are empty. this scheduling algorithm gives highest priority to any process with a cpu burst of milliseconds or less. such a process will quickly get the cpu finish its cpu burst and go off to its next i o burst. processes that need more than but less than milliseconds are also served quickly although with lower priority than shorter processes. long processes automatically sink to queue and are served in fcfs order with any cpu cycles left over from queues and . figure . multilevel feedback queues