 the mutual exclusion of critical sections ensures that the critical sections are executed atomically. that is if two critical sections are executed concurrently the result is equivalent to their sequential execution in some unknown order. although this property is useful in many application domains in many cases we would like to make sure that a critical section forms a single logical unit of work that either is performed in its entirety or is not performed at all. an example is funds transfer in which one account is debited and another is credited. clearly it is essential for data consistency either that both the credit and debit occur or that neither occur. consistency of data along with storage and retrieval of data is a concern often associated with database systems. recently there has been an upsurge of interest in using database systems techniques in operating systems. operating systems can be viewed as manipulators of data as such they can benefit from the advanced techniques and models available from database research. for instance many of the ad hoc techniques used in operating systems to manage files could be more flexible and powerful if more formal database methods were used in their place. in sections . . to . . we describe some of these database techniques and explain how they can be used by operating systems. first however we deal with the general issue of transaction atomicity. it is this property that the database techniques are meant to address. . . system model a collection of instructions or operations that performs a single logical function is called a transaction. a major issue in processing transactions is the preservation of atomicity despite the possibility of failures within the computer system. we can think of a transaction as a program unit that accesses and perhaps updates various data items that reside on a disk within some files. from our point of view such a transaction is simply a sequence of read and write operations terminated by either a commit operation or an abort operation. a commit operation signifies that the transaction has terminated its execution successfully whereas an abort operation signifies that the transaction has ended its normal execution due to some logical error or a system failure. if a terminated transaction has completed its execution successfully it is committed otherwise it is aborted. since an aborted transaction may already have modified the data that it has accessed the state of these data may not be the same as it would have been if the transaction had executed atomically. so that atomicity is ensured . atomic transactions an aborted transaction must have no effect on the state of the data that it has already modified. thus the state of the data accessed by an aborted transaction must be restored to what it was just before the transaction started executing. we say that such a transaction has been rolled back. it is part of the responsibility of the system to ensure this property. to determine how the system should ensure atomicity we need first to identify the properties of devices used for storing the various data accessed by the transactions. various types of storage media are distinguished by their relative speed capacity and resilience to failure. volatile storage. information residing in volatile storage does not usually survive system crashes. examples of such storage are main and cache memory. access to volatile storage is extremely fast both because of the speed of the memory access itself and because it is possible to access directly any data item in volatile storage. nonvolatile storage. information residing in nonvolatile storage usually survives system crashes. examples of media for such storage are disks and magnetic tapes. disks are more reliable than main memory but less reliable than magnetic tapes. both disks and tapes however are subject to failure which may result in loss of information. currently nonvolatile storage is slower than volatile storage by several orders of magnitude because disk and tape devices are electromechanical and require physical motion to access data. stable storage. information residing in stable storage is never lost never should be taken with a grain of salt since theoretically such absolutes cannot be guaranteed . to implement an approximation of such storage we need to replicate information in several nonvolatile storage caches usually disk with independent failure modes and to update the information in a controlled manner section . . here we are concerned only with ensuring transaction atomicity in an environment where failures result in the loss of information on volatile storage. . . log based recovery one way to ensure atomicity is to record on stable storage information describing all the modifications made by the transaction to the various data it accesses. the most widely used method for achieving this form of recording is write ahead logging. here the system maintains on stable storage a data structure called the log. each log record describes a single operation of a transaction write and has the following fields transaction name. the unique name of the transaction that performed the write operation data item name. the unique name of the data item written old value. the value of the data item prior to the write operation new value. the value that the data item will have after the write chapter process synchronization other special log records exist to record significant events during transaction processing such as the start of a transaction and the commit or abort of a transaction. before a transaction t starts its execution the record t s t a r t s is written to the log. during its execution any write operation by t is preceded by the writing of the appropriate new record to the log. when commits the record t commits is written to the log. because the information in the log is used in reconstructing the state of the data items accessed by the various transactions we cannot allow the actual update to a data item to take place before the corresponding log record is written out to stable storage. we therefore require that prior to execution of a write x operation the log records corresponding to x be written onto stable storage. note the performance penalty inherent in this system. two physical writes are required for every logical write requested. also more storage is needed both for the data themselves and for the log recording the changes. in cases where the data are extremely important and fast failure recovery is necessary the price is worth the functionality. using the log the system can handle any failure that does not result in the loss of information on nonvolatile storage. the recovery algorithm uses two procedures undo tj which restores the value of all data updated by transaction t to the old values redo tj which sets the value of all data updated by transaction t to the new values the set of data updated by and their respective old and new values can be found in the log. the undo and redo operations must be idempotent that is multiple executions must have the same result as does one execution to guarantee correct behavior even if a failure occurs during the recovery process. if a transaction aborts then we can restore the state of the data that it has updated by simply executing undo . if a system failure occurs we restore the state of all updated data by consulting the log to determine which transactions need to be redone and which need to be undone. this classification of transactions is accomplished as follows transaction needs to be undone if the log contains the t s t a r t s record but does not contain the t commits record. transaction t needs to be redone if the log contains both the t s t a r t s and the commits records. . . checkpoints when a system failure occurs we must consult the log to determine those transactions that need to be redone and those that need to be undone. in principle we need to search the entire log to make these determinations. there are two major drawbacks to this approach . atomic transactions . the searching process is time consuming. . most of the transactions that according to our algorithm need to be redone have already actually updated the data that the log says they need to modify. although redoing the data modifications will cause no harm due to idempotency it will nevertheless cause recovery to take longer. to reduce these types of overhead we introduce the concept of checkpoints. during execution the system maintains the write ahead log. in addition the system periodically performs checkpoints that require the following sequence of actions to take place . output all log records currently residing in volatile storage usually main memory onto stable storage. . output all modified data residing in volatile storage to the stable storage. . output a log record checkpoint onto stable storage. the presence of a checkpoint record in the log allows the system to streamline its recovery procedure. consider a transaction tj that committed prior to the checkpoint. the t commit s record appears in the log before the checkpoints record. any modifications made by tj must have been written to stable storage either prior to the checkpoint or as part of the checkpoint itself. thus at recovery time there is no need to perform a redo operation on tj. this observation allows us to refine our previous recovery algorithm. after a failure has occurred the recovery routine examines the log to determine the most recent transaction that started executing before the most recent checkpoint took place. it finds such a transaction by searching the log backward to find the first checkpoint record and then finding the subsequent ti s t a r t record. once transaction tj has been identified the redo and undo operations need be applied only to transaction tj and all transactions tj that started executing after transaction tj . we'll call these transactions set t. the remainder of the log can thus be ignored. the recovery operations that are required are as follows a for all transactions tjt in t such that the record tj commits appears in the log execute redo t t . for all transactions tj in t that have no ti commits record in the log execute undo to . . concurrent atomic transactions we have been considering an environment in which only one transaction can be executing at a time. we now turn to the case where multiple transactions are active simultaneously. because each transaction is atomic the concurrent execution of transactions must be equivalent to the case where these transactions are executed serially in some arbitrary order. this property called serializability can be maintained by simply executing each transaction within chapter process synchronization a critical section. that is all transactions share a common semaphore mutex which is initialized to . when a transaction starts executing its first action is to execute wa. t mutex . after the transaction either commits or aborts it executes signal ?z ta' although this scheme ensures the atomicity of all concurrently executing transactions it is nevertheless too restrictive. as we shall see in many cases we can allow transactions to overlap their execution while maintaining serializability. a number of different concurrency control algorithms ensure serializability. these algorithms are described below. . . . serializability consider a system with two data items a and b that are both read and written by two transactions to and t . suppose that these transactions are executed atomically in the order to followed by t . this execution sequence which is called a schedule is represented in figure . . in schedule of figure . the sequence of instruction steps is in chronological order from top to bottom with instructions of to appearing in the left column and instructions of t appearing in the right column. a schedule in which each transaction is executed atomically is called a serial schedule. a serial schedule consists of a sequence of instructions from various transactions wherein the instructions belonging to a particular transaction appear together. thus for a set of n transactions there exist n different valid serial schedules. each serial schedule is correct because it is equivalent to the atomic execution of the various participating transactions in some arbitrary order. if we allow the two transactions to overlap their execution then the resulting schedule is no longer serial. a nonserial schedule does not necessarily imply an incorrect execution that is an execution that is not equivalent to one represented by a serial schedule . to see that this is the case we need to define the notion of conflicting operations. consider a schedule s in which there are two consecutive operations o and oj of transactions t and tj respectively. we say that o and oj conflict if they access the same data item and at least one of them is a w r i t e operation. to illustrate the concept of conflicting operations we consider the nonserial tn t read a write a read b write b read a write a read b write b figure . schedule a serial schedule in which to is followed by i. . atomic transactions read a write a read a write a read b write b read b write b figure . schedule a concurrent serializable schedule. schedule of figure . . the write a operation of to conflicts with the read a operation of ti. however the write a operation of t does not conflict with the read b operation of to because the two operations access different data items. let oj and oj be consecutive operations of a schedule s. if o and o are operations of different transactions and o and oj do not conflict then we can swap the order of o and to produce a new schedule s'. we expect s to be equivalent to s' as all operations appear in the same order in both schedules except for o and oj whose order does not matter. we can illustrate the swapping idea by considering again schedule of figure . . as the write a operation of t does not conflict with the read b operation of to we can swap these operations to generate an equivalent schedule. regardless of the initial system state both schedules produce the same final system state. continuing with this procedure of swapping nonconflicting operations we get swap the read b operation of tq with the read a operation of t . swap the write b operation of to with the write a operation of t . swap the write b operation of to with the read a operation of t . the final result of these swaps is schedule in figure . which is a serial schedule. thus we have shown that schedule is equivalent to a serial schedule. this result implies that regardless of the initial system state schedule will produce the same final state as will some serial schedule. if a schedule s can be transformed into a serial schedule s' by a series of swaps of nonconflicting operations we say that a schedule s is conflict serializable. thus schedule is conflict serializable because it can be transformed into the serial schedule . . . . locking protocol one way to ensure serializability is to associate with each data item a lock and to require that each transaction follow a locking protocol that governs how locks are acquired and released. there are various modes in which a data item can be locked. in this section we restrict our attention to two modes chapter process synchronization shared. if a transaction x has obtained a shared mode lock denoted by s on data item q then can read this item but cannot write q. exclusive. if a transaction t has obtained an exclusive mode lock denoted by x on data item q then can both read and write q. we require that every transaction request a lock in an appropriate mode on data item q depending on the type of operations it will perform on q. to access data item q transaction must first lock q in the appropriate mode. if q is not currently locked then the lock is granted and t can now access it. however if the data item q is currently locked by some other transaction then may have to wait. more specifically suppose that requests an exclusive lock on q. in this case must wait until the lock on q is released. if t requests a shared lock on q then must wait if q is locked in exclusive mode. otherwise it can obtain the lock and access q. notice that this scheme is quite similar to the readers writers algorithm discussed in section . . . a transaction may unlock a data item that it locked at an earlier point. it must however hold a lock on a data item as long as it accesses that item. moreover it is not always desirable for a transaction to unlock a data item immediately after its last access of that data item because serializability may not be ensured. one protocol that ensures serializability is the two phase locking protocol. this protocol requires that each transaction issue lock and unlock requests in two phases growing phase. a transaction may obtain locks but may not release any lock. shrinking phase. a transaction may release locks but may not obtain any new locks. initially a transaction is in the growing phase. the transaction acquires locks as needed. once the transaction releases a lock it enters the shrinking phase and no more lock requests can be issued. the two phase locking protocol ensures conflict serializability exercise . . it does not however ensure freedom from deadlock. in addition it is possible that for a given set of transactions there are conflict serializable schedules that cannot be obtained by use of the two phase locking protocol. however to improve performance over two phase locking we need either to have additional information about the transactions or to impose some structure or ordering on the set of data. . . . timestamp based protocols in the locking protocols described above the order followed by pairs of conflicting transactions is determined at execution time by the first lock that both request and that involves incompatible modes. another method for determining the serializability order is to select an order in advance. the most common method for doing so is to use a timestamp ordering scheme. with each transaction t in the system we associate a unique fixed timestamp denoted by ts t . this timestamp is assigned by the system . atomic transactions before the transaction t starts execution. if a transaction has been assigned timestamp ts tj and later a new transaction enters the system then ts ts tj . there are two simple methods for implementing this scheme use the value of the system clock as the timestamp that is a transaction's timestamp is equal to the value of the clock when the transaction enters the system. this method will not work for transactions that occur on separate systems or for processors that do not share a clock. use a logical counter as the timestamp that is a transaction's timestamp is equal to the value of the counter when the transaction enters the system. the counter is incremented after a new timestamp is assigned. the timestamps of the transactions determine the serializability order. thus if ts t ts t then the system must ensure that the produced schedule is equivalent to a serial schedule in which transaction t appears before transaction t . to implement this scheme we associate with each data item q two timestamp values w timestamp q denotes the largest timestamp of any transaction that successfully executed write q . r timestamp q denotes the largest timestamp of any transaction that successfully executed read q . these timestamps are updated whenever a new read q or write q instruction is executed. the timestamp ordering protocol ensures that any conflicting read and write operations are executed in timestamp order. this protocol operates as follows suppose that transaction t issues read q o if ts t w timestamp then t needs to read a value of q that was already overwritten. hence the read operation is rejected and tj is rolled back. o if ts tj w timestamp q then the read operation is executed and r timestamp q is set to the maximum of r timestamp q and ts t . suppose that transaction issues write q o if ts t r timestamp q then the value of q that is producing was needed previously and t assumed that this value would never be produced. hence the write operation is rejected and is rolled back. if ts t w timestamp q then t is attempting to write an obsolete value of q. hence this write operation is rejected and t is rolled back. o otherwise the write operation is executed. a transaction t that is rolled back as a result of the issuing of either a read or write operation is assigned a new timestamp and is restarted. chapter process synchronization t read b read b write b read a read a write a figure . schedule a schedule possible under the timestamp protocol. to illustrate this protocol consider schedule of figure . which includes transactions and t . we assume that a transaction is assigned a timestamp immediately before its first instruction. thus in schedule ts t ts t and the schedule is possible under the timestamp protocol. this execution can also be produced by the two phase locking protocol. however some schedules are possible under the two phase locking protocol but not under the timestamp protocol and vice versa. the timestamp protocol ensures conflict serializability. this capability follows from the fact that conflicting operations are processed in timestamp order. the protocol also ensures freedom from deadlock because no transaction ever waits