 just as there are myriad threats to system and network security there are many security solutions. the solutions run the gamut from improved user education through technology to writing bug free software. most security professionals subscribe to the theory of defense in depth which states that more layers of defense are better than fewer layers. of course this theory applies to any kind of security. consider the security of a house without a door lock with a door lock and with a lock and an alarm. in this section we look at the major methods tools and techniques that can be used to improve resistance to threats. . . security policy the first step toward improving the security of any aspect of computing is to have a security policy. policies vary widely but generally include a statement of what is being secured. for example a policy might state that all outsideaccessible applications must have a code review before being deployed or that users should not share their passwords or that all connection points between a company and the outside must have port scans run every six months. without a policy in place it is impossible for users and administrators to know what is permissible what is required and what is not allowed. the policy is a road map to security and if a site is trying to move from less secure to more secure it needs a map to know how to get there. once the security policy is in place the people it affects should know it well. it should be their guide. the policy should also be a living document that is reviewed and updated periodically to ensure that it is still pertinent and still followed. . . vulnerability assessment how can we determine whether a security policy has been correctly implemented? the best way is to execute a vulnerability assessment. such assessments can cover broad ground from social engineering through risk assessment to port scans. for example risk assessment endeavors to value the assets of the entity in question a program a management team a system or a facility and determine the odds that a security incident will affect the entity and decrease its value. when the odds of suffering a loss and the amount of the potential loss are known a value can be placed on trying to secure the entity. the core activity of most vulnerability assessments is a penetration test in which the entity is scanned for known vulnerabilities. because this book is . implementing security defenses concerned with operating systems and the software that runs on them we will concentrate on those aspects. vulnerability scans typically are done at times when computer use is relatively low to minimize their impact. when appropriate they are done on test systems rather than production systems because they can induce unhappy behavior from the target systems or network devices. a scan within an individual system can check a variety of aspects of the system short or easy to guess passwords unauthorized privileged programs such as setuid programs unauthorized programs in system directories unexpectedly long running processes improper directory protections on user and system directories improper protections on system data files such as the password file device drivers or the operating system kernel itself dangerous entries in the program search path for example the trojan horse discussed in section . . changes to system programs detected with checksum values unexpected or hidden network daemons any problems found by a security scan can be either fixed automatically or reported to the managers of the system. networked computers are much more susceptible to security attacks than are standalone systems. rather than attacks from a known set of access points such as directly connected terminals we face attacks from an unknown and large set of access points a potentially severe security problem. to a lesser extent systems connected to telephone lines via modems are also more exposed. in fact the u.s. government considers a system to be only as secure as its most far reaching connection. for instance a top secret system may be accessed only from within a building also considered top secret. the system loses its topsecret rating if any form of communication can occur outside that environment. some government facilities take extreme security precautions. the connectors that plug a terminal into the secure computer are locked in a safe in the office when the terminal is not in use. a person must have proper id to gain access to the building and her office must know a physical lock combination and must know authentication information for the computer itself to gain access to the computer an example of multi factor authentication. unfortunately for systems administrators and computer security professionals it is frequently impossible to lock a machine in a room and disallow all remote access. for instance the internet network currently connects millions of computers. it is becoming a mission critical indispensable resource for many companies and individuals. if you consider the internet a club then as in any club with millions of members there are many good members and some bad chapter security members. the bad members have many tools they can use to attempt to gain access to the interconnected computers just as morris did with his worm. vulnerability scans can be applied to networks to address some of the problems with network security. the scans search a network for ports that respond to a request. if services are enabled that should not be access to them can be blocked or they can be disabled. the scans then determine the details of the application listening on that port and try to determine if each has any known vulnerabilities. testing those vulnerabilities can determine if the system is misconfigured or is lacking needed patches. finally though consider the use of port scanners in the hands of a cracker rather than someone trying to improve security. these tools could help crackers find vulnerabilities to attack. fortunately it is possible to detect port scans through anomaly detection as we discuss next. it is a general challenge to security that the same tools can be used for good and for harm. in fact some people advocate security through obscurity stating that tools should not be written to test security so that security holes will be harder to find and exploit . others believe that this approach to security is not a valid one pointing out for example that crackers could write their own tools. it seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer. for example a company could publish its entire network configuration information but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected. even here though a company assuming that such information will remain a secret has a false sense of security. . . intrusion detection securing systems and facilities is intimately linked to intrusion detection. intrusion detection as its name suggests strives to detect attempted or successful intrusions into computer systems and to initiate appropriate responses to the intrusions. intrusion detection encompasses a wide array of techniques that vary on a number of axes. these axes include the time that detection occurs. detection can occur in real time while the intrusion is occurring or after the fact. the types of inputs examined to detect intrusive activity. these may include user shell commands process system calls and network packet headers or contents. some forms of intrusion might be detected only by correlating information from several such sources. the range of response capabilities. simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activity for example killing a process engaged in apparently intrusive activity. in a sophisticated form of response a system might transparently divert an intruder's activity to a honeypot a false resource exposed to the attacker. the resource appears real to the attacker and enables the system to monitor and gain information about the attack. these degrees of freedom in the design space for detecting intrusions have yielded a wide range of solutions known as intrusion detection systems . implementing security defenses idss and intrusion prevention systems idps . ids systems raise an alarm when an intrusion is detected while idp systems act as routers passing traffic unless an intrusion is detected at which point that traffic is blocked . but just what constitutes an intrusion? defining a suitable specification of intrusion turns out to be quite difficult and thus automatic idss and idps today typically settle for one of two less ambitious approaches. in the first called signature based detection system input or network traffic is examined for specific behavior patterns or signatures known to indicate attacks. a simple example of signature based detection is scanning network packets for the string etc passwd targeted for a unix system. another example is virus detection software which scans binaries or network packets for known viruses. the second approach typically called anomaly detection attempts through various techniques to detect anomalous behavior within computer systems. of course not all anomalous system activity indicates an intrusion but the presumption is that intrusions often induce anomalous behavior. an example of anomaly detection is monitoring system calls of a daemon process to detect whether the system call behavior deviates from normal patterns possibly indicating that a buffer overflow has been exploited in the daemon to corrupt its behavior. another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user either of which may indicate that an attacker has succeeded in gaining access to that user's account. signature based detection and anomaly detection can be viewed as two sides of the same coin signature based detection attempts to characterize dangerous behaviors and detects when one of these behaviors occurs whereas anomaly detection attempts to characterize normal or non dangerous behaviors and detects when something other than these behaviors occurs. these different approaches yield idss and idps with very different properties however. in particular anomaly detection can detect previously unknown methods of intrusion so called zero day attacks . signature based detection in contrast will identify only known attacks that can be codified in a recognizable pattern. thus new attacks that were not contemplated when the signatures were generated will evade signature based detection. this problem is well known to vendors of virus detection software who must release new signatures with great frequency as new viruses are detected manually. anomaly detection is not necessarily superior to signature based detection however. indeed a significant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately. if the system is already penetrated when it is benchmarked then the intrusive activity may be included in the normal benchmark. even if the system is benchmarked cleanly without influence from intrusive behavior the benchmark must give a fairly complete picture of normal behavior. otherwise the number of false positives false alarms or worse false negatives missed intrusions will be excessive. to illustrate the impact of even a marginally high rate of false alarms consider an installation consisting of a hundred unix workstations from which records of security relevant events are recorded for purposes of intrusion detection. a small installation such as this could easily generate a million audit records per day. only one or two might be worthy of an administrator's investigation. if we suppose optimistically that each such attack is reflected in chapter security ten audit records we can then roughly compute the rate of occurrence of audit records reflecting truly intrusive activity as o intrusions . n records dav intrusion i . . q records day interpreting this as a probability of occurrence of intrusive records ' we denote it as p i that is event i is the occurrence of a record reflecting truly intrusive behavior. since p . we also know that p i l p i . . now let a denote the raising of an alarm by an ids. an accurate ids should maximize both p l a and p .j a that is the probabilities that an alarm indicates an intrusion and that no alarm indicates no intrusion. focusing on p i a for the moment we can compute it using bayes' theorem . p a i . p a i . p a i now consider the impact of the false alarm rate p a j on p i a . even with a very good true alarm rate of p a l . a seemingly good falsealarm rate of p a i . yields p i a . . that is fewer than one in every seven alarms indicates a real intrusion! in systems where a security administrator investigates each alarm a high rate of false alarms called a christmas tree effect is exceedingly wasteful and will quickly teach the administrator to ignore alarms. this example illustrates a general principle for idss and idps for usability they must offer an extremely low false alarm rate. achieving a sufficiently low false alarm rate is an especially serious challenge for anomaly detection systems as mentioned because of the difficulties of adequately benchmarking normal system behavior. however research continues to improve anomalydetection techniques. intrusion detection software is evolving to implement signatures anomaly algorithms and other algorithms and to combine the results to arrive at a more accurate anomaly detection rate. . . virus protection as we have seen viruses can and do wreak havoc on systems. protection from viruses thus is an important security concern. antivirus programs are often used to provide this protection. some of these programs are effective against only particular known viruses. they work by searching all the programs on a system for the specific pattern of instructions known to make up the virus. when they find a known pattern they remove the instructions disinfecting the program. antivirus programs may have catalogs of thousands of viruses for which they search. . implementing security defenses the tripwire file system an example of an anomaly detection tool is the tripwire file system integritychecking tool for unix developed at purdue university. ivipwire operates on the premise that many intrusions result in modification of system directories and files. for example an attacker might modify the system programs perhaps inserting copies with trojan horses or might insert new programs into directories commonly found in user shell search paths or an intruder might remove system log'files to cover his tracks. tripwire is a tool to monitor file systems for added deleted or changed files and to alert system administrators to these modifications. the operation of tripwire is controlled by a configuration file tv.conf ig that enumerates the directories and files to be monitored for changes deletions or additions. each entry in this configuration file includes a selection mask to specify the file attributes inode attributes that will be monitored for changes. for example the selection mask might specify that a file's permissions be monitored but its access time be ignored. in addition the selection mask can instruct that the file be monitored for changes. monitoring the hash of a file for changes is as good as monitoring the file itself but storing hashes of files requires far less room than copying the files themselves. when run initially tripwire takes as input the tw.config file and computes a signature for each file or directory consisting of its monitored attributes inode attributes and hash values . these signatures are stored in a. database. when run subsequently tripwire inputs both tw.config and the previously stored database recomputes the signature for each file or directory named in tw.cprif ig and compares this signature with the signature if any in the previously computed database. events reported to an administrator include any monitored file or directory whose signature differs from that in the database a changed file any file or directory in a monitored directory for which a signature does hot exist in the database an added file and any signature in .the.database.for. which the corresponding file or directory no longer exists a deleted file . although effective for a wide class of attacks tripwire does have limitations. perhaps the most obvious is the need to protect the tripwire program and its associated files especially the database file from unauthorized modification. for this reason tripwire and its associated files should be stored on some tamper proof medium such as a write protected disk or a secure server where logins can be tightly controlled. unfortunately this makes it less convenient to update the database after authorized updates to monitored directories and files. a second limitation is that some security relevant files for example system log files are supposed to change over time and tripwire does not provide a way to distinguish between an authorized and an unauthorized change. so for example an attack that modifies without deleting a system log that would normally change anyway would escape tripwire's detection capabilities. the best tripwire can do in this case is to detect certain obvious inconsistencies for example if the log file shrinks . free and commercial versions of tripwire are available from http tripwire.org and http tripvvire.com. chapter security both viruses and antivirus software continue to become more sophisticated. some viruses modify themselves as they infect other software to avoid the basic pattern match approach of antivirus programs. antivirus programs in turn now look for families of patterns rather than a single pattern to identify a virus. in fact some anti virus programs implement a variety of detection algorithms. they can decompress compressed viruses before checking for a signature. some also look for process anomalies. a process opening an executable file for writing is suspicious for example unless it is a compiler. another popular technique is to run a program in a sandbox which is a controlled or emulated section of the system. the antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored. some antivirus programs also put up a complete shield rather than just scanning files within a file system. they search boot sectors memory inbound and outbound e mail files as they are downloaded files on removable devices or media and so on. the best protection against computer viruses is prevention or the practice of safe computing. purchasing unopened software from vendors and avoiding free or pirated copies from public sources or disk exchange offer the safest route to preventing infection. however even new copies of legitimate software applications are not immune to virus infection there have been cases where disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company selling the software. for macro viruses one defense is to exchange word documents in an alternative file format called rich text format rtf . unlike the native word format rtf does not include the capability to attach macros. another defense is to avoid opening any e mail attachments from unknown users. unfortunately history has shown that e mail vulnerabilities appear as fast as they are fixed. for example in the love bug virus became very widespread by appearing to be a love note sent by a friend of the receiver. once the attached visual basic script was opened the virus propagated by sending itself to the first users in the user's e mail contact list. fortunately except for clogging e mail systems and users' inboxes it was relatively harmless. it did however effectively negate the defensive strategy of opening attachments only from people known to the receiver. a more effective defense method is to avoid opening any e mail attachment that contains executable code. some companies now enforce this as policy by removing all incoming attachments to e mail messages. another safeguard although it does not prevent infection does permit early detection. a user must begin by completely reformatting the hard disk especially the boot sector which is often targeted for viral attack. only secure software is uploaded and a signature of each program is taken via a secure message digest computation. the resulting filename and associated messagedigest list must then be kept free from unauthorized access. periodically or each time a program is run the operating system recomputes the signature and compares it with the signature on the original list any differences serve as a warning of possible infection. this technique can be combined with others. for example a high overhead antivirus scan such as a sandbox can be used and if a program passes the test a signature can be created for it. if the signatures match the next time the program is run it does not need to be virus scanned again. . firewalling to protect systems and networks . . auditing accounting and logging . auditing accounting and logging can decrease system performance but they are useful in several areas including security. logging can be general or specific. all system call executions can be logged for analysis of program behavior or misbehavior . more typically suspicious events are logged. authentication failures and authorization failures can tell us quite a lot about break in attempts. accounting is another potential tool in a security administrator's kit. it can be used to find performance changes which in turn can reveal security problems. one of the early unix computer break ins was detected by cliff stoll when he was examining accounting logs and spotted an anomaly. . firewalling to protect systems and networks we turn next to the question of how a trusted computer can be connected safely to an untrustworthy network. one solution is the use of a firewall to separate trusted and untrusted systems. a firewall is a computer appliance or router that sits between the trusted and the untrusted. a network firewall limits network access between the two security domains and monitors and logs all connections. it can also limit connections based on source or destination address source or destination port or direction of the connection. for instance web servers use http to communicate with web browsers. a firewall therefore may allow only http to pass from all hosts outside the firewall to the web server within the firewall. the morris internet worm used the f inger protocol to break into computers so f inger would not be allowed to pass for example. in fact a network firewall can separate a network into multiple domains. a common implementation has the internet as the untrusted domain a semitrusted and semi secure network called the demilitarized zone dmz as another domain and a company's computers as a third domain figure . . connections are allowed from the internet to the dmz computers and from the company computers to the internet but are not allowed from the internet or dmz computers to the company computers. optionally controlled communications may be allowed between the dmz and one company computer or more. for instance a web server on the dmz may need to query a database server on the corporate network. with a firewall however access is contained and any dmz systems that are broken into still are unable to access the company computers. of course a firewall itself must be secure and attack proof otherwise its ability to secure connections can be compromised. furthermore firewalls do not prevent attacks that tunnel or travel within protocols or connections that the firewall allows. a buffer overflow attack to a web server will not be stopped by the firewall for example because the http connection is allowed it is the contents of the http connection that house the attack. likewise denial ofservice attacks can affect firewalls as much as any other machines. another vulnerability of firewalls is spoofing in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion. for example if a firewall rule allows a connection from a host and identifies that host by its ip address then another host could send packets using that same address and be allowed through the firewall. chapter security internet access from company's computers dmz access from internet access between dmz and company's computers figure . domain separation via firewall. in addition to the most common network firewalls there are other newer kinds of firewalls each with its pros and cons. a personal firewall is a software layer either included with the operating system or added as an application. rather than limiting communication between security domains it limits communication to and possibly from a given host. a user could add a personal firewall to her pc so that a trojan horse would be denied access to the network to which the pc is connected. an application proxy firewall understands the protocols that applications speak across the network. for example smtp is used for mail transfer. an application proxy accepts a connection just as an smtp server would and then initiates a connection to the original destination smtp server. it can monitor the traffic as it forwards the message watching for and disabling illegal commands attempts to exploit bugs and so on. some firewalls are designed for one specific protocol. an xml firewall for example has the specific purpose of analyzing xml traffic and blocking disallowed or malformed xml. system call firewalls sit between applications and the kernel monitoring system call execution. for example in solaris the least privilege feature implements a list of more than fifty system calls that processes may or may not be allowed to make. a process that does not need to spawn other processes can have that ability taken away for instance. . computer security classifications the u.s. department of defense trusted computer system evaluation criteria specify four security classifications in systems a b c and d. this specification is widely used to determine the security of a facility and to model security solutions so we explore it here. the lowest level classification is division d or minimal protection. division d includes only one class and is used for systems . computer security classifications that have failed to meet the requirements of any of the other security classes. for instance ms dos and windows . are in division d. division c the next level of security provides discretionary protection and accountability of users and their actions through the use of audit capabilities. division c has two levels cl and c . a cl class system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data. a cl environment is one in which cooperating users access data at the same levels of sensitivity. most versions of unix are cl class. the sum total of all protection systems within a computer system hardware software firmware that correctly enforce a security policy is known as a trusted computer base tcb . the tcb of a cl system controls access between users and files by allowing the user to specify and control sharing of objects by named individuals or defined groups. in addition the tcb requires that the users identify themselves before they start any activities that the tcb is expected to mediate. this identification is accomplished via a protected mechanism or password the tcb protects the authentication data so that they are inaccessible to unauthorized users. a c class system adds an individual level access control to the requirements of a cl system. for example access rights of a file can be specified to the level of a single individual. in addition the system administrator can selectively audit the actions of any one or more users based on individual identity. the tcb also protects itself from modification of its code or data structures. in addition no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system. some special secure versions of unix have been certified at the c level. division b mandatory protection systems have all the properties of a classc system in addition they attach a sensitivity label to each object. the bl class tcb maintains the security label of each object in the system the label is used for decisions pertaining to mandatory access control. for example a user at the confidential level could not access a file at the more sensitive secret level. the tcb also denotes the sensitivity level at the top and bottom of each page of any human readable output. in addition to the normal user namepassword authentication information the tcb also maintains the clearance and authorizations of individual users and will support at least two levels of security. these levels are hierarchical so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance. for example a secret level user could access a file at the confidential level in the absence of other access controls. processes are also isolated through the use of distinct address spaces. a b class system extends the sensitivity labels to each system resource such as storage objects. physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located. in addition a b system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel. a b class system allows the creation of access control lists that denote users or groups not granted access to a given named object. the tcb also contains a mechanism to monitor events that may indicate a violation of chapter security security policy. the mechanism notifies the security administrator a id if necessary terminates the event in the least disruptive manner. the highest level classification is division a. architecturally a class al system is functionally equivalent to a b system but it uses formal design specifications and verification techniques granting a high degree of assurance that the tcb has been implemented correctly. a system beyond class al might be designed and developed in a trusted facility by trusted personnel. the use of a tcb merely ensures that the system can enforce aspects of a security policy the tcb does not specify what the policy should be. typically a given computing environment develops a security policy for certification and has the plan accredited by a security agency such as the national computer security center. certain computing environments may require other certification such as that supplied by tempest which guards against electronic eavesdropping. for example a tempest certified system has terminals that are shielded to prevent electromagnetic fields from escaping. this shielding ensures that equipment outside the room or building where the terminal is housed cannot detect what information is being displayed by the terminal. . an example windows xp microsoft windows xp is a general purpose operating system designed to support a variety of security features and methods. in this section we examine features that windows xp uses to perform security functions. for more information and background on windows xp see chapter . the windows xp security model is based on the notion of user accounts. windows xp allows the creation of any number of user accounts which can be grouped in any manner. access to system objects can then be permitted or denied as desired. users are identified to the system by a unique security id. when a user logs on windows xp creates a security access token that includes the security id for the user security ids for any groups of which the user is a member and a list of any special privileges that the user has. examples of special privileges include backing up files and directories shutting down the computer logging on interactively and changing the system clock. every process that windows xp runs on behalf of a user will receive a copy of the access token. the system uses the security ids in the access token to permit or deny access to system objects whenever the user or a process on behalf of the user attempts to access the object. authentication of a user account is typically accomplished via a user name and password although the modular design of windows xp allows the development of custom authentication packages. for example a retinal or eye scanner might be used to verify that the user is who she says she is. windows xp uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user is authorized to have. a subject is used to track and manage permissions for each program that a user runs it is composed of the user's access token and the program acting on behalf of the user. since windows xp operates with a client server model two classes of subjects are used to control access simple subjects and server subjects. an example of a simple subject is the typical application program that a user executes after she logs on. the simple subject is assigned a security . an example windows xp context based on the security access token of the user. a server subject is a process implemented as a protected server that uses the security context of the client when acting on the client's behalf. as mentioned in section . auditing is a useful security technique. windows xp has built in auditing that allows many common security threats to be monitored. examples include failure auditing for login and logoff events to detect random password break ins success auditing for login and logoff events to detect login activity at strange hours success and failure write access auditing for executable files to track a virus outbreak and success and failure auditing for file access to detect access to sensitive files. security attributes of an object in windows xp are described by a security descriptor. the security descriptor contains the security id of the owner of the object who can change the access permissions a group security id used only by the posix subsystem a discretionary access control list that identifies which users or groups are allowed and which are not allowed access and a system access control list that controls which atiditing messages the system will generate. for example the security descriptor of the file foo.bar might have owner avi and this discretionary access control list avi all access group cs read write access user cliff no access in addition it might have a system access control list of audit writes by everyone. an access control list is composed of access control entries that contain the security id of the individual and an access mask that defines all possible actions on the object with a value of accessauowed or accessdenied for each action. files in windows xp may have the following access types readdata writedata appenddata execute readextendedattribute writeextendedattribute readattributes and w r i t e a t t r i b u t e s . we can see how this allows a fine degree of control over access to objects. windows xp classifies objects as either container objects or noncontainer objects. container objects such as directories can logically contain other objects. by default when an object is created within a container object the new object inherits permissions from the parent object. similarly if the user copies a file from one directory to a new directory the file will inherit the permissions of the destination directory. noncontainer objects inherit no other permissions. furthermore if a permission is changed on a directory the new permissions do not automatically apply to existing files and subdirectories the user may explicitly apply them if she so desires. the system administrator can prohibit printing to a printer on the system for all or part of a day and can use the windows xp performance monitor to help her spot approaching problems. in general windows xp does a good job of providing features to help ensure a secure computing environment. many of these features are not enabled by default however which may be one reason for the myriad security breaches on windows xp systems. another reason is the vast number of services windows xp starts at system boot time and the number of applications that typically are installed on a windows xp system. chapter security for a real multiuser environment the system administrator should forrnulate a security plan and implement it using the features that windows xp provides and other security tools