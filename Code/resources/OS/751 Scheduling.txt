 thus if clone is passed the flags clone fs cl ne vm clone sig and and clone files the parent and child tasks will share the same file system information such as the current working directory the same memory space the same signal handlers and the same set of open files. using clone in this fashion is equivalent to creating a thread in other systems since the parent task shares most of its resources with its child task. however if none of these flags is set when clone is invoked no sharing takes place resulting in functionality similar to the f ork system call. the lack of distinction between processes and threads is possible because linux does not hold a process's entire context within the main process data structure rather it holds the context within independent subcontexts. thus a process's file system context file descriptor table signal handler table and virtual memory context are held in separate data structures. the process data structure simply contains pointers to these other structures so any number of processes can easily share a subcontext by pointing to the same subcontext as appropriate. the arguments to the clone system call tell it which subcontexts to copy and which to share when it creates a new process. the new process always is given a new identity and a new scheduling context according to the arguments passed however it may either create new subcontext data structures initialized to be copies of the parent's or set up the new process to use the same subcontext data structures being used by the parent. the fork system call is nothing more than a special case of clone o that copies all subcontexts sharing none. . scheduling scheduling is the job of allocating cpu time to different tasks within an operating system. normally we think of scheduling as being the running and interrupting of processes but another aspect of scheduling is also important to linux the running of the various kernel tasks. kernel tasks encompass both tasks that are requested by a running process and tasks that execute internally on behalf of a device driver. . . process scheduling linux has two separate process scheduling algorithms. one is a time sharing algorithm for fair preemptive scheduling among multiple processes the other is designed for real time tasks where absolute priorities are more important than fairness. the scheduling algorithm used for routine time sharing tasks received a major overhaul with version . of the kernel. prior to version . the linux kernel ran a variation of the traditional unix scheduling algorithm. among other issues problems with the traditional unix scheduler are that it does not provide adequate support for smp systems and that it does not scale well as the number of tasks on the system grows. the overhaul of the scheduler with version . of the kernel now provides a scheduling algorithm that runs in constant time known as o l regardless of the number of tasks on the system. the new scheduler also provides increased support for smp including chapter the linux system numeric relative time priority priority quantum highest ms real time tasks other tasks lowest ms figure . the relationship between priorities and time slice length. processor affinity and load balancing as well as maintaining fairness and support for interactive tasks. the linux scheduler is a preemptive priority based algorithm with two separate priority ranges a real time range from to and a nice value ranging from to . these two ranges map into a global priority scheme whereby numerically lower values indicate higher priorities. unlike schedulers for many other systems linux assigns higher priority tasks longer time quanta and vice versa. because of the unique nature of the scheduler this is appropriate for linux as we shall soon see. the relationship between priorities and time slice length is shown in figure . . a runnable task is considered eligible for execution on the cpu so long as it has time remaining in its time slice. when a task has exhausted its time slice it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta. the kernel maintains a list of all runnable tasks in a runqueue data structure. because of its support for smp each processor maintains its own runqueue and schedules itself independently. each runqueue contains two priority arrays active and expired. the active array contains all tasks with time remaining in their time slices and the expired array contains all expired tasks. each of these priority arrays includes a list of tasks indexed according to priority figure . . the scheduler chooses the task with the highest priority from the active array for execution on the cpu. on multiprocessor machines this means that each processor is scheduling the highest priority task from its own runqueue structure. when all tasks have exhausted their time slices that is the active array is empty the two priority arrays are exchanged as the expired array becomes the active array and vice versa. tasks are assigned dynamic priorities that are based on the nice value plus or minus up to the value based upon the interactivity of the task. whether a value is added to or subtracted from a task's nice value depends on the interactivity of the task. a task's interactivity is determined by how long it has been sleeping while waiting for i o. tasks that are more interactive typically have longer sleep times and therefore are more likely to have an adjustment closer to as the scheduler favors such interactive tasks. conversely tasks with shorter sleep times are often more cpu bound and thus will have their priorities lowered. . scheduling active expired array array priority task lists priority task lists o o o o o okx o o o o figure . list of tasks indexed according to priority. the recalculation of a task's dynamic priority occurs when the task has exhausted its time quantum and is to be moved to the expired array. thus when the two arrays are exchanged all tasks in the new active array have been assigned new priorities and corresponding time slices. linux's real time scheduling is simpler still. linux implements the two realtime scheduling classes required by posix.lb first come first served fcfs and round robin sections . . and . . respectively . in both cases each process has a priority in addition to its scheduling class. processes of different priorities can compete with one another to some extent in time sharing scheduling in real time scheduling however the scheduler always runs the process with the highest priority. among processes of equal priority it runs the process that has been waiting longest. the only difference between fcfs and round robin scheduling is that fcfs processes continue to run until they either exit or block whereas a round robin process will be preempted after a while and will be moved to the end of the scheduling queue so round robin processes of equal priority will automatically time share among themselves. unlike routine time sharing tasks real time tasks are assigned static priorities. linux's real time scheduling is soft rather than hard real time. the scheduler offers strict guarantees about the relative priorities of real time processes but the kernel does not offer any guarantees about how quickly a real time process will be scheduled once that process becomes runnable. . . kernel synchronization the way the kernel schedules its own operations is fundamentally different from the way it schedules processes. a request for kernel mode execution can occur in two ways. a running program may request an operating system service either explicitly via a system call or implicitly for example when a page fault occurs. alternatively a device driver may deliver a hardware interrupt that causes the cpu to start executing a kernel defined handler for that interrupt. the problem posed to the kernel is that all these tasks may try to access the same internal data structures. if one kernel task is in the middle of accessing some data structure when an interrupt service routine executes then that service routine cannot access or modify the same data without risking data corruption. this fact relates to the idea of critical sections portions of code that access shared data and that must not be allowed to execute concurrently. as a result kernel synchronization involves much more than just process chapter the linux system scheduling. a framework is required that allows kernel tasks to run wfthout violating the integrity of shared data. prior to version . linux was a nonpreemptive kernel meaning that a process running in kernel mode could not be preempted even if a higherpriority process became available to run. with version . the linux kernel became fully preemptive so a task can now be preempted when it is running in the kernel. the linux kernel provides spinlocks and semaphores as well as readerwriter versions of these two locks for locking in the kernel. on smp machines the fundamental locking mechanism is a spinlock the kernel is designed so that the spinlock is held only for short durations. on single processor machines spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption. that is on single processor machines rather than holding a spinlock the task disables kernel preemption. when the task would otherwise release the spinlock it enables kernel preemption. this pattern is summarized below single processor multiple processors disable kernel preemption. acquire spin lock. enable kernel preemption. release spin lock. linux uses an interesting approach to disable and enable kernel preemption. it provides two simple system calls preempt.disable and preempt .enable for disabling and enabling kernel preemption. however in addition the kernel is not preemptible if a kernel mode task is holding a lock. to enforce this rule each task in the system has a thread info structure that includes the field preempt count which is a counter indicating the number of locks being held by the task. when a lock is acquired preempt count is incremented. likewise it is decremented when a lock is released. if the value of preempt .count for the task currently running is greater than zero it is not safe to preempt the kernel as this task currently holds a lock. if the count is zero the kernel can safely be interrupted assuming there are no outstanding calls to preempt disable . spinlocks along with enabling and disabling kernel preemption are used in the kernel only when the lock is held for short durations. when a lock must be held for longer periods semaphores are used. the second protection technique that linux uses applies to critical sections that occur in interrupt service routines. the basic tool is the processor's interrupt control hardware. by disabling interrupts or using spinlocks during a critical section the kernel guarantees that it can proceed without the risk of concurrent access of shared data structures. however there is a penalty for disabling interrupts. on most hardware architectures interrupt enable and disable instructions are expensive. furthermore as long as interrupts remain disabled all i o is suspended and any device waiting for servicing will have to wait until interrupts are reenabled so performance degrades. the linux kernel uses a synchronization architecture that allows long critical sections to run for their entire duration without having interrupts disabled. this ability is especially useful in the networking code an . scheduling top half interrupt handlers bottom half interrupt handlers kernel system service routines preemptible user mode programs preemptible figure . interrupt protection levels. interrupt in a network device driver can signal the arrival of an entire network packet which may result in a great deal of code being executed to disassemble route and forward that packet within the interrupt service routine. linux implements this architecture by separating interrupt service routines into two sections the top half and the bottom half. the top half is a normal interrupt service routine and runs with recursive interrupts disabled interrupts of a higher priority may interrupt the routine but interrupts of the same or lower priority are disabled. the bottom half of a service routine is run with all interrupts enabled by a miniature scheduler that ensures that bottom halves never interrupt themselves. the bottom half scheduler is invoked automatically whenever an interrupt service routine exits. this separation means that the kernel can complete any complex processing that has to be done in response to an interrupt without worrying about being interrupted itself. if another interrupt occurs while a bottom half is executing then that interrupt can request that the same bottom half execute but the execution will be deferred until the one currently running completes. each execution of the bottom half can be interrupted by a top half but can never be interrupted by a similar bottom half. the top half bottom half architecture is completed by a mechanism for disabling selected bottom halves while executing normal foreground kernel code. the kernel can code critical sections easily using this system. interrupt handlers can code their critical sections as bottom halves and when the foreground kernel wants to enter a critical section it can disable any relevant bottom halves to prevent any other critical sections from interrupting it. at the end of the critical section the kernel can reenable the bottom halves and run any bottom half tasks that have been queued by top half interrupt service routines during the critical section. figure . summarizes the various levels of interrupt protection within the kernel. each level may be interrupted by code running at a higher level but will never be interrupted by code running at the same or a lower level except for user mode code user processes can always be preempted by another process when a time sharing scheduling interrupt occurs. . . symmetric multiprocessing the linux . kernel was the first stable linux kernel to support symmetric multiprocessor smp hardware allowing separate processes to execute in parallel on separate processors. originally the implementation of smp imposed chapter the linux system the restriction that only one processor at a time could be executing kernel anode code. in version . of the kernel a single kernel spinlock sometimes termed bkl for big kernel lock was created to allow multiple processes running on different processors to be active in the kernel concurrently. however the bkl provided a very coarse level of locking granularity. later releases of the kernel made the smp implementation more scalable by splitting this single kernel spinlock into multiple locks each of which protects only a small subset of the kernel's data structures. such spinlocks are described in section . . . the . kernel provided additional smp enhancements including processor affinity and load balancing algorithms