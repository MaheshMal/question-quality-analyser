 our coverage of scheduling so far has focused primarily on soft real time systems. as mentioned though scheduling for such systems provides no guarantee on when a critical process will be scheduled it guarantees only that the process will be given preference over noncritical processes. hard real time systems have stricter requirements. a task must be serviced by its deadline service after the deadline has expired is the same as no service at all. we now consider scheduling for hard real time systems. before we proceed with the details of the individual schedulers however we must define certain characteristics of the processes that are to be scheduled. first the processes are considered periodic. that is they require the cpu at constant intervals periods . each periodic process has a fixed processing time t once it acquires the cpu a deadline d when it must be serviced by the cpu and a period p. the relationship of the processing time the deadline and the period can be expressed as t d p. the rate of a periodic task is p. figure . illustrates the execution of a periodic process over time. schedulers can take advantage of this relationship and assign priorities according to the deadline or rate requirements of a periodic process. what is unusual about this form of scheduling is that a process may have to announce its deadline requirements to the scheduler. then using a technique known as an admission control algorithm the scheduler either admits the process guaranteeing that the process will complete on time or rejects the request as impossible if it cannot guarantee that the task will be serviced by its deadline. in the following sections we explore scheduling algorithms that address the deadline requirements of hard real time systems. . real time cpu scheduling p p j time period i period? period figure . periodic task. . . rate monotonic scheduling the rate monotonic scheduling algorithm schedules periodic tasks using a static priority policy with preemption. if a lower priority process is running and a higher priority process becomes available to run it will preempt the lower priority process. upon entering the system each periodic task is assigned a priority inversely based on its period the shorter the period the higher the priority the longer the period the lower the priority. the rationale behind this policy is to assign a higher priority to tasks that require the cpu more often. furthermore rate monotonic scheduling assumes that the processing time of a periodic process is the same for each cpu burst. that is every time a process acquires the cpu the duration of its cpu burst is the same. let's consider an example. we have two processes pi and p?. the periods for p and pt are and respectively that is f and pz . the processing times are t for pi and tz for pi. the deadline for each process requires that it complete its cpu burst by the start of its next period. we must first ask ourselves whether it is possible to schedule these tasks so that each meets its deadlines. if we measure the cpu utilization of a process pi as the ratio of its burst to its period tj pi the cpu utilization of pi is . and that of p is . for a total cpu utilization of percent. therefore it seems we can schedule these tasks in such a way that both meet their deadlines and still leave the cpu with available cycles. first suppose we assign p a higher priority than p . the execution of pi and p? is shown in figure . . as we can see p starts execution first and completes at time . at this point pi starts it completes its cpu burst at time . however the first deadline for pi was at time so the scheduler has caused pi to miss its deadline. now suppose we use rate monotonic scheduling in which we assign p a higher priority than pi since the period of pi is shorter than that of p?. deadlines pi i figure . scheduling of tasks when p has a higher priority than p . chapter real time systems deadlines p p p p p p i i i i figure . rate monotonic scheduling. the execution of these processes is shown in figure . . pi starts first and completes its cpu burst at time thereby meeting its first deadline. p starts running at this point and runs until time . at this time it is preempted by pi although it still has milliseconds remaining in its cpu burst. pi completes its cpu burst at time at which point the scheduler resumes p . p completes its cpu burst at time also meeting its first deadline. the system is idle until time when pi is scheduled again. rate monotonic scheduling is considered optimal in the sense that if a set of processes cannot be scheduled by this algorithm it cannot be scheduled by any other algorithm that assigns static priorities. let's next examine a set of processes that cannot be scheduled using the rate monotonic algorithm. assume that process pi has a period of p and a cpu burst of fi . for p the corresponding values are p and t . rate monotonic scheduling would assign process pi a higher priority as it has the shorter period. the total cpu utilization of the two processes is . and it therefore seems logical that the two processes could be scheduled and still leave the cpu with percent available time. the gantt chart showing the scheduling of processes pi and p is depicted in figure . . initially pi runs until it completes its cpu burst at time . process p then begins running and runs until time when it is preempted by pi. at this point p still has milliseconds remaining in its cpu burst. process pi runs until time however p misses the deadline for completion of its cpu burst at time . despite being optimal then rate monotonic scheduling has a limitation cpu utilization is bounded and it is not always possible to fully maximize cpu resources. the worst case cpu utilization for scheduling n processes is . with one process in the system cpu utilization is percent but it falls to approximately percent as the number of processes approaches infinity. with two processes cpu utilization is bounded at about percent. combined cpu utilization for the two processes scheduled in figures . and . is percent and therefore the rate monotonic scheduling algorithm is guaranteed deadlines p p p p p i i i figure . missing deadlines with rate monotonic scheduling. . real time cpu scheduling to schedule them so that they can meet their deadlines. for the two processes scheduled in figure . combined cpu utilization is approximately percent therefore rate mono tonic scheduling cannot guarantee that they can be scheduled so that they meet their deadlines. . . earliest deadline first scheduling earliest deadline first edf scheduling dynamically assigns priorities according to deadline. the earlier the deadline the higher the priority the later the deadline the lower the priority. under the edf policy when a process becomes runnable it must announce its deadline requirements to the system. priorities may have to be adjusted to reflect the deadline of the newly runnable process. note how this differs from rate monotonic scheduling where priorities are fixed. to illustrate edf scheduling we again schedule the processes shown in figure . which failed to meet deadline requirements under rate monotonic scheduling. recall that pj has values of p and t and that p has values pi and t . the edf scheduling of these processes is shown in figure . . process pi has the earliest deadline so its initial priority is higher than that of process pi. process pi begins running at the end of the cpu burst for p . however whereas rate monotonic scheduling allows pi to preempt p at the beginning of its next period at time edf scheduling allows process p to continue running. p now has a higher priority than pi because its next deadline at time is earlier than that of p at time . thus both pi and p have met their first deadlines. process pi again begins running at time and completes its second cpu burst at time also meeting its second deadline at time . pi begins running at this point only to be preempted by pi at the start of its next period at time . p? is preempted because pi has an earlier deadline time than p time . at time pi completes its cpu burst and pj resumes execution finishing at time and meeting its deadline as well. the system is idle until time when p is scheduled to run once again. unlike the rate monotonic algorithm edf scheduling does not require that processes be periodic nor must a process require a constant amount of cpu time per burst. the only requirement is that a process announce its deadline to the scheduler when it becomes runnable. the appeal of edf scheduling is that it is theoretically optimal theoretically it can schedule processes so that each process can meet its deadline requirements and cpu utilization will be percent. in practice however it is impossible to achieve this level of cpu utilization due to the cost of context switching between processes and interrupt handling. deadlines p p p p p i figure . earliest deadline first scheduling. chapter real time systems . . proportional share scheduling proportional share schedulers operate by allocating t shares among all applications. an application can receive n shares of time thus ensuring that the application will have n t of the total processor time. as an example assume that there is a total of t shares to be divided among three processes a b and c. a is assigned shares b is assigned shares and c is assigned shares. this scheme ensures that a will have percent of total processor time b will have percent and c will have percent. proportional share schedulers must work in conjunction with an admission control policy to guarantee that an application receives its allocated shares of time. an admission control policy will only admit a client requesting a particular number of shares if there are sufficient shares available. in our current example we have allocated shares of the total of shares. if a new process d requested shares the admission controller would deny d entry into the system. . . pthread scheduling the pos x standard also provides extensions for real time computing posix.lb. in this section we cover some of the posix pthread api related to scheduling real time threads. pthreads defines two scheduling classes for real time threads sched.fifo schedjrr sched fifo schedules threads according to a first come first served policy using a fifo queue as outlined in section . . . however there is no time slicing among threads of equal priority. therefore the highest priority real time thread at the front of the fifo queue will be granted the cpu until it terminates or blocks. sched rr for round robin is similar to sched fifo except that it provides time slicing among threads of equal priority. pthreads provides an additional scheduling class sched.other but its implementation is undefined and system specific it may behave differently on different systems. the pthread api specifies the following two functions for getting and setting the scheduling policy pthread attr getsched policy pthread attr t attr int policy pthread attr getsched policy pthread attr t attr int policy the first parameter to both functions is a pointer to the set of attributes for the thread. the second parameter is either a pointer to an integer that is set to the current scheduling policy for pthread attr getsched policy or an integer value sched.fifo sched rr or schedx ther for the pthread attr getsched policy function. both functions return non zero values if an error occurs. . real time cpu scheduling include pthread.h include stdio.h define num.threads int main int argc char argv int i policy pthread t tid num.threads pthread attr t attr get the default attributes pthread attr init j attr get the current scheduling policy if pthread attr getschedpolicy attr kpolicy ! fprintf stderr unable to get policy. n else if policy sched other printf sched other rl else if policy sched rr printf schedj rvn else if policy sched fifo printf sched fifo n set the scheduling policy fifo rr or other if pthread.attr setschedpolicy attr sched other ! fprintf stderr unable to set policy. n create the threads for i i num threads i pthread create tid i iattr runner hull now join on each thread for i i numjthreads i pthread join tid i null each thread will begin control in this function void runner void param do some work ... pthread exit figure . pthread scheduling api. chapter real time systems in figure . we illustrate a pthread program using this apr this program first determines the current scheduling policy followed by setting the scheduling algorithm to sched.other. . vxworks .x in this section we describe vxworks a popular real time operating system providing hard real time support. vxworks commercially developed by wind river systems is widely used in automobiles consumer and industrial devices and networking equipment such as switches and routers. vxworks is also used to control the two rovers spirit and opportunity that began exploring the planet mars in . the organization of vxworks is shown in figure . . vxworks is centered around the wind microkernel. recall from our discussion in section . . that microkernels are designed so that the operating system kernel provides a bare minimum of features additional utilities such as networking file systems and graphics are provided in libraries outside of the kernel. this approach offers many benefits including minimizing the size of the kernel a desirable feature for an embedded system requiring a small footprint. the wind microkernel supports the following basic features processes. the wind microkernel provides support for individual processes and threads using the pthread api . however similar to linux vxworks does not distinguish between processes and threads instead referring to both as tasks. embedded real time application wind microkernel hardware level pentium power pc mips customized etc. figure . the organization of vxworks. . vxworks .x real time linux the linux operating system is being used increasingly in real time environments. we have already covered its soft real time scheduling features section . . whereby real time tasks are assigned the highest priority in the system . additional features in the . release of the kernel make linux increasingly suitable for embedded systems. these features include a fully preemptive kernel and a more efficient scheduling algorithm which runs in time regardless of the number of tasks active in the system. the . release also makes it easier to port linux to different hardware architectures by dividing the kernel into modular components. another strategy for integrating linux into real time environments involves combining the linux operating system with a small real time kernel thereby providing a system that acts as both a general purpose and a real time system. this is the approach taken by the rtlinux operating system. in rtlinux the standard linux kernel runs as a task in a small real time operating system. the real time kernel handles all interrupts directing each interrupt to a handler in the standard kernel or to an interrupt handler in the real time kernel. furthermore rtlinux prevents the standard linux kernel from ever disabling interrupts thus ensuring that it cannot add latency to the real time system. rtlinux also provides different scheduling policies including rate monotonic scheduling section . . and earliest deadline first scheduling section . . . scheduling. wind provides two separate scheduling models preemptive and nonpreemptive round robin scheduling with different priority levels. the scheduler also supports the posix api for real time threads covered in section . . . interrupts. the wind microkernel also manages interrupts. to support hard real time requirements interrupt and dispatch latency times are bounded. interprocess communication. the wind microkernel provides both shared memory and message passing as mechanisms for communication between separate tasks. wind also allows tasks to communicate using a technique known as pipes a mechanism that behaves in the same way as a fifo queue but allows tasks to communicate by writing to a special file the pipe. to protect data shared by separate tasks vxworks provides semaphores and mutex locks with a priority inheritance protocol to prevent priority inversion. outside the microkernel vxworks includes several component libraries that provide support for posrx java tcp ip networking and the like. all components are optional allowing the designer of an embedded system to customize the system according to its specific needs. for example if networking is not required the tcp ip library can be excluded from the image of the operating system. such a strategy allows the operating system designer to chapter real time systems include only required features thereby minimizing the size or footprint of the operating system. vxworks takes an interesting approach to memory management supporting two levels of virtual memory. the first level which is quite simple allows control of the cache on a per page basis. this policy enables an application to specify certain pages as non cacheable. when data are being shared by separate tasks running on a multiprocessor architecture it is possible that shared data can reside in separate caches local to individual processors. unless an architecture supports a cache coherency policy to ensure that the same data residing in two caches will not be different such shared data should not be cached and should instead reside only in main memory so that all tasks maintain a consistent view of the data. the second level of virtual memory requires the optional virtual memory component vxvmi figure . along with processor support for a memory management unit mmu . by loading this optional component on systems with an mmu vxworks allows a task to mark certain data areas as private. a data area marked as private may only be accessed by the task it belongs to. furthermore vxworks allows pages containing kernel code along with the interrupt vector to be declared as read only. this is useful as vxworks does not distinguish between user and kernel modes all applications run in kernel mode giving an application access to the entire address space of the system