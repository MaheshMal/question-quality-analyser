 recently used page will be used last or perhaps never again. instead sequential access can be optimized by techniques known as free behind and read ahead. free behind removes a page from the buffer as soon as the next page is requested. the previous pages are not likely to be used again and waste buffer space. with read ahead a requested page and several subsequent pages are read and cached. these pages are likely to be requested after the current page is processed. retrieving these data from the disk in one transfer and caching them saves a considerable amount of time. one might think a track cache on the controller eliminates the need for read ahead on a multiprogrammed system. however because of the high latency and overhead involved in making many small transfers from the track cache to main memory performing a read ahead remains beneficial. the page cache the file system and the disk drivers have some interesting interactions. when data are written to a disk file the pages are buffered in the cache and the disk driver sorts its output queue according to disk address. these two actions allow the disk driver to minimize disk head seeks and to write data at times optimized for disk rotation. unless synchronous writes are required a process writing to disk simply writes into the cache and the system asynchronously writes the data to disk when convenient. the user process sees very fast writes. when data are read from a disk file the block i o system does some read ahead however writes are much more nearly asynchronous than are reads. thus output to the disk through the file system is often faster than is input for large transfers counter to intuition. . recovery files and directories are kept both in main memory and on disk and care must taken to ensure that system failure does not result in loss of data or in data inconsistency. we deal with these issues in the following sections. . . consistency checking as discussed in section . some directory information is kept in main memory or cache to speed up access. the directory information in main memory is generally more up to date than is the corresponding information on the disk because cached directory information is not necessarily written to disk as soon as the update takes place. consider then the possible effect of a computer crash. cache and buffetcontents as well as i o operations in progress can be lost and with them any changes in the directories of opened files. such an event can leave the file system in an inconsistent state the actual state of some files is not as described in the directory structure. frequently a special program is run at reboot time to check for and correct disk inconsistencies. the consistency checker a systems program such as f sck in unix or chkdsk in ms dos compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds. the allocation and free space management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them. for instance if linked allocation is used and there is a link from any block to its next block chapter file system implementation then the entire file can be reconstructed from the data blocks and the directory structure can be recreated. in contrast the loss of a directory entry on an indexed allocation system can be disastrous because the data blocks have no knowledge of one another. for this reason unix caches directory entries for reads but any data write that results in space allocation or other metadata changes is done synchronously before the corresponding data blocks are written. of course problems can still occur if a synchronous write is interrupted by a crash. . . backup and restore magnetic disks sometimes fail and care must be taken to ensure that the data lost in such a failure are not lost forever. to this end system programs can be used to back up data from disk to another storage device such as a floppy disk magnetic tape optical disk or other hard disk. recovery from the loss of an individual file or of an entire disk may then be a matter of restoring the data from backup. to minimize the copying needed we can use information from each file's directory entry. for instance if the backup program knows when the last backup of a file was done and the file's last write date in the directory indicates that the file has not changed since that date then the file does not need to be copied again. a typical backup schedule may then be as follows day . copy to a backup medium all files from the disk. this is called a full backup. day . copy to another medium all files changed since day . this is an incremental backup. day . copy to another medium all files changed since day . day n. copy to another medium all files changed since day n . then go back to day . the new cycle can have its backup written over the previous set or onto a new set of backup media. in this manner we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups. of course the larger the value of n the greater the number of tapes or disks that must be read for a complete restore. an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day. the length of the cycle is a compromise between the amount of backup medium needed and the number of days back from which a restore can be done. to decrease the number of tapes that must be read to do a restore an option is to perform a full backup and then each day back up all files that have changed since the full backup. in this way a restore can be done via the most recent incremental backup and. the full backup with no other incremental backups needed. the trade off is that more files will be modified