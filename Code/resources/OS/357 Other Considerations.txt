 . other considerations the major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy which we discussed earlier in this chapter. there are many other considerations as welt and we discuss several of them here. . . prepaging an obvious property of pure demand paging is the large number of page faults that occur when a process is started. this situation results from trying to get the initial locality into memory. the same situation may arise at other times. for instance when a swapped out process is restarted all its pages are on the disk and each must be brought in by its own page fault. prepaging is an attempt to prevent this high level of initial paging. the strategy is to bring into memory at one time all the pages that will be needed. some operating systems notably solaris prepage the page frames for small files. in a system using the working set model for example we keep with each process a list of the pages in its working set. if we must suspend a process due to an i o wait or a lack of free frames we remember the working set for that process. when the process is to be resumed because i o has finished or enough free frames have become available we automatically bring back into memory its entire working set before restarting the process. prepaging may offer an advantage in some cases. the question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults. it may well be the case that many of the pages brought back into memory by prepaging will not be used. assume that s pages are prepaged and a fraction a of these s pages is actually used a . the question is whether the cost of the s a saved page faults is greater or less than the cost of prepaging s a unnecessary pages. if a is close to prepaging loses if a is close to prepaging wins. . . page size the designers of an operating system for an existing machine seldom have a choice concerning the page size. however when new machines are being designed a decision regarding the best page size must be made. as you might expect there is no single best page size. rather there is a set of factors that support various sizes. page sizes are invariably powers of generally ranging from to bytes. how do we select a page size? one concern is the size of the page table. for a given virtual memory space decreasing the page size increases the number of pages and hence the size of the page table. for a virtual memory of mb for example there would be pages of bytes but only pages of bytes. because each active process must have its own copy of the page table a large page size is desirable. memory is better utilized with smaller pages however. if a process is allocated memory starting at location and continuing until it has as much as it needs it probably will not end exactly on a page boundary. thus a part of the final page must be allocated because pages are the units of allocation. but will be unused creating internal fragmentation . assuming independence chapter virtual memory of process size and page size we can expect that on the average halfof the final page of each process will be wasted. this loss is only bytes for a page of bytes but is bytes for a page of bytes. to minimize internal fragmentation then we need a small page size. another problem is the time required to read or write a page. i o time is composed of seek latency and transfer times. transfer time is proportional to the amount transferred that is the page size a fact that would seem to argue for a small page size. however as we shall see in section . . latency and seek time normally dwarf transfer time. at a transfer rate of mb per second it takes only . milliseconds to transfer bytes. latency time though is perhaps milliseconds and seek time milliseconds. of the total i o time . milliseconds therefore only percent is attributable to the actual transfer. doubling the page size increases i o time to only . milliseconds. it takes . milliseconds to read a single page of bytes but . milliseconds to read the same amount as two pages of bytes each. thus a desire to minimize i o time argues for a larger page size. with a smaller page size though total i o should be reduced since locality will be improved. a smaller page size allows each page to match program locality more accurately. for example consider a process kb in size of which only half kb is actually used in an execution. if we have only one large page we must bring in the entire page a total of kb transferred and allocated. if instead we had pages of only byte then we could bring in only the kb that are actually used resulting in only kb transferred and allocated. with a smaller page size we have better resolution allowing us to isolate only the memory that is actually needed. with a larger page size we must allocate and transfer not only what is needed but also anything else that happens to be in the page whether it is needed or not. thus a smaller page size should result in less i o and less total allocated memory. but did you notice that with a page size of byte we would have a page fault for each byte? a process of kb that used only half of that memory would generate only one page fault with a page size of kb but page faults with a page size of byte. each page fault generates the large amount of overhead needed for processing the interrupt saving registers replacing a page queueing for the paging device and updating tables. to minimize the number of page faults we need to have a large page size. other factors must be considered as well such as the relationship between page size and sector size on the paging device . the problem has no best answer. as we have seen some factors internal fragmentation locality argue for a small page size whereas others table size i o time argue for a large page size. however the historical trend is toward larger page sizes. indeed the first edition of operating systems concepts used bytes as the upper bound on page sizes and this value was the most common page size in . however modern systems may now use much larger page sizes as we will see in the following section. . . tlb r e a c h in chapter we introduced the hit ratio of the tlb. recall that the hit ratio for the tlb refers to the percentage of virtual address translations that are resolved in the tlb rather than the page table. clearly the hit ratio is related . other considerations to the number of entries in the tlb and the way to increase the hit ratio is by increasing the number of entries in the tlb. this however does not come cheaply as the associative memory used to construct the tlb is both expensive and power hungry. related to the hit ratio is a similar metric the tlb reach. the tlb reach refers to the amount of memory accessible from the tlb and is simply the number of entries multiplied by the page size. ideally the working set for a process is stored in the tlb. if not the process will spend a considerable amount of time resolving memory references in the page table rather than the tlb. if we double the number of entries in the tlb we double the tlb reach. however for some memory intensive applications this may still prove insufficient for storing the working set. another approach for increasing the tlb reach is to either increase the size of the page or provide multiple page sizes. if we increase the page size say from kb to kb we quadruple the tlb reach. however this may lead to an increase in fragmentation for some applications that do not require such a large page size as kb. alternatively an operating system may provide several different page sizes. for example the ultrasparc supports page sizes of kb kb kb and mb. of these available pages sizes solaris uses both kb and mb page sizes. and with a entry tlb the tlb reach for solaris ranges from kb with kb pages to mb with mb pages. for the majority of applications the kb page size is sufficient although solaris maps the first mb of kernel code and data with two mb pages. solaris also allows applications such as databases to take advantage of the large mb page size. providing support for multiple pages requires the operating system not hardware to manage the tlb. for example one of the fields in a tlb entry must indicate the size of the page frame corresponding to the tlb entry. managing the tlb in software and not hardware comes at a cost in performance. however the increased hit ratio and tlb reach offset the performance costs. indeed recent trends indicate a move toward software managed tlbs and operating system support for multiple page sizes. the ultrasparc mips and alpha architectures employ software managed tlbs. the powerpc and pentium manage the tlb in hardware. . . inverted page tables section . . introduced the concept of the inverted page table. the purpose of this form of page management is to reduce the amount of physical memory needed to track virtual to physical address translations. we accomplish this savings by creating a table that has one entry per page of physical memory indexed by the pair process id page number . because they keep information about which virtual memory page is stored in each physical frame inverted page tables reduce the amount of physical memory needed to store this information. however the inverted page table no longer contains complete information about the logical address space of a process and that information is required if a referenced page is not currently in memory. demand paging requires this information to process page faults. for the information to be available an external page table one per process chapter virtual memory must be kept. each such table looks like the traditional per process page table and contains information on where each virtual page is located. but do external page tables negate the utility of inverted page tables? since these tables are referenced only when a page fault occurs they do not need to be available quickly. instead they are themselves paged in and out of memory as necessary. unfortunately a page fault may now cause the virtual memory manager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store. this special case requires careful handling in the kernel and a delay in the page lookup processing. . . program structure demand paging is designed to be transparent to the user program. in many cases the user is completely unaware of the paged nature of memory. in other cases however system performance can be improved if the user or compiler has an awareness of the underlying demand paging. let's look at a contrived but informative example. assume that pages are words in size. consider a c program whose function is to initialize to each element of a by array. the following code is typical int i j int data for j j j for i i i data i j notice that the array is stored row major that is the array is stored data data data data l data l data . for pages of words each row takes one page. thus the preceding code zeros one word in each page then another word in each page and so on. if the operating system allocates fewer than frames to the entire program then its execution will result in x page faults. in. contrast changing the code to int i j int data for i i i for j j j data i j zeros all the words on one page before starting the next page reducing the number of page faults to . careful selection of data structures and programming structures can increase locality and hence lower the page fault rate and the number of pages in the working set. for example a stack has good locality since access is always made to the top. a hash table in contrast is designed to scatter references producing bad locality. of course locality of reference is just one measure of the efficiency of the use of a data structure. other heavily weighted factors . other considerations include search speed total number of memory references and total number of pages touched. at a later stage the compiler and loader can have a significant effect on paging. separating code and data and generating reentrant code means that code pages can he read only and hence will never he modified. clean pages do not have to be paged out to be replaced. the loader can avoid placing routines across page boundaries keeping each routine completely in one page. routines that call each other many times can be packed into the same page. this packaging is a variant of the bin packing problem of operations research try to pack the variable sized load segments into the fixed sized pages so that interpage references are minimized. such an approach is particularly useful for large page sizes. the choice of programming language can affect paging as well. for example c and c use pointers frequently and pointers tend to randomize access to memory thereby potentially diminishing a process's locality. some studies have shown that object oriented programs also tend to have a poor locality of reference. . . i o interlock when demand paging is used we sometimes need to allow some of the pages to be locked in memory. one such situation occurs when i o is done to or from user virtual memory. i o is often implemented by a separate i o processor. for example a controller for a usb storage device is generally given the number of bytes to transfer and a memory address for the buffer figure . . when the transfer is complete the cpu is interrupted. buffer figure . the reason why frames used for i o must be in memory. chapter virtual memory we must be sure the following sequence of events does not occur a process issues an i o request and is put in a queue for that i o device. meanwhile the cpu is given to other processes. these processes cause page faults and one of them using a global replacement algorithm replaces the page containing the memory buffer for the waiting process. the pages are paged out. some time later when the i o request advances to the head of the device queue the i o occurs to the specified address. however this frame is now being used for a different page belonging to another process. there are two common solutions to this problem. one solution is never to execute i o to user memory. instead data are always copied between system memory and user memory. i o takes place only between system memory and the i o device. to write a block on tape we first copy the block to system memory and then write it to tape. this extra copying may result in unacceptably high overhead. another solution is to allow pages to be locked into memory. here a lock bit is associated with every frame. if the frame is locked it cannot be selected for replacement. under this approach to write a block on tape we lock into memory the pages containing the block. the system can then continue as usual. locked pages cannot be replaced. when the i o is complete the pages are unlocked. lock bits are used in various situations. frequently some or all of the operating system kernel is locked into memory as many operating systems cannot tolerate a page fault caused by the kernel. another use for a lock bit involves normal page replacement. consider the following sequence of events a low priority process faults. selecting a replacement frame the paging system reads the necessary page into memory. ready to continue the low priority process enters the ready queue and waits for the cpu. since it is a low priority process it may not be selected by the cpu scheduler for a time. while the low priority process waits a high priority process faults. looking for a replacement the paging system sees a page that is in memory but has not been referenced or modified tt is the page that the low priority process just brought in. this page looks like a perfect replacement it is clean and will not need to be written out and it apparently has not been used for a long time. whether the high priority process should be able to replace the low priority process is a policy decision. after all we are simply delaying the low priority process for the benefit of the high priority process. however we are wasting the effort spent to bring in the page for the low priority process. if we decide to prevent replacement of a newly brought in page until it can be used at least once then we can use the lock bit to implement this mechanism. when a page is selected for replacement its lock bit is turned on it remains on until the faulting process is again dispatched. using a lock bit can be dangerous the lock bit may get turned on but never turned off. should this situation occur because of a bug in the operating system for example the locked frame becomes unusable. on a single user system the overuse of locking would hurt only the user doing the locking. multiuser systems must be less trusting of users. for instance solaris allows locking hints but it is free to disregard these hints if the free frame pool becomes too small or if an individual process requests that too many pages be locked in memory