 we move next to the issue of concurrency control. in this section we show how certain of the concurrency control schemes discussed in chapter can be modified for use in a distributed environment. the transaction manager of a distributed database system manages the execution of those transactions or subtransactions that access data stored in a local site. each such transaction may be either a local transaction that is a transaction that executes only at that site or part of a global transaction that is a transaction that executes at several sites . each transaction manager is responsible for maintaining a log for recovery purposes and for participating in an appropriate concurrency control scheme to coordinate the conciirrent execution of the transactions executing at that site. as we shall see the concurrency schemes described in chapter need to be modified to accommodate the distribution of transactions. . . locking protocols the two phase locking protocols described in chapter can be used in a distributed environment. the only change needed is in the way the lock manager is implemented. here we present several possible schemes. the first deals with the case where no data replication is allowed. the others apply to the more general case where data can be replicated in several sites. as in chapter we assume the existence of the shared and exclusive lock modes. . . . nonreplicated scheme if no data are replicated in the system then the locking schemes described in section . can be applied as follows each site maintains a local lock manager whose function is to administer the lock and unlock requests for those data items stored in that site. when a transaction wishes to lock data item q at site si it simply sends a message to the lock manager at site s requesting a lock in a particular lock mode . if data item q is locked in an incompatible mode then the request is delayed until that request can be granted. once it has been determined that the lock request can be granted the lock manager sends a message back to the initiator indicating that the lock request has been granted. . concurrency control this scheme has the advantage of simple implementation. it requires two message transfers for handling lock requests and one message transfer for handling unlock requests. however deadlock handling is more complex. since the lock and unlock requests are no longer made at a single site the various deadlock handling algorithms discussed in chapter must be modified these modifications are discussed in section . . . . . single coordinator approach several concurrency control schemes can be used in systems that allow data replication. under the single coordinator approach the system maintains a single lock manager that resides in a single chosen site say s all lock and unlock requests are made at site s . when a transaction needs to lock a data item it sends a lock request to s . the lock manager determines whether the lock can be granted immediately. if so it sends a message to that effect to the site at which the lock request was initiated. otherwise the request is delayed until it can be granted and at that time a message is sent to the site at which the lock request was initiated. the transaction can read the data item from any one of the sites at which a replica of the data item resides. in the case of a write operation all the sites where a replica of the data item resides must be involved in the writing. the scheme has the following advantages simple implementation. this scheme requires two messages for handling lock requests and one message for handling unlock requests. simple deadlock handling. since all lock and unlock requests are made at one site the deadlock handling algorithms discussed in chapter can be applied directly to this environment. the disadvantages of the scheme include the following bottleneck. the site s becomes a bottleneck since all requests must be processed there. vulnerability. if the site s fails the concurrency controller is lost. either processing must stop or a recovery scheme must be used. a compromise between these advantages and disadvantages can be achieved through a multiple coordinator approach in which the lockmanager function is distributed over several sites. each lock manager administers the lock and unlock requests for a subset of the data items and the lock managers reside in different sites. this distribution reduces the degree to which the coordinator is a bottleneck but it complicates deadlock handling since the lock and unlock requests are not made at a single site. . . . majority protocol the majority protocol is a modification of the nonreplicated data scheme presented earlier. the system maintains a lock manager at each site. each manager controls the locks for all the data or replicas of data stored at that site. when a transaction wishes to lock a data item q that is replicated in n different chapter distributed coordination sites it must send a lock request to more than one half of the n sites in which q is stored. each lock manager determines whether the lock can be granted immediately as far as it is concerned . as before the response is delayed until the request can be granted. the transaction does not operate on q until it has successfully obtained a lock on a majority of the replicas of chl . this scheme deals with replicated data in a decentralized manner thus avoiding the drawbacks of central control. however it suffers from its own disadvantages implementation. the majority protocol is more complicated to implement than the previous schemes. it requires n messages for handling lock requests and n messages for handling unlock requests. deadlock handling. since the lock and unlock requests are not made at one site the deadlock handling algorithms must be modified section . . in addition a deadlock can occur even if only one data item is being locked. to illustrate consider a system with four sites and full replication. suppose that transactions t and t wish to lock data item q in exclusive mode. transaction t may succeed in locking q at sites s and s while transaction t may succeed in locking q at sites sj and s . each then must wait to acquire the third lock and hence a deadlock has occurred. . . . biased protocol the biased protocol is similar to the majority protocol. the difference is that requests for shared locks are given more favorable treatment than are requests for exclusive locks. the system maintains a lock manager at each site. each manager manages the locks for all the data items stored at that site. shared and exclusive locks are handled differently. shared locks. when a transaction needs to lock data item q it simply requests a lock on q from the lock manager at one site containing a replica oichls . exclusive locks. when a transaction needs to lock data item q it requests a lock on q from the lock manager at each site containing a replica of chl . as before the response to the request is delayed until the request can be granted. the scheme has the advantage of imposing less overhead on read operations than does the majority protocol. this advantage is especially significant in common cases in which the frequency of reads is much greater than the frequency of writes. however the additional overhead on writes is a disadvantage. furthermore the biased protocol shares the majority protocol's disadvantage of complexity in handling deadlock. . . . primary copy yet another alternative is to choose one of the replicas as the primary copythus for each data item q the primary copy of q must reside in precisely one site which we call the primary site ofq. when a transaction needs to lock a data . concurrency control item q it requests a lock at the primary site of chl . as before the response to the request is delayed until the request can be granted. this scheme enables us to handle concurrency control for replicated data in much the same way as for unreplicated data. implementation of the method is simple. however if the primary site of q fails q is inaccessible even though other sites containing a replica may be accessible. . . timestamping the principal idea behind the timestamping scheme discussed in section . is that each transaction is given a unique timestamp which is used to decide the serialization order. our first task then in generalizing the centralized scheme to a distributed scheme is to develop a method for generating unique timestamps. our previous protocols can then be applied directly to the nonreplicated environment. . . . generation of unique timestamps two primary methods are used to generate unique timestamps one is centralized and one is distributed. in the centralized scheme a single site is chosen for distributing the timestamps. the site can use a logical counter or its own local clock for this purpose. in the distributed scheme each site generates a local unique timestamp using either a logical counter or the local clock. the global unique timestamp is obtained by concatenation of the local unique timestamp with the site identifier which must be unique figure . . the order of concatenation is important! we use the site identifier in the least significant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site. compare this technique for generating unique timestamps with the one we presented in section . . for generating unique names. we may still have a problem if one site generates local timestamps at a faster rate than do other sites. in such a case the fast site's logical counter will be larger than those of other sites. therefore all timestamps generated by the fast site will be larger than those generated by other sites. a mechanism is needed to ensure that local timestamps are generated fairly across the system. to accomplish the fair generation of timestamps we define within each site s a logical clock lc which generates the local timestamp see section . . . to ensure that the various logical clocks are synchronized we require that a site s advance its logical clock whenever a transaction t with timestamp x y local unique timestamp site identifier global unique identifier figure . generation of unique timestamps. chapter distributed coordination visits that site and x is greater than the current value of ld. in this case ?site s advances its logical clock to the value x . if the system clock is used to generate timestamps then timestamps are assigned fairly provided that no site has a system clock that runs fast or slow. since clocks may not be perfectly accurate a technique similar to that used for logical clocks must be used to ensure that no clock gets far ahead or far behind another clock. . . . timestamp ordering scheme the basic timestamp scheme introduced in section . can be extended in a straightforward manner to a distributed system. as in the centralized case cascading rollbacks may result if no mechanism is used to prevent a transaction from reading a data item value that is not yet committed. to eliminate cascading rollbacks we can combine the basic timestamp scheme of section . with the pc protocol of section . to obtain a protocol that ensures serializability with no cascading rollbacks. we leave the development of such an algorithm to you. the basic timestamp scheme just described suffers from the undesirable property that conflicts between transactions are resolved through rollbacks rather than through waits. to alleviate this problem we can buffer the various read and write operations that is delay them until a time when we are assured that these operations can take place without causing aborts. a read x operation by t must be delayed if there exists a transaction ty that will perform a write e operation but has not yet done so and ts ty ts tj . similarly a wr ite x operation by t must be delayed if there exists a transaction t that will perform either a read x or a write x operation and ts t ts ti . various methods are available for ensuring this property. one such method called the conservative timestamp ordering scheme requires each site to maintain a read queue and a write queue consisting of all the read and write requests that are to be executed at the site and that must be delayed to preserve the above property. we shall not present the scheme here. again we leave the development of the algorithm to you