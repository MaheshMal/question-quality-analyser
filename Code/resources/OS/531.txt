Linux I/O

      In general terms, the Linux I/O kernel facility is very similar to that of other UNIX
      implementation, such as SVR4. The Linux kernel associates a special file with each
      I/O device driver. Block, character, and network devices are recognized. In this sec-
      tion, we look at several features of the Linux I/O facility.

     Disk Scheduling
     The default disk scheduler in Linux 2.4 is known as the Linux Elevator, which is
     a variation on the LOOK algorithm discussed in Section 11.5. For Linux 2.6, the
     Elevator algorithm has been augmented by two additional algorithms: the deadline
     I/O scheduler and the anticipatory I/O scheduler [LOVE04]. We examine each of
     these in turn.
     THE  ELEVATOR   SCHEDULER  The elevator scheduler maintains a single queue
     for disk read and write requests and performs both sorting and merging functions
     on the queue. In general terms, the elevator scheduler keeps the list of requests
     sorted by block number. Thus, as the disk requests are handled, the drive moves in
     a single direction, satisfying each request as it is encountered. This general strategy
     is refined in the following manner. When a new request is added to the queue, four
     operations are considered in order:
     1.   If the request is to the same on-disk sector or an immediately adjacent sector
          to a pending request in the queue, then the existing request and the new re-
          quest are merged into one request.
     2.   If a request in the queue is sufficiently old, the new request is inserted at the
          tail of the queue.
     3.   If there is a suitable location, the new request is inserted in sorted order.
     4.   If there is no suitable location, the new request is placed at the tail of the
          queue.
     DEADLINE SCHEDULER       Operation 2 in the preceding list is intended to prevent
     starvation of a request, but is not very effective [LOVE04]. It does not attempt to
     service requests in a given time frame but merely stops insertion-sorting requests
     after a suitable delay. Two problems manifest themselves with the elevator scheme.
     The first problem is that a distant block request can be delayed for a substantial
     time because the queue is dynamically updated. For example, consider the following
     stream of requests for disk blocks: 20, 30, 700, 25. The elevator scheduler reorders
     these so that the requests are placed in the queue as 20, 25, 30, 700, with 20 being the
     head of the queue. If a continuous sequence of low-numbered block requests arrive,
     then the request for 700 continues to be delayed.
          An even more serious problem concerns the distinction between read and
     write requests. Typically, a write request is issued asynchronously. That is, once
     a process issues the write request, it need not wait for the request to actually be
     satisfied. When an application issues a write, the kernel copies the data into an
     appropriate buffer, to be written out as time permits. Once the data are captured
     in the kernel's buffer, the application can proceed. However, for many read oper-
     ations, the process must wait until the requested data are delivered to the appli-
     cation before proceeding. Thus, a stream of write requests (e.g., to place a large
     file on the disk) can block a read request for a considerable time and thus block a
     process.
          To   overcome  these  problems,     the  deadline  I/O  scheduler  makes             use
     of three queues (Figure 11.14). Each incoming request is placed in the sorted

Sorted (elevator) queue
Read FIFO queue
Write FIFO queue
Figure 11.14       The Linux Deadline I/O Scheduler
elevator queue, as before. In addition, the same request is placed at the tail of a
read FIFO queue for a read request or a write FIFO queue for a write request.
Thus, the read and write queues maintain a list of requests in the sequence in
which the requests were made. Associated with each request is an expiration
time, with a default value of 0.5 seconds for a read request and 5 seconds for a
write request. Ordinarily, the scheduler dispatches from the sorted queue. When
a request is satisfied, it is removed from the head of the sorted queue and also
from the appropriate FIFO queue. However, when the item at the head of one of
the FIFO queues becomes older than its expiration time, then the scheduler next
dispatches from that FIFO queue, taking the expired request, plus the next few
requests from the queue. As each request is dispatched, it is also removed from
the sorted queue.
The deadline I/O scheduler scheme overcomes the starvation problem and
also the read versus write problem.
ANTICIPATORY I/O SCHEDULER           The original elevator scheduler and the deadline
scheduler both are designed to dispatch a new request as soon as the existing request
is satisfied, thus keeping the disk as busy as possible. This same policy applies to all
of the scheduling algorithms discussed in Section 11.5. However, such a policy can
be counterproductive if there are numerous synchronous read requests. Typically,
an application will wait until a read request is satisfied and the data available before
issuing the next request. The small delay between receiving the data for the last
read and issuing the next read enables the scheduler to turn elsewhere for a pending
request and dispatch that request.
Because of the principle of locality, it is likely that successive reads from the
same process will be to disk blocks that are near one another. If the scheduler were
to delay a short period of time after satisfying a read request, to see if a new nearby
read request is made, the overall performance of the system could be enhanced.
This is the philosophy behind the anticipatory scheduler, proposed in [IYER01],
and implemented in Linux 2.6.

          In Linux, the anticipatory scheduler is superimposed on the deadline sched-
     uler. When a read request is dispatched, the anticipatory scheduler causes the
     scheduling system to delay for up to 6 ms, depending on the configuration. During
     this small delay, there is a good chance that the application that issued the last
     read request will issue another read request to the same region of the disk. If
     so, that request will be serviced immediately. If no such read request occurs, the
     scheduler resumes using the deadline scheduling algorithm.
          [LOVE04] reports on two tests of the Linux scheduling algorithms. The first
     test involved the reading of a 200-MB file while doing a long streaming write in
     the background. The second test involved doing a read of a large file in the back-
     ground while reading every file in the kernel source tree. The results are listed in
     the following table:
          I/O Scheduler and Kernel           Test 1                       Test 2
          Linux elevator on 2.4              45 seconds            30 minutes, 28 seconds
          Deadline I/O scheduler on 2.6      40 seconds                3 minutes, 30 seconds
          Anticipatory I/O scheduler on 2.6  4.6 seconds               15 seconds
          As can be seen, the performance improvement depends on the nature of
     the workload. But in both cases, the anticipatory scheduler provides a dramatic
     improvement.
     Linux Page Cache
     In Linux 2.2 and earlier releases, the kernel maintained a page cache for reads and
     writes from regular file system files and for virtual memory pages, and a separate
     buffer cache for block I/O. For Linux 2.4 and later, there is a single unified page
     cache that is involved in all traffic between disk and main memory.
          The page cache confers two benefits. First, when it is time to write back dirty
     pages to disk, a collection of them can be ordered properly and written out effi-
     ciently. Second, because of the principle of temporal locality, pages in the page
     cache are likely to be referenced again before they are flushed from the cache, thus
     saving a disk I/O operation.
          Dirty pages are written back to disk in two situations:
       ·  When free memory falls below a specified threshold, the kernel reduces the
          size of the page cache to release memory to be added to the free memory pool.
       ·  When dirty pages grow older than a specified threshold, a number of dirty
          pages are written back to disk.
