 swapping was first presented in section . where wre discussed moving entire processes between disk and main memory. swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes which are usually selected because they are the least active are moved from memory to swap space to free available memory. in practice very few modern operating systems implement swapping in this fashion. rather systems now combine swapping with virtual memory techniques chapter and swap pages not necessarily entire processes. in fact some systems now use the terms swapping and paging interchangeably reflecting the merging of these two concepts. swap space management is another low level task of the operating system. virtual memory uses disk space as an extension of main memory. since disk access is much slower than memory access using swap space significantly decreases system performance. the main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system. in this section we discuss how swap space is used where swap space is located on disk and how swap space is managed. . . swap space use swap space is used in various ways by different operating systems depending on the memory management algorithms in use. for instance systems that implement swapping may use swap space to hold an entire process image including the code and data segments. paging systems may simply store pages that have been pushed out of main memory. the amount of swap space needed on a system can therefore vary depending on the amount of physical memory the amount of virtual memory it is backing and the way in which the virtual memory is used. it can range from a few megabytes of disk space to gigabytes. note that it may be safer to overestimate than to underestimate the amount of swap space required because if a system runs out of swap space it may be forced to abort processes or may crash entirely. overestimation wastes disk space that could otherwise be used for files but it does no other harm. some systems recommend the amount to be set aside for swap space. solaris for example sviggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory. historically linux suggests setting swap space to double the amount of physical memory although most linux systems now use considerably less swap space. in fact there is currently much debate in the linux community about whether to set aside swap space at all! some operating systems including linux allow the use of multiple swap spaces. these swap spaces are usually put on separate disks so the load placed on the i o system by paging and swapping can be spread over the system's i o devices. . . swap space location a swap space can reside in one of two places it can be carved out of the normal file system or it can be in a separate disk partition. if the swap space is simply a large file within the file system normal file system routines . swap space management can be used to create it name it and allocate its space. this approach though easy to implement is inefficient. navigating the directory structure and the disk allocation data structures takes time and potentially extra disk accesses. external fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image. we can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap file but the cost of traversing the file system data structures still remains. alternatively swap space can be created in a separate raw partition as no file system or directory structure is placed in this space. rather a separate swap space storage manager is used to allocate and deallocate the blocks from the raw partition. this manager uses algorithms optimized for speed rather than for storage efficiency because swap space is accessed much more frequently than file systems when it is used . internal fragmentation may increase but this trade off is acceptable because the life of data in the swap space generally is much shorter than that of files in the file system. swap space is reinitialized at boot time so any fragmentation is short lived. this approach creates a fixed amount of swap space during disk partitioning. adding more swap space requires repartitioning the disk which involves moving the other file system partitions or destroying them and restoring them from backup or adding another swap space elsewhere. some operating systems are flexible and can swap both in raw partitions and in file system space. linux is an example the policy and. implementation are separate allowing the machine's administrator to decide which type of swapping to use. the trade off is between the convenience of allocation and management in the file system and the performance of swapping in raw partitions. . . swap space management an example we can illustrate how swap space is used by following the evolution of swapping and paging in various unix systems. the traditional unix kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory. unix later evolved to a combination of swapping and paging as paging hardware became available. in solaris sunos the designers changed standard unix methods to improve efficiency and reflect technological changes. when a process executes text segment pages containing code are brought in from the file system accessed in main memory and thrown away if selected for pageout. it is more efficient to reread a page from the file system than to write it to swap space and then reread it from there. swap space is only used as a backing store for pages of anonymous memory which includes memory allocated for the stack heap and uninitialized data of a process. more changes were made in later versions of solaris. the biggest change is that solaris now allocates swap space only when a page is forced out of physical memory rather than when the virtual memory page is first created. this scheme gives better performance on modern computers which have more physical memory than older systems and tend to page less. chapter mass storage structure swap area page slot swap partition ! ill ! m hi !m or swap file j j j . ' ' . ' . . m'. . ' ' iilil. swap map f figure . the data structures for swapping on linux systems. linux is similar to solaris in that swap space is only used for anonymous memory or for regions of memory shared by several processes. linux allows one or more swap areas to be established. a swap area may be in either a swap file on a regular file system or a raw swap partition. each swap area consists of a series of kb page slots which are used to hold swapped pages. associated with each swap area is a swap map an array of integer counters each corresponding to a page slot in the swap area. tf the value of a counter is the corresponding page slot is available. values greater than indicate that the page slot is occupied by a swapped page. the value of the counter indicates the number of mappings to the swapped page for example a value of indicates that the swapped page is mapped to three different processes which can occur if the swapped page is storing a region of memory shared by three processes . the data structures for swapping on linux systems are shown in figure . . . raid structure disk drives have continued to get smaller and cheaper so it is now economically feasible to attach .many disks to a computer system. having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written if the disks are operated in parallel. furthermore this setup offers the potential for improving the reliability of data storage because redundant information can be stored on multiple disks. thus failure of one disk does not lead to loss of data. a variety of disk organization techniques collectively called redundant arrays of inexpensive disks raids are commonly used to address the performance and reliability issues. in the past raids composed of small cheap disks were viewed as a cost effective alternative to large expensive disks today raids are used for their higher reliability and higher data transfer rate rather than for economic reasons. hence the i in raid now stands for independent instead of inexpensive. . . improvement of reliability via redundancy let us first consider the reliability of raids. the chance that some disk out of a set of n disks will fail is much higher than the chance that a specific single disk will fail. suppose that the mean time to failure of a single disk is hours. then the mean time to failure of some disk in an array of disks . raid structure ptpgg j f idi? xaitt fe a s slejii can. fes e ilisftsmrect mtycfeti t itg.bases .'lit ih'ft ca e tfit epgiattftg 'scte usuallyl s i r t d l s k t . f t h a fh c f l e t ! f s ih ecgf vfa's h a r h i i 'tfea will be hours or . days which is not long at all! if we store only one copy of the data then each disk failure will result in loss of a significant amount of data and such a high rate of data loss is unacceptable. the solution to the problem of reliability is to introduce redundancy we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information. thus even if a disk fails data are not lost. the simplest but most expensive approach to introducing redundancy is to duplicate every disk. this technique is called mirroring. a logical disk then consists of two physical disks and every write is carried out on both disks. if one of the disks fails the data can be read from the other. data will be lost only if the second disk fails before the first failed disk is replaced. the mean time to failure where failure is the loss of data of a mirrored volume made up of two disks mirrored depends on two factors. one is the mean time to failure of the individual disks. the other is the mean time to repair which is the time it takes on average to replace a failed disk and to restore the data on it. suppose that the failures of the two disks are independent that is the failure of one disk is not connected to the failure of the other. then if the mean time to failure of a single disk is hours and the mean time to repair is hours the mean time to data loss of a mirrored disk system is hours or years! you should be aware that the assumption of independence of disk failures is not valid. power failures and natural disasters such as earthquakes fires and floods may result in damage to both disks at the same time. also manufacturing defects in a batch of disks can cause correlated failures. as disks age the probability of failure grows increasing the chance that a second disk will fail while the first is being repaired. in spite of all these considerations however mirrored disk systems offer much higher reliability than do singledisk systems. power failures are a particular source of concern since they occur far more frequently than do natural disasters. even with mirroring of disks if writes are in progress to the same block in both disks and power fails before both blocks are fully written the two blocks can be in an inconsistent state. one solution to this problem is to write one copy first then the next so that one chapter mass storage structure of the two copies is always consistent. another is to add a nonvolatile' ram nvram cache to the raid array. this write back cache is protected from data loss during power failures so the write can be considered complete at that point assuming the nvram has some kind of error protection and correction. such as ecc or mirroring. . . improvement in performance via parallelism now let's consider how parallel access to multiple disks improves performance. with disk mirroring the rate at which read requests can be handled is doubled since read requests can be sent to either disk as long as both disks in a pair are functional as is almost always the case . the transfer rate of each read is the same as in a single disk system but the number of reads per unit time has doubled. with multiple disks we can improve the transfer rate as well or instead by striping data across the disks. in its simplest form data striping consists of splitting the bits of each byte across multiple disks such striping is called bit level striping. for example if we have an array of eight disks we write bit ' of each byte to disk . the array of eight disks can be treated as a single disk with sectors that are eight times the normal size and more important that have eight times the access rate. in such an organization every disk participates in every access read or write so the number of accesses that can be processed per second is about the same as on a single disk but each access can read eight times as many data in the same time as on a single disk. bit level striping can be generalized to include a number of disks that either is a multiple of or divides . for example if we use an array of four disks bits and i of each byte go to disk . further striping need not be at the bit level. for example in block level striping blocks of a file are striped across multiple disks with n disks block of a file goes to disk mod n . other levels of striping such as bytes of a sector or sectors of a block also are possible. block level striping is the most common. parallelism in a disk system as achieved through striping has two main goals . increase the throughput of multiple small accesses that is page accesses by load balancing. . reduce the response time of large accesses. . . raid levels mirroring provides high reliability but it is expensive. striping provides high data transfer rates but it does not improve reliability. numerous schemes to provide redundancy at lower cost by using the idea of disk striping combined with parity bits which we describe next have been proposed. these schemes have different cost performance trade offs and are classified according to levels called raid levels. we describe the various levels here figure . shows them pictorially in the figure p indicates error correcting bits and c indicates a second copy of the data . in all cases depicted in the figure four disks' worth of data are stored and the extra disks are used to store redundant information for failure recovery. . raid structure a raid non redundant striping. b raid mirrored disks. c raid memory style error correcting codes. d raid bit interleaved parity. e raid block interleaved parity. f raid block interleaved distributed parity. g raid p q redundancy. figure . raid levels. raid level . raid level refers to disk arrays with striping at the level of blocks but without any redundancy such as mirroring or parity bits as shown in figure . a . raid level . raid level refers to disk mirroring. figure . b shows a mirrored organization. raid level . raid level is also known as memory style error correctingcode ecc organization. memory systems have long detected certain errors by using parity bits. each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to is even parity or odd parity . if one of the bits in the byte is damaged either a becomes a or a becomes a the parity of the byte changes and thus will not match the stored parity. similarly if the stored parity bit is damaged it will not match the computed parity. thus all single bit errors are detected by the memory system. error correcting chapter mass storage structure schemes store two or more extra bits and can reconstruct the data if a single bit is damaged. the idea of ecc can be used directly in disk arrays via striping of bytes across disks. for example the first bit of each byte can be stored in disk the second bit in disk and so on until the eighth bit is stored in disk the error correction bits are stored in further disks. this scheme is shown pictorially in figure . c where the disks labeled p store the error correction bits. if one of the disks fails the remaining bits of the byte and the associated error correction bits can be read from other disks and used to reconstruct the damaged data. note that raid level requires only three disks' overhead for four disks of data unlike raid level which requires four disks' overhead. raid level . raid level or bit interleaved parity organization improves on level by taking into account the fact that unlike memory systems disk controllers can detect whether a sector has been read correctly so a single parity bit can be used for error correction as well as for detection. the idea is as follows if one of the sectors is damaged we know exactly which sector it is and we can figure out whether any bit in the sector is a or a by computing the parity of the corresponding bits from sectors in the other disks. if the parity of the remaining bits is equal to the stored parity the missing bit is otherwise it is . raid level is as good as level but is less expensive in the number of extra disks required it has only a one disk overhead so level is not used in practice. this scheme is shown pictorially in figure . d . raid level has two advantages over level . first the storage overhead is reduced because only one parity disk is needed for several regular disks whereas one mirror disk is needed for every disk in level . second since reads and writes of a byte are spread out over multiple disks with a way striping of data the transfer rate for reading or writing a single block is n times as fast as with raid level . on the negative side raid level supports fewer i os per second since every disk has to participate in every i o request. a further performance problem with raid and with all paritybased raid levels is the expense of computing and writing the parity. this overhead results in significantly slower writes than with non parity raid arrays. to moderate this performance penalty many raid storage arrays include a hardware controller with dedicated parity hardware. this controller offloads the parity computation from the cpu to the array. the array has an nvram cache as well to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles. this combination can make parity raid almost as fast as non parity. in fact a caching array doing parity raid can outperform a non caching non parity raid. raid level . raid level or block interleaved parity organization uses block level striping as in raid and in addition keeps a parity block on a separate disk for corresponding blocks from a! other disks. this scheme is diagramed in figure . e . if one of the disks fails the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk. . raid structure a block read accesses only one disk allowing other requests to be processed by the other disks thus the data transfer rate for each access is slower but multiple read accesses can proceed in parallel leading to a higher overall i o rate. the transfer rates for large reads are high since all the disks can be read in parallel large writes also have high transfer rates since the data and parity can be written in parallel small independent writes cannot be performed in parallel. an operating system write of data smaller than a block requires that the block be read modified with the new data and written back. the parity block has to be updated as well. this is known as the read modify write cycle. thus a single write requires four disk accesses two to read the two old blocks and two to write the two new blocks. wafl chapter uses raid level because this raid level allows disks to be added to a raid set seamlessly. if the added ciisks are initialized with blocks containing all zeros then the parity value does not change and the raid set is still correct. raid level . raid level or block interleaved distributed parity differs from level by spreading data and parity among all n disks rather than storing data in n disks and parity in one disk. for each block one of the disks stores the parity and the others store data. for example with an array of five disks the parity for the nth block is stored in disk n mod the nth blocks of the other four disks store actual data for that block. this setup is shown in figure . l f where the ps are distributed across all the disks. a parity block cannot store parity for blocks in the same disk because a disk failure would result in loss of data as well as of parity and hence the loss would not be recoverable. by spreading the parity across all the disks in the set raid avoids the potential overuse of a single paritydisk that can occur with raid . raid is the most common parity raid system. raid level . raid level also called the p q redundancy scheme is much like raid level but stores extra redundant information to guard against multiple disk failures. instead of parity error correcting codes such as the reed solomon codes are used. in the scheme shown in figure . g bits of redundant data are stored for every bits of data compared with parity bit in level and the system can tolerate two disk failures. raid level . raid level refers to a combination of raid levels and . raid provides the performance while raid provides the reliability. generally this level provides better performance than raid . it is common in environments where both performance and. reliability are important. unfortunately it doubles the number of disks needed for storage as does raid so it is also more expensive in raid a set of disks are striped and then the stripe is mirrored to another equivalent stripe. another raid option that is becoming available commercially is raid level in which disks are mirrored in pairs and then the resulting mirror pairs are striped. this raid has some theoretical advantages over raid . for example if a single disk fails in raid the entire chapter mass storage structure stripe mirror stripe a raid with a single disk failure. stripe mirror mirror mirror mirror b raid with a single disk failure. figure . raid and . stripe is inaccessible leaving only the other stripe available. with a failure in raid the single disk is unavailable but its mirrored pair is still available as are all the rest of the disks figure . . numerous variations have been proposed to the basic raid schemes described here. as a result some confusion may exist about the exact definitions of the different raid levels. the implementation of raid is another area of variation. consider the following layers at which raid can be implemented. volume management software can implement raid within the kernel or at the system software layer. in this case the storage hardware can provide a minimum of features and still be part of a full raid solution. parity raid is fairly slow when implemented in software so typically raid or is used. raid can be implemented in the host bus adapter hba hardware. only the disks directly connected to the hba can be part of a given raid set. this solution is low in cost but not very flexible. raid can be implemented in the hardware of the storage array. the storage array can create raid sets of various levels and can even slice these sets into smaller volumes which are then presented to the operating system. . raid structure the operating system need only implement the file system on each f the volumes. arrays can have multiple connections available or can be part of a san allowing multiple hosts to take advantage of the array's features. raid can be implemented in the san interconnect layer by disk virtualization devices. in this case a device sits between the hosts and the storage. it accepts commands from the servers and manages access to the storage. it could provide mirroring for example by writing each block to two separate storage devices. other features such as snapshots and replication can be implemented at each of these levels as well. replication involves the automatic duplication of writes between separate sites for redimdancy and disaster recovery. replication can be synchronous or asynchronous. in synchronous replication each block must be written locally and remotely before the write is considered complete whereas in asynchronous replication the writes are grouped together and written periodically. asynchronous replication can result in data loss if the primary site fails but is faster and has no distance limitations. the implementation of these features differs depending on the layer at which raid is implemented. for example if raid is implemented in software then each host may need to implement and manage its own replication. if replication is implemented in the storage array or in the san interconnect however then whatever the host operating system or features the hosts data can be replicated. one other aspect of most raid implementations is a hot spare disk or disks. a hot spare is not used for data but is configured to be used as a replacement should any other disk fail. for instance a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail. in this way the raid level can be reestablished automatically without waiting for the failed disk to be replaced. allocating more than one hot spare allows more than one failure to be repaired without human intervention. . . selecting a rasd level given the many choices they have how do system designers choose a raid level? one consideration is rebuild performance. if a disk fails the time needed to rebuild its data can be significant and will vary with the raid level used. rebuilding is easiest for raid level since data can be copied from another disk for the other levels we need to access all the other disks in the arrayto rebuild data in a failed disk. the rebuild performance of a raid system may be an important factor if a continuous supply of data is required as it is in high performance or interactive database systems. furthermore rebuild performance influences the mean time to failure. rebuild times can be hours for raid rebuilds of large disk sets. raid level is used in high performance applications where data loss is not critical. raid level is popular for applications that require high reliability with fast recovery. raid and are used where both performance and reliability are important for example for small databases. due to raid l's high space overhead raid level is often preferred for storing large volumes of data. level is not supported currently by many raid implementations but it should offer better reliability than level . chapter mass storage structure raid system designers and administrators of storage have to make several other decisions as well. for example how many disks should be in a given raid set? how many bits should be protected by each parity bit? if more disks are in an array data transfer rates are higher but the system is more expensive. if more bits are protected by a parity bit. the space overhead due to parity bits is lower but the chance that a second disk will fail before the first failed disk is repaired is greater and that wall result in data loss. . . extensions the concepts of raid have been generalized to other storage devices including arrays of tapes and even to the broadcast of data over wireless systems. when applied to arrays of tapes raid structures are able to recover data even if one of the tapes in an array is damaged. when applied to broadcast of data a block of data is split into short units and is broadcast along with a parity unit if one of the units is not received for any reason it can be reconstructed from the other units. commonly tape drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time. . . the insarv titicin in an effort to provide betfer fasfei and less expeifisiye ohitions ge arr y ffobn pftr. uln ifce mqst other storage array thb jnsei v does not require that a set nf disks be' coh igua ed a t a specific rmd level jptfier a e h d i k iisrokeh i iiloi a mb iciaunklie l rllp isaheniapplied at the chu hklgt lev i l a disk can thus piiftic ipated n.miiiiiiple and variou s raid teyfefajas ferhiirnklefis afe usedfar multiple vaiumes. . v ! . i s ipserv al d pmiddes snapsha toy the vy pl lliowiing hpiti pledis s ts tc iii unt nc pdgs idf a giygii fits gvstlark i ai e ed i big theiiriidwnikbpiies mi. the jrt ti rei fflc systbnfi mny changes a ther goplesj ' ' . ' ? v' sh rink! fti these file systems ttieioriginal sizeisthedniy size and an y changes ceciudre copying data niadrninistiratcitvcan canfigure imserv toip'i'ovide a attiiount p phygical storage. s the htsi si ar t a afig ln ti gt ffage uinhsed disk area llpeatefltb he hml iip fe bggjhii llipinlleve! . jnithisimanneiva vgs can l t lteve that i! li asi a targe .fixt d sta .rage space ereaite'lts file systerfis there ajtd so on disks gan h .added q.f fenk ved froin th.e file system by tse re wifhbut the le systeens noticing the change t jis feature can reduce the number of drives needed by hosts or at least delay tbe purchase of disks