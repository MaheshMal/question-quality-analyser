Multicore and Multithreading

     The use of a multicore system to support a single application with multiple threads,
     such as might occur on a workstation, a video-game console, or a personal computer
     running a processor-intense application, raises issues of performance and applica-
     tion design. In this section, we first look at some of the performance implications
     of a multithreaded application on a multicore system and then describe a specific
     example of an application designed to exploit multicore capabilities.
     Performance of Software on Multicore
     The potential performance benefits of a multicore organization depend on the
     ability to effectively exploit the parallel resources available to the application. Let
     us focus first on a single application running on a multicore system. Amdahl's law
     (see Appendix E) states that:
     Speedup =  time to execute program on a single processor                 =  1
                time to execute program on N parallel processors                 (1 - f ) +                  f
                                                                                                             N
     The law assumes a program in which a fraction (1 - f) of the execution time
     involves code that is inherently serial and a fraction f that involves code that is infi-
     nitely parallelizable with no scheduling overhead.
     This law appears to make the prospect of a multicore organization attractive.
     But as Figure 4.7a shows, even a small amount of serial code has a noticeable impact.
     If only 10% of the code is inherently serial ( f = 0.9), running the program on a
     multicore system with eight processors yields a performance gain of only a factor
     of 4.7. In addition, software typically incurs overhead as a result of communication
     5The movement of processes or threads among address spaces, or thread migration, on different machines
     has become a hot topic in recent years. Chapter 18 explores this topic.

                                      8                                                                0%
                                                                                                             2%
                                      6                                                                   5%
                    Relative speedup                                                                         10%
                                      4
                                      2
                                      0    1    2        3  4  5                  6   7                   8
                                                            Number of processors
                                           (a)  Speedup  with 0%, 2%, 5%, and 10% sequential portions
                                      2.5
                                      2.0                                                                    5%
                                                                                                             10%
                                                                                                             15%
                    Relative speedup                                                                         20%
                                      1.5
                                      1.0
                                      0.5
                                      0    1    2        3  4  5                  6   7                8
                                                            Number of processors
                                                         (b) Speedup with overheads
                    Figure 4.7                  Performance Effect of Multiple Cores
     and distribution of work to multiple processors and cache coherence overhead. This
     results in a curve where performance peaks and then begins to degrade because
     of the increased burden of the overhead of using multiple processors. Figure 4.7b,
     from [MCDO07], is a representative example.
     However, software engineers have been addressing this problem and there are
     numerous applications in which it is possible to effectively exploit a multicore sys-
     tem. [MCDO07] reports on a set of database applications, in which great attention

            64                                      Oracle DSS 4-way join
                                                    TMC data mining
                                                    DB2 DSS scan & aggs
                                                    Oracle ad hoc insurance OLTP
            48
   Scaling  32             perfect scaling
            16
            0   0  16      32               48  64
                       Number of CPUs
   Figure 4.8      Scaling of Database Workloads on Multiple-Processor Hardware
was paid to reducing the serial fraction within hardware architectures, operating
systems, middleware, and the database application software. Figure 4.8 shows the
result. As this example shows, database management systems and database applica-
tions are one area in which multicore systems can be used effectively. Many kinds of
servers can also effectively use the parallel multicore organization, because servers
typically handle numerous relatively independent transactions in parallel.
   In addition to general-purpose server software, a number of classes of applica-
tions benefit directly from the ability to scale throughput with the number of cores.
[MCDO06] lists the following examples:
·  Multithreaded native applications: Multithreaded applications are charac-
   terized by having a small number of highly threaded processes. Examples
   of threaded applications include Lotus Domino or Siebel CRM (Customer
   Relationship Manager).
·  Multiprocess applications: Multiprocess applications are characterized by
   the presence of many single-threaded processes. Examples of multiprocess
   applications include the Oracle database, SAP, and PeopleSoft.
·  Java applications: Java applications embrace threading in a fundamental way.
   Not only does the Java language greatly facilitate multithreaded applications,
   but the Java Virtual Machine is a multithreaded process that provides sched-
   uling and memory management for Java applications. Java applications that
   can benefit directly from multicore resources include application servers such
   as Sun's Java Application Server, BEA's Weblogic, IBM's Websphere, and
   the open-source Tomcat application server. All applications that use a Java 2
   Platform, Enterprise Edition (J2EE platform) application server can immedi-
   ately benefit from multicore technology.

     ·  Multiinstance applications: Even if an individual application does not scale
        to take advantage of a large number of threads, it is still possible to gain from
        multicore architecture by running multiple instances of the application in
        parallel. If multiple application instances require some degree of isolation,
        virtualization technology (for the hardware of the operating system) can be
        used to provide each of them with its own separate and secure environment.
     Application Example: Valve Game Software
     Valve is an entertainment and technology company that has developed a number
     of popular games, as well as the Source engine, one of the most widely played game
     engines available. Source is an animation engine used by Valve for its games and
     licensed for other game developers.
        In recent years, Valve has reprogrammed the Source engine software to use
     multithreading to exploit the power of multicore processor chips from Intel and
     AMD [REIM06]. The revised Source engine code provides more powerful support
     for Valve games such as Half Life 2.
        From Valve's perspective, threading granularity options are defined as follows
     [HARR06]:
     ·  Coarse threading: Individual modules, called systems, are assigned to individ-
        ual processors. In the Source engine case, this would mean putting rendering
        on one processor, AI (artificial intelligence) on another, physics on another,
        and so on. This is straightforward. In essence, each major module is single
        threaded and the principal coordination involves synchronizing all the threads
        with a timeline thread.
     ·  Fine-grained threading: Many similar or identical tasks are spread across mul-
        tiple processors. For example, a loop that iterates over an array of data can be
        split up into a number of smaller parallel loops in individual threads that can
        be scheduled in parallel.
     ·  Hybrid threading: This involves the selective use of fine-grained threading for
        some systems and single threading for other systems.
        Valve found that through coarse threading, it could achieve up to twice the
     performance across two processors compared to executing on a single processor.
     But this performance gain could only be achieved with contrived cases. For real-
     world gameplay, the improvement was on the order of a factor of 1.2. Valve also
     found that effective use of fine-grained threading was difficult. The time per work
     unit can be variable, and managing the timeline of outcomes and consequences
     involved complex programming.
        Valve found that a hybrid threading approach was the most promising and
     would scale the best, as multicore systems with 8 or 16 processors became available.
     Valve identified systems that operate very effectively being permanently assigned
     to a single processor. An example is sound mixing, which has little user interaction,
     is not constrained by the frame configuration of windows, and works on its own set
     of data. Other modules, such as scene rendering, can be organized into a number
     of threads so that the module can execute on a single processor but achieve greater
     performance as it is spread out over more and more processors.

                                  Render
   Skybox     Main view                            Monitor                       Etc.
              Scene list
                                  For each object
                                                        Particles
                                                                   Sim and draw
                                                   Character
                                                                   Bone setup
                                                                   Draw
                                                        Etc.
Figure 4.9  Hybrid Threading for Rendering Module
   Figure 4.9 illustrates the thread structure for the rendering module. In this hier-
archical structure, higher-level threads spawn lower-level threads as needed. The
rendering module relies on a critical part of the Source engine, the world list, which
is a database representation of the visual elements in the game's world. The first task
is to determine what are the areas of the world that need to be rendered. The next
task is to determine what objects are in the scene as viewed from multiple angles.
Then comes the processor-intensive work. The rendering module has to work out
the rendering of each object from multiple points of view, such as the player's view,
the view of TV monitors, and the point of view of reflections in water.
   Some of the key elements of the threading strategy for the rendering module
are listed in [LEON07] and include the following:
· Construct scene-rendering lists for multiple scenes in parallel (e.g., the world
   and its reflection in water).
·  Overlap graphics simulation.
·  Compute character bone transformations          for  all  characters  in  all  scenes     in
   parallel.
·  Allow multiple threads to draw in parallel.
   The designers found that simply locking key databases, such as the world list, for
a thread was too inefficient. Over 95% of the time, a thread is trying to read from a data
set, and only 5% of the time at most is spent in writing to a data set. Thus, a concurrency
mechanism known as the single-writer-multiple-readers model works effectively.

