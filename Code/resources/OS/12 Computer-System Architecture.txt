 in section . we introduced the general structure of a typical computer system. a computer system may be organized in a number of different ways which we can categorize roughly according to the number of general purpose processors used. . . single processor systems most systems vise a single processor. the variety of single processor systems may be surprising however since these systems range from pdas through mainframes. on a single processor system there is one main cpu capable of executing a general purpose instruction set including instructions from user processes. almost all systems have other special purpose processors as well. they may come in the form of device specific processors such as disk keyboard and graphics controllers or on mainframes they may come in the form of more general purpose processors such as i o processors that move data rapidly among the components of the system. all of these special purpose processors run a limited instruction set and do not run user processes. sometimes they are managed by the operating system in that the operating system sends them information about their next task and monitors their status. for example a disk controller microprocessor receives a sequence of requests from the main cpu and implements its own disk queue and scheduling algorithm. this arrangement relieves the main cpu of the overhead of disk scheduling. pcs contain a microprocessor in the keyboard to convert the keystrokes into codes to be sent to the cpu. in other systems or circumstances special purpose processors are low level components built into the hardware. the operating system cannot communicate with these processors they do their jobs autonomously. the use of special purpose microprocessors is common and does not turn a single processor system into a multiprocessor. if there is only one general purpose cpu then the system is a single processor system. . . multiprocessor systems although single processor systems are most common multiprocessor systems also known as parallel systems or tightly coupled systems are growing in importance. such systems have two or more processors in close communication sharing the computer bus and sometimes the clock memory and peripheral devices. multiprocessor systems have three main advantages . increased throughput. by increasing the number of processors we expect to get more work done in less time. the speed up ratio with n processors is not n however rather it is less than n. when multiple processors cooperate on a task a certain amount of overhead is incurred in keeping all the parts working correctly. this overhead plus contention for shared resources lowers the expected gain from additional processors. similarly n programmers working closely together do not produce n times the amount of work a single programmer would produce. . computer system architecture . economy of scale. multiprocessor systems can cost less than equivalent multiple single processor systems because they can share peripherals mass storage and power supplies. if several programs operate on the same set of data it is cheaper to store those data on one disk and to have all the processors share them than to have many computers with local disks and many copies of the data. . increased reliability. if functions can be distributed properly among several processors then the failure of one processor will not halt the system only slow it down. if we have ten processors and one fails then each of the remaining nine processors can pick up a share of the work of the failed processor. thus the entire system runs only percent slower rather than failing altogether. increased reliability of a computer system is crucial in many applications. the ability to continue providing service proportional to the level of surviving hardware is called graceful degradation. some systems go beyond graceful degradation and are called fault tolerant because they can suffer a failure of any single component and still continue operation. note that fault tolerance requires a mechanism to allow the failure to be detected diagnosed and if possible corrected. the hp nonstop system formerly tandem system uses both hardware and software duplication to ensure continued operation despite faults. the system consists of multiple pairs of cpus working in lockstep. both processors in the pair execute each instruction and compare the results. if the results differ then one cpu of the pair is at fault and both are halted. the process that was being executed is then moved to another pair of cpus and the instruction that failed is restarted. this solution is expensive since it involves special hardware and considerable hardware duplication. the multiple processor systems in use today are of two types. some systems use asymmetric multiprocessing in which each processor is assigned a specific task. a master processor controls the system the other processors either look to the master for instruction or have predefined tasks. this scheme defines a master slave relationship. the master processor schedules and allocates work to the slave processors. the most common systems use symmetric multiprocessing smp in which each processor performs all tasks within the operating system. smp means that all processors are peers no master slave relationship exists between processors. figure . illustrates a typical smp architecture. an example of the smp system is solaris a commercial version of unix designed by sun microsystems. a solaris system can be configured to employ dozens of processors all running solaris. the benefit of this model is that many processes gpu gpu cpu memory figure . symmetric multiprocessing architecture. chapter introduction can run simultaneously n processes can run if there are n cpus without causing a significant deterioration of performance. however we must carefully control i o to ensure that the data reach the appropriate processor. also since the cpus are separate one may be sitting idle while another is overloaded resulting in inefficiencies. these inefficiencies can be avoided if the processors share certain data structures. a multiprocessor system of this form will allow processes and resources such as memory to be shared dynamically among the various processors and can lower the variance among the processors. such a system must be written carefully as we shall see in chapter . virtually all modern operating systems including windows windows xp mac os x and linux now provide support for smp. the difference between symmetric and asymmetric multiprocessing may result from either hardware or software. special hardware can differentiate the multiple processors or the software can be written to allow only one master and multiple slaves. for instance sun's operating system sunos version provided asymmetric multiprocessing whereas version solaris is symmetric on the same hardware. a recent trend in cpu design is to include multiple compute cores on a single chip. in essence these are multiprocessor chips. two way chips are becoming mainstream while n way chips are going to be common in high end systems. aside from architectural considerations such as cache memory and bus contention these multi core cpus look to the operating system just as n standard processors. lastly blade servers are a recent development in which multiple processor boards i o boards and networking boards are placed in the same chassis. the difference between these and traditional multiprocessor systems is that each blade processor board boots independently and runs its own operating system. some blade server boards are multiprocessor as well which blurs the lines between types of computers. in essence those servers consist of multiple independent multiprocessor systems. . . clustered systems another type of multiple cpu system is the clustered system. like multiprocessor systems clustered systems gather together multiple cpus to accomplish computational work. clustered systems differ from multiprocessor systems however in that they are composed of two or more individual systems coupled together. the definition of the term clustered is not concrete many commercial packages wrestle with what a clustered system is and why one form is better than another. the generally accepted definition is that clustered computers share storage and are closely linked via a local area network lan as described in section . or a faster interconnect such as infiniband. clustering is usually used to provide high availability service that is service will continue even if one or more systems in the cluster fail. high availability is generally obtained by adding a level of redundancy in the system. a layer of cluster software runs on the cluster nodes. each node can monitor one or more of the others over the lan . if the monitored machine fails the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine. the users and clients of the applications see only a brief interruption of service