 each day so each successive incremental backup involves more files and more backup media. a user may notice that a particular file is missing or corrupted long after the damage was done. for this reason we usually plan to take a full backup from time to time that will be saved forever. it is a good idea to store these permanent backups far away from the regular backups to protect against hazard such as a fire that destroys the computer and all the backups too. and if the backup cycle reuses media we must take care not to reuse the media too many times if the media wear out it might not be possible to restore any data from the backups. . log structured file systems computer scientists often find that algorithms and technologies originally used in one area are equally useful in other areas. such is the case with the database log based recovery algorithms described in section . . . these logging algorithms have been applied successfully to the problem of consistency checking. the resulting implementations are known as log based transaction oriented or journaling file systems. recall that a system crash can cause inconsistencies among on disk filesystem data structures such as directory structures free block pointers and free fcb pointers. before the use of log based techniques in operating systems changes were usually applied to these structures in place. a typical operation such as file create can involve many structural changes within the file system on the disk. directory structures are modified fcbs are allocated data blocks are allocated and the free counts for all of these blocks are decreased. these changes can be interrupted by a crash and inconsistencies among the structures can result. for example the free fcb count might indicate that an fcb had been allocated but the directory structure might not point to the fcb. the fcb would be lost were it not for the consistency check phase. although we can allow the structures to break and repair them on recovery there are several problems with this approach. one is that the inconsistency may be irreparable. the consistency check may not be able to recover the structures resulting in loss of files and even entire directories. consistency checking can require human intervention to resolve conflicts and that is inconvenient if no human is available. the system can remain unavailable until the human tells it how to proceed. consistency checking also takes system and clock time. terabytes of data can take hours of clock time to check. the solution to this problem is to apply log based recovery techniques to file system metadata updates. both ntfs and the veritas file system use this method and it is an optional addition to lfs on solaris and beyond. in fact it is becoming common on many operating systems. fundamentally all metadata changes are written sequentially to a log. each set of operations for performing a specific task is a transaction. once the changes are written to this log they are considered to be committed and the system call can return to the user process allowing it to continue execution. meanwhile these log entries are replayed across the actual filesystem structures. as the changes are made a pointer is updated to indicate which actions have completed and which are still incomplete. when an entire chapter file system implementation committed transaction is completed it is removed from the log file which is actually a circular buffer. a circular buffer writes to the end of its space and then continues at the beginning overwriting older values as it goes. we would not want the buffer to write over data that has not yet been saved so that scenario is avoided. the log may be in a separate section of the file system or even on a separate disk spindle. it is more efficient but more complex to have it under separate read and write heads thereby decreasing head contention and seek times. if the system crashes the log file will contain zero or more transactions. any transactions it contains were not completed to the file system even though they were committed by the operating system so they must now be completed. the transactions can be executed from the pointer until the work is complete so that the file system structures remain consistent. the only problem occurs when a transaction was aborted that is was not committed before the system crashed. any changes from such a transaction that were applied to the file system must be undone again preserving the consistency of the file system. this recovery is all that is needed after a crash eliminating any problems with consistency checking. a side benefit of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the on disk data structures. the reason for this improvement is found in the performance advantage of sequential i o over random i o. the costly synchronous random metadata writes are turned into much less costly synchronous sequential writes to the log structured file system's logging area. those changes in turn are replayed asynchronously via random writes to the appropriate structures. the overall result is a significant gain in performance of metadata oriented operations such as file creation and deletion