 operating system simply needs a free block so that it can allocate thatblock to a file so the first block in the free list is used. the fat method incorporates free block accounting into the allocation data structure. no separate method is needed. . . grouping a modification of the free list approach is to store the addresses of n free blocks in the first free block. the first n of these blocks are actually free. the last block contains the addresses of another n free blocks and so on. the addresses of a large number of free blocks can now be found quickly unlike the situation when the standard linked list approach is used. . . counting another approach is to take advantage of the fact that generally several contiguous blocks may be allocated or freed simultaneously particularly when space is allocated with the contiguous allocation algorithm or through clustering. thus rather than keeping a list of n free disk addresses we can keep the address of the first free block and the number n of free contiguous blocks that follow the first block. each entry in the free space list then consists of a disk address and a count. although each entry requires more space than would a simple disk address the overall list will be shorter as long as the count is generally greater than . . efficiency and performance now that we have discussed various block allocation and directorymanagement options we can further consider their effect on performance and efficient disk use. disks tend to represent a major bottleneck in system performance since they are the slowest main computer component. in this section we discuss a variety of techniques used to improve the efficiency and performance of secondary storage. . . efficiency the efficient use of disk space depends heavily on the disk allocation and directory algorithms in use. for instance unix inodes are preallocated on a volume. even an empty disk has a percentage of its space lost to inodes. however by preallocating the inodes and. spreading them across the volume we improve the file system's performance. this improved performance results from the unix allocation and free space algorithms which try to keep a file's data blocks near that file's inode block to reduce seek time. as another example let's reconsider the clustering scheme discussed in section . which aids in file seek and file transfer performance at the cost of internal fragmentation. to reduce this fragmentation bsd unix varies the cluster size as a file grows. large clusters are used where they can be filled and small clusters are used for small files and the last cluster of a file. this system is described in appendix a. the types of data normally kept in a file's directory or inode entry also require consideration. commonly a 'last write date is recorded to supply information to the user and to determine whether the file needs to be backed chapter file system implementation up. some systems also keep a last access date so that a user can determine when the file was last read. the result of keeping this information is that whenever the file is read a field in the directory structure must be written to. that means the block must be read into memory a section changed and the block written back out to disk because operations on disks occur only in block or cluster chunks. so any time a file is opened for reading its directory entry must be read and written as well. this requirement can be inefficient for frequently accessed files so we must weigh its benefit against its performance cost when designing a file system. generally every data item associated with a file needs to be considered for its effect on efficiency and performance. as an example consider how efficiency is affected by the size of the pointers used to access data. most systems use either or bit pointers throughout the operating system. these pointer sizes limit the length of a file to either kb or bytes gb . some systems implement bit pointers to increase this limit to bytes which is a very large number indeed. however bit pointers take more space to store and in turn make the allocation and free space management methods linked lists indexes and so on use more disk space. one of the difficulties in choosing a pointer size or indeed any fixed allocation size within an operating system is planning for the effects of changing technology. consider that the ibm pc xt had a mb hard drive and an ms dos file system that could support only mb. each fat entry was bits pointing to an kb cluster. as disk capacities increased larger disks had to be split into mb partitions because the file system could not track blocks beyond mb. as hard disks with capacities of over mb became common the disk data structures and algorithms in ms dos had to be modified to allow larger file systems. each fat entry was expanded to bits and later to bits. the initial file system decisions were made for efficiency reasons however with the advent of ms dos version millions of computer users were inconvenienced when they had to switch to the new larger file system. sun's zfs file system uses bit pointers which theoretically should never need to be extended. the minimum mass of a device capable of storing ' s bytes using atomic level storage would be about trillion kilograms. as another example consider the evolution of sun's solaris operating system. originally many data structures were of fixed length allocated at system startup. these structures included the process table and the open file table. when the process table became full no more processes could be created. when the file table became full no more files could be opened. the system would fail to provide services to users. table sizes could be increased only by recompiling the kernel and rebooting the system. since the release of solaris almost all kernel structures have been allocated dynamically eliminating these artificial limits on system performance. of course the algorithms that manipulate these tables are more complicated and the operating system is a little slower because it must dynamically allocate and deallocate table entries but that price is the usual one for more general functionality. . . performance even after the basic file system algorithms have been selected we can still improve performance in several ways. as will be discussed in chapter . efficiency and performance ! v pech.o readf s te i ! pace cacne file system figure . i o without a unified buffer cache. most disk controllers include local memory to form an on board cache that is large enough to store entire tracks at a time. once a seek is performed the track is read into the disk cache starting at the sector under the disk head reducing latency time . the disk controller then transfers any sector requests to the operating system. once blocks make it from the disk controller into main memory the operating system may cache the blocks there. some systems maintain a separate section of main memory for a buffer cache where blocks are kept under the assumption that they will be used again shortly. other systems cache file data using a page cache. the page cache uses virtual memory techniques to cache file data as pages rather than as file system oriented blocks. caching file data using virtual addresses is far more efficient than caching through physical disk blocks as accesses interface with virtual memory rather than the file system. several systems including solaris linux and windows nt and xp use page caching to cache both process pages and file data. this is known as unified virtual memory. some versions of unix and linux provide a unified buffer cache. to illustrate the benefits of the unified buffer cache consider the two alternatives for opening and accessing a file. one approach is to use memory mapping section . the second is to use the standard system calls reado and write . without a unified buffer cache we have a situation similar to figure . . here the read and write system calls go through the buffer cache. the memory mapping call however requires using two caches the page cache and the buffer cache. a memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache. because the virtual memory system does not interface with the buffer cache the contents of the file in the buffer cache must be copied into the page cache. this situation is known as double caching and requires caching file system data twice. not only does it waste memory but it also wastes significant cpu and i o cycles due to the extra data movement within system memory. in add ition inconsistencies between the two caches can result in corrupt files. in contrast when a unified chapter file system implementation memory mapped i o figure . i o using a unified buffer cache. buffer cache is provided both memory mapping and the read and write system calls use the same page cache. this has the benefit of avoiding double caching and it allows the virtual memory system to manage file system data. the unified buffer cache is shown in figure . . regardless of whether we are caching disk blocks or pages or both leu section . . seems a reasonable general purpose algorithm for block or page replacement. however the evolution of the solaris page caching algorithms reveals the difficulty in choosing an algorithm. solaris allows processes and the page cache to share unused inemory. versions earlier than solaris . . made no distinction between allocating pages to a process and allocating them to the page cache. as a result a system performing many i o operations used most of the available memory for caching pages. because of the high rates of i o the page scanner section . . reclaimed pages from processes rather than from the page cache when free memory ran low. solaris . and solaris optionally implemented priority paging in which the page scanner gives priority to process pages over the page cache. solaris applied a fixed limit to process pages and the file system page cache preventing either from forcing the other out of memory. solaris and again changed the algorithms to maximize memory use and minimize thrashing. this real world example shows the complexities of performance optimizing and caching. there are other issvies that can affect the performance of i o such as whether writes to the file system occur synchronously or asynchronously. synchronous writes occur in the order in which the disk subsystem receives them and the writes are not buffered. thus the calling routine must wait for the data to reach the disk drive before it can proceed. asynchronous writes are done the majority of the time. in an asynchronous write the data are stored in the cache and control returns to the caller. metadata writes among others can be synchronous. operating systems frequently include a flag in the open system call to allow a process to request that writes be performed synchronously. for example databases use this feature for atomic transactions to assure that data reach stable storage in the required order. some systems optimize their page cache by using different replacement algorithms depending on the access type of the file. a file being read or written sequentially should not have its pages replaced in lru order because the most