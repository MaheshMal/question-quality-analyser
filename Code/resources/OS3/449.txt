Thrashing
     computer systems, that is not the case. Often, in systems with multiple CPUs
     (Section 1.3.2), a given CPU can access some sections of main memory faster
     than it can access others. These performance differences are caused by how
     CPUs and memory are interconnected in the system. Frequently, such a system
     is made up of several system boards, each containing multiple CPUs and some
     memory. The system boards are interconnected in various ways, ranging from
     system buses to high-speed network connections like InfiniBand. As you might
     expect, the CPUs on a particular board can access the memory on that board with
     less delay than they can access memory on other boards in the system. Systems
     in which memory access times vary significantly are known collectively as
     non-uniform memory access (NUMA) systems, and without exception, they
     are slower than systems in which memory and CPUs are located on the same
     motherboard.
     Managing which page frames are stored at which locations can significantly
     affect performance in NUMA systems. If we treat memory as uniform in such
     a system, CPUs may wait significantly longer for memory access than if we
     modify memory allocation algorithms to take NUMA into account. Similar
     changes must be made to the scheduling system. The goal of these changes is
     to have memory frames allocated "as close as possible" to the CPU on which
     the process is running. The definition of "close" is "with minimum latency,"
     which typically means on the same system board as the CPU.
     The algorithmic changes consist of having the scheduler track the last CPU
     on which each process ran. If the scheduler tries to schedule each process onto
     its previous CPU, and the memory-management system tries to allocate frames
     for the process close to the CPU on which it is being scheduled, then improved
     cache hits and decreased memory access times will result.
     The picture is more complicated once threads are added. For example, a
     process with many running threads may end up with those threads scheduled
     on many different system boards. How is the memory to be allocated in this
     case? Solaris solves the problem by creating lgroups (for "latency groups") in
     the kernel. Each lgroup gathers together close CPUs and memory. In fact, there
     is a hierarchy of lgroups based on the amount of latency between the groups.
     Solaris tries to schedule all threads of a process and allocate all memory of a
     process within an lgroup. If that is not possible, it picks nearby lgroups for the
     rest of the resources needed. This practice minimizes overall memory latency
     and maximizes CPU cache hit rates.
9.6  