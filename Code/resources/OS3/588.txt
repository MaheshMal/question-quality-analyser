Efficiency and Performance
      Now     that  we  have  discussed    various  block-allocation  and  directory-
      management options, we can further consider their effect on performance
      and efficient disk use. Disks tend to represent a major bottleneck in system
      performance, since they are the slowest main computer component. In this
      section, we discuss a variety of techniques used to improve the efficiency and
      performance of secondary storage.
      12.6.1  Efficiency
      The efficient use of disk space depends heavily on the disk-allocation and
      directory algorithms in use. For instance, UNIX inodes are preallocated on
      a volume. Even an empty disk has a percentage of its space lost to inodes.
      However, by preallocating the inodes and spreading them across the volume,
      we improve the file system's performance. This improved performance results
      from the UNIX allocation and free-space algorithms, which try to keep a file's
      data blocks near that file's inode block to reduce seek time.
      As another example, let's reconsider the clustering scheme discussed in
      Section 12.4, which improves file-seek and file-transfer performance at the cost
      of internal fragmentation. To reduce this fragmentation, BSD UNIX varies the
      cluster size as a file grows. Large clusters are used where they can be filled, and
      small clusters are used for small files and the last cluster of a file. This system
      is described in Appendix A.
      The types of data normally kept in a file's directory (or inode) entry also
      require consideration. Commonly, a "last write date" is recorded to supply
      information to the user and to determine whether the file needs to be backed
      up. Some systems also keep a "last access date," so that a user can determine
      when the file was last read. The result of keeping this information is that,
      whenever the file is read, a field in the directory structure must be written
      to. That means the block must be read into memory, a section changed, and
      the block written back out to disk, because operations on disks occur only in
      block (or cluster) chunks. So any time a file is opened for reading, its directory
      entry must be read and written as well. This requirement can be inefficient for
      frequently accessed files, so we must weigh its benefit against its performance
      cost when designing a file system. Generally, every data item associated with
      a file needs to be considered for its effect on efficiency and performance.
      Consider, for instance, how efficiency is affected by the size of the pointers
      used to access data. Most systems use either 32-bit or 64-bit pointers throughout
      the operating system. Using 32-bit pointers limits the size of a file to 232, or 4
      GB. Using 64-bit pointers allows very large file sizes, but 64-bit pointers require



                     12.6                  