Distributed File Systems
      local, remote, and mobile users. Once the authentication is complete, facilities
      like desktop virtualization allow users to see their desktop sessions at remote
      facilities.
           Still another issue is scalability--the capability of a system to adapt to
      increased service load. Systems have bounded resources and can become
      completely saturated under increased load. For example, with respect to a file
      system, saturation occurs either when a server's CPU runs at a high utilization
      rate or when disks' I/O requests overwhelm the I/O subsystem. Scalability
      is a relative property, but it can be measured accurately. A scalable system
      reacts more gracefully to increased load than does a nonscalable one. First,
      its performance degrades more moderately; and second, its resources reach a
      saturated state later. Even perfect design cannot accommodate an ever-growing
      load. Adding new resources might solve the problem, but it might generate
      additional indirect load on other resources (for example, adding machines to
      a distributed system can clog the network and increase service loads). Even
      worse, expanding the system can call for expensive design modifications. A
      scalable system should have the potential to grow without these problems. In
      a distributed system, the ability to scale up gracefully is of special importance,
      since expanding the network by adding new machines or interconnecting two
      networks is commonplace. In short, a scalable design should withstand high
      service load, accommodate growth of the user community, and allow simple
      integration of added resources.
           Scalability is related to fault tolerance, discussed earlier. A heavily loaded
      component can become paralyzed and behave like a faulty component. In
      addition, shifting the load from a faulty component to that component's
      backup can saturate the latter. Generally, having spare resources is essential
      for ensuring reliability as well as for handling peak loads gracefully. Thus, the
      multiple resources in a distributed system represent an inherent advantage,
      giving  the  system  a  greater   potential  for  fault  tolerance  and  scalability.
      However, inappropriate design can obscure this potential. Fault-tolerance and
      scalability considerations call for a design demonstrating distribution of control
      and data.
           Facilities like the Hadoop distributed file system were created with this
      problem in mind. Hadoop is based on Google's MapReduce and Google
      File System projects that created a facility to track every web page on the
      Internet. Hadoop is an open-source programming framework that supports
      the  processing  of  large  data  sets  in  distributed  computing  environments.
      Traditional systems with traditional databases cannot scale to the capacity and
      performance needed by "big data" projects (at least not at reasonable prices).
      Examples of big data projects include mining Twitter for information pertinent
      to a company and sifting financial data to look for trends in stock pricing.
      With Hadoop and its related tools, thousands of systems can work together to
      manage a distributed database of petabytes of information.
17.9  